Data,Labels
"I don't see that being the case. Almost the opposite, that is to say separating *Nazism* from the military institutions of the period. It must be remembered that after the war, the Western powers were complicit in the growth of the '*Myth of the Clean Wehrmacht*', through various means, including the extensive use of former German soldiers within the US Army Historical Division, most (in)famously Franz Halder. The intention, in no small part, was driven by the needs of the Cold War and the desire to rearm (West) Germany as a bulwark against the Soviet Union for the coming hot war in Europe. They didn't aim to do these by divorcing the post-war German military from the Nazi-era legacy, but rather by trying to pain the Wehrmacht as not being complicit in war crimes, and as having fought the war honorably, while the crimes of the Nazi regime were carried out by the Waffen-SS and other such groups.

So you actually *are* onto something here, but coming at it from the wrong direction. Evans talks about the mysticism and romanticism, and in the case of the term *Wehrmacht*, this was a very large part of its development in the post-war years. [This thread](https://www.reddit.com/r/AskHistorians/comments/5799li/did_the_rommel_myth_and_clean_wehrmacht_myth_and/) has some stuff from both me and /u/commiespaceinvader may be of interest, as well as [this one from me](https://www.reddit.com/r/AskHistorians/comments/bytjch/how_much_history_is_the_english_speaking_world/eqmsxb2/) on how Western perceptions of the Eastern Front were shaped in the period.",0
"Never, ever, underestimate the role of fashion in sword design. Long swords were elite weapons. (""Long"" here refers literally to the length, and not to any particular style of weapon.) Elite weapons were desirable because they announced your status merely by wearing them, without even fighting. And in martial cultures, you spent a *lot* more time standing around wearing swords than you actually spent in combat using them. So their social utility was at least as important as their martial utility in terms of understanding why they looked the way they did.

Medieval European sword design inherited from Roman antecedents, and in particular the *spatha*, which was a design inspired by Celtic long swords. As a long sword, the *spatha* was expensive and well-suited to cavalry, so it established itself as the continental sword of the aristocracy quite easily, and the aristocratic sidearms of Europe were dominated by long, straight, double-edged blades for many centuries thereafter.

Curved swords existed side-by-side with medieval long swords for most of this time, and they were, in fact, very popular, so it is untrue that European swords *in general* were in the straight, crucifix style. But curved swords were not considered ""elite"" weapons. They were short, cheap, practical weapons like falchions, which drew from a long tradition of agricultural or sickle-style weapons that were associated with peasantry and commoners. They were looked down on as status symbols, even when regarded as useful weapons. Even those who could afford horses and fancy long swords would often carry a short curved sword like a falchion with them for when things got down and dirty in the melee.

The Muslim world (as encountered by Europeans) overlapped substantially with the ancient domains of the Roman Empire, and its notions of elite swords were not dissimilar. During the Crusades, Arab swords were typically long, straight, double-edged, and single handed. They had a bit less of the crucifix-style hilt going on, but that's mostly just the guard design, and that didn't come from the Romans, anyway. So there really wasn't much in the way of ""international influence"" to steer western sword design away from the straight long sword.

Until the Turks. Although curved ""sickle swords"" were known throughout the world, their roots in agricultural labour didn't give them a lot of social cachet. The Turks may have been the first to lengthen the curved sword into an elite cavalry weapon, somewhere around the 8th Century. (There is also a short Turkish curved sword called a yataghan, but it did not have nearly so much influence.) But even so, the peoples of the Asian steppes were treated as barbarians by most of the cultures and empires who encountered them, so the mere existence of a long, curved sword that was useful from horseback would not have been enough to convince western aristocrats that it was a proper badge of status.

But after the Mongols and then the Turks overran much of western Asia and established their own empires, perceptions about elite weapons began to shift. What does ""elite"" mean, after all, other than ""associated with the ruling class""? After Turks have been running the show for a while, their weapon styles began to redefine the general conception of ""aristocratic cool"". (And kicking some highly respected ass all through the region certainly didn't hurt the reputation of their weaponry, either.) This happened earliest in lands like Persia and India (giving us weapons like the scimitar and talwar), but it eventually came to Europe as well, with Ottoman incursions into, and rule over, eastern Europe in the 14th through 19th Centuries. 

This led to the European take on the Turkish sword, which is generally known as the sabre, and is so solidly ingrained into our own cultural patterns that we hardly recognize its Turkish influences anymore. That's partly because the sword was popularized in Western Europe by Hussar regiments, which were modelled after the forces that drove the Turks *out* of Eastern Europe. Part of its popularity was that it was perceived as the weapon that defeated the Turks, when in fact the opposite was closer to the truth. 

So fashionable was the new curved sabre that it became the dominant style of western military sword during the 19th Century, and although the classic straight-bladed, doubled-edged sword (by then called a broadsword) did manage to survive, it mostly did so by rebranding itself as a ""sabre"". (See, for example, the Patton saber, and the 1908 trooper sabre, both of which are basically rapiers masquerading as broadswords, while calling themselves sabres.)

(This post covers a lot of ground, but *Swords and Hilt Weapons* is a good overview of the broad historical development of the sword, with *Barbarians and Christians* and *17th Century Europe*, both by Anthony North, covering the main influences on western sword shapes.)",0
"To add to this excellent answer, we encounter a similar problem even when we are dealing with people who there is better sourcing and less ambiguity about

Mineral Point, Wisconsin was a prominent mining town in the 1830s-50s, with many of the miners hailing from Cornwall, in the UK. These Cornish miners brought a distinct architectural style to Mineral Point, particularly the miner's cottages. But the main mine in town was closed in the 1850s, and the town declined, with many of those cottages abandoned

In the 1930s, Robert Neal and Edward Hellum together took an interest in the crumbling but unique Cornish architecture, and bought one of the cottages and restored it, turning it into a cornish-themed restaurant they named *Pendarvis*. The restaurant was enormously successful, becoming an elite destination that attracted prominent patrons from across the midwest, and they used the proceeds to buy up and restore more workers' cottages and turn them into hotel rooms. Pendarvis helped turn Mineral Point from a crumbling mining town to a flourishing tourist destination. Today, Pendarvis is a preserved historic site

By modern standards, Neal and Hellum were gay men in a romantic relationship. There's no ambiguity to this - they lived together for most of their lives, lived pretty openly as a couple, and we have access to their writings to each other which are unambiguously talking about each other in a romantic fashion. The people of Mineral Point knew this and didn't seem to make *too* big a deal of it, though it wasn't uncontroversial either. Neal and Hellum also were patrons of a sort of underground gay and artist society, hosting private parties at Pendarvis for their friends

But here's where the issue of terminology comes in. Neal and Hellum lived through the gay liberation movement. Neal died in the 1980s, and Hellum in the 2000s. Despite that, there's little evidence that they identified specifically as ""gay"", even after it became (more) socially acceptable - certainly not publicly, but probably not even privately

The standard approach for a situation like Neal and Hellum's would be for modern scholars to label them as ""queer"" - a catchall for non-traditional sexualities. But this has its own problems, for Neal and Hellum weren't living in Ancient Greece where the word didn't exist. During most of Neal and Hellum's lives, ""queer"" was one of the most prominent slurs used against LGBTQ people. Even if the word has been reclaimed since then, for Neal and Hellum, it was a slur. Can we today label these men with a word they'd consider a particularly nasty slur, in a quest to publicize their sexuality, which they seemed to be interested in keeping quiet and private?

And this isn't a purely academic question either! As I mentioned, Pendarvis is an active historical site, giving tours and all that. It originally focused on the Cornish and mining history of the town - but there's been more of a push to include both Neal and Hellum in that history, and specifically to talk about the *gay* history of Neal and Hellum, and how they fit into a wider ""queer"" history in a time and place (pre 1960s, rural Wisconsin) where gay history is silent. But the people who run Pendarvis are skittish about this - Mineral Point is a conservative town in a conservative area and mostly caters to visitors who are conservative - should Pendarvis risk alienating its visitors, risking the very legacy that Neal and Hellum left behind, to talk about a sexuality that Neal and Hellum themselves sought to keep private? Is it our place, whether as academic or public historians, to give these men and their relationship modern labels? Labels that not only were they *aware of*, but that they rejected, or even that they found demeaning?

But at the same time, by tacitly ignoring their relationship - calling Neal and Hellum ""partners"" (calling to mind ""business partners"", which they were) - are we just perpetuating a culture where LGBTQ people are expected to remain quiet and closeted, a culture that created an atmosphere where Neal and Hellum never felt able to express their sexuality and identity openly?

Some sources about Neal and Hellum:

https://www.chicagotribune.com/news/ct-xpm-2003-08-29-0308280430-story.html

https://www.intomore.com/culture/the-pattern-at-pendarvis-demystifying-the-heritage-of-gay-culture-keepers/

https://online.ucpress.edu/tph/article-abstract/41/2/70/90720/Queer-Public-History-in-Small-Town-WisconsinThe?redirectedFrom=fulltext",0
"California is known today to be one of the strictest states in terms of laws regulating the ownership of firearms, but much of the core laws that underpin their regulations post-date the *Terminator* films.

The most important one for our purposes would be the Roberti-Roos Assault Weapons Control Act of 1989, passed in some year that I can't remember. 

It placed heightened restrictions on a number of semi-automatic firearms ('Assault Weapons', which is a legal term used to define certain semi-automatic firearms based on certain features such as grip-types, barrel-shrouds, and so on. Not to be confused with 'Assault Rifle' which refers to a select-fire rifle capable of automatic fire, chambering an intermediate cartridge. AWs are often semi-auto versions of ARs, but they aren't quite the same thing), including several which you name-check here (confession, it has been years since I saw the movie, so taking your word for what shows up). This includes the UZI, the SPAS 12, and the AR-180 (the AR-18 is the full-auto version, which we'll get to soon). The ownership and sale of these guns was prohibited, with the inclusion of a grandfather clause for those which existed, and were registered with the state.

So in simplest terms, those firearms were legal in California at the time, and laws only began to affect most of them - the M1911 wouldn't be impacted by the AWCA - in 1989 (note, they are no longer legal as designed, kept off the list of approved handguns that began to regulate what was allowed in 2001, but 'California Legal' M1911s exist on the market). There had been earlier attempts, including a similar 1984 bill following a mass shooting in San Ysidro, which if it had passed might have stymied what was available to the Terminator, but it didn't pass. Likewise with a 1982 attempt to prevent new sale of handguns.

*But*, that of course assumes, as you hinted, that these are all semi-automatics, and because of his obvious armory abilities, the Terminator was able to convert them to automatic fire. The fact that you reference an *AR-18*, and not an *AR-180*, hints that this one at least is in fact fully-automatic weapon (the UZI, too, which I'll address below), and hence would be regulated by these additional laws (I'd note here that while it might be something which, legally, could have been sold, I'm incredulous that a gun-store would have just kept a fully-automatic weapon just out on display with everything else, so that at least we can be 'lol' about. But then again, this is a gun store owner who left ammo lying around for the gun he handed to the customer, so maybe just not very bright. Would need to see the clip again, I guess, but it does seem weird).

Federally, these weapons are controlled by the National Firearms Act of 1934, the Gun Control Act of 1968, and the Firearm Owners Protection Act of 1986. That last one is the most important one though, and as noted, it again takes effect *after* the setting of the film. Prior to its passage with the included Hughes Amendment, while regulated, the manufacture of fully-automatic firearms for civilian purchase was possible, with payment of a tax and a lot of paperwork. It was only with the Hughes Amendment that the registry was 'closed'. Those weapons already on the market were, essentially 'locking the market'. Existing weapons were grandfathered in, and can continue to be transferred with paperwork and a tax, but for all intents and purposes, no *new* weapons could enter the civilian market. This means that it is possible, if he showed up after 1986, he might have found a used one he could have purchased, but no new examples would have been available.

The UZI specifically presents an interesting conundrum. Based on the quick shot of it in the scene, I honestly couldn't tell you whether it is a semi-auto or fully-auto, but some kind souls have made a site called the  Internet Movie Firearms Database (thanks /u/Steve_Wilcox), which provides some stills, which others have poured over *obsessively*, and offered some commentary on. The model used for the scene is a short-barreled, fully-automatic. This of course would be covered by the aforementioned laws concerning fully-automatic weapons (for sale/transfer, not the Hughes Amendment), *but* the implications of the script, and Doylean knowledge about the making of the film indicate they *considered* it to be semi-automatic and, as OP notes, then converted by the Terminator. 

So although a fully-automatic UZI, is shown, we should pretend it is a semi-automatic UZI. This would make it *not* a machine gun, but it would still be regulated by the same laws. As it has a stock, with the short barrel (SBR) that is present on the fully-automatic UZI, it would still have the same laws regulating it, just for reasons of length, and not for being fully-automatic capable. This is the only point that is an major inaccuracy, as in the *script*, treating it as a semi-auto, the expectation almost certainly was to have an UZI Carbine as the prop, which would have been a 16"" barrel for civilian sales (originally, the NFA required 18"" inches for a rifle, but was amended to 16"" in 1960). Just before being shot, the store owner tells /u/GovSchwarzenegger that he can take the rifles and shotguns that day, but this wouldn't have been the case for an SBR, while it *would* have been the case for a 16"" UZI Carbine, so what we have here is a mismatch between what the script wanted, and what was shown on screen, but it is somewhat explained by what was happening outside the film.

In any case, this all segues us to the third, and final issue to tackle.

Now, Arnie doesn't *pay* for anything. He loads one of the firearms and kills the owner of the shop. If the Terminator was programmed to be a law abiding citizen who just wanted to collect firearms though, he would have run into some trouble. Especially if any of these were automatic firearms/SBRs, it seems unlikely he would have been able to purchase them. As a non-resident, non-citizen, in the country without documentation, he *definitely* would not have been able to fill out the proper paperwork as required by the NFA/GCA for the transfer of such firearms. Even if he somehow was able to produce the right documents, he wouldn't have been able to walk out of the store that day. Purchase of Title II regulated items can take months before the buyer is able to take possession currently, and although I'm unable to find anything giving specific wait times circa 1984, it was certainly more than a few hours.

So let's go back, and assume that the end decision is to only buy non-NFA regulated semi-automatics. Are things OK now? Well, again moving past the issue where the Terminator likely has no identity and no money, if he was able to produce that stuff... probably! There was no Federal waiting period for firearms prior to the 1993 Brady Act, and in California, it was only handguns which were so regulated, with a 15-day waiting period having been passed in the wake of the political assassinations of the '60s (a bill to change that to all firearms would come about in 1990). This means that the Terminator would not have been able to walk out with the a pistol that day, having to wait 15 days to come pick it up, but he would have been able to take any non-NFA regulated rifles and shotguns he ended up deciding on buying and leave with them that day.

So the sum of it is that there isn't anything particularly unusual about anything going on in that store. None of the firearms are ones which would have been illegal to sell or own, either Federally or in California. An honest buyer would have possibly faced problems if they wanted some of them *that day* due to the 15 day waiting period for handguns in California, as well as the National Firearms Act/Gun Control Act, but as Arnie was most certainly *not* there in good faith, he sidestepped that issue.

**Sources**

Fafarman, Keith R. (1991) ""State Assault Rifle Bans and the Militia Clauses of the United States Constitution,"" *Indiana Law Journal*
Vol. 67 : Iss. 1 , Article 9.

Godwin, Marcia L. & Jean Reith Schroedel. ""Gun Control Politics in California"", in *The Changing Politics of Gun Control*. eds John M. Bruce, Clyde Wilcox. 1998.

Ingram, Carl. ""[Assault Gun Ban Wins Final Vote : Deukmejian’s Promised Approval Would Make It 1st Such U.S. Law](https://www.latimes.com/archives/la-xpm-1989-05-19-mn-112-story.html)"" LA Times. May 19, 1989 

Kopel, David B. “The Great Gun Control War of the Twentieth Century--and Its Lessons for Gun Laws Today.” *Fordham Urban Law Journal* 39, no. 5 (October 1, 2012).

ETA: Keep finding typos when I reread through it.

ETA II: Got some clarification on what the UZI was, so expanded slightly on that)",0
"/u/sunagainstgold strikes again with a succinct, well-written, and definitive answer... is there anything she *doesn't* know? I almost fear her responding to some query about left-wing German terrorism of the 1970s because it will probably blow my response out of the water; and that's literally all I've studied for 15-20 years or so.

One thing that her response brought up is how awful the situation was in San Francisco in the late 60s and early 70s. While it was becoming a mecca for gay people, many in the police department were especially brutal to gay people.

This Awl piece recounts the story of a serial killer on the loose in the gay community in the early 70s, and it talks about the terrible police response. It mentions a particular officer, with the ironic last name of ""Gay"":

>But the San Francisco Police Department would not leave well enough alone. Officers Cornelius Lucy and William Gay, for example, practiced a creative form of entrapment. Officer Gay, as the Advocate put it, would “drive slowly through [Golden Gate Park] in a pickup truck and stop near a strolling male. Then he would stretch out…and show a bulging ‘basket’ in his tight Levis.” Once an advance was made, Officer Gay would make an arrest.

>Or worse. Lucy and Gay dragged Lawrence Candler from his car after a minor traffic incident and beat him so badly he suffered brain damage. Neither Lucy nor Gay were charged in the beating because Candler declined to file a formal complaint. However, a San Francisco jury eventually awarded Candler $264,500.

https://theawl.com/the-untold-story-of-the-doodler-murders-6efbb99f4a24

It's really a remarkable story; about a serial killer that found his victims in gay bars; and the reluctance of many victims and witnesses to cooperate because of the notorious brutality of the local police.",0
"From the perspective of someone in the Germany of 1930, the antisemitic beliefs of the Nazi Party wouldn't appear all that different from other conservative parties. After all, even the Centre Party was conservative and antisemitic to a significant degree. As a Catholic party, the Centre Party fought pornography, birth control and defended Catholic interests at a conservative level.

After the Nazis seized power in 1933, they intensified their antisemitism, using it as a tool to complete their total takeover of the government. 

From the perspective of someone before the takeover, there would've been few ways to distinguish the Nazis from other conservative, right-wing figures like Ludendorff who also believed that Jews had stabbed the German Army in the back in 1918, that Jews were profiting while others starved during the Depression, that Jews were the figures behind the Communist movement in Germany.

As Evans writes: 

>""While the overwhelming majority of Germans still rejected the use of physical force against Jews during the Weimar Republic, the language of antisemitism became embedded in mainstream political discourse as never before. 

>The ‘stab-in-the-back’, the ‘November traitors’, the ‘Jewish Republic’, the ‘Jewish-Bolshevik conspiracy’ to undermine Germany - all these and many similar demagogic slogans could be regularly read in the papers, whether as expressions of editorial opinion or in reporting of political incidents, speeches and trials. They could be heard day after day in legislative assemblies, where the rhetoric of the Nationalists, the second largest party after the Social Democrats during the middle years of the Republic, was shot through with antisemitic phrases. 

>These were more extreme and more frequently employed than they had been by the Conservatives before the war, and were amplified by splinter groups of the right that collectively enjoyed much more support than the antisemitic parties of Ahlwardt, Böckel and their ilk. Closely allied to many of these groups was the German Protestant Church, deeply conservative and nationalist by conviction and also prone to outbursts of antisemitism; but Catholic antisemitism also took on new vigour in the 1920s, animated by fear of the challenge of Bolshevism, which had already launched violent attacks on Christianity in Hungary and Russia at the end of the war. 

>There were large swathes of the German electorate on the right and in the centre that fervently desired a rebirth of German national pride and glory after 1918. They were to a greater or lesser degree convinced as a result that this had to be achieved by overcoming the spirit of ‘Jewish’ subversion that had supposedly brought Germany to its knees at the end of the war.""",0
"> Kind of throttling discussion isn’t it? Why ?

This isn't a subreddit for discussion; it's one for high quality answers to historical questions. If you want to casually discuss history, r/history or r/askhistory is your best bet. Thanks!",0
"This was crossposted on r/AcademicBiblical so I'll just copy+paste my response from there.

*Lucifer qui mane oriebaris* is the Latin translation of the Hebrew *hȇlēl ben šāḥar*. The LXX renders the name *ἑωσφόρος ὁ πρωὶ ἀνατέλλων*. The literal translations are: ""the Morning Star, that rose in the morning"" (LXX), ""The Morning Star, that used to rise in the morning"" (Vulgate), and ""Day Star, son of Dawn"" (NRSV) in Hebrew. This refers to the planet Venus, which rose before dawn and was called Lucifer in Latin. The idea that Dawn begets the morning star is found in classical literature, where Eos (called Erigineia, ""Early-born"", an epithet for 'Dawn') ""brought forth... the Star Eosphoros"".

But the folklore here isn’t Roman, it’s Canaanite. John Day, *Yahweh and the Gods and Goddesses of Canaan*:

>That the verses represent an excerpt from a myth that was first recognized by Herder and it was Gunkel who first suggested either a Babylonian or a Phoenician origin. On the basis of a number of words and phrases used it is now generally accepted that the origin of the myth must be sought specifically in Canaanite mythology, and this has especially become clear in the light of Ugaritic parallels. Thus, Zaphon (Isa. 14.13) is well known from the Ugaritic texts as the mountain which constituted Baal's throne (cf. *KTU*^2 1.5.I.11; 1/6/I/57-9, etc.), and the words 'I shall ascend above the heights of the clouds' (Isa. 14.14a) again recall Baal, one of whose stock epithets was *rkb 'rpt*, 'Rider of the clouds' (*KTU*^2 1.5.II.7,etc.). 

[...]

>Is it possible to identify the morning  star Venus with a figure from Canaanite mythology? It is very probable that this role was filled by the god Athtar, even though this is nowhere explicitly stated. In South Arabia the god Athtar was certainly identified with Venus, and in Mesopotamia the cognate deity, the goddess Ishtar (sometimes represented as male) likewise represented the planet Venus. Similarly, the Canaanite female equivalent of Athtar, Astarte (Athtart), was equated with the Greek goddess Aphrodite (=Venus). It is probably that Athtar and Astarte represent Venus as the morning and the evening star respectively. Interestingly, Athtar was equated in the Ugaritic pantheon list with the Hurrian war god Ashtabi, which fits the warlike context of Isaiah 14.

[...]

>Now, it so happens that we possess a Canaanite myth from Ugarit, part of the Baal cycle, which speaks of Athtar's abortive attempt to occupy Baal's throne on Mt Zaphon and this has most commonly been thought to be the prototype of the myth in Isa. 14.12-15. It is to be found in the Ugaritic text *KTU*^2 1.6.I43-67, where after Baal's descent into the underworld the god Athtar was appointed by El and Athirat to the kingship in succession to Baal on Mt Zaphon, but he proved to be too small to occupy Baal's throne and therefore had to descend to the earth and rule from there.

The association of Athtar with Venus is also supported by Mark S. Smith in *The Origins of Biblical Monotheism: Israel's Polytheistic Background and the Ugaritic Texts*:

>At Emar Athtar is once called *Aš-tar* MUL, ""Ashtar of the stars,"" and Aramaic texts from the ninth century onward attest to *'tršmn*, ""Athtar of heaven,"" apparently a reference to the god's astral character. References to the astral character of Ishtar in Mesopotamian sources are also commonly used to bolster a case for Athtart as an astral god. Taken together, such textual references lend credence to the old view that Athtar and Athtart represent the morning and evening ""star"" (Venus).

This is also bolstered by Athtar being a member of the family of El and Athirat, which was composed of astral deities. Which segues into the next connection: ""above the stars of El"" (*mimma'al lěkōkěbě-'ēl*) refers to the council of divine beings, referenced again (indirectly) in v.13's ""mount of assembly"" on Zaphon (the Greek parallel would be Mount Olympus). See also Job 38:67,

>Who set its cornerstone when the morning stars sand together, and all the divine beings [*běnȇ 'ělōhîm*, lit.: ""sons of god(s)""] shouted for joy?

edit: As to the question about the connection between this figure and Satan, I'd refer to Philip Harland's [Religions of the Ancient Mediterranean](http://www.philipharland.com/Blog/religions-of-the-ancient-mediterannean-podcast-collection-page/) podcast for a good overview on the development of Satan.",0
"There's more that could be said, but allow me to paste in a previous answer of mine ...

The road is long and winding!

It begins with Gothic architecture, although this is something of an incidental step. Gothic architecture, developed and used from the High Middle Ages through the Renaissance, was characterized by an impression of narrowness and height; buildings in the Gothic style managed to be higher and narrower than earlier, Romanesque ones through the use of the slender and pointed Gothic or ogival arch, clusters of slender columns, and flying buttresses (basically, the skeleton of an arch, placed perpendicularly to a wall in order to support it). During the Early Modern period, the humanities and sciences of the Middle Ages - then conceived of as a savage ""dark age"" between the enlightened eras of the Roman Empire and the classically-inspired Renaissance - were typically rated on a scale from ""utterly barbaric"" to ""less barbaric, but still barbaric"", and by the time Batty Langley wrote *Ancient Architecture, Restored, and Improved* in 1742, this style was seen as ""coarse"" and ""artless"" and needed to be rehabilitated; where today we see it as having important feats of engineering, it was then generally considered to be ugliness incarnate essentially derived from the loss of culture caused by the destruction of Rome by the (see?) Visigoths.

As the love of Classical art and architecture built up to an extreme at the end of the eighteenth century, however, people started to see a good side to not just the Middle Ages, but a wilder and less pared-down aesthetic in general, and ""Gothic"" came to take on a new connotation. For instance, Richard Hurd's 1762 *Letters on Chivalry and Romance* described Edmund Spenser's *The Faerie Queen*, written and published at the end of Queen Elizabeth I's reign, as a ""Gothic poem"". The English started to see the Gothic era as one characterized by, well, chivalry and romance, and particularly important to their country's history, taking it from a degenerate period to one with important figures to remember and a tradition worth reviving.

And it certainly was revived. In architecture, the Gothic style was being used from the time of Langley, above (that's what he was writing for - he also published [a book of designs for Gothic architecture and interior design](https://books.google.com/books?id=oolMAQAAMAAJ)), and exploded in a big way in the early nineteenth century. The Lake Poets, led by William Wordsworth, Samuel Taylor Coleridge, and Robert Southey, took inspiration from medieval and English Renaissance literature, and were sometimes referred to as poets of the ""second Gothic"". Sir Walter Scott was an early adopter of the fascination with the medieval in both fiction and poetry; his early poems, *The Lay of the Last Minstrel* (1805) and *The Lady of the Lake* (1810), were set in Scotland in the sixteenth century, and he also wrote a number of historical novels. I must also mention that women's fashion was moving away from the Neoclassical and making many references to medieval and Renaissance dress, with slashed sleeves, starched white ruffs, and fuller skirts. Outside of England, others were responding to the same impulses against Neoclassical rationality: the German *Sturm und Drang*, the American Hudson River School and Transcendentalist philosophers.

All of this together - the historical settings and references, and the love of untamed nature - is also known as the Romantic movement. Gothic/Romantic artwork was lush and wild; novels were full of the kind of excitement that modern people living in the prosaic Industrial Revolution felt was gone from the world. This movement also saw the invention of the *historical reenactment*: a full-on medieval tournament was put on by the Earl of Eglinton in 1839, and several years later, Queen Victoria and Prince Albert held a [*bal costumé*](https://www.royalcollection.org.uk/collection/404540/queen-victoria-and-prince-albert-at-the-bal-costume-of-12-may-1842) where the pair dressed as Philippa of Hainault and Edward III and encouraged guests to also dress from the same period.

Romanticism faded away from popularity, not to return again until members of the hippie counter culture in the late 1960s adopted long, flowing skirts, peasant blouses, and long hair on both men and women. This was soon co-opted by the fashion industry, leading to a more mainstream take in the early 1970s. (Think Laura Ashley and Gunne Sax, cravats and ascots, those men's shirts with ruffles down the front.) The New Romantics of the 1980s - Spandau Ballet, Boy George, et al. - then took this to an extreme, as did Vivienne Westwood in her 1981 ""Pirate"" collection, with ""puffy shirts"" and eighteenth-century-style jackets.

Another aspect of the Romantic era, one which persisted through the late nineteenth century, was an interest in the dark, mysterious, and supernatural. Mary Shelley's *Frankenstein* (1818), for instance, is a classic Gothic horror story; *The Monk* (1796) and *The Mysteries of Udolpho* (1794), two Gothic novels referenced in Jane Austen's Gothic parody *Northanger Abbey*, were also classics of the genre and featured evil villains, persecuted innocents, and a lot of implausible but exciting situations happening in dark castles. This is the meaning of ""Gothic"" that the twentieth century goths were drawing from.

The modern gothic subculture began with gothic rock artists and fans, an offshoot of punk that started in 1979-80, and then went on to combine with the style of the New Romantics - like goth rock and punk, both a musical and a fashionable movement. Goths pulled from Victorian clothing, wearing black PVC corsets, velvet, lace, and high boots; they also added a lot of crosses in jewelry and tattoos, ripped fishnet stockings, and of course a heavy use of pale foundation and black makeup, none of which were really in use in the nineteenth century. Modern goths are a post-modern pastiche of the people involved in the literary Gothic movement, many steps removed from the original ancient Europeans.

For further reading, I want to suggest *Gothic to Goth: Romantic Era Fashion & Its Legacy* - it's out of print, sadly, but as it's a catalogue from an exhibition at the Wadsworth Athenaeum it is *beautiful*. You might also enjoy Valerie Steele's *Gothic: Dark Glamour*.

If you enjoyed this, you might like some other recent answers of mine:

[At what point in European history did it become necessary for a woman to accept a proposal of marriage (as opposed to/in addition to her father or guardian)](https://www.reddit.com/r/AskHistorians/comments/ku49fx/at_what_point_in_european_history_did_it_become/gjmy6w9/)

[I am a monarch in medieval western Europe, but woe is the kingdom, for the royal womb is barren! What is done to remedy this? Medicine? Prayers? Witchcraft? Henrytheeighthing? And what happens if the years go by and the queen approaches menopause?](https://www.reddit.com/r/AskHistorians/comments/kx3env/i_am_a_monarch_in_medieval_western_europe_but_woe/gjipjwu/)

[What was the quality of life like for Mary, Queen of Scots throughout the 19 years in which she was imprisoned?](https://www.reddit.com/r/AskHistorians/comments/krpbtg/what_was_the_quality_of_life_like_for_mary_queen/gjf1y0b/)

[How were illegitimate children named in the regency era?](https://www.reddit.com/r/AskHistorians/comments/kr540o/how_were_illegitimate_children_named_in_the/giq62is/)

[I'm a younger member of a royal family in the 1300s. I'm not really in line to be the next king. What are my options?](https://www.reddit.com/r/AskHistorians/comments/koencj/im_a_younger_member_of_a_royal_family_in_the/gi0um6g/)",0
"# The Cit(ies) That Surrendered

Having left Ogodei and Chagatai to deal with the offending governor of Otrar, Genghis himself, along with his youngest son Tolui, rode on, moving their main force southwest and across the (likely frozen, and thus easily crossable) Syr Darya River to the fortress at Zarnuq. Though heavily fortified, it surrendered without offering resistance, and after occupying and reducing its population, turned away from the road to Samarkand, and instead followed several hired Turcoman guides up the little-used road to Nur. It too would surrender without a fight to the forces led by Subudei, and would become one of the few towns across Khwarazmia that would not face the Mongol extermination command, and indeed was demanded a comparatively low tribute.

(More info [here](https://www.reddit.com/r/AskHistorians/comments/c27hqp/i_am_a_governor_of_a_village_at_the_height_of/erj8j8o?utm_source=share&utm_medium=web2x))

So, Nur found out the *best* possible outcome. Surrender - complete, totally, and right away - and live.

They'd be among the very few... let's look on...

# The Fate of Unfaithful Soldiers

The greater part of the Khwarazmian armies were Turkish mercenaries, who had rather early on in the conflict begin rapidly doing the calculus in their heads and determining that – regardless of what amount of payment was promised them by the Amir – it was a bad deal if they were too dead to spend it later. As such, especially early on in the war, entire city garrisons of ostensibly Khwarazmian troops closed and sealed their city gates, only to begin negotiations with the Mongol armies upon their arrival. They appeared to think that, given the fact that large numbers of the Mongol armies were likewise their Turkic brethren, that they could bargain their way out of the fate that awaited them, and join the winning side – perhaps even getting a better deal in the process.

The Mongol emissaries were more than willing to hear these garrison commanders out, as well as their terms. Sure thing, open the gates, and we’ll let you live, heck you can join us, we’ll pay you guys well, shiny new hats while we’re at it – uh-huh. Whatever you say. Yeah, but first, open the gates. When the Turk soldiers were sufficiently satisfied that they were totally about to be on the ins with the Mongols and join up, they’d throw open the gates as promised and let the conquerors inside to be duly disarmed and mustered in the city square along with the rest of the populace… only to realize far too late that those honeyed words they’d been told from the other side of the wall now meant exactly nothing from this side. Moreover, what possible use could the Great Khan or any of his commanders have for cowardly soldiers who fought only for money, and would turn their cloaks at the first sign of trouble? No, the Khan valued above all else loyalty… and just because you cheated on your prince *for* me, doesn’t make you any less of a cheater. Their terms of their negotiation were therefore retroactively *amended* – and they would one and all receive precisely the reward for their “service” to the Khan that their actions deserved: a traitor’s execution, to the last man. De Hartog notes that such a brutal – and arguably underhanded - tactic was a military necessity early in the war, and while the Khwarazmian army remain as-yet unbroken. “The numerical inferiority with which \[Genghis\] marched into Khwarazm demanded \[…\] absolutely loyal troops. In 1221, when he no longer had much to fear from the enemy’s army and it was necessary to replace his losses, Turkish troops were indeed accepted from the Khwarazm army, even in fairly large numbers.”

# The Governor Who Ate Gold

Otrar would fall in the night through one of the many acts of self-serving treachery by the Turkic guardsmen opening the gates to the Mongols in an attempt to spare their own skins. In short order and seemingly without much internal resistance, as much as 90% of the soldiers garrisoned within the city were killed either as they slept, or in the subsequent slaughter. That left perhaps as many as 2-6,000 troops alive and within the central citadel, who were able to bar the gates and man the walls with the governor ensconced within. For five long months the defenders held out, until at last they slipped up… once again through an act of self-serving cowardice. A senior commander of the guard attempted to flee through a postern gate, but was immediately caught and executed – for treachery, of course – by the Mongol besiegers. Then, they forced their way in. From John Man, “Their quarry “Little Lord” Inalchuk, barricaded himself in the inner sanctum with several hundred defenders. Since the Mongol had orders to take Inalchuk alive, there followed a slow, methodical attack that lasted another month. Realizing they were doomed, the defenders staged suicide assaults on Mongol spearmen and bowmen, 50 at a time, until finally Inalchuk and his few surviving bodyguards were trapped on the upper floors.” By that point, they had been reduced to hurling little more than bricks loosened from the walls down at their ever-encroaching foes. At last, by the Great Khan’s order, Inalchuq was taken alive and in chains. After the appropriate pomp and ceremony for one so infamous to the Mongols for his crimes against him, he faced his public execution. The more colorful telling of the tale of the fall of Otrar have a truly karmic death for the greedy governor. All of this had been started over his lust for gold and silver, and so let it end. The governor was restrained, ingots of the precious metals were melted down in cauldrons, and then – yes, exactly like what Khal Drogo did to Prince Viserys in Game of Thrones – the molten stream was poured into his eyes and ears. Awesome? Yes. Brutal? Definitely! Likely? Not so much… Such an end – creative and poetic though it certain would have been -  would have been rather uncharacteristically un-Mongol of Inalchuq’s executioners. There is a certain brutal elegance and calculus to Mongol dealings with death – unlike many other medieval societies where the pain of the torture is the point of an execution… Mongols tended to view such thing much more simply and dispassionately. Why waste the time and the gold on such an exotic method, when the simple stroke of a sword would accomplish the same goal? Or – even more likely – given the Mongol aversion to shedding noble blood, a far more characteristic method would have been to sew him up – perhaps in one of his own fine carpets – and then either have a herd of Mongols horses repeatedly trample him to death, or throw him into a river to drown. Less flashy, yes, but that’s a much more typical Mongol method of execution.

# The Punishment of God

In February of 1220, the dawn of the Year of the Dragon, Genghis Khan reached the great city of Bukhara, with a population of some 300,000 almost rivaling that of Baghdad itself, and where the Amir’s ill-gotten treasures had been taken and sold off. The Mongols invested the city in a siege, which lasted for three days before the garrison force within, purportedly as many as 20,000, sallied forth and attempted to break through the Mongol lines to freedom. Though a few did manage to reach the far banks of the nearby Amu Darya and kept their lives, the vast majority were wiped out in the attempt by the Mongols, who then turned back to the now completely undefended city. Abandoning all hope, the populace thereafter surrendered and opened its gates, allowing the conquerors inside. Given its singular position in the mind of the Great Khan – the very embodiment of defiance against his will – Genghis broke with a tradition of his, of studiously avoiding setting foot in the disgusting and claustrophobic confines of a settled city. It’s evident that Genghis well understood the importance of the city beyond it’s position in his own or his people’s minds, as well. “Noble Bukhara” had long been famed across the Muslim world as a center of piety and the “ornament and delight to all Islam,” and had been called by 11th century anthologist al-Tha’alabi the “focus of slendor, the shrine of empire, the meeting-place of the most unique intellects of the age.” Probably the greatest of Noble Bukhara’s gifts to the world had been the 11th century scholar and scientist Ibn Sina – better known across Europe by his Spanish moniker *Avicenna* \- whose compendium on maladies and their treatments, the *Canons of Medicine*, would be ***The*** medical textbook for the subsequent 500 years.

Thus it was very much the 13th century version of “playing to the cameras” in a grand and sweeping propagandistic gesture that he rode through the city gates, surrounded by his honor guard of *nökör* Companions, and to the city’s central square.  The townspeople had been herded like cattle by their conquerors to amass there, and upon the Khan’s arrival, he called out to the hushed and huddled masses that before any other act was undertaken, “the countryside is empty of fodder, feed our horses bellies.” Provide it at once - by demanded such and having it provided by the people, it signaled the submission of the populace, and the Mongols’ acceptance of their surrender  -that they were now the vassals of the Khan, subject to his commands and entitled to his protection.

When this token of submission had been completed, Genghis proceeded to the largest and most ornate building in the square – and indeed in the city as a whole – and asked if it was the house of the ruler. When informed that it was, in fact, the Great Mosque of the city, and the house not of an earthly ruler, but of God Himself, Genghis made no reply. He dismounted and proceeded – in the only known instance of him having done so in his life – into the building of stone. He then commanded that the scholars and priests within likewise tend and provide for his horses at once – again, symbolically placing them under his aegis of protection and vassaldom.",0
"Even with the defeat, schools remained opened for the beginning of the 1940 school year, so you would have had to teach starting October 1st. Your classes' effectives would have been severely reduced, as many students (those whose family had the means) would have fled to the countryside. Many would stay away from Paris for the whole war.

There is a good chance you would not have been able to attend the beginning of classes, though, as many teachers had been mobilized in the French army. Many of them were in prison camps. You could have been one of the 250 000 (about 12%) who evaded German captivity or you could have been among the 330 000 prisoners repatriated for medical reasons in March 1941.

For the year 1940-1941, nothing much would have changed, except that you would see the ""vert-de-gris"", German uniforms, here and there, and that you would sense a mood of hostility against teachers. Many commentators, such as Paul Claudel in *Paris-Soir*, were accusing you and your colleagues of being responsible for the defeat. Being back alive from the front would have made you look like a coward in the eyes of conservatives, because you were supposed to show the example and die for your country. In the *Revue des Deux-Mondes* during the Summer, Philippe Pétain would accuse teachers of inciting defeat by spreading pacifism and socialism, and some measures would be taken against them.

The first of these measures weren't that bad. According to an order sent on August 9th 1940, those who had ""spread outdated ideas"" were moved to another posting in order to start anew. Okay. On August 16th, inspectors were mandated to find teachers ""not worthy enough to teach to French youth"". Not too bad either, but the climate for the beginning of this first school year under the occupation is shitty, a few colleagues ratting to the authorities about the others' bad ideas or bad behavior.

On October 11th, the first coercitive measure is taken against women, as all female public servants are fired (to take care of their families). So you might lose one or two of your colleagues in the beginning of the year, having to take in their students in your half-full classes. On November 15th, you might lose another colleague or two as ""past involvement"" in ""anti-French"" groups such as the Front Populaire becomes a fireable offense. Still, it's not that much, as it is thought to amount to about 1000 teachers on the 30 000 in France, a mere 3,33%. Nothing compared to your 13 000 colleagues in German prison camps.

On October 31th, an order is given to identify all Jews teaching in French schools. For this, they rely on colleagues telling on each other. Their jobs would have to be cancelled by the beginning of December. Same thing goes for communists and freemasons. In class, though, nothing much has changed as the major reforms would come in for 1941-1942, but the figure of Pétain becomes omnipresent : songs, pictures depict the Maréchal, new heroes like Jeanne d'Arc and Vercingétorix become important. Some books are now forbidden, both by the Vichy regime and the Germans, you have to slightly change your syllabus.

Starting October 1941, every public servant has to pledge allegiance to Philippe Pétain. You do it with mental fingers crossed. Why lose your job ? A portrait of the Maréchal and a French flag are now mandatory in every class. Every Monday morning, the flag has to be raised in the schoolyard in a ceremony. Every Saturday afternoon, it has to be folded to mark the end of the week. Big reforms are planned to happen in the coming years, the main theme of teaching has to be ""travail, famille, patrie"", work, family, fatherland, but the new program is announced for 1946. Anyway, you teach as you always did, pretending to care about the new regime when inspectors or colleagues you can't trust are around. Repression against your Jewish students starts to intensify, but you might not see it firsthand. Some of them might disappear by the end of 1942, either because they fled to the free zone or worse.

Meanwhile, your life conditions worsen. After a short period of improvement after the Summer of 1940, when 2/3 of the city had fled and everything was missing in stores, many things become scarce. Curfews whenever some German officer gets shot by a ""terrorist"", no more sweet Virginian tobacco, only that crappy stuff that tastes like sawdust. Coffee ? Tea ? Chocolate ? Dream on. Maybe on the black market. Soap ? Save it. Coal ? Paris is so cold and wet during winter... You had a car ? No you hadn't, you're only a teacher. Anyway, there is no more gas for those who had one. Everybody's riding a bicycle. You even see horses coming back in the streets. Your cat ? Gone missing since two weeks. You suspect the neighbors ate it. You go back to school. Monday, somewhere in 1943. You know the Germans are losing the war since Stalingrad, some even whisper a version of ""Lili Marlene"" saying the Russians are coming for them. Where are the Americans ? Where are the British ? You follow them on ""Radio Londres"" in your friends' apartment. What are they doing in Africa ?

It's Monday. You have to raise the flag and sing ""Maréchal, nous voilà!"" with your students. Some are missing. You have no news from your Jewish ex-colleague who was sent in a work camp last June. Hopefully they feed him well. You are luckier, you teach. At least, you have a job for as long as you can put up with the Pétain thing.

Jean-Michel Barreau, *Vichy contre l’école de la République*, Flammarion, 2000.

Stéphanie Calcagni. *Éducation et enseignement sous le régime de Vichy, 1940-1944*. Education. 2013.

Rémy Handourtzel. *Vichy ou l'échec de l'""école nationale"" (été 1940-été 1944)* In: *L’école et la nation: Actes du séminaire scientifique international. Lyon, Barcelone, Paris, 2010* \[online\]. Lyon: ENS Éditions, 2013.",0
"With gratitude to SepehrNS for the tag! I have a couple of earlier answers that might help. The closest one is:

* [Since most of our physical and visual perceptions of hell come from Dante or later works, what did earlier medieval European Christians associate hell with in a visual or physical sense?](https://www.reddit.com/r/AskHistorians/comments/ii4zw6/our_current_concept_of_hell_as_a_blazing_inferno/g34le5r/)

~~

Yeah, yeah, Dante always gets all the credit. Don't get me wrong--the breathtaking scope, literary dexterity, theological and political ambition of the *Commedia* make it a suitable and attention-grabbing reference point. But Dante's versions of hell, purgatory, and heaven are deeply rooted in more than a millennium of Christian visions of the afterlife (influenced by various pre-Christian and philosophical traditions as well). For the early Middle Ages up through 1200, in fact, visions of heaven and hell were the single most popular form of religious vision narrative in the Latin west. (They don't exactly diminish in popularity afterwards, but other forms of visions become cultural tsunamis). Don't worry; for this answer I will concentrate on hell. ;)

The Bible does give us the idea of a ""lake of fire,"" but it's remarkably scant on other details. Nevertheless, already in the New Testament apocrypha (texts written a bit later than the canonical NT books), there are extensive visions of the afterlife in the *Apocalypse of Peter* and in the *Shepherd of Hermas*. The *Apocalypse of Peter* is intriguing because it already develops the idea of the contrapasso, or ""the punishment symbolizes the sin."" 

Early medieval voyages to the afterlife often take on, as a whole, the tone of a morality tale. A newly deceased sinner is given a cosmic tour, only to come back to life to spread the message of the joys of paradise/horrors of hell (and, of course, they inevitably clean up their life). One of the innovations of this era, that Dante will also draw out, is the use of hell visions as political weapons. More often this is much vaguer than Dante, like placing all the bishops who buy and sell offices in hell. But there's a reason for that. These visions are written and passed down as experiences that ""actually happened""--as genuine witnesses to the underworld.

By the high Middle Ages, there's a new wrinkle in the medieval visionary tradition: the idea that ""living saints,"" particularly women, are specially graced by God with divine visions. Although much of the women's visionary corpus is Christocentric, many visionaries receive (or ask for and are denied, as with Julian of Norwich in the 14-15C) a short trip through heaven and/or hell. Women's visionaries of purgatory, meanwhile, often involve these living saints actually *pulling people out* according to God's will and then by their own prayerful and ascetic intercession. Mechthild of Hackeborn, a late 13th century German visionary whose visions of paradise sparkle with gemstones and light, is typically proposed as Dante's ""Matilda""--that is, that he was familiar with her and her visionary text, the Liber specialis gratie.

But for my money, the best high medieval vision of hell belongs to Mechthild of *Magdeburg*, an independent religious women slightly older than her co-named monastic counterpart. Mechthild has extensive and lavishly described visions of paradise and purgatory as well, but her hell is top-notch. You can read a fair portion of [Frank Tobin's translation of it](https://books.google.com/books?id=vH3dYiGG-5sC&pg=PA127&lpg=PA127&dq=mechthild+of+magdeburg+%22its+name+is+eternal+hate%22&source=bl&ots=QUE2qXzOxh&sig=5dVj1DwrqfHRqFvz-mTh23YTCiY&hl=en&sa=X&ved=0ahUKEwj72MyVstLSAhUI-GMKHdxVD7UQ6AEIHDAA#v=onepage&q=mechthild%20of%20magdeburg%20%22its%20name%20is%20eternal%20hate%22&f=false) on Google Books (scroll down a bit on that first page), and you absolutely should.

> I have seen a city / its name is eternal hate

> It was built in the deepest abyss / From all kinds of stones of huge capital sins

> [...] Lucifer sits bound by his guilt in the deepest abyss. There flows unceasingly out of his fiery heart and out of his mouth all the sins, torments, sickness, and shame in which hell, purgatory, and the earth are so wretchedly entangled. In the bottommost part of hell, the fire, gloom, stench, shuddering, and all kinds of intense pain are the greatest.

> [...][Lucifer] grabs the proud one and thrusts him under his tail and says: ""I have not sunk so deep that I shall not lord it over you."" All the sodomites pass down his throat and live in his belly. Whenever he draws a breath, they slide into his belly. But when he coughs, they are expelled again.

> The false saints he puts upon his lap, kisses them hideously...Unceasingly he gnaws the usurer and rebukes him fo rhaving have been moved by mercy. The thief is strung up by the feet to serve in hell as a beacon, but the damned do not see the better for it.

> [...] At the top, hell has a head that is hideous and has on it numerous fierce eyes which shoot forth flames.

Oh, Mechthild!

I introduce that last bit because there is a parallel tradition that passes down ideas of (especially) hell in the Middle Ages: art. Indeed, depictions of hell were a standard feature in church sculpture, generally as part of a Last Judgment scene. And while Dante's ""gates of hell"" have their place in iconography (though note that Satan's jaws have a starring role in the lowest level of hell), Mechthild's reference to hell having a head arrives at *the* iconographic symbol of hell up to the Reformation: the hellmouth. That's right. Long before it was terrorizing the students of Sunnydale High School, hellmouths were [all over](https://www.google.com/search?q=hellmouth&source=lnms&tbm=isch&sa=X&ved=0ahUKEwj58OiJtNLSAhUWz2MKHTdHCZMQ_AUICCgB&biw=1160&bih=552) medieval manuscripts, churches, and imaginations. Sometimes they are more reptilian, sometimes more lion-like, sometimes just abstract evil with eyes.

As far as conceptions of the afterlife go, Dante's importance is less in creativity and more in sealing the deal. The insane popularity of the *Commedia* and its enduring hold on western imaginations means we are likely to continue to talk of hell in terms of his metaphors and descriptions. But those literary devices have deep and fertile roots.",0
"Yes, there are a few notable examples of nude statues from Greek and Roman antiquity that depicted female genitalia, although they are relatively rare compared to the number of male statues with explicit genitalia. One such example is the marble statue known as the Venus of Willendorf, dating back to around 28,000 BCE, which clearly shows the vulva and pubic area. However, it is important to note that this statue predates the Greek and Roman civilizations.",1
"Possibly. The question of whether a portrait was inaccurate became a factor in at least one high-profile annulment, actually. 

In 1539, Henry VIII of England was in the market for his fourth wife. His most recent wife, Jane Seymour, had died in childbirth and he had remained unmarried for a little while after her death. Because Jane had finally provided him with the male heir he desired, he was able to spend some time looking around for a politically advantageous match. He sent ambassadors hither and yon to look at the most eligible princesses of Europe ([and these ambassadors returned with very detailed descriptions of the ladies, down to pox scars and hairstyles\).](https://englishhistory.net/tudor/monarchs/anne-of-cleves/) Finally, after a considerable amount of politicking that we can skip over, he dispatched one of the greatest court painters of all time - [Hans Holbein the Younger](https://en.wikipedia.org/wiki/Hans_Holbein_the_Younger) - to the Duchy of Cleves (a part of what was then the Holy Roman Empire and what is now Germany) to paint the daughters of the Duke of Cleves. Their names were Anne and Amelia. 

Holbein brought the portraits back and showed them to Henry -
 [here is the picture he painted of Anne.](https://englishhistory.net/images/tudor/cleves-bio1.jpg) When Henry saw it, he was entranced by Anne and decided to bring her to England to become his fourth wife. But when he met her in person, he was completely disgusted, telling Thomas Cromwell that she was ""nothing so fair as she had been reported,"" her ""body was disordered"" and was ""indisposed to excite and provoke any lust” in him. He ""like[d] her not"" and he was furious at the courtiers whom he felt had misrepresented her and misled him into believing that she was a great beauty. Although he held his nose and married her, he never consummated the marriage and it was dissolved within six months. (Anne of Cleves wound up being Henry's luckiest wife, though, living the rest of her life in luxury in England as ""the king's sister."") We can see from the portrait that Anne wasn't portrayed as unpleasantly fat, ugly, or saggy, which is how Henry found her upon arrival. So was this a case of Henry and Holbein having radically different perspectives, or did Holbein really paint Anne in an unrealistically attractive light (perhaps having been pressured by someone like Thomas Cromwell, who was desperate for the alliance with Cleves)? We can't know. However - and perhaps tellingly - some of Anne's contemporaries (other than Henry) did describe her as ""pockmarked,"" which she absolutely isn't in Holbein's portrait.

Another example where we can tell 'massaging' probably happened by a painter might be [Charles II of Spain.] (http://en.wikipedia.org/wiki/Charles_II_of_Spain) The last Habsburg king of Spain, generations of Habsburg inbreeding had rendered him *severely* disabled - by this point they had more or less completely stopped marrying anyone outside the family, so he was more inbred than someone whose parents are siblings. He was known for terrible hygiene and had significant, incapacitating mental and physical deformities (he could barely chew or speak). [Yet check out some portraits of him.](https://en.wikipedia.org/wiki/Charles_II_of_Spain#/media/File:Rey_Carlos_II_de_Espa%C3%B1a.jpg) He appears to be more or less a normal royal guy, albeit one with a giant Habsburg chin. You can't tell from his portraits how profoundly disabled he was.

I suspect it is more or less like having professional photographs taken of you today. If you were having your portrait taken at a studio, you'd wear your best clothes, the photographer would be careful to light you well, and after the picture was taken they might use a little judicious Photoshop to clean you up a little. It might not be a real representation of the daily, perhaps scruffier ""you,"" but it's recognizably you. 

(If this comment piqued your interest, you might enjoy the stellar ""The Six Wives of Henry VIII"" by Alison Weir, which contains plenty of primary source material about the ladies themselves and the politics of the time, as well as plenty of additional entertaining stories about Henry.) ",0
"So the leather scene started after WWII, as a machismo rejection of the current American culture and values. It became popular in the mid 1950s, with the emergence of the Satyrs MC and NYMC. Add in the Hollister riots, a film with Marlon Brando wearing a leather jacket, and you have a cultural movement. 

This style doesn't actually ""hit"" the gay community until 1958 when Chuck Renslow opened *The Gold Coast*, the first gay leather bar in the US, in Chicago. Then in 1961, the first one in San Francisco opened, *The Tool Box*, and is frequented by a lot of gay motorcycle clubs. This is the point where you see the bars full of gay leathermen, and the culture starts expanding rapidly.

The 1970s roll around, and activists like Cynthia Slater and Pat Califia start pushing for lesbian acceptance in the community, and before 1980 *Dykes on Bikes* and *Sanois* (a lesbian MC, and a lesbian, feminists BDSM community, respectively) enters the scene. At this point there are BDSM/Leather/Biker publications everywhere, the movement has a lot of crossover with gay rights, the AIDS epidemic, women's rights, motorcycle clubs, and fetishism, all sharing this one community in common.

As far as the decline of the leather scene, specifically in the gay community, it is a combination of several factors. The initial gut punch was the AIDS outbreak, which led to many of the leather and sex clubs closing, many never to reopen, and ALL of them being raided regularly by police. The scene also lost people organically through the 80s and 90s, with the emergence of PVC and latex fetishism.. In addition to this, the internet put people's fetishism and sexuality much more in their own hands, where the leather club wasn't the only place you could get your particular rocks off. The biggest factor affecting your specific question though, I believe, is that the average gay man's interest in a scene centered around hyper masculinity and machismo has declined considerably since the 1970s. Society had decided then that homosexuals weren't ""manly"", so many wore leather to ""reclaim"" their masculinity, and displayed it for all to see. While homophobia is alive and well today, our culture has rejected, for the most part, that a man being gay makes him less of a man. A strong plurality of gay men now feel confident expressing their sexuality, the leather community is no longer the singular safe space for gays, lesbians, fetishists, bikers, and whoever else. Most leathermen/women now are over 40, with younger homosexuals and fetishists finding spaces they identify with more. So I don't think the leather scene died as much, as it gave birth to dozens of other groups and movements that represent and celebrate other people's individuality as well.

If you would like a really insightful read on this subject, *The Rise and Fall of Gay Culture* by Daniel Harris goes into these factors in much greater detail, and covers the rise and fall of numerous gay movements, as well as AIDS impact on these things, though it is about the entirety of gay culture, not just the leather scene. But it would be a good read for you as you are specifically interested in the decline of the culture.",0
"Out of curiosity, if the freed slave had a child after he had been freed, did that child have any connection or obligation to the Master? Or is the child just considered a citizen since they were born to a freedman?",0
"Yes and no. As you suggested, this is presented for simplicity. While it's true that the main gods can be related to Greek equivalents, I'd like to think it's more complex than just the Romans stole the Greek gods and re-named them. And the Roman's are well known for essentially stealing pieces of other cultures and then branding it as their own, so it's not totally wrong, however they adopted from other cultures as well. 

For instance, if we look into the history and etymology of the Roman god Minerva, thought to be the equivalent of the Greek's Athena, she actually has origins from the Etruscan culture. Etruscan culture also existed in Italy (Tuscan region) prior to when the Roman's were first thought to have settled (and perhaps originated from if we don't take Virgil's Aeneid at face value). Here she was known locally as Menrva. It is thought that over the centuries she took on more and more of the common epithets & ritualistic traits known from Athena, eventually becoming the Minerva/Athena that was born from the head of Jupiter/Zeus. It is also thought that perhaps when Rome was founded - likely by a few Etruscan men in the area - Menrva came with them to form the backbone of the newly forming Roman culture. Her worship can also be dated back to the founding of Rome in the 8th-7th century BCE where she formed part of the Capitoline Triad along with Jupiter and Juno (who also likely came from the Etruscan pantheon where they were known as Tinia and Uni, respectively). Who she represented then is largely unknown as we don't have much written history pertaining to the early days of Rome, but it's likely that the early Minerva of Rome's foundation was completely different from the Minerva of the Republican era, and this Minerva was different to the Minerva of the Imperial era. It's likely that throughout the centuries, the gods were adapted and altered as the Romans came into contact with other cultural groups, and even more likely that the gods were Hellenized by the Romans to give them a more courageous & interesting backstory like the Greeks boasted so that the Romans could look even more prestigious and favored by the gods than their Greek rivals. 

Rome was known to let it's newly conquered people retain their culture and local pantheon of gods as long as they recognized Rome as their head of state. So it's assumed that as Rome came into contact with these cultures, the 'cool gods' of these conquered states were of interest to Roman citizens and would gain traction and eventually become known as 'that Roman god' instead of 'that Greek god' or 'that Egyptian god' where it originated. Sometimes this meant that a god was literally adopted from them (such as Greek's Apollo or Egypt's Isis) or it could mean that one of Rome's existing gods took on more of their traits and epic mythology (like Minerva, Jupiter, and Juno). However, not all Roman gods morphed into Hellenized gods over time - the Roman gods Quirinus and Janus, who were very popular during the Republican era of Rome, seem to be distinctly Roman and don't directly have a Greek or other counterpart. Their history also dates back to the early days of Rome suggesting they may have been invented and then popularized as gods by it's early kings.

We should also remember that Rome thrived for over 1000 years as a culture. Over this time Rome had the opportunity to come into contact with a number of cultures and ~~adopt them~~ conquer them to grow their empire. The Romans also admittedly forgot their roots over the centuries as written history wasn't common like it is today and most of history was reported orally throughout the centuries.It's thought that as the stories were passed down, they become increasingly exaggerated and the truth slowly deteriorates from the narrative. It's much easier and more fun to tell the tale an epic adventure of a god than it is say that a god was adopted from that local tribe 200 years ago. This is why Virgil's Aeneid was so popular since he presented a narrative about Rome's foundation that was otherwise unexplained to the Roman's of the 1st century, and he presented a Rome that was founded by a man from Troy, directed to this land by a Greek deity, and told in the epic format invented by the Greeks.",0
"(1/2) If you're interested in learning more about the BPP, I would recommend reading From The Bullet to the Ballot by Dr. Jakobi Williams. He is a civil rights activist and one of the leading BPP historians. Most of my information in this comment is sourced from primary/secondary sources that we utilized during a class I took with him about the history and development of the Black Panther Party. Bullet to the Ballot is an in depth look at the Chicago chapter specifically. My favorite overview is Black  Against Empire by Joshua Bloom and Waldo Martin - comprehensive but accessible enough that I recommend it to my high school students. Kathleen Cleaver has also written a fantastic history of the BPP, titled Liberation, Imagination, and the Black Panther Party. Bobby Seale, Eldridge Cleaver, and Huey P. Newton have all written extensively. Their initial political writings laid the foundation for the BPP, and they have also all written autobiographies. The autobiographies should probably be taken with a grain of salt, particularly anything written by Newton as he was kind of a piece of shit, but the original writings are all really important for understanding the core of the BPP. The FBI has a ""vault"" of publicly accessible information regarding the BPP and COINTELPRO if anybody wants to view those, but the book The COINTELPRO Papers by Ward Churchill is much more informative. There are also quite a few breakdowns on the role of gender within the BPP. My favorite is Linda Lumsdens Good Mothers With Guns, but a lot of people really enjoy reading things from/about Afeni Shakur, as her son Tupac is...uh....fairly well known. 

The issue with our modern perception of the BPP is twofold. One, history is told by the victors, and the BPP did not win. Two, the actions of Huey Newton after the FBI systematically dismantled the BPP from within were uh...not great. As the BPP faded away, the last actions that were widely associated with the BPP was Huey Newton being shitty. That doesn't lend itself to a positive representation. This extra sucks because Huey Newtons initial role as a founding member, and then as a martyr, was key in uniting the community. He was an initial force behind community mobilization and dissemination of information, who was then accused/convicted of a murder that most believe he did not do. His consequential imprisonment led to Free Huey campaigns and protests that spread the ideals of the BPP further than they had before. This also meant that for the most important years of the party's activity, he was an imprisoned figurehead, a martyr, and more of a rallying cry than an actual leader. His return to the BPP and actions from about 1971 - onwards did a lot of harm to the party.

In order to see the positive impacts of the Panthers, you need to look not at their downfall but at their origins. The Panthers originated from a desire within black communities to take care of their own, in the ways that the federal government refused to do. Once the party was established, different chapters were loosely linked by a name and some shared beliefs, but they didn't have a significant overarching structure. There were a few leaders and thinkers (Bobby Seale and Huey P. Newton primarily) that are commonly referenced as the leaders of the BPP, but it wasn't as strict of a hierarchy as most modern interpretations would have you believe. Chapters behaved relatively independently, as the main focus was on the communities within which members lived. Independence was also encouraged under the belief that it was safer for members. Actions and beliefs could vary widely from chapter to chapter, with the 10 Point Program and Platform as the foundation. It follows:

**What We Want Now!**

1. We want freedom. We want power to determine the destiny of our Black Community.
2. We want full employment for our people.
3. We want an end to the robbery by the capitalists of our black and oppressed communities.
4. We want decent housing, fit for shelter of human beings.
5. We want education for our people that exposes the true nature of this decadent American society. We want education that teaches us our true history and our role in the present day society.
6. We want all Black men to be exempt from military service.
7. We want an immediate end to POLICE BRUTALITY and MURDER of Black people.
8. We want freedom for all Black men held in federal, state, county and city prisons and jails.
9. We want all Black people when brought to trial to be tried in court by a jury of their peer group or people from their Black Communities, as defined by the Constitution of the United States.
10. We want land, bread, housing, education, clothing, justice and peace.

**What We Believe:**

1. We believe that Black People will not be free until we are able to determine our own destiny.
2. We believe that the federal government is responsible and obligated to give every man employment or a guaranteed income. We believe that if the White American business men will not give full employment, the means of production should be taken from the businessmen and placed in the community so that the people of the community can organize and employ all of its people and give a high standard of living.
3. We believe that this racist government has robbed us and now we are demanding the overdue debt of forty acres and two mules. Forty acres and two mules was promised 100 years ago as redistribution for slave labor and mass murder of Black people. We will accept the payment in currency which will be distributed to our many communities: the Germans are now aiding the Jews in Israel for genocide of the Jewish people. The Germans murdered 6,000,000 Jews. The American racist has taken part in the slaughter of over 50,000,000 Black people; therefore, we feel that this is a modest demand that we make.
4. We believe that if the White landlords will not give decent housing to our Black community, then the housing and the land should be made into cooperatives so that our community, with government aid, can build and make a decent housing for its people.
5. We believe in an educational system that will give our people a knowledge of self. If a man does not have knowledge of himself and his position in society and the world, then he has little chance to relate to anything else.
6. We believe that Black people should not be forced to fight in the military service to defend a racist government that does not protect us. We will not fight and kill other people of color in the world who, like Black people, are being victimized by the White racist government of America. We will protect ourselves from the force and violence of the racist police and the racist military, by whatever means necessary.
7. We believe we can end police brutality in our Black community by organizing Black self-defense groups that are dedicated to defending our Black community from racist police oppression and brutality. The second Amendment of the Constitution of the United States gives us the right to bear arms. We therefore believe that all Black people should arm themselves for self-defense.
8. We believe that all Black people should be released from the many jails and prisons because they have not received a fair and impartial trial.
9. We believe that the courts should follow the United States Constitution so that Black people will receive fair trials. The 14th Amendment of the U.S Constitution gives a man a right to be tried by his peers. A peer is a persons from a similar economic, social, religious, geographical, environmental, historical, and racial background. To do this the court will be forced to select a jury from the Black community from which the Black defendant came. We have been, and are being tried by all-white juries that have no understanding of ""the average reasoning man"" of the Black community.
10. When in the course of human events, it becomes necessary for one people to dissolve the political bonds which have connected them with another, and to assume among the powers of the earth, the separate and equal station to which the laws of nature and nature's god entitle them, a decent respect to the opinions of mankind requires that they should declare the causes which impel them to separation. We hold these truths to be self-evident, and that all men are created equal, that they are endowed by their creator with certain unalienable rights, that among these are life, liberty, and the pursuit of happiness. That to secure these rights, governments are instituted among men, deriving their just powers from the consent of the governed, that whenever any form of government becomes destructive of these ends, it is the right of the people to alter or abolish it, and to institute new government, laying its foundation on such principles and organizing its power in such a form as to them shall seem most likely to effect their safety and happiness. Prudence, indeed, will dictate that governments long established should not be changed for light and transient causes; and accordingly all experience hath shewn, that mankind are more disposed to suffer, while evils are sufferable, than to right themselves by abolishing the forms to which they are accused. But when a long train of abuses and usurpations, pursuing invariably the same object, evinces a design to reduce them under absolute despotism, it is their right, and their duty, to throw off such government, and to provide new guards of their future security",0
"(4 / 4)

At the time, Republican arguments were that the court was violating the free states' right to protect their citizens as free people. They believed there was nothing at all like a *given* that the Constitution required that federal slave laws superseded state laws. As Lincoln made mention of in his First Inaugural Address, the Constitution's ""full faith and credit clause"" *could* (and, he believed, *should*) be given more weight than the Fugitive Slave Clause, i.e., if a free state recognized a person as free, then the Fugitive Slave Clause couldn't apply, since other states owed ""full faith and credit"" to recognize the laws of the individual states as they applied to those state's individual citizens. This also explains the convoluted logic of the *Dred Scott* case, and why they had to come to a decision that black Americans, under any circumstances, were not considered U.S. citizens.—even those who resided in the North and had been born free.

So, contrary to the Republican argument, the Supreme Court never remotely interpreted the Constitution as one that protected free states' rights to protect their own citizens. The court's membership was virtually always one with a slave state majority, and slavery always took precedence over any free ""state's rights"", even if those same Democrat-appointed justices were quick to make ""state's rights"" arguments whenever they could on non-slavery matters before the court.

So, there was a situation where the United States was populated by a large, and increasing, majority of anti-slavery people, but with a court which was deciding cases on a narrow view that slavery trumps all, and that the legality of slavery was more important than the popular will. This created severe tension between legality vs. legitimacy, since the United States is founded on a basic principle of ""consent of the governed"". As Michael P. Zuckert writes in ""Legality and Legitimacy in Dred Scott: The Crisis of the Incomplete Constitution"", published in the *Chicago-Kent Law Review*:

> ""...[T]he Constitution did indeed give slavery a place, several places in the established legality, but the institution remained outside the broader consensus on the basic principles of legitimacy upon which the Constitution was erected.""

Thus, especially after the *Dred Scott* decision, there began to be a public perception of a lack of legitimacy of the court, since it was not at all representative of the people, and increasingly lacked the ""consent of the governed"". This was reflected in [contemporary newspaper editorials](https://books.google.com/books?id=nxZQAQAAMAAJ&pg=PA81) in reaction to many of the slavery-related cases of the era. As a clear example, in 1859, the *New York Tribune* criticized the Supreme Court with:

> ""This Court, as now arranged, is scandalously sectional, grossly partial, a mockery of the Constitution, a serf of the slave power, and a disgrace to the country. A truly National Administration will not fail to reform it so as to regain for it the confidence of the people...""

In reprinting the *Tribune* article, the *Cleveland Morning Leader* agreed, [adding](https://chroniclingamerica.loc.gov/lccn/sn83035143/1859-03-29/ed-1/seq-2/): ""The Supreme Court is the last great bulwark of slavery"".

There are many criticisms of the *Dred Scott* ruling being based upon unsound Constitutional interpretation, even at the time, when slavery was legal, but the elephant in the room is that it didn't really matter *how* the court came to their reasoning. They came to a conclusion, and it was up to the people—the governed—to give their consent to that ruling, and abide by it. There had been many other cases where it had been difficult for the public to abide by the rulings (such as *Prigg v. Pennsylvania*, or *Ableman v. Booth*), but the people had done so. But with *Dred Scott*, and other contentious legal actions by the government (the John Brown case, the Anthony Burns case, etc.), it was increasingly becoming a situation of: if the court does not represent the people, then do the people have a responsibility to respect their rulings as legitimate?

So, then, the 1860 election essentially became a matter of what to do about slavery's expansion, and how to reconcile that with the *Dred Scott* ruling, which had ruled the Missouri Compromise was unconstitutional. Four candidates gave four very different interpretations of what the executive and legislative branches of the federal government had a right to do in light of *Dred Scott*.

In his [First Inaugural Address](https://avalon.law.yale.edu/19th_century/lincoln1.asp), Abraham Lincoln touched on this. The passage is too lengthy to reproduce here in full, but to summarize it, Lincoln recognized the duty of the American people to respect the legitimacy of the court and their decisions, but there is only so far that the American people can be pushed before the court loses any of its power, which would be a very bad thing for the future of democracy. The people must be able to retain the power to overrule the Supreme Court through other legal means, such as through new legislation. (CTRL+F: ""I do not forget"" to find this passage of the speech.)

So, whether justified or not, this was the thought process behind adding a seat to the Supreme Court, and then denying slave-stater Andrew Johnson the right to fill an empty seat later on. Partisanship surely played a role, but this was the underlying reasoning behind that partisanship. In the Republican Party's eyes, a continuation of the court being representative of a shrinking, and very decided, minority, to the contrary of the popular will, was dangerous for the continued operation of a government ""of the people, by the people, and for the people"". It had become ""of the states, by the states, and for the states"", where some of those states were controlled by the ""Slave Power"", whose power was of questionable democratic origin. 

Or to put it more concisely, the American people were against an interpretation of the Constitution that supported the enforcement of slave laws in free states, but the Supreme Court was always controlled by a pro-slavery majority, who were consistent in interpreting the Constitution in a pro-slavery way. In effect, it had begun to strain the legitimacy of the court, and Republicans believed this endangered the continued preservation of democratic government. 

There have been arguments critical of the Republican remedy to this perceived situation, but that was their thought process. The worst case scenario was that the executive and legislative branches, and the states themselves, would start ignoring the courts' rulings altogether, so that the U.S. would have no operational justice system at all. 

In a way, that *is* what ultimately happened. The Confederacy could have filed a lawsuit to try to secede, but A) they knew it would probably lose, and B) they would consider such a courtroom loss as illegitimate anyway. In contrast to Northerners, they thought the courts were too soft on slavery-related issues and Southern ""states' rights"", so why bother filing a lawsuit in an illegitimate justice system? Instead, they resorted to a ""might makes right"" legal remedy, trying to win the case for secession through an act of war.

> I know most (not all) politicians and military leaders chose state over nation and joined the side their state went with.

This question could fill a whole separate post - and this is four posts long already - so this is better addressed as its own topic. But suffice it to say, there was no consensus on this in the South, and in the North, it's not particularly true. ""Most"", sure, but it wasn't a supermajority or anything. There were two parties that won over 40% of Southern votes in the 1860 election. One of those parties—the Southern Democrats—appealed to ""state over nation"". The other—the Constitutional Unionists—appealed to ""nation over state"". As exemplified by the actions of Wayne and Catron, as well as military men such as George Thomas and Winfield Scott, there were plenty of Southerners who believed loyalty to nation took precedence over state. It wasn't some tiny fraction. The Confederacy took active measures to suppress this dissent. The South was severely divided on loyalty.

**SOURCES**:

Battle, George Gordon. ""Review of *James Moore Wayne, Southern Unionist*."" *Fordham Law Review*, 1944.

Farber, Daniel A. *Lincoln's Constitution*. University of Chicago Press, 2003.

Fehrenbacher, Don E. ""Roger B. Taney and the Sectional Crisis."" The Journal of Southern History, Nov 1977.

Fehrenbacher, Don E. *Slavery, Law, and Politics: The Dred Scott Case in Historical Perspective*. Oxford University Press, 1981.

Huebner, Timothy S. *The Taney Court: Justices, Ruling Legacy*. ABC-CLIO, 2003. 

Lawrence, Alexander A. *James Moore Wayne, Southern Unionist*. University of North Carolina Press, 1943.

Martin, David L. ""When Lincoln Suspended Habeas Corpus"". *American Bar Association Journal*, January 1974.

McClintock, Russell A. *Lincoln and the Decision for War: The Northern Response to Secession*. University of North Carolina Press, 2008.

McGinty, Brian. *Lincoln and the Court*. Harvard University Press, 2008.

McGinty, Brian. *The Body of John Merryman*. Harvard University Press, 2011.

Saunders, Robert, Jr. *John Archibald Campbell: Southern Moderate, 1811–1889*. University of Alabama Press, 1997.

Silver, David M. *Lincoln's Supreme Court*. University of Illinois Press, 1957.

Simon, James F. *Lincoln and Chief Justice Taney: Slavery, Secession, and the President’s War Powers*. Simon & Schuster, 2006.

Warren, Charles. *The Supreme Court in United States History, Volume 3*. Little, Brown, 1923.

Zuckert, Michael P. ""Legality and Legitimacy in Dred Scott: The Crisis of the Incomplete Constitution"". *Chicago-Kent Law Review*, Dec 2006.",0
"Hey all,

If you frequent the sub, you know the drill. If you're here from /r/all (or /r/trendingsubreddits, which seems to like us today), or browse only occasionally, please be aware we have strict rules here intended to enforce the very high bar we expect from comments, so before posting, please read our [rules](/r/AskHistorians/wiki/rules). We remove comments which don't comply, and consider everyone forewarned. If you have feedback or commentary about how things are run here, please don't post it in this thread. We'll just remove it. We love to hear *thoughtful* and *constructive* feedback via [modmail](http://www.reddit.com/message/compose?to=%2Fr%2FAskHistorians&subject=Question%20Regarding%20Rules) however.

It can take time for [an answer to show up](/r/AskHistorians/comments/6a5duv/a_statistical_analysis_of_10000_raskhistorians/), so we thank you for your patience. We know you're here because the question sounds interesting, and we eagerly await an answer just like you! While you wait though, there is tons of great content already written, which you can find through our [Twitter](http://twitter.com/askhistorians), the [Sunday Digest](http://www.reddit.com/r/AskHistorians/search?q=title%3A%22Sunday+Digest%22&restrict_sr=on&sort=new&t=all), the [Monthly ""Best Of""](https://www.reddit.com/r/AskHistorians/wiki/bestof) feature, and [Facebook](https://www.facebook.com/askhistorians/). If you don't want to forget to check back late, consider a [Private Message](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLINK%20INSIDE%20SQUARE%20BRACKETS%20else%20default%20to%20FAQs%5D%0A%0ANOTE:%20Don%27t%20forget%20to%20add%20the%20time%20options%20after%20the%20command.%0A%0ARemindMe!) to the [Remind-Me bot](https://www.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/), or the 'RES' Subscribe feature.

Again though, please remember the rules, and be conscious of them while you browse or write. If you don't like how this subreddit is run, keep in mind that this method has seen us continue to succeed and grow for years, and isn't going to change, so at least try and make your complaint original. /r/AskHistory exists, so whining about the rules to us is like going into a fancy restaurant to throw a tantrum because they don't sell chicken nuggets, even though Chick-fil-A is nextdoor.

**Edit:** Report ""*1: dont shit on chick-fil-a man, love their food. Say mcdonalds or smth*""

LOL!

Seriously though, don't get the impression that I'm shitting on /r/AskHistory here (or by analogy, Chick-fil-A)! Some people would rather have chicken nuggets than chicken cordon-bleu, and that is *totally fine!*. The point is that multiple subreddits exist on this site which complement each other, and allow users to find the experience that *they* want.  If I was continuing the analogy, then McDonalds I assume would be /r/ShittyAskHistory.",0
"The individual who cast the sole dissenting vote against Adolf Hitler's assumption of power within the Nazi party was Otto Strasser. He was a prominent figure within the party during its early years and had ideological differences with Hitler. After Hitler's rise to power, Strasser was marginalized and eventually expelled from the party in 1930. He went on to form his own faction called the Black Front, which sought to pursue a more radical socialist direction for the Nazi party.",1
"I agree that this is an excellent question. Most of what we have from 1952 is so positive about the new Queen, that we should wonder about any likely alternative approaches.  Two initial indicators:

Churchill initially remarked that he was worried about the Queen's age an experience.  

1/3rd of the population apparently viewed the new monarch as a sign from God.  

1952 was a very difficult economic time for the UK, and while the focus was not as much on the new Queen (as it was initially on the death of her father, on the economy &c), it seems that Elizabeth brilliantly rose to the occasion, charming everyone in her path, proving that she could be a public relations blessing for the nation, rather than a concern.  The extent of her political power was not a major concern, it seems, and this helped with the acceptance of her as Queen. 

Here is an initial resource: http://www.williamshawcross.org/index.php?page=timesqueen",0
"I will attempt to explain a similar situation in a broad sense, that may illuminate why puzzling public support occurs; there are a LOT of specifics to the Waco standoff that I'm obviously not addressing; I'm just showing a related example and what was learned from it. Mods; if I'm going off the rails I will not be offended if you delete my comments.

My area of study is left-wing European terrorism of the 1970s era. The dominant group active in Europe of this time was the Red Army Faction; commonly called ""The Baader-Meinhof Group.""

Their stated goal was to be the vanguard of a violent revolution, dedicated to retaking the German state on behalf of the people, and ushering in an era of pure socialism. Early in the history of the group they were associated with violence; the serious injury of an elderly library shot when co-leader Andreas Baader was broken out of police custody, weapons training in a Palestinian camp, shootouts with police offers.

And while this was going on, the group was pilloried, along with most leftists, by the conservative Springer Press. Founded by ~~Lord~~ Axel Springer, the Springer Press newspapers were, far and away, the most dominant news outlets in the country, surpassing 50% newspaper readership on Sunday alone. In many ways they were the 1970s German equivalent of the Rupert Murdoch's media empire, only with a much, much larger reach.

So in June of 1971 the [Institut für Demoskopie Allensbach](http://www.ifd-allensbach.de/service/english/summary.html), a very well respected market research and public opinion organization, asked Germans their thoughts on the Baader-Meinhof Group.

Remember, this is a group that has injured people with guns, engaged in shootouts with German police, and spoke openly of their coming violent Revolutionary War in West Germany.

The results were remarkable. 20 percent of Germans under 30 expressed ""a certain sympathy with the group,"" and one in ten young Germans said they'd be willing to shelter a group member for the night. If you tally up all the results, 14 percent of Germans said they'd either be willing to shelter a group member or would be willing to consider it. Essentially 8 million people out of a population of 60 million were either willing to materially support a violent group with the stated goal of revolution, or were willing to consider it.

Now one year later, after the group began their ""war"" in earnest, killing four US soldiers, and maiming several dozen civilians, police officers, and other soldiers over a one month period, that public support essentially ceased to exist (the entire leadership was caught over the next few weeks and imprisoned).

So what was the deal? How could so many people support this group that was so clearly embraced deadly violence? Were people truly supportive of violent revolution?

My research shows that no, people weren't necessarily supportive of violent revolution in practice. But many Germans had an extremely strong belief in Socialism. The upheavals in university campuses during the 1960s across the globe were especially notable in Germany. Students were extremely well-versed in Marxist theory. The dominant left-leaning party, the SPD, had socialist revolution a part of their party platform well into the 1950s. My point is that you had a significant part of the population where the nebulous notion of ""revolution"" was a vague end goal. So when this group comes along saying ""we will be the vanguard of this revolution,"" it was easy to express a certain support of their work, despite their occasional violence. Because this portion of the population had been primed to support the vague goal of Revolution, when asked about this group that was taking baby steps trying to fight for that revolution in the real world, it would have perhaps been more a surprise had they NOT expressed support.

So my takeaway, and how I relate this to the Waco situation (though again, this is NOT my area of study), is that at the time of the Waco standoff, it essentially was a completely unknown situation to the general public prior to the initial deadly shootings that killed several ATF agents and several Branch Davidians. Much like in Germany there was a sizable portion of the US population predisposed to opposition to the US government, the ATF, the FBI, and other agencies; particularly in light of the tragic events at Ruby Ridge the year earlier. So on the news comes word of a standoff, between a group of people, several now dead, who claim they just want to be left alone, and government agents telling stories of meth labs (later shown to be false), unlawful weapons, and stories of child sexual abuse... the people inclined to support the Davidians were just as inclined to discount any information provided by the government.

Another interesting thing happened in Germany; after the bombing campaign of May 1972--their public support completely ceased. BUT... in the coming years many of their original supporters ""returned to the fold"" to an extent. They wouldn't publicly support violence or bombings, but their support morphed into an opposition to how the government was treated the imprisoned terrorists (and the government DID treat many of the terrorists truly terribly).

How I relate this Waco is this: during the siege it was likely VERY easy to discount anything the government was saying, if you were so inclined to disbelieve them. But certainly at some point even the most hardened of these folks likely accepted that Koresh was leading a group where he and others had engaged in sexual abuse of children. But because the government had managed to kill so many of the Davidians, and because the siege was handled so poorly, it trumped any concerns they had about sexual abuse. It's not that they were dismissive of it (though likely many did feel the accusations were made up); it's just that everything else felt so much more important to them.

I talked about this a bit more here: https://www.reddit.com/r/AskHistorians/comments/21ka7d/the_baadermeinhof_gang_what_wasis_german_public/

Further Reading:

Aust, Stefan. ""The Baader-Meinhof Complex"" 1988, updated 2007

EDIT: Downgraded Axel Springer from Nobleman status; thanks u/LBo87 !",0
"British and Irish people were perfectly aware of the problems that emigration to India would probably cause. Central statistics were not kept in this period and it's actually quite hard to find reliable stats for precise numbers of people leaving the country in 1850, or even the 1850s as a whole. However, every source I have consulted agrees that the US was, by far, the main point of arrival, with Canada, Australia and New Zealand the next most popular destinations.

Looking for example at Irish emigration, which has been more extensively studied than its British equivalent, it is estimated that between 900,000 and 1m Irish people left the country between 1850 and 1855. Some of those went to Britain, but of those who left Britain and Ireland altogether, 80% went to the US, 10% to Canada, and almost the whole of the balance to either Australia or New Zealand. British figures, seen in figures for emigration from the main UK seaport, Liverpool, show a very similar pattern; in 1851 150,000 people left the country of whom the vast majority headed to the United States and the balance almost entirely to Australia and New Zealand.

Marshall, who covers emigration of British people to India, notes no precise figures are available for the period before 1871. He agrees that the numbers were ""minuscule"" in comparison to those leaving for Anglophone countries, or even South America; the main reason for this, he thinks, was climate rather than opportunity. He agrees that the Indian market was ""an extremely difficult one for Europeans to penetrate"" and one that ""provided very few opportunities for the deployment of European capital and technology before the late nineteenth century.""

Most of those who did go to India were soldiers. About 20,000 men from the regular British army were stationed in India by 1851. About another 40,000 men officered the Indian army maintained by the EIC, or ran the Company's civil service, and they are estimated to have been accompanied by some thousands of women and children. The balance of the ""unofficial"" European community in the first half of the century was only about 2,150 across the whole of India, a tiny but growing number, of whom the great majority were either seamen or independent merchants based in port cities. By 1851, according to the census taken in that year, the total had reached 10,006. Of course we don't know how many of that total were independent working class emigrants of the sort posited by this query, but it seems safe to presume the actual number was essentially negligible. 

Practically all of those who had emigrated to India did so for work and with the intention of eventually returning to Britain. Again, the main reason for this was the climate – this was before the invention of air conditioning or refrigeration, and before the development of a rail network that would at least allow the European population to escape the heat of the hot season by retreating to the hills, as began to happen in the second half of the 19th century – and the associated risk of disease.

With regard to the help available for such people if they got into trouble – the numbers involved were so small there seem to have been no special charities or foundations aimed at relieving such want. However, as you suggest, it certainly would be the case that even working class Brits down on their luck could have turned to sources of help not available to the average Indian. The most obvious, perhaps only, source of charity in such circumstances would have been the church.

**Sources**

PJ Marshall, ""British immigration into India in the Nineteenth Century,"" *Itinerario* 14 (1990)

Arthur Redford, *Labour Migration in England, 1800-1850* (Manchester, 1926)",0
"Aha! A subject I have knowledge of! The difference you are seeing between the ""common depiction"" of a pointy crown and the ""search result"" of a crown with arches stems from a variety of sources. 

And let me start by limiting our scope; I am only going to speak of crowns from the European Christian tradition. There are numerous examples of crowns and ceremonial headgear from cultures outside of the European Christian tradition that will be different and not relevant to my discussions.

To start out with, there needs to be a distinction made between different types of ""crowns"". Type and ornamentation of crowns has historically been limited both by the rank of the wearer, and the occasion of wear. Specifics could vary significantly over place and time, so I'm going to be working with more general design features.

The arched crown (what you called a domed cage) is what is most properly called a ""crown"". Its wear is restricted to currently reigning monarchs (\*with an exception for the Virgin Mary). Probably the best-known example is the British [St. Edward's Crown](https://www.hrp.org.uk/tower-of-london/history-and-stories/the-crown-jewels/#gs.8yf892).  (Papal mitres/tiaras also share an ancestor with domed crowns, but that's a whole other post!)

The ""spiked crown"" and all its variants, are most properly called ""coronets"", literally small-crowns, defined by the lack of domed arches. These could be worn by reigning monarchs, heirs, and varying degrees of nobility.

In the modern European tradition, the arched crown is often worn with a velvet and ermine cap called the ""Cap of Maintenance"". Although it is most often displayed with the crown, the cap is properly a separate piece. 

Studying the history of crowns is complicated not only by the relative scarcity of these objects, but by the fact that a large percentage of extant pieces which have survived from medieval or pre-medieval times have been passed down and used by rulers for centuries, and been reworked and repaired numerous times.

The earliest hard examples of a crown in the European tradition can be traced back to the Byzantine Empire. These consisted of a wide and heavily ornamented ring. [Here is an image of Byzantine Emperor Justinian I.](https://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Meister_von_San_Vitale_in_Ravenna.jpg/800px-Meister_von_San_Vitale_in_Ravenna.jpg) Justinian ruled from 527-565ce. The image is from a mosaic in a church in Ravenna, Italy, and dates from the 6th century, so roughly contemporaneous to his rule. 

An extant example of this style is the [Iron Crown of Lombardy](https://en.wikipedia.org/wiki/Iron_Crown_of_Lombardy#/media/File:Iron_Crown.JPG), which resides in Milan, Italy. The Iron Crown is a ""reliquary"" crown - legend says it was ordered to be forged by St. Helena, mother of Emperor Constantine, and contains a nail of the True Cross.  Already in the European royal tradition, crowns are associated with Christianity and the Divine Right to rule. 

Dating of this piece seems to indicate it was assembled of separate components dating to the 4th-5th century CE. The Iron Crown was used as a coronation crown for the Kings of the Holy Roman Empire, including Charlemagne. It continued in use until the 19th century, with it last crowning Ferdinand I. When not in use for coronations and funerals, it was held safe by church officials.

The first ""arched"" crown seems to have been developed around the 11th-12th century. Two examples survive from this time. The first is [the Holy Crown of Hungary](https://upload.wikimedia.org/wikipedia/commons/2/21/A_Szent_Korona_el%C3%B6lr%C5%91l_2.jpg). The enamelwork plaques are of Byzantine make, and indicate the crown was gifted from the Byzantine emperor Michael VII Doukas to Hungarian King Geza I \~1070CE. The rest of the crown, however, shows signs of reworking. It likely reached its final form in the 12th-13th centuries. It probably began its life looking closer to the Iron Crown of Lombardy. It was used to crown monarchs until the early 20th century. 

The second arched crown of this era is the [Imperial Crown of the Holy Roman Empire](https://upload.wikimedia.org/wikipedia/commons/0/07/Holy_Roman_Empire_Crown_%28Imperial_Treasury%292.jpg). It seems to have suffered from fewer repairs over the centuries. It has only a single arch crossing the dome of the crown, and the solid arched panels are suggestive of Byzantine design influence, though the crown itself was most likely manufactured in Germany. It dates roughly to the late 10th-early 11th century. It was used until the fall of the Holy Roman Empire.

Holy Roman Emperors continued to use a arched crowns throughout the course of the Empire. [Portrait of Frederick III (1468)](https://en.wikipedia.org/wiki/Frederick_III,_Holy_Roman_Emperor#/media/File:Hans_Burgkmair_d._%C3%84._005.jpg) [Imperial Crown of Austria (1602)](https://upload.wikimedia.org/wikipedia/commons/6/68/Imperial_Crown_of_Austria_%28Vienna%29.JPG)

The earliest reference I can find for the use of an arched crown in Britain is a [coin from the reign of Henry VII Tudor](http://publish.illinois.edu/canderson/files/2015/11/Henry_VII_groat.jpg). His son [Henry VIII](http://publish.illinois.edu/canderson/files/2015/11/08henry81.jpg), and granddaughter [Elizabeth I](http://publish.illinois.edu/canderson/files/2015/11/Elizabeth_I_in_coronation_robes.jpg), both wore arched crowns for their coronations. The arched crown is not attested to in England before the Tudor era, and may reflect a deliberate choice to separate their dynasty from that of their predecessors. By the end of Elizabeth's rule, the modern form of the arched crown has appeared. This portrait of [James I](http://publish.illinois.edu/canderson/files/2015/11/800px-James_I_of_England_404446.jpg) may show the first iteration of the St. Edward's Crown. 

The original St. Edward's Crown does not survive; it was stripped of jewels and melted down under Oliver Cromwell. Here is [Charles II](http://publish.illinois.edu/canderson/files/2015/11/Charles_II_of_England_in_Coronation_robes.jpg), wearing a recreated St. Edward's Crown for his coronation. This is the same crown I linked to in the beginning of this reply; over the past 4 centuries it has been reworked and modified for successive rulers many times. It supposedly contains pieces of the pre-Cromwell crown, which may be true.

Outside of Britain, the use of arched crowns seems to have gained popularity in the 16th century. [The Crown of Eric XIV of Sweden](https://upload.wikimedia.org/wikipedia/commons/7/7f/Royal_crown_of_Sweden.jpg) was created in 1561. The [Crown of Louis XV of France](https://upload.wikimedia.org/wikipedia/commons/e/e5/Crowns%2C_Mus%C3%A9e_du_Louvre%2C_April_2011blackened.jpg) was created in 1722. The [Corona tumular](https://upload.wikimedia.org/wikipedia/commons/1/1e/Spanish_Royal_Crown_1crop.jpg) of Spain was commissioned by Carlos III \~1766, but has literally never been worn.

Outside of coronation regalia, arched crowns were confined to use in churches and cathedrals, as votives to adorn statues of the Virgin Mary. The Virgin Mary, as ""Queen of Heaven"" was entitled to wear an arched crown, like any other sitting monarch. This extended even to colonial areas; the [Crown of the Andes](https://upload.wikimedia.org/wikipedia/commons/d/d9/Crown_of_the_Virgin_of_the_Immaculate_Conception%2C_known_as_the_Crown_of_the_Andes_MET_DP365520.jpg) was created for a Mary in Popoyan, Columbia. In the 1800s, Queen Isabella II of Spain donated her [diamond and topaz crown](https://brocantevintage.tumblr.com/image/149497639040) as a votive to the Virgin of Atocha in Madrid. 

I'll make a separate post to discuss some of the history related to coronets, as this is already running quite long.",0
"(1/2)

Broadly speaking, the assumption that American country music used to be significantly more left-wing is incorrect. Commercial American country music has generally been focused around the Nashville establishment that coalesced in the 1940s around music publishers like Acuff/Rose and the outposts established by record companies in the wake of the 1920s/1930s success of the likes of Jimmie Rodgers and the Carter Family. Broadly speaking, this Nashville establishment, and the network of commercial radio stations, live venues (Nashville's Grand Old Opry, most famously), and record stores that serves this music's audience has *always* pandered to its audience, which is predominantly white, relatively rural, and Southern. As such, where that music touches on politics, it almost always has done so based on a mix of the establishment's politics and their understanding of the political and cultural orientations of their audience, which they saw as being focused around the traditions of the South (which of course in the 1940s and 1950s *did* include various Jim Crow laws). To the extent that the Southern audience for country music's values change over time, the politics of the music will also likely change over time.

Of course, the existence of an establishment almost guarantees the existence of rebels against that establishment of some form or another, those discontented with the way things are, whether because they think the Nashville establishment is *too* focused on tradition, or whether they think it's fundamentally ignoring the important part of the tradition, etc. The South as an area of America, obviously, contains a lot of people with a variety of different cultural backgrounds and outlooks, and not everyone there sees themselves in the same way, even if they still identify with the area and thus the musical origins of the area. Finally, of course, there's plenty of fans of country music who aren't Southern, who are fascinated by the way the music sounds, but don't always share the shared cultural background of the Nashville establishment. Broadly speaking, we might call the tradition of musicians strongly influenced by country music - and most often influenced by the country music of the past, rather than what's currently selling - 'Americana'.

Americana is, understandably, less likely to be embraced by the country music establishment, as it often has quite different values both in the music and in the attitude and orientation of the music. Jason Isbell, who you mention as a modern left-wing country artist, is an example of Americana; [assuming the facts on his Wikipedia page are accurate](https://en.wikipedia.org/wiki/Jason_Isbell#Discography), he's been nominated for over a dozen Americana Music awards and has only ever been nominated for one Country Music Association Award; he has #1 country albums (which these days is based on his being categorised as such by Billboard - you may remember some controversy about the classification of Lil Nas X's 'Old Town Road'), but doesn't have hit singles on the country charts (which requires airplay on country music stations). But as the example of Jason Isbell also suggests, Americana often can gain a wider audience than establishment country music; Jason Isbell will get the opportunity to, say, get interviewed on coastal NPR stations in a way more establishment country stars likely wouldn't, because Isbell is interesting to their audiences. 

In terms of the Nashville establishment, one illustration of their conservatism is in their support for Senator George Wallace, the Governor of Alabama who ran on a segregationist third-party platform in the 1968 election, winnnig the states of Alabama, Arkansas, Georgia, Louisiana, and Mississippi. Tammy Wynette sang 'Stand By Your Man' at a George Wallace appearance (and which became an unofficial Wallace anthem) and Hank Snow ('I've Been Everywhere') went on tour with Wallace during his Presidential campaign. 

Wallace, in 1968, was fond of joking about long-haired hippies as they were protesting him, saying things like:

> That’s alright, that’s alright honey – that’s right sweetie-pie – oh, that’s a he. I thought you were a she

and

>  You come up when I get through and I'll autograph your sandals for you. That is, if you got any on . . . You need a good haircut. That's all that's wrong with you. . . There are two four-letter words I bet you folks don't know: 'w-o-r-k' and 's-o-a-p.'

In 1972, after Wallace had been seriously injured after being shot, a crowd of 7,000 gathered at 'Wallace's Woodstock' at the Old Plantation Music Park near Highland City in Florida (basically part of George Jones and Tammy Wynette's property) to hold a benefit for Wallace, which featured performances from Jones and Wynette, Ferlin Husky, Del Reeves, and George Wallace Jr.

Not everyone in country music was a Wallace supporter in 1968; Roy Acuff, one of the biggest figures in the Nashville establishment, was a prominent supporter of Richard Nixon during the 1968 election. Nixon ran ads on Porter Wagoner's television show - one of the most prominent country music shows in the south - during the campaign, warning that a vote for Wallace was a vote for Hubert Humphrey, the (Northern) Democratic candidate.

In essentially this same era, there were several prominent country hits that were strongly pro-Vietnam (which had what you'd describe as 'America fuck yeah' vibes), in marked contrast to all those protesting long-haired hippies Wallace was making fun of: 

* 'Hello Vietnam' by Johnny Wright (a #1 country single in October-November 1965), which featured the lyrics 'America has trouble to be stopped...we must stop Communism in that land'. 

* 'Tell Them What We're Fighting For' by Dave Dudley (which hit #4 on the country charts in mid-January 1966), with lyrics from the perspective of the soldier writing home: why, he asks, is it the case that 'people marching are in our streets?' He exhorts: 'Mama, tell them what we're fighting for'.

* Dave Dudley's 'Viet Nam Blues' (a #12 on the Country chart in April 1966), which narratively is a soldier reacting in disbelief to protestors in Washington DC (who he thinks support the Communists instead).

* Autry Inman's 1968 'The Ballad Of Two Brothers' (a #14 on the country chart, and a #48 on the mainstream chart), which portrays a serving soldier in Vietnam and his draft resister brother - the draft resister changes his mind in favour of war after his brother dies valiantly.

* 1970's 'The Fightin' Side Of Me' by Merle Haggard (a #1 country hit), which attacks those 'harping on about wars', saying 'if you don't love it, leave it: let this song...be a warning'.

As these examples suggest, the country music establishment in the 1960s and 1970s was perfectly happy with intrinsically right-wing sentiments in terms of the big issues of the day.",0
"While this is a good answer I take issue with this part:

> The fact that Baum waited to release this ""smoking gun"" quote is peculiar, but not necessarily unheard of.

Baum said: ”At the time, I was writing a book about the politics of drug war.” Baum's book was titled ""Smoke and Mirrors: The War on Drugs and the Politics of Failure"". Baum told CNN his current reason for not including this story in the book in 1996 was [""Baum...said he left out the Ehrlichman comment from the book because it did not fit the narrative style focused on putting the readers in the middle of the backroom discussions themselves, without input from the author.""](https://www.cnn.com/2016/03/23/politics/john-ehrlichman-richard-nixon-drug-war-blacks-hippie/index.html)

I’m sorry if this comes across as hyperbolic but...is Dan Baum openly admitting to being the worst journalist alive? He must be, because the only logical alternative is he is making up that story. 

If you’re a journalist and a top Nixon advisor tells you the drug war was explicitly designed from the start to be racist while you’re writing a book on the matter, that’s the point you start mentally writing your submission letter for the Pulitzer. That's your career getting made. That’s not far behind finding the lost Watergate tapes or exonerating Gary Webb’s Dark Alliance news story. If it doesn’t fit the narrative style of the book, you change how you are writing the damn book. Not go ""well I don't know how to fit this in. Guess I'll just forget about it for 20 years until [""after Baum remembered them while going back through old notes for the Harper's story.""](https://www.cnn.com/2016/03/23/politics/john-ehrlichman-richard-nixon-drug-war-blacks-hippie/index.html).

The fact that Baum waited 20 years and after Erlichman's death to publish really stretches the bounds of credibility.",0
"In Judges the story is about an Israelite staying the night in a town of benjanites, he is finally taken in along with his concubine. Once they are inside men knock on the door demanding the Israelite to be handed over for sex. They give them the concubine instead, she is raped and beaten to death. Would you consider that to be a historical account or a metaphorical story? The rest of the book lead to a civil war among the tribes where most of the benjamites are wiped out, they surrender and retreat. One of the tribes didn’t show up to the reunion so to repopulate, the tribe they steal the wives of the tribe that didn’t show. This leads into the story of Ruth. If it is a metaphorical story, what is the metaphor?

Edit: Levite instead of Israelite ",0
"Hi! Very interesting and broad question.

I will answer from my area of expertise: medieval literature, and more specifically Scandinavian medieval literature. First, I must emphasis that no text from the medieval Scandinavian literature can be defined as a ""dystopic"" narrative. None of these texts is explicitly about describing the worst possible society, but some of the most important examples of medieval Scandinavian literature are explicitly concerned with politics and the role of the state. As such the authors described and contrasted good and bad examples of government. Your question is specifically about whether bad form of government was characterized as authoritarian. I must also specify that the examples of novel you provided are depiction of totalitarianism, which is not exclusive with authoritarianism but not equivalent either. I will solely speak of authoritarianism and not of totalitarianism.

So, is authoritarianism presented as a political evil? As often, it depends: one of the earliest examples of Medieval Scandinavian literature is the Gesta Danorum (the deeds of the Danes) by Saxo Grammaticus, which was written around 1210. The work is a very lengthy history of Denmark through its heroes and kings, the first half of the work is mainly legendary while the second dwells with more historical matter. In this text Saxo speaks a lot about the institution of monarchy. Most of the time his kings are portrayed in a rather negative ways, one of the negative traits of the bad rulers is tyranny. According to him, the king Harald bluetooth has enslaved people to haul a huge stone as a memorial to his deceased mother. This act of tyranny is presented as one of the major sources of his fall as well as an immoral example of government. It is however not only the authoritarian aspect of his action which is at stake, Harald is also criticized for neglect, prioritizing this non urgent task over his military duties. Excessive authoritarianism is only one of the flaws of bad government, and Saxo is primarily concerned with Christian morality and civil peace. Above all, the good king is the one who listen to the church (Saxo was himself working for Absalom, the archbishop of Lund) and one which preserve peace in his kingdom. One way to preserve peace in the kingdom may be to bring war outside of the kingdom. For instance, Saxo had nothing against authoritarianism used against the Baltic pagan people which often partook in pirate expeditions against Denmark. When describing the Danish expedition against those pagan Balts he portrayed with a lot of admiration how the Danish invaders destroyed the pagan sanctuaries and forced the Baltic people to practice Christianity. To him, authoritarianism is not an intrinsically bad thing, it can be used to attain good means, but it can certainly be abused if performed for self-interest. In that regards authoritarianism is not intrinsically bad if it serves the Church or is used to defend the country from civil war or foreign invaders. It can however be abused if performed for self-interest.

Another, more famous, example of medieval Scandinavian literature on political power is the Heimskringla (often called The History of the Kings of Norway in English) written by the Icelandic chieftain Snorri Sturluson around 1230. In this massive piece of literature Snorri often describes the struggles between local political rulers and the kings, which tried to establish their political rules all over Norway. It is not always easy to know what Snorri’s personal opinion on the matter is. The narrators of the Icelandic sagas are presented as a neutral point of view merely witnessing the events unfolding before his eyes. Of course, despite this seemingly neutral point of view the texts often convey political opinions. As Saxo Snorri is not against every form of authoritarianism, the first king of Norway, Harald Fair-Hair is a generally positive character, and his main achievement is submitting every Norwegian chieftain to his authority, often with violent means. The unification of Norway is presented as an honorable deed, but it must be noted that Snorri, as other Icelandic authors, present the colonization of Iceland as the result of Harald's harsh rules: wealthy farmers, were dissatisfied by their sudden, and mandatory submission to a king, and preferred to settle on a desert island rather than suffer his rule. (I must specify that historical and archeological data show that the tyranny of Harald is not the cause for the settlement of Iceland, but Icelandic authors certainly believed it was.) These Icelandic settlers are hardly portrayed as negative rebels, and Snorri empathizes with their desire for independence and self governance.

On the other hand, Snorri also depicted some of the most praised rulers of Norway as authoritarian kings. For instance, one of the first king to introduce Christianity to Norway, Olaf Tryggvason, would be a fanatical tyrant by our modern standards. Snorri describes him bullying and torturing chieftains unwilling to accept Christianity and his rule (submission to his rule being mostly equivalent to conversion to Christianity). Olaf is even portrayed as trespassing some of the most holy rules of the medieval Scandinavian society such as hospitality, as he used violence against his hosts. The same is for his successors, Saint Olaf, who achieved Norway's conversion and bullied the last rebel chieftains into submission. I would personally argue that according to Snorri, the two Olaf kings were not entirely positive and that some of their actions were meant to be perceived as excessive, but in anyway their reign must be seen as globally positive as they introduced Christianity in the kingdom. Once again, authoritarianism, even excessive, may be an acceptable mean to attain desirable goals.

Yet the authority of the king is not always presented in a positive light, and one of the most famous passage of the Heimskringla is a harsh criticism of the ambition of Norwegian kings to rule over Iceland:

>‘The reason I have had little to say about this business is that no one has called upon me to speak about it. But if I am to give my opinion, then I think that the course for us dwellers in this land is not to submit here to the taxes paid to King Ólafur and all the burdens such as he has imposed on people in Norway. And we shall be causing this deprivation of freedom not only to ourselves, rather both to ourselves and our sons and all our families that inhabit this land, and this bondage will never go away or disappear from this land. So, though this king may be a good man, as I firmly trust that he is, yet it will happen from now on as it has before now, when there is a change of ruler, that they turn out differently, some well, some badly. But if the people of this country wish to keep their freedom, which they have had since this land was settled, then it will be best to grant the king no foothold on it, either in possession of land here or by payment from here of specific taxes which may be interpreted as acknowledgement of allegiance. But this I declare to be quite proper, that people should send the king friendly gifts, those who wish to, hawks or horses, hangings or sails or other such things that are suitable to send. It is making good use of these things, if they are rewarded by friendship. But as for Grímsey, there is this to say, if nothing is transported from there that can be used as food, then a host of men could be maintained there. And if a foreign army is there and they come from there with longships, then I think many a cottager would feel that oppression was at hand.’

Here quoted in the translation of Alison Finlay: Snorri Sturluson, Heimskringla II: Óláfr Haraldsson (The Saint), trans. Alison Finlay and Anthony Faulkes, vol. 2, 3 vols. (London: Viking society for northern research, 2014). pp. 143-144.

When the authority of the king threatens the interest of Iceland, or of the wealthy farmers, it is soon perceived as illegitimate or tyrannical.

This is only a superficial overview of the topic and much more could be said on that matter. To sum it up I could say that medieval Scandinavian authors were not aiming at describing dystopic societies. They however portrayed imperfect examples of rulership. One aspect of what made a ruler imperfect, or frankly bad, was a misuse, or abuse of his authority. The authority of the king was however perceived as a safeguard against political chaos and civil war. Unlike in modern novels, the most dreadful political danger was civil war and political chaos, not tyrannical governments. In that regards, strong kings have been seen a necessary inconvenience against the greater danger of generalized violence.

EDIT: lot of typos and grammar",0
"> How was the Church able to handle it, and even grow exponentially, when this didn't happen?

1 Thessalonians addresses exactly this problem: What do you say to Christians who are disappointed that Jesus hasn't yet returned? In chapter four, Paul addresses the Thessalonians' concerns about “those who sleep in death” \(4.13\). It appears \(based on Paul's reply; we don't have the letter to which he is responding\) that the Thessalonians had been given the impression that Christ's second coming was imminent; when members of the community died before Christ's return, the survivors worried that their fallen brothers and sisters would therefore be excluded from the new kingdom. Paul reassures them that

>we who are still alive, who are left until the coming of the Lord, will certainly not precede those who have fallen asleep. For the Lord himself will come down from heaven, with a loud command, with the voice of the archangel and with the trumpet call of God, and the dead in Christ will rise first. After that, we who are still alive and are left will be caught up together with them in the clouds to meet the Lord in the air. And so we will be with the Lord forever \(4.15\-17\).

In other words, there are contingencies in place for those who succumb to death before the end of the world. There is a plan, Paul reassures them, for an extended wait.

Paul also makes the timeline a little more vague. In the next chapter, he reminds the Thessalonians of a particular aspect of his earlier teaching, telling them that “about times and dates we do not need to write to you, for you know very well that the day of the Lord will come like a thief in the night” \(5.1\-2\). That is, no one knows when the end will come, as like a thief it will come by surprise. This might strike you as somewhat surprising itself, as speculation about the end of the world typically includes all kinds of signs that indicate *when* the end is nigh \(cf. 2 Thessalonians 2, where Paul offers a number of signs \(one reason some scholars doubt its authenticity\)\). Why would Paul preach to the Thessalonians that the exact timing of the end of the world is impossible to predict? Well, if the world does not end when he says it will, he's going to have a lot of disappointed people. Better to be vague. This is sometimes referred to as “delayed eschatology:” The end of the world is DEFINITELY coming, but it's coming at some ill\-defined point in the future. That way, Christianity can have the end of the world as central to its beliefs without it dominating present considerations.

There were many other strategies employed, but these \(& the realized eschatology of Luke\) are the earliest & some of the most influential.

>At what point was this no longer the predominant expectation of new converts?

It's hard to say what the majority of early converts believed, as we simply don't have access to that sort of data. What evidence we do have –\- a handful of 1st century Christian texts –\- doesn't express too much interest in imminent eschatology. As 1 Thessalonians suggests, even the earliest converts were dissuaded from thinking of the end of the world as something that would occur soon. Similarly, in 1 Timothy there's lots of talk about establishing a church hierarchy, rearing children, planning for the future, etc. You wouldn't expect Christians to be doing those sorts of things if they thought the world was going to end next week.

That said, there's never been a point in Christian history when someone wasn't preaching that the world was about to end. Indeed, this sort of message is especially effective at attracting converts. The problem is that imminent eschatology doesn’t lend itself to the creation of the sort of enduring institutions that a religion needs to grow and prosper. There's an excellent modern example of this in Seventh\-Day Adventism \(see Lawson, “The Persistence of Apocalypticism Within a Denominationalizing Sect”\): Converts are drawn in with a promise of imminent destruction, but comparatively little is said about the end of the world among established members of the church. It's difficult to get people to plan and build and foster community if they think God is about to wipe everything away.

Rather than think of imminent eschatology as a Christian belief that was superseded, then, it might be better to think of it as a persistent Christian belief that is specifically targeted at potential converts. Telling people that the world is about to end is a great way to scare the crap out of them and get them to join, but the messaging usually shifts once they're on the inside. Hook 'em with the fire and brimstone, then give them their assignment for next week's bake sale.",0
"So hopefully someone else can add more to this since I know little about the individual in question and don't speak French but I can (hopefully) elaborate a bit on questions 1-3. My primary source for this is ""Vichy France: Old Guard and New Order"" by Robert Paxton. I will tackle your questions a little out of order and I apologize for that but it seems the best way to construct this.  


What's the historical consensus on the Vichy Bureaucracy exactly? Well that could frankly be an entire post of its own, but simply put it is a very hot-button and controversial subject. While it is not the universal opinion I would say that academic consensus has mostly shifted to the views of Robert Paxton which is that the Vichy Government was not merely a collaborationist/puppet government of Nazi Germany but was rather its own fascist state which pursued its own independent agenda (albeit with limited ability to do so). The efforts of the Petain government were not widely condemned internally during the contemporary period: this is to say the Petain government had considerably more support (especially initially) than the Free France movement which in the beginning numbered only about 7,000 troops with the majority of French troops in the UK requesting repatriation to either the occupied territories or the Vichy territories.  


So were Vichy Bureaucrats active supporters or people just doing their job? Well that's unfortunately difficult to make a generalization on, and it will vary case-to-case. The reality in France is that the vast-majority of the population (Vichy or occupied) engaged in some form of collaboration with the Nazis at some point or another. (I'd point to u/mikedash's [answer](https://www.reddit.com/r/AskHistorians/comments/7u5cvz/comment/dtijaps/?utm_source=share&utm_medium=web2x&context=3) on the effectiveness of the resistance for a lil more detail on this) But basically: for many in France active resistance just wasn't worth it, especially early in the occupation. Meanwhile the Vichy government acted with a degree of autonomy: they administered the vast majority of the colonies which had remained loyal to them initially, they were not *officially* a member of the Axis, and were able to pursue their own policies (albeit while still being largely kowtowed to Germany). Their autonomy would increasingly diminish after Operation Torch and Case Anton, but there did nonetheless remain a strong collaborationist strain throughout France prior to Operation Overlord. They formed their own paramilitary death squads in 1943 known as the Milice and prior to the German occupation in Case Anton the Vichy government pursued their own policy of oppression against Jews, Communists, Romani and others. Indeed the degree of relative autonomy afforded to the Vichy government in regards to internal affairs and the manner in which they conducted themselves with the support (or at least complicity) of the majority of the population was and continues to be a point of controversy and shame in French discourse. There was a large attempt to pave over this in the post-war years for the sake of reconciliation but this often had to involve censorship or half-truths to maintain the illusion that support for the fascist movements was vanishingly thin and the resistance wildly popular. One notable example of this would be the Holocaust documentary ""Night and Fog"" which was censored upon release from showing a picture which clearly depicts a French officer guarding a concentration camp as seen [here](https://upload.wikimedia.org/wikipedia/en/0/02/Night_and_fog_Pithiviers_uncensored.jpg).  


So now on to your ancestor. According to the wikipedia page you linked he was General Secretary for the ""Services de l'Équipement National"" (National Equipment Services) which I *believe* is referring to the General Delegation for National Equipment (DGEN) though again, information on this is difficult to come-by especially without speaking French. This would effectively put him in charge of managing French economic and industrial development from February of 1941 to November of 1942. This involved building bridges, hydroelectric dams, and tunnels. This would have benefitted the Axis only tangentially, insofar as the economic developments would contribute towards the Vichy payment for German occupation (yes they had to pay to Germans to occupy Northern France) and in the form of an improved economy and infrastructure once southern France had also been occupied; this is however far from damning. To this end, it seems unlikely that he was involved in much of the decision making in regards to collaboration, racial policy, or other *questionable* government actions in Vichy France. Notably the policies and plans of development led by the DGEN were largely incorporated and continued by the Provisional government and later the fourth republic. Which, to my mind at least, would say that overall what your ancestor was in charge of was at least not outright dastardly.  


Would he have been in the room for these decisions? It's difficult to say as I'm not overly familiar with how exactly the prime ministers and Petain preferred to operate their cabinets, couple this with the fact that the position Giraud occupied was far from the most prestigious or important (as well as being largely irrelevant for many of these decisions) would lead me to believe that he wouldn't be ""In-the-room"" as it were for these decisions, and if he was I doubt he would be consulted for input on it. Absence of evidence is not evidence of absence however, so take that with a grain of salt. I would however say that given his relatively high standing in the government there is very little chance that he was not at least *aware* of the kinds of policies this government was pursuing. For instance he was in office during July of 1942 when the Vichy government deported over 10,000 Jews to Germany to be sent to the death camps. While I doubt he was involved in this decision, I also think it's unlikely that he was not at least partially aware of it. But this gets back to the trickiness of collaboration and knowledge of the Holocaust, the unfortunate reality is that many people had at least *some* knowledge of it and went along with it to some extent, be it out of fear, indifference, or convenience.  


Did he ever publicly express support for this racial program? This is a realm in which I must bow out and say that I truly have no idea. Even for question 3, I had to base my answer off very limited information. He died on the day of Case Anton which means that he died before the worst of it, which makes it even harder to gauge. I genuinely hope someone else who speaks French and has access to some kind of archives or interviews could perhaps elaborate on that point for you, assuming that such sources exist.  


For sources:  
""Vichy France: Old Guard and New Order, 1940-1944"" by Robert Paxton  


[This French Govt Website](https://www.siv.archives-nationales.culture.gouv.fr/siv/rechercheconsultation/consultation/ir/pdfIR.action?irId=FRAN_IR_017032) which confirms which agency your ancestor worked for (Thank you google translate)  


[Another Govt Website](http://www.archives.developpement-durable.gouv.fr/delegation-generale-a-l-equipement-national-dgen-r6883.html) detailing the agency, its role, and its accomplishments  


If anyone else could possibly elaborate on the last question I would really appreciate it as I'm sure it is probably the point you're most concerned with. Hope this helps, please let me know if there's anything else you'd like to know and I'll try my best.",0
"Many enslaved women in the late medieval Mediterranean were forced into work as wet nurses. This is an excerpt from a longer article I wrote on wet nursing in Latin Europe:

For enslaved women forced to serve as wet nurses, the situation was even more demeaning than that of the municipal nurses. Since 1179, canon law had forbidden Christians from hiring Jews and Muslims to nurse their children (although the frequency of reiterations—from Christian and Jewish authorities alike—points to the frequently-informal shared breastfeeding duties among Christian and Jewish women neighbors). And yet from the thirteenth century into the early modern era, slaves in the Christian Mediterranean were almost always purchased or captured Muslims. To Christian parents, the solution was simple: forced baptism. 

And even beyond this indignity, evidence concerning baptized Muslim wet nurses indicates the growing tendency to classify and judge people by skin color. The humour and heat imbalances in darker-skinned women made them bad mothers, according to medical authorities. And since wet nurses were understood as little more than biological appendages of real mothers, over and over, slave purchase and rental contracts demonstrate a keen preference for light-skinned baptizatae of nursing age.

Yes, rental contracts. Enslaved women pressed into service as wet nurses weren’t always just serving the family they at least knew and were comfortable with. Owners might hire out a *baptizata* as a nurse for some extra cash, or sell her altogether if the price was attractive enough.

And if no wet nurse was available? The brutal, systematic, and endemic sexual exploitation of enslaved women in Iberia could be—was—turned to economic use. When the foundling hospital of Perpignan was so financially overwhelmed in 1456 that it appealed to the city for extra money, the rectors made it bleedingly clear that aristocratic men bringing in their own bastard children were to blame. One man in 1400 Barcelona openly admitted sending away his own child so the baby's mother, an enslaved woman, would be free to nurse his legitimate heir.

And therein lay the cruelest blow of medieval wet nursing practice. Medical thought and popular religious teaching alike forbade women from nursing more than one child at the same time. All these wet nurses, all these “mothers who weren’t,” were mothers. They were mothers who weaned their children too early and quickly in order to make some money for the family; they were mothers who lost a child in infancy and had milk but no one suckle; they were mothers sold away from their newborns forever.

*(n.b. I'm super busy today so it will be a little while before I can get to follow-up questions if there are any; my apologies in advance for the delay.)*",0
"There is unfortunately a fairly simple answer to this question, which I wrote some time ago in response to [How did Hugo Boss not go out of business after the Nazis lost WW2?](https://www.reddit.com/r/AskHistorians/comments/811x3n/how_did_hugo_boss_not_go_out_of_business_after/duzwduk/) and will quote below:

I think you've been misled by the listicle-pop-history version of Hugo Boss's involvement with the Nazis. [Please note - the preceding sentence was specifically in reference to the question in the preceding link, which stated that Hugo Boss had *designed* the uniforms. This *is* a listicle-pop-history take, as said listicles typically assume that the Hugo Boss company of 1935 was known for making extremely sharp suits, as today.] To quote from a previous answer of mine ([How was the clothing industry (especially haute couture) affected by WWII both during and afterwards?](https://www.reddit.com/r/AskHistorians/comments/78ho4c/how_was_the_clothing_industry_especially_haute/dow60t2/)):

> Despite these noble intentions, the firms that stayed open and catered to the Nazis and collaborators generally tried to bury the fact that they did so, or rather, they just never talked about it. Hugo Boss had opened a ready-made menswear store in 1923, joined the Nazi Party in 1931, and started making, though not designing, uniforms on government contracts soon after, eventually using forced labor - as did, it has to be said, many other menswear/uniform manufacturers. Unlike the French companies, Boss suffered some legal penalties for his Nazi ties afterward, though he did get them ameliorated eventually. To say that either Dior or Boss ""capitalized greatly"" misses the context that they were not what they are today at the time: Dior was an employee of Lelong, and Hugo Boss-the-company didn't even get into men's suits until after Hugo died in 1948. Yes, they benefited and it's possible that their later success would not have been able to happen if they'd gotten out of the clothing industry for the duration of the war, but it didn't finance some kind of ultra-luxurious lifestyle for them at the time.

Hugo Boss did not actually design the SS uniforms - this is an assumption that's been made based on his firm's connection to Nazi uniforms and the brand's present-day reputation for being really sharp. His factory had been making cheap men's ready-to-wear in the 1920s, and he won contracts to produce uniforms (in part and in whole) that were, after 1940, produced with forced labor, and these contracts saved his business/family from bankruptcy. There was very little to set the firm off from any other German company led by loyal but non-military members of the Nazi Party.

On that more general note, you may be interested in the answer to [Was the fact that companies like Kodak, Hugo Boss, Volkswagen ect were part of the Nazi war effort used against them by their competitors in the post war years?](https://www.reddit.com/r/AskHistorians/comments/3aaro7/was_the_fact_that_companies_like_kodak_hugo_boss/csbjq8t/) written by /u/kieslowskifan. This deals with what happened with German industries associated with the Nazis following WWII.

(End quoted transmission)

~~~

Hugo Boss (1885-1948) was raised by parents who ran a shop that sold linens, shirts, and undergarments; he took control of the shop in 1908, and didn't get into menswear (which was produced in a factory that he owned, rather than purchased from suppliers) until 1923. It has to be understood, because of the context of what ""Hugo Boss"" refers to today, that this was basic clothing for the middle-class man and not high-end tailoring. His early orders included brown shirts for the Nazi Party, but at the time he was also making uniforms on contracts for other parties and for branches of the government; by 1928, though, he'd become an official supplier to the Nazis.

He appears to have become more closely entangled with the Nazis in 1931, after the Depression had hit and caused him to declare bankruptcy. He was able to restart his business while still in debt, and he joined the Nazi Party himself, most likely out of both a belief in Hitler's plans and a desire to use it as a business connection - which worked. He got contracts to produce SS uniforms (though he did not design them), more brown shirts for brownshirts, and Hitler Youth outfits early in the 1930s, and by the end of the decade was producing uniforms for the army as well, although his factory was still on the smaller side and he was far from the only clothing manufacturer supplying the Nazi state. During the war years, in order to fulfill all of these contracts he took on enslaved labor in the form of foreigners (some POWs, some simply transported from occupied countries), who worked under horrific conditions. Following the war, he was tried and condemned as a Nazi activist (though later retried and found to be only a ""follower""), which resulted in his business being taken over by his son and son-in-law, as he was no longer legally able to run it.

It's not until the 1960s that it began to produce the kind of suits it's famous for, and the level of prestige it has now seems to date largely from the tenure of the original Hugo's grandsons, who took over in 1972 and got the brand involved with fine Italian fabrics as well as racing sponsorships. By that point, they were generations out from the man who had joined up with the Nazis and operating on a more global scale, with customers who had no idea of the firm's bad history.

(For more detail on Boss's life, I referred to *Perpetrating the Holocaust: Leaders, Enablers, and Collaborators*, by Paul R. Bartrop and Eve E. Grimm.)",0
"These next charts from ESV rather than ISSP so they exact numbers are different even if the general message is the same. The ISSP gives four options—""Always"" ""Almost Always"" ""Only Sometimes"" and ""Not Wrong at All""—and asks whether homosexuality is *wrong*. The ESV  asks you to put your opinion on a ten point scale with 10 being ""Always *Justified*"" and 1 being ""Never *Justified*"". In some of the research I looked at, the researchers choose to analyze the numbers by giving the mean value from this 10 point scale, but in the tables below, we're just tallying up the percentage of population that gave a 1 or 2 (i.e. strongly saying ""Never Justified""), which I think is probably more useful for what you're asking about. Notice, though, that these sorts of little variations in wording have big changes in values—more than 10 points—even if the same overall pattern remains clear.

##Percent of Respondents Saying Homosexuality ""Never Justified""

Country/Group| 1981| 1990| 1999
:--|:--|:--|:--
Republic of Ireland/All|62|56|38
Republic of Ireland/Catholic|.|.|42
Republic of Ireland/Protestant|.|.|52
Republic of Ireland/Other|.|.|27
Northern Ireland|69|66|45
Northern Ireland/Catholic|.|.|43
Northern Ireland/Protestant|.|.|57
Northern Ireland/Other|.|.|40

 * [Source](https://books.google.com.tr/books?hl=en&lr=&id=AVoxwn9fdOYC&oi=fnd&pg=PP11&dq=%22european+values+study%22+homosexuality+northern+ireland&ots=GWQU6GjfKb&sig=F2Zdm9r7PV653olPQXlb4Rs5lwQ&redir_esc=y#v=onepage&q=homosexuality&f=false)

Here's the thing though: I don't know from *the Derry Girls*, so I can't get into exact demographics. But I think the most important thing I want you to get from this section is that we see *big* variations between demographic groups. I believe they're Catholics? So Catholics in Northern Ireland are more tolerant than Protestants on this issue. And I don't think they're mass every Sunday Catholics, and also non-Church going Catholics are more accepting of homosexuality than Church-going Catholics. You also see huge gaps by age. Let's look at more data, again, from the 1999 survey, for Northern Ireland only, again for homosexuality being ""never justified"", this time by education and age:

#Homosexuality ""Never Justified"" in NI by age and education

.|Unfinished Secondary School|At least Some Tertiary [post-high school]
:--|:--|:--
Under 45|42|23
Over 45|65|39

(Remember: NI average for 1999 was 45%. Age and education had even larger effects in the Republic.)

I don't have the exact numbers in front of me, but I saw in one of these sets of data influenced heavily by gender. In some countries (mainly in Eastern Europe), we see no difference of opinion between men and women on acceptance of same sex relationships, but in Northern Ireland, there are clear gender differences. I didn't see this issue addressed directly in any of this data, and I can't say for sure that this is true for Northern Ireland in this period, but I've read elsewhere that often opinions (of men in particular) are more positive towards lesbian woman than gay men.

Which is to say, Northern Ireland in the late 90's was one of the most conservative countries in Western Europe in its views about sexuality in general, including toward homosexuality. However, these opinions were changing fast and opinions about homosexuality varied greatly among different parts of society in Northern Ireland. Opinions from non-Church-going young, university educated Catholics about lesbians are starkly different from Church-going older, high school drop-out Protestants about gay men. These values were also in flux throughout this period, probably particularly as we get towards the end, meaning it's very hard to pin down what it was like ""in Northern Ireland"" as a whole.

For more recent numbers, you can see 2014 (with some data going back to 2001) report *[Public attitudes towards LGB equality](https://www.ark.ac.uk/ARK/sites/default/files/2018-07/update106_1.pdf)* [in Northern Ireland] and 2014 *[Queering the Family: Attitudes towards Lesbian and Gay Relationships and Families in Northern Ireland](https://www.ark.ac.uk/ARK/sites/default/files/2018-08/Update89.pdf)* from the same research group. I think the most telling graph is the Figure 1 in the second pdf: ""Figure 1: Percentage of respondents who believe that sexual relations between two adults of the same sex is ‘always wrong’"". You can just see the percentage of people who say same-sex relationships are ""always wrong"" start at 75-80% in 89/90 plummet to less than 30% in 2012. The period 1994-2004—the period you're asking about, essentially—is where we see the biggest change. Starting in 1994 from 70+% of respondents in Northern Ireland thinking same-sex relationships are wrong, we're suddenly down to around 45% thinking the same in 2004. This precipitous drop certainly happened unevenly, as mentioned above, so what exact community *within* Northern Ireland we're talking about probably matters a great deal. The more educated, Catholic, non-church-going, and female, the less likely they are to have strong negative opinions about same sex attraction.

But in the end, because of all these variations by demographic, and the rapid sea change of opinions, and how little I know about the show, it's honestly very hard to pin down, precisely. Northern Ireland was among the conservative places in Western Europe sexually, but it was rapidly becoming a more typically Western European in its reactions to sex, and there are plenty of people in Northern Ireland by the late 90's who wouldn't have made a big deal of same sex relationships.",0
"(2 / 3)

It's probably no coincidence that the percentage of New Yorkers who were Dutch decreased significantly during this period. Van der Sijs claims that, by 1703, Dutch New Yorkers made up less than 50% of the colony's population. He doesn't cite a source and I'm too lazy to fact-check it at the moment, but it's probably based on an analysis of the surviving New York census of that year. Van der Sijs claims that this percentage had fallen to about a third of the New York population on the eve of the American Revolution. After the war, the percentage fell dramatically, and rapidly. In the article ""The Ethnic Origins of the American People, 1790"" by Forrest and Ellen Shapiro McDonald, published in *The William and Mary Quarterly* in 1980, the authors analyzed the 1790 U.S. Census to claim that about 17.5% of New Yorkers were ethnically Dutch. The New Jersey returns in that year's census no longer exist, so they don't give a percentage for that state, but it would have been something less than New York, but probably somewhere over 10%. Third on the list was Pennsylvania, which had a Dutch (non-German) population of 1.8%. No other state even crossed the 1% threshold.

But again, throughout this period, the Dutch community remained widely bilingual. One piece of evidence comes from the 1808-09 book *Memoirs of an American Lady* by Anne Grant. Grant had been born in Scotland in 1755, but from the ages of three to fifteen, she had lived in New York state (mostly in Albany) while her father, a military officer, was stationed there. While there, she was taught by a private tutor, Margarita Schuyler, who she called ""Aunt Schuyler"". Grant recounts that she learned to speak Dutch from other children her age in Albany, but that English was also universally spoken, and this had been the case even in Aunt Schuyler's youth before 1710, and even in the more remote New York countryside. [She writes](https://babel.hathitrust.org/cgi/pt?id=nyp.33433081794087&view=1up&seq=51&q1=dutch):

> ""It was observed at that time [i.e., 1709] very difficult to procure the means of instruction in those inland districts; female education of consequence was conducted on a very limited scale; girls learnt needle work...from their mothers and aunts; they were taught too at that period to read, in Dutch, the bible and a few Calvinist tracts of the devotional kind. But in the infancy of the settlement few girls read English; when they did, they were thought accomplished; they generally spoke it, however imperfectly, and few were taught writing.""

But anyway, by the mid-1700s, the Dutch community was no longer a reliable, relatively single-minded political coalition, as they had been under the Leislerian faction. There was nothing like the Bloc Québécois in modern day Canada, or the Scottish National Party in the modern day United Kingdom. As discussed in my earlier answer on Martin Van Buren, by the 1750s, the New York Dutch community had become embroiled in a religious schism, over whether the New York Dutch church should Americanize, Anglicize, and cut ties with the mother church in Amsterdam, or whether they should remain affiliated with the Dutch language and organization in the Netherlands. The two competing church factions that arose during this time were known as the pro-English language *Coetus* and the pro-Dutch language *Conferentie*. The *Coetus* believed the church would die off if it didn't anglicize, by alienating the youth. The *Conferentie* believed anglicization would kill the church faster, by alienating the broader community and not having a cultural reason to remain parishioners. The issue became more concering throughout the 1760s up to the Revolution. Van der Sijs writes:

> ""In 1763, the council wrote to the Amsterdam mother church to report that hardly anyone understood Dutch anymore, and to ask for permission to appoint a dominie who could also preach in English. That permission was granted in 1764 and, although some of the members walked out, the tide could not be stemmed...""

Van der Sijs goes on to write that in that same year, 1763, a New York Dutch clergyman wrote a theological publication entirely in the English language - the first time that had happened, signifying what was to come. In fact, after that, there was no such publicly-facing Dutch language publications from the New York church, though internal documents continued to be written in Dutch.

The *Coetus* and *Conferentie* factions largely split along the wider political landscape as well. While there would certainly be some crossover, in general, the *Coetus* identified with the Livingston party, and then with the Patriots during the American Revolution, while the *Conferentie* identified with the Delancey party, and then with the Loyalists during the Revolution.

Since the pro-Dutch language faction was largely the Loyalist faction, who lost the war, this had a significant effect on the community post-Revolution. Many former *Conferentie* left for Canada as Loyalists, settling in new, more mixed communities where the Dutch language was almost immediately lost. In the remaining Dutch communities in New York and New Jersey, their homes were often taken up by new, monolingual English neighbors. Moreover, the groundswell of patriotism toward the new United States meant an emphasis on the common language — English — of the new country, which further endangered the Dutch language and culture. In 1785, the last schoolteacher at a Dutch church in New York City who taught exclusively in the Dutch language resigned; the city's churches never hired a Dutch-only teacher after that. In 1792, the Dutch Reformed Church in America broke ties with the parent church in the Netherlands, becoming independent, and largely emphasizing bilingual sermons. According to Jaap Jacobs' book *The Colony of New Netherland: A Dutch Settlement in Seventeenth-Century America*, the last Dutch language sermon at any of New York Dutch Reformed churches (in Tappan, NY, by Rev. Nicholas Lansing) was given in 1835 (though Charles Gehring says in his article ""The Survival of the Dutch Language in New York and New Jersey"" the last verifiable Dutch sermon was given in 1833).
 
An 1826 edition of the *Magazine of the Reformed Dutch Church* included [an article](https://books.google.com/books?id=tvu1F0jxTXsC&pg=PA31&dq=language) (likely a fictional parable) about the transition to the Dutch language in the church. It quoted a Dutch farmer who had been resistant to the anglicization of the community in the post-war period, until he eventually succumbed, reasoning:

> ""We are all Americans – happy Americans. The language of the majority must prevail. Our sons must learn the English, if ever they would aspire after honors and important stations in life. The venerable language of my fathers, and my country—music indeed to my ears,—is fast fading away! Our children cannot now use it in their associations in life. They do not understand the language of their fathers and mothers. Religious instructions, in that language, can no longer reach their minds intelligibly. I submit. I sacrifice my own wishes and pleasures for the general good.""

There was also the perception that it was something of a ""country bumpkin"" language. Jokes with a stupid Dutchman as the butt of the joke were common in 18th and early 19th century New York; the idiom ""in Dutch"" meaning ""in trouble"" or ""disgraced"" developed out of this. In the introduction to *A Jersey Dutch Vocabulary*, author James Storms recollects of his rural, mid-19th century upbringing by parents whose first language was Jersey Dutch:

> ""As a small boy at home I never attempted to reply in Jersey Dutch, as my parents frowned upon it and wanted me to speak English, and might look upon my use of their patois as an act of defiance, or an attempt to ridicule them, yet I understood everything they said in it...""

In addition to the affiliation of the pro-Dutch language *Conferentie* largely to the Loyalist cause, and the rustic perception of the language by Americans, another significant factor, as Van der Sijs points out, is what happened to New York City itself during the American Revolution. The city was occupied by the British military for more than seven years, from September 1776 until Evacuation Day, November 25, 1783. While many Loyalists had elected to stay, many more of the city's residents had fled, and lived seven years in exile. When the war was over, the people who returned weren't the same as those who left. Many never came back, particularly if they hadn't been property owners. And even among property owners, many only came back to arrange the sale of their property and then left again. They had set up a new life, often bought new property in the Hudson River Valley, or New Jersey, or even in Pennsylvania, and no longer cared to return to the city life, in a city that had to be mostly rebuilt from the ground up. (The Great Fire of 1776, and the British military's misuse of the city's buildings meant that much of the city's structures were unusable.)

In the first decades after the war, there are several lamentations of old New Yorkers reminiscing about the city of their youth, and how different it became after the war. And one of those big differences was its relative lack of a Dutch community. For instance, in his 1809 book *Knickerbocker's History of New York*, Washington Irving gives a little aside about the lack of Dutch-ness of New York City in those post-war years:

> ""The good old Dutch festivals, those periodical demonstrations of an overflowing heart and a thankful spirit, which are falling into sad disuse among my fellow citizens...""",0
"**Part 2**

As for **question number 2**: I have gone into the history of Holocaust denial in [this answer](https://www.reddit.com/r/AskHistorians/comments/68uu11/how_prevalent_is_holocaust_denial_and_how_does_it/) and first efforts definitely appeared directly after the war. One effort was closely tied to the West German governement's effort [to push the Clean Wehrmacht myth](https://www.reddit.com/r/AskHistorians/comments/5799li/did_the_rommel_myth_and_clean_wehrmacht_myth_and/d8suoyk/) and re-integrate former Nazis into German society as well as the efforts of those Nazis that minimized numbers and responsibility. But there were others too.

Another political agenda that used Holocaust denialism as its tool right after the war, was a certain strand of proto-fascist and right-wing extremist thinkers who wanted to clean fascism and their ideology from the strain of being associated with Hitler and the Holocaust. Douglas Reed is such an example. Reed, who was a prominent journalist in Great Britain, was against Hitler but not against Nationalsocialism (he favored the Otto Strasser position). In the late 40s, early 50s he started publishing books which claimed Hitler had been a Zionist agent and his policy of killing the Jews was a Jewish plot to justify the creation of Israel and which was done against the wishes of many Nazis. At some point it became increasingly hard for him to find publishers, so he moved to South Africa and became involved in supporting apartheid politics in SA and Rhodesia.

Another -- and rather odd -- strand of denialism comes from a pacifists. Pacifism had been very popular during the time between the World Wars because of the effects of WWI and after World War Two, a couple of people of the radical pacifist movement saw their positions threatened because the crimes of the Nazis were a major reason why the war against Nazi Germany was portrayed as a moral and necessary war. In the United States, a former mainstream historian and pacifist activist, Harry Elmer Barnes, started publishing literature that claimed the Holocaust was an Allied invention to justify their war against German, which they had started in 1939.

All in all though, Holocaust Denial as we know it today, meaning the total denial of all that occurred while presenting itself as ""scientific"" work is a phenomenon of the 70s since even at the IMT people like Göring didn't outright deny what had occured but rather were keen on deflecting responsibility of it to others not present with Hitler, Himmler, and Eichmann being favorites.

As for **question number 3**: The Dachau reprisals are the only known such reprisal killing by American troops after the war.

When liberation had become inevitable, several guards in Dachau tried to disguise themselves as prisoners in order to escape being arrested. According to several accounts several prisoners of the camp took offense to that and started basically beating them to death under the eyes of the American troops. Similarly, the US troops in Dachau killed a number of former guards by executing them on the spot.

After finding 29 box cars full with about 2000 skeletal corpses, an unknown number, estimates range from 35-50, of guards was summarily executed by the American troops at Dachau. The US Army investigated the incidents and briefly considered to put the responsible members of the Army before a court martial but then gave up on the idea considering a proper defense would have included finding box cars full of dead skeletal corpses with according to several testimonies ""brain matter scattered around"" and the information that basically the people in these box cars had been stuffed in there by these guards and they in effect let them starve.

As for the impact liberation had on soldiers and commanders, Susan L. Carruthers' recent book *The Good Occupation* offers insight into the thinking of regular American soldiers and officers when confronted with the liberation and we have a similar wealth of accounts from the British liberation of Buchenwald. The overarching tenor is the horror and helplessness they experienced, especially in Buchenwald where in the weeks following liberation several thousand prisoners perished due to the effects of starvation and disease until the British managed to get the situation under control. I write more about Buchenwald [here](https://www.reddit.com/r/AskHistorians/comments/6h4srl/during_the_liberation_of_concentration_camps_at/) but as far as long term studies of the impact go, I am not aware of any (that isn't to say that there aren't, just that I am not well-versed in psychological literature in the field).

Tying also into **question 4**, these resources, especially Carruthers' book do deal with the impact on Jewish soldiers and the sources they left behind. You can find a lot of those [on the USHMM website where survivors and liberators detail their stories of liberation in form of written accounts and interviews](https://www.ushmm.org/wlc/en/gallery.php?ModuleId=10005131&MediaType=oi).


As for **question number 5**: I believe that what I have shown above shows how much care and effort the various Allied governments put into the documentation of the Holocaust: Press reports, systematic collection of evidence, trials, all these went a long way to spread this knowledge and to build a basis for the vast amoutns of scholarship we have today. In this sense, the emergence of Holocaust denial is certainly not tied into the lack of documentation of these crimes. It wasn't then and it isn't know.

Rather Holocaust Denial originates with with a clear ideologically driven agenda that bends and ignores the truth in service of fascist and Nazist ideology, especially as the emergence of its modern form in the 60s and 70s shows and its close ties to neo-fascism and neo-nazism shows. Holocaust Denial was inevitable in as far as these ideologies continued to exist and were on occasion even used for political purposes in the Cold War. But that they experienced such a resurgence in the 90s and even today has to do with other factors rather than the lack of documentation since there isn't a lack of documentation and there never was really.

**Edit**: I was writing this answer before you edited your question text, so here's some more answer on what you were asking:

The initial concern of Eisenhower was, as far as we can reconstruct it, not so much prompted by any outright denial he witnessed on part of the Americans and British publics – the Germans were another matter – but by the lack of care for the issue during the war. He had been privy to a lot of the info that I mentioned above that showed that the Allied leadership knew about the Holocaust but did not really take it in consideration during war time. Additionally, it is important to understand the context behind it: Eisenhower had just witnessed and fought a war against a regime that was masterful at bending and distorting the truth and was hyper-aware that this occurred only 20 years after the last war against Germany. While the Allies took every step to ensure to document these crimes, nobody was certain about the future of fascism as an ideology. Would it resurface? Would it make a huge comeback in post-war Europe or elsewhere? Hence, the documentation of these crimes was a large emphasis for him and others as a means to prevent this ideology from ever becoming popular again.

As for his presidency, Eisenhower oversaw the last stretch of the Displaced Persons System in Germany and repatriation and emigration of the last Holocaust survivors. According to his daughter, he also kept some photographs of the liberation of camps where he was present at his bookshelve but in terms of actual policy, it needs to be acknowledged how  context had changed. Eisenhower was a Cold War president and the Soviet threat loomed large on the mind of the American public with the atrocities of the last war and their remembrance taking a back seat to this new threat. Pam Parry in her book *Eisenhower: The Public Relations President* however makes the point that some of the most remembered aspects of the Eisenhower presidency, including his condemnation of the military-industrial complex were in parts shaped by a staunch sense of morality influenced by the war and atrocities he had witnessed.

And Eisenhower certainly testified, not in front of congress but in public through press releases, his orders, and his speeches in 1945 which directly tackled what he had personally seen head-on. In a press conference in 1945 he said:

> When I found the first camp like that I think I never was so angry in my life. The bestiality displayed there was not merely piled up bodies of people that had starved to death, but to follow out the road and see where they tried to evacuate them so they could still work, you could see where they sprawled on the road. You could go to their burial pits and see horrors that really I wouldn't even want to begin to describe. I think people ought to know about such things. It explains something of my attitude toward the German war criminal. I believe he must be punished, and I will hold out for that forever.

and the [Eisenhower library keeps a further list of public utterances and reports of Eisenhower on the Holocaust](https://www.eisenhower.archives.gov/research/online_documents/holocaust.html).

Sources:

* Dan Stone: The Liberation of the Camps.

* Shephard, Ben. 'The Long Road Home: The Aftermath of the Second World War.' (Bodley Head, 2010).

* Stefan Hördler: Ordnung und Inferno. Das KZ-System im letzten Kriegsjahr. Göttingen: Wallstein, 2015.

* Nichaolas Wachsmann: KL. A History of the Concentration Camps.

* Deborah Lipstadt: Denying the Holocaust.

* Richard Evans: Lying about Hitler.

* Carruthers: The Good Occupation.

* Ian Kersahw: The End.",0
"After the Taliban banned many forms of entertainment and recreational activities in Afghanistan, people had to find alternative ways to have fun and pass the time. Here are some activities that became popular during this time:

1. Storytelling: Afghan culture has a rich tradition of oral storytelling. People would gather in homes or community spaces to hear stories and legends passed down through generations. This was not only a form of entertainment but also a way to preserve their history and culture. 2.",1
"I can actually answer this one.  Like most questions, the answer depends on the context. Did people hack and slash like madmen? Yes. Did people fight with skill? Also yes. I point first to the manuscript commonly known as Pseudo - peter von Danzig. This contains an anonymous explanation of Johannes Liechtenauer's poem of the use of the long sword. It describes in great detail the skilled use of the weapon, primarily against other masters. It does describe at points the tactics of ""those who run in"" and ""Buffaloes"" who are *not* skilled with the sword. Swords have a principal of leverage to them and while strength absolutely does matter, skill tends to get you farther.  In Fiore di Liberi's manuscript (the Getty, If I recall correctly) he similarly describes the ""peasant strike"" as an over committed downward strike.

But these are weapons made to kill and they are effective at it. The manuscripts both describe ""plays"" in which an attack will lead to a counter, which in turn leads to a counter and so on. The longest one I can think of is the Zwerchhau play that ends in one death and another wound. It is approximately five distinct moves Long and takes seconds to execute. Most sword fights started and ended quickly. No fighting manuscript from medieval Europe seems to indicate that such affairs would be lengthy, and often emphasize quick bursts of strength to avoid drawn out conflict, Liechtenauer's students say frequently to ""come before him quickly and strike to the head or breast so he may not come to blows""  or to ""drive at him thus with all your strength."" 

Ms. 3227a in particular spells out a brutal message:

>You should execute begin, medium and end in one rush without break and without being stifled by your adversary and under no circumstances let him gain the opportunity to strike. 

And

> fence upwards from the left side with the whole body and with all strength, to the head or to the body wherever he may hit. And never strike to the sword but just work like would not have a weapon or if you don´t see it, and should not avoid Zeckrühr or taps, and permanently be in motion, work and contact, so the opponent may not come to strikes.

The plays described are complex thereafter. Describing quite verbosely how one is to execute each technique.  

> Gloss: Mark, this is called the twofold Failer, drive it thus: when you come to him with the pre-fencing, then stand with your left foot before and hold your sword on your right shoulder, and when he is even to you, then spring well against him with your right foot on his left side, and do as if you would hew him with a free Thwart-strike to the left side of his head, but pull the hew before it hits, and spring with your left foot on his right side, and strike there to his head. If he parries and you hit his sword, then spring over to the same side near him, and slice him in his mouth with the short edge, behind his sword with the Doubling. Or fall in with your sword over both arms with the slice.

You can read the manuscripts mentioned here over on Wiktenauer, a collection of treatises for western martial arts. Here is the link to Danzig.
http://wiktenauer.com/wiki/Pseudo-Peter_von_Danzig


We can deduce from these descriptions that, like fighting today, it is done with varying levels of skill.

If you are further interested, please join us on /r/HEMAscholar, a collection of papers and articles on the topic or /r/wma, the less stringent community devoted to the revival of these arts for many reasons, not all of which are devoted to their historical context. I would warn scholars reading this that the community is prone to conjecture, and while there are many historians and anthropologists involved, they are outnumbered by less academic enthusiasts.",0
"Ok, so as far as I can tell the vote was anonymous, thus the problem is that we might never know who did not vote for Hitler in that meeting.

When reviewing the fragmentary early membership lists of the NSDAP, there are a few possibilities who could have voted against Hitler but the most likely is Anton Drexler.

Drexler was one of the  founders of the NSDAP (then named Deutsche Arbeiter Partei, German Workers' Party, DAP) in 1919 and Hitler's main rival for party chairman in 1921. Drexler who had been the second chair of the party had taken over first chair in 1920 when his predecessor Karl Harrer resigned over conflicts with Hitler. A similar fate awaited Drexler, who was unable to mount any serious resistance to Hitler. Hitler had parctically become indispensable for the party by 1921 because of his success as an orator and he guarded that role as well as what he understood as ideological purity of the movement jealously.

In 1920 and 21 the NSDAP was plagued by a shortage of money and by being one among the many völkisch movements in Germany. To alleviate both of these problems, they attempted to merge with other, similar, parties such as the DSP and the Deutsche Werksgemeinschaft. Hitler opposed this because it threatened his supremacy in the party, so he used his own role as leverage. Successfully crashing the DSP negotiations, he attempted the same in connection with the DWG negations but failed and so, in a fit of rage, he threatened to resign several times in 1920 and 1921. Every time, fearing the loss of their popular orator and really, the only thing that set them apart form the dime a dozen völisch parties in Germany, the NSDAP leadership placated Hitler by giving him what he wanted.

Again sent to to convince Hitler not to resign, Drexler asked him in July 1921 what he wanted. Hitler demanded chairmanship and extraordinary powers within the party. While Drexler and Hitler showed their unity on July 1926, Drexler as the current chairman was obviously the choice for those who opposed Hitler within the party and there seem to have been at least some because according to Ian Kershaw, 3000 anonymous pamphlets attacking Hitler had been prepared by the time, the members' meeting rolled around.

So, it might have been Drexler who voted against Hitler, possibly expecting a victory of himself, which obviously didn't pan out. Drexler left the NSDAP when it was dissolved after the Hitler Putsch and only rejoined in 1933 never again to gain any kind of political role until his death in 1942.

Other possibilities include Gottfired Feder, prominent member of the anti-Capitalist movement within the NSDAP, who later served in minor roles in the Nazi adminsitration; the first chairman, Harrer, though it is unknown if he was even there (he died of natural causes in 1926); or one of the lesser known early members about whom little information can be found such as Hans Baumann or Karl Beggel; or it could have been Hitler himself following convention in some parties of abstaining to vote for himself though given his personality that seems rather unlikely. In the end, it probably is not possible to find out with any kind of certainty.

Edit: Apparently, Robert Payne claims that the dissenting vote came from a man called Rudolf Posch, a party member and librarian (who is not to be confused with the Austrian Social Democrat of the same name). I have no idea where this claim comes from. Neither Payne nor Joseph Howard Tyson who also mentions this in his book on Dietrich Eckhart have that fact footnoted, though it is likely that one of them got that fact from the other. I could go and look up his Nazi party file though according to the information in the search engine of the German Bundesarchiv it is rather thin and it seems that man never made any sort of impact on the history of the NSDAP again and what became of him is unknown.

2nd edit: for more on the conflict between Hitler and others in the party see /u/boffcheese comment [here](https://www.reddit.com/r/AskHistorians/comments/5bs2hk/hitler_won_control_of_the_nazi_party_with_an/d9r1se1/)

Sources:

* Ian Kershaw: Hitler.

* Wolfgang Horn: Der Marsch zur Machtergreifung. Die NSDAP bis 1933.

* Mathias Rösch: Die Münchner NSDAP 1925–1933. Eine Untersuchung zur inneren Struktur der NSDAP in der Weimarer Republik.

* Ludolf Herbst: Hitlers Charisma: Die Erfindung eines deutschen Messias. ",0
"Jacopo da Acqui's report of Marco Polo's last testament should be put in its proper context. His family members were trying to get him to repent and disown all the lies he'd written, to which Polo sneered that he had not told even *half* of what he'd seen.

Assuming the anecdote is true (or even if it's an invention by Jacopo, defending his subject, which seems just as likely), there are two ways to take it. First, if Polo told the other half, it would be realistic enough to make the whole story seem plausible. Second, the stories in the other half were *even wilder*.

Fortunately, we have ways to investigate both possibilities. Boring one first.

**Scribes Gonna Scribe**

The manuscript tradition of the Travels is a mess. It's maybe not as bad as Piers Plowman, but it's a mess. We don't have the original. There's no single surviving manuscript from which all others derive. Which is to say, there is at least something missing from/added to every existing version.

Some changes are surely accidental, or the result of translation problems. Others, though, are substantial omissions/additions. In those cases, it seems likely that somewhere along the way, a scribe/translator either thought the text needed that story, or saw that story and thought it did not belong for whatever reason.

One good example is the large void in the overall narrative, which occurs when Polo is basically hanging out in the East. Some manuscripts don't really say anything. Others explain that he was specially chosen as governor of a city for three years. Oh, and that he, his father, and his uncle pretty much single-handedly won a siege for the Khan by reinventing the trebuchet.

A passage like this one, especially since it was almost certainly added to some rather than omitted from the rest, suggests a couple of possibilities for reconciling skepticism/maintaining the book's veneer of ""plausibility,"" fully aware it was just a veneer and part of the genre.

First, it fills in a large temporal gap at least in part. Useful in and of itself. Second, it casts the Europeans in a *really* good light. It's usually thought that even when pointing out good qualities of non-Christians, medieval European travel narratives relate tales and descriptions in ways that emphasize their Otherness. Massaging the awesomeness of the Polos serves those ends quite well--especially useful in a post-fall of Acre world.

A lot of additions seem to point to scribes feeling that the version they had in front of them was just missing a few details. Like a description for how horse thieves or bar brawlers were punished will have the procedure for punishing murderers added to it in later recensions. If the scribes thought the story needed it, Polo's friends and family could well have thought the same.

But really what you're here for is shipwrecks and cannibals, right?

**Diamond Poop, or, 1001 Mediterranean Nights**

In [this earlier answer](https://www.reddit.com/r/AskHistorians/comments/5570ic/in_one_story_of_sindbad_the_sailor_and_in_marco/d88riob/), I discuss how one episode that Marco Polo recounts traces back in time to amazing 12th century Persian poet Nizami, to a 10th century natural history text in Arabic, to a 4th century Christian bishop, to...Herodotus. From Polo:

> Among these mountains there are certain great and deep valleys, to the bottom of which there is no access. Wherefore the men who go in search of the diamonds take with them pieces of flesh, as lean as they can get, and these they cast into the bottom of a valley...When the eagles [who also eat the deadliest snakes known to the ENTIRE WORLD] see the meat thrown down they pounce upon it and carry it up to some rocky hill-top...

> The people go to the nests of those white eagles, of which there are many, and in their droppings they find plenty of diamonds which the birds have swallowed in devouring the meat that was cast into the valleys.

The details of the story change (Polo's diamonds are Herodotus's cinnamon sticks), but the underlying ""plot"" is the same.

This matters for present purposes because it shows how stories like this one are circulating around the Mediterranean-Asian world, crossing geographic and linguistic barriers as though they didn't exist. Ibn Battuta, Marco Polo, any interested traveler might well have heard the same basic story, with different details (snakes? no snakes? Alexander the Great? Random peasants?) from multiple sources. Heck, the valley of the diamonds story will even eventually be recorded in *1001 Nights* (the messiest manuscript tradition yet), although the MS is more recent than Polo but the story was probably part of the collection beforehand.

The Mediterranean-Asian world was a world of stories. So in addition to whatever Polo did/did not see with his *eyes* firsthand, there's what he ""saw"" in the sense of having heard or overheard. And thus, the question becomes: what stories or what kinds of stories might we expect, that Polo nevertheless leaves out?

Looking at a 10th century Arabic text known as the *Marvels of India*, I'm going to suggest that what he left out includes shipwrecks and cannibals.

*Marvels of India*, like Polo and Ibn Battuta, is a collection of anecdotes, although it doesn't really attempt to be a cohesive travel narrative of any sort. The reason I think it's particularly useful for present purposes is that taken as a whole, the book is *really, really repetitive.* How many stories do you need that emphasize THIS FISH IS REALLY BIG?  (At least four in a row, at one point, to say nothing of elsewhere in the text). It's a mishmash of tales, of the ""1001 Mediterranean Nights,"" just like Marco Polo. (And yes, it includes the 'valley of the diamonds' legend.)

And some of the most common themes suicide, monkeys, snakes, REALLY BIG FISH...shipwrecks and cannibals.

Mind you, cannibals and shipwrecks both appear in Marco Polo's books. But not like this. The cannibals of *Marvels* sometimes have tails. Sometimes the book's author goes into *way* too much detail about how the cannibals cook their meals. And over and over, we read the trope--that survives to day--of the shipwrecked sailors on the island of the cannibal king.

In a world of travelers, merchants, and educated people across cultures (or the product of one author with a really thorough education), Marco Polo would have heard a lot of stories from a lot of people. One way or another, some of them wound up in his *Travels.* And one way or another, Jacopo da Acqui had his character Polo defend the ""truth"" in his own voice--whether that truth was what Polo saw--or what he had heard from someone who had heard it from someone.",0
I've seen an image floating around of Nazi soldiers being made to watch pictures or footage from concentration camps to make them face what they were involved in. Is that true? Did that have a significant effect on getting rid of the Nazi mentality?,0
"This post has generated significant interest and been up for 13 hours without an answer. I am not a professional historian, but my work involves researching China and some parts of its modern history. I’d like to try to give people clarity on this issue as best I can with a few of the sources I have on hand. Let the mods decide if it’s acceptable to leave up.

Apologies in advance for typos. I’m on mobile. Sources listed at the bottom of the post.

The 8 non-Communist political parties in China are collectively called the “democratic parties.” They have a total membership of about 700,000 members, mostly professionals, intellectuals, scientists, artisans, and entrepreneurs.(1) Membership allegedly requires sponsorship by senior members of the party a person wishes to join.(2) These parties do not vie for power over the state, but express the concerns and ideas of their members as they relate to policy, regulation, and governance to the incumbent state power—namely, the Chinese Communist Party (CCP).(1)

The principal mechanism through which these parties exercise their voice is the Chinese People’s Political Consultative Congress (CPPCC). The easiest way to understand the CPPCC is as a forum for the CCP to hear feedback on the effects of their policies and collect ideas on what reforms might be beneficial to certain constituents. All 8 democratic parties are represented in the CPPCC, but their voices compete with 25 other non-CCP constituencies represented in the CPPCC. The CPPCC is charter-bound to accept the CCP’s leadership.(1)

Still, their views can affect things in small ways or on specific policy points. For example, a CPPCC member once complained about travel restrictions on scholars, arguing that the restrictions hampered other CCP efforts to make Chinese ideas “go global.” The restrictions were modified based on this criticism.(3) I’ll note there are probably more substantive examples, especially on issues like environmental challenges, but I’m not aware of any specific instances.

Simply put, you can think of the democratic parties as lobbying groups that represent some segments of Chinese society. That is how they function. Their members meet at semi-regular intervals to discuss issues, conduct research, and submit proposals to be taken to the CCP for consideration (again, mostly through the CPPCC).(2) They also allegedly influence policy at the local levels through those governing bodies and are sometimes sought out for comment by CCP officials during the policy-making process.(2)

The existence of the democratic parties has its roots in the CCP’s revolutionary history, political necessity, and the CCP’s concept of “democracy.” The preamble of China’s constitution is pretty clear on the first two the first two points: “In the long years of revolution and construction, there has been formed under the leadership of the Communist Party of China a broad patriotic united front that is composed of democratic parties and people's organizations.... The Chinese People's Political Consultative Conference is a broadly representative organization of the united front, which has played a significant historical role and will continue to do so in the political and social life of the country, in promoting friendship with the people of other countries and in the struggle for socialist modernization and for the reunification and unity of the country.”(4) In the simplest terms, at a time when the CCP was just one of many factions vying for control of China, they found it necessary to create allies with non-Communist peers. This method of receiving working with non-communists to ensure victory (in this case, a stable society) continues today. (There is a lot more to say about the role of the united front, but it’s off topic.)

The longer answer as to why these specific parties exist and why they are called democratic parties requires more knowledge of the period before and just after 1949. I think—but do not have a source for—that at least some of the parties existed in some form prior to the CCP’s victory in the civil war and believed they would be able to compete for power with the CCP in a multi-party system. This did not happen and they were given the current consultative status.

To the third point—that the parties exist because of and in relation to the CCP’s concept of “democracy”—the CCP thinks of democracy as serving the will of the people. As long as it can claim to be responsive to the citizenry, the CCP can claim it is a democratic representative of the people. Since the CCP believes it leads in the interest of the people, then China is democratic.(5) Without multi-party elections there is an obviously circular logic to this, but that’s what it is.

For the CCP to be responsive, then, it needs mechanisms like the CPPCC to collect feedback and hear complaints, and it allows groups like the democratic parties to exist. To be thorough, I’ll note that there are many other feedback mechanisms and even elections are held at the most grassroots level of Chinese society.(5)

Sources:
(1) Chinese Politics in the Xi Jinping Era — Cheng Li (book)
(2) What do China’s Democratic Parties Actually Do — Wang Xiaofeng (online)
(3) The Third Revolution: Xi Jinping and the New Chinese State — Elizabeth Economy (book)
(4) The Constitution of the People’s Republic of China (online)
(5) Is China a Democracy? A Long (and Better) Answer — Alan Wong (online)",0
"I will try to answer some of your questions, but not all unfortunately.

First of all, almost all of these surnames do not sound like insulting or negative. When Russian hears ‘Chekhov’, he doesn’t think about old word for sneeze (also, I don’t think this one is true at all), it’s just a surname for him/her without any real meaning.

Second, a lot of Russian surnames are like -son surnames. So, Blokhin is actually son of Blokha, Zhukov is actually son of someone who was named Zhuk and so on.

Tolstoy is just Tolstoy, not someone’s son.

Some of these surnames maybe were initially created as nicknames but now Russians often do not use or do not know the words these nicknames were based on.

So, surnames like Nabokov, Bunin, Sholokhov, Bulgakov, Chekhov, or Khrushchev are almost meaningless for the modern Russian.

Gorbachev doesn’t look like something that comes from hunchback, because modern form of this word is gorbun, not gorbach. Tolstoy is quite close to fat, but still the real modern word for fat is tolstyi, not quite the same.

Also, I think that your hypothesis that some surname comes from some noun only because they sound alike is not very strong.

Komarov is not really related to mosquitos, it’s a polish surname (Komar) with Jewish roots (possibly). Nekrasov is not directly related to ugly, this surname comes from the name Nekras (extremely outdated now), and that name means ‘non-beautiful’ but such names were given by parents to their kids to defend these kids from evil: my kid is ugly and non-interesting, go away, evil spirit, find someone else.

I think it’s quite possible that some Russian nobles created funny and insulting surnames for their servants but most of your examples are something different.",0
"Prepare yourself, this is an extremely lengthy and unnescessarily complicated explanation. Also, it didn't fit in a single comment, so I continued it in the replies. Click the link at the bottom of each comment for a short-cut.

\---

The likely explanation is that Social Scientists and National Governments just use different vocabularies to talk about this stuff.

In my experience, having done archaeology on the famine, it comes down to what you'd consider genocidal intent. The British heavily bought into Malthusian theory, which they viewed as the cause of the famine. Malthusian theory says that famines are the result of the population becoming too large for the land to support (this is incorrect; we now understand that famines are more commonly the result of chain failures in complex societal systems). Many British intellectuals blamed the Irish peasantry for having too many children while living in a state of poverty, which they viewed as irresponsible (again, not accurate). This is one of the reasons why the British instituted the workhouse system, which I'll cycle back to in a sec.

So in actuality the famine was caused by monoculture of the potato, which existed because it was the only economical way for poor farmers to operate within the tenant system utilized in British Ireland. The tenant system involved large landowners renting to middlemen under a fairly firm renting agreement, giving stability to the middlemen but requiring them to make rent consistently. They achieved this by renting at-will to small tenant farmers. Since middlemen were constantly competing for land, the ones most aggressive in returning profits would win out. The middlemen kept dividing the land into smaller and smaller parcels, so that they could rent to as many tenant farmers as possible. Now obviously there was a lower limit on how much they could shrink the land parcels, because there needed to be at least enough land for the tenant farmers to feed themselves and to make money for rent. But there were different ways in which tenant farmers could feed themselves, and these all involved different efficiencies of land use. The best way to feed yourself using the smallest amount of land possible is to grow potatoes. So since all these tenant farmers were competing for increasingly small parcels of land, the system gradually pushed them to all adopt potato monoculture to feed themselves. Which from a nutrition angle actually worked reasonably well for them, as potatoes are quite nutrient and calorie dense. But from a stability angle, it was catastrophic. When the potato blight came, these subsistence crops failed completely, and the tenant farmers were left starving. But *here's the thing*. Remember, the tenant farmers weren't just using their land to feed themselves, but also to make money to pay rent. Also, there were parts of Ireland which weren't as potato focused, and they continued to be productive. It's debatable whether or not Ireland actually remained a net exporter of food during the famine (meaning that they exported more than they imported). But they still continued to export. In the late 1700s, the British halted exports from Ireland to combat a famine, but actively opted against this for the great famine, on the basis of preferring market solutions. It might not have been possible to eliminate the great famine by halting exports, but it would have helped reduce the severity. Coupled with the fact that tenancy policy drove potato monoculture in the first place, and the famine was unquestionably a human-made tragedy.

So why didn't they just distribute the food which they had? Well, because again, the British blamed poor moral character for the famine. They thought that by just handing out food, it would encourage laziness, resulting in even more overpopulation as people ignored the consequences of having more kids. So the British insisted on food being conditioned on entering a workhouse, in order to build an ethic of able employment in exchange for assistance. Problem is that the workhouses basically involved cramming a ton of malnourished and thus immunocompromised people together in poor living conditions. Disease was rampant. The workhouses were seen as a place that people went to die. Since any person without tenancy or an occupation ended up in the workhouses, tenant farmers became even more desperate to retain tenancy on their small parcels. What's more, the British also provided for some limited relief to be given to people who remained working on their lands, without the requirement of entering a poorhouse. The problem? The landholders were expected to pay for it, which seems great on face value, but combined with the workhouses, the system resulted in landholders evicting tenants into the workhouses rather than pay to support them. This only exacerbated the underlying factors contributing to the famine.

So was the famine a genocide? It's complicated. We have two main factors to consider, and both involve intent. Question one. Was the intent of the British in formulating their Irish policy during the famine to eliminate the Irish people? Question two. What actually is a genocide, and does intent matter?

Let's talk about the intent of the British. Prejudice and power relations were unquestionably a major contributing factor to the famine. But the British weren't trying to kill the Irish people, at least not overtly. In theory, the British were trying to 'save' the Irish. This is speaking as a matter of intent. As a matter of actual events, the British were definitely making things worse.

Except here's where it gets complicated. See, the British were justifying a lot of their efforts on the basis of Malthus, and Malthus' theory states that famine is caused by overpopulation. So the British were, by their own basis of justification, definitely intending to deliberately reduce the Irish population. They weren't even shy about that. But the British of the time would tell you that their plan wasn't to reduce the Irish population by killing lots of people, but rather by instilling strong moral character in the Irish so that they'd be responsible enough not to have more children than they can support (ahem, prejudice). But that's still intentional reduction of population. Is that genocide? Well, right now the West finances plenty of programs in the developing world to educate people about contraceptives and provide them access to family planning resources. That carries overt intent to limit population. We can probably agree that these programs don't constitute a genocide. But on the other hand, consider the American Eugenics Program, where women were forcibly sterilized (a practice which we see echoes of today in ICE actions). That's something we hopefully can agree is abhorrent. So what's the difference between forced sterilization and contraception education? The main thing is consent.

Okay, so here's another question. At one point does power become an instrument of coersion so effective that it functionally limits the ability for consent? In theory, Irish tenant farmers had the choice whether or not to switch onto potato monoculture. In theory, Irish tenant farmers had the choice to negotiate their contracts. In theory, Irish tenant farmers had the choice to find a job, or a tenancy, in order to avoid the workhouses. But *did they really*? I once participated in an archaeological excavation of famine-era huts which we discovered to actually be converted from reclaimed shale pits (not a great way to live ... no drainage, no land for crops, and based on the crude fire pits we found, they would have been filled with smoke). We have records of these pits being used for shale up until soon before the famine, meaning that the people who moved into those sites did so during famine times, likely after losing tenant land. So when the Irish had a choice, *any* choice, they did exercise it. This is the length people would go to in order to avoid the workhouse. Meaning that most of those who entered the workhouse really did so because they had no other option. What does this mean? Well, let's say we buy the British explanation that they genuinely thought that the workhouses would be beneficial to the Irish. Is this more like contraception education in the developing world, or is it more like forced sterilization?

And this all assumes, incidentally, that the British actually really did genuinely think that the workhouses were helpful. Now, for what it's worth, I actually do suspect that plenty of British people were just straight-up true believers. Malthus was considered 'cutting-edge' at the time, thus many British considered themselves to be operating from a rational and scientific basis. Incidentally, stuff like this is why it's so important to be skeptical about appeals to a more rational temperament, such as in the ""facts over feelings"" type of slogan. Science is a process, not a personality trait. Many people at the time definitely bought into the latter, a problem which people today still replicate. Most records of the policy discussion showed a strong focus on Malthus and market solutions to economic problems, so it's likely that large parts of Parliament were genuinely just wrong. So if you buy that, the situation looks more like a misguided intent to do good, rather than a guided attempt to do bad.

[Continued in Reply](https://www.reddit.com/r/AskHistorians/comments/pqjz96/the_irish_potato_famine_18451852_while_often/hddihwt/?utm_source=reddit&utm_medium=web2x&context=3)",0
"Also, because I feel like I have only partially answered your question.

There certainly was a change in the way the problem of Fascism as a ""cultural thing"" was approached within the context of historical research. While this does not really address the dichotomy between a Mussolini ""renaissance man"" and a Mussolini ""brutish moron"" (which seems mostly a derivation of opposite propaganda excesses), it's probably worth mentioning in the context of your question. 

There certainly was a tradition which tended to accept a view of Fascism as supposedly ""anti-culture"" in so far as it was ""a-cultural"" - to adopt, in other words, Croce's argument that it was very difficult to define a Fascist ideology because there was none. After the war another prominent philosopher like Norberto Bobbio had curtly concluded that it was ""impossible to talk both of culture and Fascism"".  

Now, there is an obvious reluctance in addressing the ""cultural side"" of Fascism. From an intellectual perspective, one is brought to regard culture as an inherently ""positive"" value, and therefore to remove ""negative"" dis-values from the ""cultural"" sphere and to ascribe them to ""a-cultural"" impulses. And it's also true that many of those intellectuals had experienced directly or indirectly the consequences of the pressure, oppression and at times persecution conducted by the Fascist Regime against non-aligned intellectuals; which does explain how many of them could feel almost a moral obligation to speak out: ""no, true culture is something else"". 

Furthermore, contemporary historiography was largely influenced by a certain view which - intentionally or not - tended to identify culture with the cultural values of Western liberalism, and therefore to interpret deviation from the historical process (progress) as aberrations due to the surfacing of a magma of amorphous underlying impulses (which is to say, rationalism vs. irrationalism) and therefore to interpret Fascism as some sort of epi-phenomenon of these incoherent forces of chaos. This was certainly the viewpoint of someone like Croce (albeit expressed in a quite more articulate way). But similar arguments underlay the old interpretations of Fascism as a moral (or even physio-psichological) degeneration.


This is no longer a dominant view. And certainly not in an effort to re-evaluate Fascism, which is both a vain and clumsy intellectual endeavor. But, once we accept the reality that the historical process is quite often a magmatic and irrational thing itself, it becomes impossible to establish a clear distinction between the elements representing a moral and intellectual advancement of humankind and those which instead represent a regression, a degeneration, an aberration. If we can't rely on progress to tell us the right way, then we must figure it out for ourselves. 


In this endeavor, intellectuals may appear at times better equipped than the common folks, but that doesn't automatically lead to the conclusion that one should deny the patent of intellectual to anything related to Fascism. Men like Gentile, Heidegger, Eliot, Pound, Lindbergh, and many others, including some like Toscanini or Croce himself who had looked with some sympathy or at least with cautious reservation at the new fascist phenomenon weren't extraneous to the world of culture. 

In very basic terms, culture - a culture - is not always and necessarily ""our culture"" or high culture. Even if one is taken aback by the character of ""fascist culture"" or looks with horror at the idea of associating the gruesome spectacles of the Second World War and of the Holocaust with the word ""culture"", it makes sense to investigate Fascism from the perspective of a cultural phenomenon. 

This ""revision"" - in an obviously positive acception - was probably heralded by George Mosse in the late 1960s. And continued with a series of proficuos investigations in the ""culture of Fascism"" until now. Most notably with the works of Emilio Gentile, who has approached the issue of a ""fascist ideology"" from the perspective of a ""political religion"", Roger Griffin, who chose to interpret Fascism as a phenomenon of ""modernity"" in contrast to a tradition of ""anti-modernist"" interpretations, or Zeev Sternhell, who has thorougly examined the connections between the intellectual culture of early XX Century in France and Italy and the Fascist phenomenon to posit an almost direct filiation. It goes without saying that those positions themselves are subject to criticism and represent a deliberate choice in an investigative direction, which is not without consequences. In other words, looking for a fascist culture leads one to find a fascist culture, even by mere collection, accretion and composition. And yet, it is - I dare say - without question that one can speak of ""fascist culture"", nor should the supposed anti-intellectualism of Fascism led to dispute this position. After all, men of no culture (if we admit such thing) may express a distaste for culture and intellectuals, but are extremely unlikely to articulate anti-intellectual dispositions; or, in other words, anti-intellectualism is an intellectual position, and anti-intellectual culture is culture (which isn't to say that Fascism was only, and always anti-intellectual). 


For general sources on Mussolini's culture, education and publicist years: 

De Felice, R. - *Mussolini*

Di Scala, S.E. ; Gentile, E. - *Mussolini 1883-1915*

Gentile, E. - *Il mito dello stato nuovo, dall'antigiolittismo al fascismo*

Gentile, E. - *Le origini dell'ideologia fascista*

Milza, P. - *Mussolini*


More specifically to the additional part of my reply.


Esposito, F. - *Fascism, Aviation and Mythical Modernity*

Gentile, E. - *Le origini dell'ideologia fascista*

Gentile, E. - *Il mito dello stato nuovo, dall'antigiolittismo al fascismo*

Gentile, E. - *Mussolini contro Lenin*

Gentile, E. - *Storia del partito fascista: 1919-22, movimento e milizia*

Gentile, E. - *Fascismo di Pietra*

Griffin, R. - *Modernism and Fascism*

Griffin, R. - *The Nature of Fascism*

Isnenghi, M. - *L'Italia del Fascio*

Sternhell, Z. - *The birth of Fascist ideology*

Vivarelli, R. - *Il fallimento del liberalismo*

Vivarelli, R. - *Storia delle origini del Fascismo*",0
"I have a past answer on this, which I'll paste below:

In the seventeenth century, it was indeed generally understood that women were voracious sexual creatures. This was particularly true when it came to non-virgins, a trope that would actually continue through the eighteenth and nineteenth centuries: a woman who had been introduced to carnality by a man was supposed to be fundamentally changed, to have been transformed into someone who needed to be restrained from leaping into bed whenever a man seemed at all interested. Widows compounded the issue by having been initiated into lust and then left bereft of a man to take care of her needs.

>It is very easie for him which never experienced himself that vain Pleasure, or repenting Pleasure, chuse you whether I mean the accompanying of lewd Women, but such as are exercised and experimented in that kind of Drudgery; they I say, have a continual desire and Temptation is ready at hand: Therefore stake heed at the first, suffer not thy self to be led away into lustful Folly; for it is more easie for a young Man or Maid to forbear carnal Act than it is for a Widow ...

*The arraignment of lewd, idle, froward, and unconstant women*, by Joseph Swetnam (1615)

But according to Aristotle, who was still considered an important natural philosopher in this period, once virgin maids reached the age of menstruation they began to have their passions raised - which is why they needed to marry, so that there would be a man lawfully allowed to service them. (Though even in his time, he made the point that teenagers might be physically able to conceive, but that it wasn't good for their bodies to give birth until closer to twenty.) A good woman would restrain herself from acting on these urges, but women as a whole were understood to feel them - and the ""weakness"" and ""frailty"" understood to be inherent to women extended to their ability to resist. And a truly depraved woman would deliberately not resist in order to seduce a man into doing what she wanted, flouting the natural order of things by taking the active and commanding role. (Unmarried adult women, it should be noted, were seen as big problems, in part because of their uncontrolled sexuality. It was generally assumed that a single woman trying to live without the authority of a man, either a parent or employer, was a prostitute.)

Another point is that early moderns understood the concept of the female orgasm, and drew conclusions from it that we'd now consider bonkers. Since women had a shorter refractory period than men and were capable of multiple orgasms, and men's physicality was considered the norm, women could be seen as needing multiple partners in order to be fully sated. ""Though they be weaker vessels, yet they will overcome 2, 3 or 4 men in satisfying of their carnal appetites,"" Thomas Wythorne, Elizabethan musician and tutor, wrote in the sixteenth century. By contrast, a man was capable of being sated by a single woman, and indeed, was pretty much always one and done.

But we know that attitudes did change. As with a number of issues, this comes down to societal changes in the second half of the eighteenth century, changes often called the ""cult of sensibility"" - ""sensibility"" in this sense refers to emotionality, kindness, and refined feeling. In a sentimental novel of the period, it was important for both male and female characters to display how strong their emotions were by fainting and crying at every opportunity; in real life, few could really match the sensibility of a character like Richardson's Pamela, but women of genteel backgrounds were considered to have larger reserves of the quality, and to be inherently more delicate than women of the lower orders and all men. That is, *weaker*, but in a positive sense. This weakness, rather than targeting their moral susceptibility to temptation, affected the nerves and the body - including their physical capacity for sex. This carried the seeds for the ""cult of domesticity"": good women were physically weak but morally strong, and therefore suited to stay at home and tend to the well-being of her husband and children.

In light of these developments, women on the whole could not be seen as inherently carnal beings. Women whose marriages had been consummated or who had sex outside of marriage were still seen as having been awakened into a new state of sexuality, but the strong moral sense of the women who insisted on being married before engaging in sex prevented them from becoming insatiable; the women who were ""ruined"", on the other hand, lacked that moral sense and were generally seen as as rapacious as all women had been seen a century earlier.

(Regarding the tidbit about the cigar clubs: that's a weird, misogynistic thing for your professor to say. Women weren't seen as ""the lustier sex"" because they actually were begging for it constantly, but because a number of interlocking cultural factors, as discussed above; they actually had lives, professions, and hobbies of their own to occupy themselves with.)",0
"**Part II: Victorians, Sexuality, and Social Reform**

To bring us back to the bits about sex, all of this concern about family and idealization of the family means that anything that threatens to destabilize the family was particularly menacing for Victorians. We can easily imagine how same sex relationships are threatening—men preferring to have sex with men were potentially turning away from their role as head of house. Men who had sex with men were a monstrous perversion of the social order. The later Victorian period experienced what might be considered a minor moral panic regarding gay men and how they represented a growing threat to this nicely-ordered ideal of society with the family at its base. Likewise, sex outside of marriage had the potential to leave society with inconvenient single mothers who couldn’t be fitted into the ideal model of the family. For the Victorians, wrong sex was potentially a social problem. 

Speaking of social problems... the Victorian era was a period in time with a particularly active reform movement that set out to solve social problems. Reformers' goals (speaking generally) were to change minds and change laws. Thus, the kinds of sexual behavior that were regarded as social problems were often met with legislative solutions. One such legislative solution was the Criminal Law Amendment of 1885, which I think is what you’re referring to when you ask why homosexuality was made illegal. 

To explain this bill, we’re going to need some context and some clarification. First, the Criminal Law Amendment did not make ""homosexuality"" illegal as such, in that today we understand ""homosexuality"" to be a sexual orientation, the exclusive attraction to people the same gender as you. This bill didn’t make it illegal to be gay; it extended existing laws that criminalized sexual activity between men. Note I say men here: the ""gross indecency"" amendment did not apply to women having sex with women. It’s important to know that sodomy had been illegal for centuries, but the Victorians knew, as we do, that there’s a lot of sexual activity available to men outside of sodomy. Thus, the bill criminalized what we might politely refer to as non-penetrative sex between men (the bill itself criminalized ""gross indecency""). 

This provision was essentially tacked on to a bill that was mainly concerned with criminalizing child sexual abuse. The bill was prompted by the publication of a piece of investigative journalism called ""The Maiden Tribute of Modern Babylon,"" written in 1885 by W.T. Stead. Stead was a reformer and journalist largely concerned with the abuse, sexual and otherwise, of women and girls in Victorian society. In this campaign, he was joined by early feminist Josephine Butler. Stead, Butler, and the strain of reform they belonged to often framed their concerns in terms of purity because they operated in a culture that placed a high value on purity. So, Stead might speak of the degradation of prostitutes and other ""fallen women,"" he saw this degradation not as a simple ""prostitutes are bad,"" but as a symptom of a culture that saw ""unpure"" women as outside of polite society (that ideal middle class home where the woman is the Angel in the House). These women and girls were not only subject to rape and other violence but social ruination and exclusion. 

""The Maiden Tribute of Modern Babylon"" was an expose on this problem as it specifically applies to *children*. Stead's pieces describe girls being abducted, sold by uncaring (often alcoholic) parents, or tricked into brothels where they were drugged, raped, and trapped. In fact, ""The Maiden Tribute"" offers up stories of girls who, after having been subjected to this horrific treatment, felt they had nowhere to go because they were ruined. In the process of confirming these accounts, Stead himself purchased a girl, whom he thankfully didn’t abuse and turned over to the care of the Salvation Army. But, on discovering that Stead had actually *done* this, the authorities chose to prosecute him and he served a short jail sentence. 

As I’m sure you can imagine, all of this caused a sensation, and there was a loud and immediate demand for laws to be changed so that this child sexual abuse could be prosecuted. (The age of consent at this time was 13, so many of these girls were, in a legal sense, perfectly able to consent to sex.) This problem was the main focus of the bill that extended the criminalization of homosexual activity. The ""gross indecency"" bit was tacked on by an MP named Henry Labouchere who had a particularly intense hatred of gay men and thought that *male* prostitutes were a particular problem. The bill passed, and this is the law under which Oscar Wilde and Alan Turing were both prosecuted. 

Now, I’ve said all of this, and I’ve made it sound as though Victorian England was a homogenous society in which everyone was a stuck up prude, and that is simply not true. London had a gay subculture of its own. Prostitution was an enormous social problem because *so many men were looking for socially proscribed sex* (and the poor women who provided it were vulnerable to their whims). Victorians didn’t object to a husband and wife seeing each other naked. Certain kinds of sex were socially sanctioned, and underneath the veneer of middle class respectability was a teeming world of unsanctioned sex. Some of the Victorian era's harshest critics were Victorians themselves. 

So, why do we have a stereotype of Victorians as so stuffy and prudish they’re having sex with all their clothes on? I’m going to offer two theories. The first is that the Victorians were great codifiers and classifiers. The Victorians might not have invented their ideas about sex from whole cloth—they evolved from earlier ideas, but because they wrote the book, we think they invented it all.  

Secondly, our cultural memory of the Victorians was very much established in the early twentieth century, in which the next generation very much tried to have a hard break from the Victorian past. Much of the high art of the early twentieth century was fervently *anti* Victorian. I think of, for example, James Joyce's Nausicaa episode in *Ulysses*, where he ruthlessly mocks Victorian sentimental literature, or Virginia Woolf's *To the Lighthouse,* where the stodgy Victorian Mrs Ramsey has the refrain of ""People must marry."" It’s understandable that this younger generation would react to the ideas and culture they were raised in. We all, I think, do this to some extent. But the culture has accepted this narrative of stodgy Victorians rather uncritically. 

*Sources* 

Chase, Karen and Michael Levenson. *The Spectacle of Intimacy: A Public Life for the Victorian Family.* Princeton UP, 2000.

Cook, Matt. *London and the Culture of Homosexuality, 1885-1914*. Cambridge UP, 2003. 

Foldy, Michael S. *The trials of Oscar Wilde : deviance, morality, and late-Victorian society*. Yale UP, 1997. 

Koven, Seth. *Slumming: Sexual and Social Politics in Victorian London.* Princeton UP, 2004.

https://attackingthedevil.co.uk/ is a good resource if you’re interested in reading W.T. Stead's journalism. ",0
"> I have no fucking idea!

While we appreciate the honesty, we've removed your response. We expect answers in this subreddit to be [in-depth and comprehensive](https://www.reddit.com/r/AskHistorians/comments/f7ffl8/rules_roundtable_ii_the_four_questions_what_does/), and to demonstrate a [familiarity](https://www.reddit.com/r/AskHistorians/comments/g3peg6/rules_roundtable_x_informed_complete_answers_the/) with the [current, academic understanding](https://www.reddit.com/r/AskHistorians/comments/fj1ieh/rules_roundtable_v_sources_what_is_required/) of the topic at hand. Before contributing again, please take the time to better familiarize yourself with [the rules](http://www.reddit.com/r/AskHistorians/wiki/rules#wiki_write_an_in-depth_answer), as well as our expectations for an answer such as featured on [Twitter](http://www.twitter.com/askhistorians) or in the [Sunday Digest](http://www.reddit.com/r/AskHistorians/search?q=title%3A%22Sunday+Digest%22&restrict_sr=on&sort=new&t=all).",0
"There are two ways to attack the subject at the heart of your excellent question: by using early medieval primary sources (which are always too rare) and by considering early modern folklore collected in the North Sea area and projecting backwards (which has its own problems). At the heart of the problem with projecting backwards using early modern sources is that folklore is not monolithic, by its nature. Folk traditions, beliefs, and the narratives that express those beliefs are subject to change because there is no single dogmatic doctrine that provides an anchor from which the tradition cannot drift. Similarly, it is too easy - and error ridden - to generalize about traditions using rare early-medieval sources. The two types of evidence provide glimpses at a tradition in motion, and yet, they do point to some observations.

There is a reasonably homogeneous tradition of supernatural beings, which people believed lived in family groups and/or communities and which is documented in historic and pre-modern professional collections, ranging from wherever Scandinavian languages are spoken, Britain, Ireland, nearby islands, and Brittany.  Because this generic tradition (acknowledging enormous variation over time and geography and also a wide variety of terms for the entities) is so widespread and appears in some of the oldest documents from the area, so that it is reasonable to conclude that the roots of the tradition is pre-conversion. 

At the same time, it seems inevitable that conversion affected the tradition and shaped it over time. That said, the Church appears to have failed at the one thing it strived to achieve - namely, to group these supernatural beings together with the forces of evil. We see evidence of this effort in the passage in the Anglo-Saxon poem, Beowulf, when the poet writes that elves, like the monster Grendel, were descendants of Cain:

>From him the evil brood were all born,

>Giants and elves and evil spirits,

>And also the giants who fought against God

>For a long time; He paid them retribution for that.

That clerical point of view did not generally affect the folk tradition, which maintained that the elves (fairies, sidhe, pixies, hidden folk, or other names) were neither good nor evil. Mind you, people feared them, for they were extraordinarily dangerous and were capable of inflicting terrible harm, not the least of which was abduction, which could last for eternity and deprive the poor soul of salvation. The elves had to be treated with caution, respect, and deference, and most of all, they needed to be avoided. But they were not in league with Satan - that was a different kettle of supernatural beings.

In pre-modern collections from North Sea cultures, we find numerous explanations of the origin of the elves (with all their names), some of which are placed in a Biblical context: the elves were angels who refused to take a side in the battle between God and Satan, so they were cast out of heaven to live on earth (but not Hell) apart from the Christian struggle between good and evil; or the elves were the children of Eve who, when she realized God was coming for a visit, was only able to clean some of her children, and so she hid the others, and God answered that those who were hidden from Him would be hidden from mankind for all time. At the same time, there is a widespread legend that tells of the possible salvation of the elves: a young man is traveling through the forest and sees the elves celebrating, so he calls out and says that they have no reason to celebrate since they cannot know salvation, at which point the elves begin to cry. He goes home and tells his father what happened, and his father tells him to return with word that they will know salvation, which the boy does, and the elves resume their party. Some times the boy tells them that they have no more chance at salvation than does his walking stick have a chance to sprout leaves. The next morning the walking stick is full of leaves, and so the boy goes back and tells the elves the news.

The repeated message in these stories is that the elves are not evil, and that if they lean in one direction or the other, it is on the side of salvation. The Church, however, persisted in its attempt to say that all supernatural beings not specifically identified as being on God's side in the bible are by necessity on the side of evil - maintaining a strict division of the universe into those two halves. During the witch trials, for example, people could be accused of cavorting with the elves, with the consequence being the same as cavorting the demons. Despite that dark chapter in folk belief, the folk persisted in seeing the elves as theologically neutral, and while they were to be feared, there were also stories of these supernatural beings rewarding people of good character and generosity.

That leaves us with the odd tradition of incorporating the word ""Aelf"" into Anglo-Saxon names - Alfred, being one of the more common that descends to this time. There can be no question that people understood the meaning of these names, but that does not mean that they took them to mean that in a literal way. We have names including Rose and Marigold, but that doesn't mean that we automatically think of those flowers when we encounter those people. I knew a fellow named, of all things, Rex King (parents with a sense of humor or a lack of imagination!), and all his schoolmates no doubt understood that Rex meant king, but we didn't think along those terms when we encountered him - or at least I didn't. 

It has always struck me as odd that the Anglo-Saxon people might be summoning dangerous supernatural beings with a name that uses their name, but we have to set that aside as something that apparently cannot be resolved, and we must merely accept the fact that the Anglo-Saxon naming tradition included this word, and people did not think of it as taunting the dangerous supernatural beings.

edit: Thanks for the anonymous gilding of the answer - and for the platinum! Much appreciated!",0
"Blackbeard, the infamous pirate, is often associated with the myth of consuming rum and gunpowder. However, it is important to note that this claim is primarily a legend and not historically accurate. Gunpowder, typically made from sulfur, charcoal, and potassium nitrate, was not regularly consumed as a beverage or food additive in the past. Gunpowder was primarily used for military purposes, such as propelling projectiles or igniting firearms. Its main purpose was never for consumption.",1
"George Whittell, Jr., (1881-1969) was the sole heir of two enormous fortunes from Gold-Rush-era San Francisco (both his grandfathers had made millions as investors). Whittell refused to work and was, simply, an obnoxious playboy who did little to manage his investments, although he frequently had to buy off women for sexual assault or men who threatened to sue him when his pet lion, ""Bill,"" mauled them. 

Whittell inherited large swaths of real estate on the San Francisco peninsula, but it was easy after the crash to be land rich but cash poor, and even the rich could sometimes find it impossible to pay real estate taxes. In what was perhaps his only act of initiative and business acumen, Whittell realized in August 1929 that it was time to draw his money out of the stock market. He later claimed that he sensed that the party was over and that when people stop drinking and having a good time, it's time to pull back: he withdrew $50 mil out of the stock market, an incredible fortune for the time, and thus avoided the really horrible effects of the crash.

This gave him the funds to continue living extravagantly and yet still pay his taxes, etc., so that his fortune was not jeopardized (he purchased six Duesenbergs and a DC2, for example). He also bought 27 miles of Lake Tahoe shoreline, land that extended from water's edge to the crest of the mountain - real estate worth billions today. He claimed that he wanted to build a hotel casino on the Lake Tahoe shore (his holdings represented almost all of the Nevada side of the Lake), but he never got around to doing it, committed as he was to doing nothing productive. He did manage to build himself the ""Castle,"" a lavish estate on the edge of the lake.

All this meant that he was either aware or lucky enough to watch the economic disaster from afar while not having it affect him in any substantial way - except to make the cash he had on hand being even more valuable for purchasing power. 

In short, to weather economic disaster: Step 1. pick your parents well so that you start with lots of money; Step 2. be lucky enough or perceptive enough to consolidate your holdings in the right way before the crash; Step 3. continue your life as a totally reprehensible expression of humanity.

My wife and I wrote a biography about Whittell together with a portrait of his Lake Take estate: [Castle in the Sky](https://www.amazon.com/Castle-sky-George-Whittell-Thunderbird/dp/0971804702), which is the accumulation of research on this not very pleasant person.

edit: because of the unexpected interest in this post, [here is a page related to Whittell's speedboat, ""Thunderbird](https://thunderbirdtahoe.org/index.php/yacht-history) - one of the wildest rides I ever took outside an amusement park. And here is a page on the [history of Whittell and what has become known as the Thunderbird Lodge](https://thunderbirdtahoe.org/index.php/thunderbird-lodge-history-page), what Whittell called his Castle.",0
"For this response, I'm going to write only about what scholars of history can determine from documentary material.  I'm not going to take a position one way or another on the theological questions involved, as this isn't the place for that.

> As far as I know from my Christian upbringing, when it comes to authors of the New Testament, Matthew, Mark, Luke, John, James, Jude and Peter all supposedly knew Jesus intimately for years.

I'm afraid your Christian upbringing left a few gaps.  The traditional Christian understanding is that Mark and Luke were not written by anyone who knew Jesus personally.  The gospel of Luke even starts out with an explanation that he is collecting accounts of what happened from others, that the author himself is not an eyewitness.

What we have are a number of writings regarding the early Christian movement.  Some of them claim to be written by particular people, while others are anonymous.  Unfortunately we have no writings that scholars generally recognize to be from anyone who ever met Jesus.

The majority position of scholarship today is that Jesus, Peter, and Paul almost certainly existed.  (Though be aware that there is a small number of scholars who have concluded that Jesus himself was a myth.  I don't find their arguments persuasive, but you should at least know that they exist.)

-----

There are many different gospels of Jesus, but here I'll focus on the big four, the ones that were most accepted by early Christians and are now included in Bibles today.

Scholars today almost all recognize that Mark was the first gospel to be written, a position known as Markan priority.  We don't actually know the name of the author for sure -- all four canonical gospels are anonymous -- but it was attributed to Paul's assistant John Mark around a hundred years after the time of Jesus.

Most of the material in Mark is copied in Matthew and Luke, not just the events, but the particular phrasing, with many stretches of word-for-word agreement.  For a number of reasons (that I'd be happy to get into) scholars are generally convinced that Matthew and Luke both had access to manuscripts of Mark to copy from, along with one or more other sources that are now lost to us.

The gospel of John is generally believed to be the last of these four gospels composed, around 60 to 80 years after Jesus' day.  Within a generation or so it was attributed to the apostle John.  Most scholars are unconvinced of this authorship -- the sophisticated Greek writing, for example, seems unlikely to be the work of an Aramaic-speaking fisherman.

-----

The two letters attributed to Peter are also in fluent Greek, while Peter is described in the book of Acts as being an illiterate fisherman.  1st Peter was generally accepted by the early church, but 2nd Peter's authenticity has been disputed since at least the time of Origen, writing around 200 AD.

Peter himself is one of the more widely attested figures from the New Testament, being mentioned by many different early Christian writers.

The letter of Jude is a very brief text in polished Greek.  It does give the name of an author, Judas the brother of James.  (Jude and Judas are two different English renderings of the same Greek name.)  Unfortunately we're not sure exactly who this is supposed to be -- the author of Luke mentions a *Judas, brother of James*, but there is some dispute as to whether this is the same person as Matthew and Mark's *Judas, brother of Jesus* or John's *Judas (not Judas Iscariot)*.

Jude was not universally accepted as valid by the early church.  Some, such as Tertullian and Clement of Alexandria, accepted it, while others, such as Eusebius and Origen, did not.  One sticking point was that Jude quotes the book of Enoch as an authority, a popular book of the day that most Christians no longer consider canonical.

The letter of James is similarly difficult to attribute to an author.  Three of the gospels state that two of Jesus' disciples were named James, and a number of different texts mention James the brother of Jesus.  We simply don't know which one of these the letter of James is supposed to have been written by, and the different Christian traditions disagree on this point.

A number of early Christian writers mention at least one James, though there is often some confusion as to the identity of the different Jameses.  Josephus, a non-Christian writer of the first century, mentions James the brother of Jesus being among those stoned to death by the Sanhedrin for breaking the law.

-----

As you mentioned, Paul himself never met Jesus.  Paul claims to have had an extraordinary experience of Jesus, which is also reported with varying details in the book of Acts.  Whether you believe Jesus actually revealed himself to Paul is a good question, but not the sort of thing that historians can determine.

Paul is mentioned by many early Christian authors, like Peter.  (And like Peter, I'm not aware of any non-Christian authors who mention Paul early on.)

-----

Overall, I think we have abundant documentary evidence that Paul, Peter, and Jesus existed.  We have many different writings that speak of them that are probably from within a hundred years or so of the events described, and we have manuscript fragments of some of those writings from not much later.

As for theories about how people could come to believe that Jesus was raised from the dead without an actual resurrection, this quickly gets into religious questions, but historians can (and sometimes do) speculate.  Maybe Jesus actually came back from the dead.  Maybe Peter, out of grief and trauma, had a vision of Jesus.  Religious beliefs sometimes spread very quickly.  As for whether those beliefs are founded on truly miraculous events, I'll leave that to you to decide.",0
"Octavian's early career was not only anomalous, it was outright illegal under normal circumstances. First of all, armies could not legally be raised and led by private citizens, only by elected magistrates and promagistrates holding *imperium*. Moreover, Octavian received the consulship on August 19, 43 at the age of 20. The minimum age for the consulship, as established by the *lex Vilia* in 180, was 43. Furthermore, Octavian had not held the quaestorship, formally a requirement for entry into senatorial magistracies since Sulla, and was not even of age to offer his candidacy, as the minimum age for the quaestorship was 31. Octavian's meteoric early career was made possible, if only barely acceptable to a relatively few, by special provisions passed in response to the state of intense political feuding and near-anarchy that Caesar's murder left the Republican state. Octavian arrived in Italy by the fall of 44 a complete newcomer to politics. He had never held any magistracy and only with great difficulty--and the aid of Cicero, who was strongly opposed to Antony--managed to secure the fortune left him in Caesar's will. As Cicero delivered the *Philippics* condemning Antony and eventually driving him from Italy entirely, Octavian was gathering a private army of his own accord and from his own (inherited) fortune, consisting mainly of Caesarian veterans and serving Caesarian legions which defected from Antony. This army was entirely illegal, as Octavian had no legal status allowing him to maintain an army--he was not even a senator yet, much less a praetor or consul. But with Antony gathering his forces in Cisalpine Gaul, Cicero and his supporters saw Octavian as something of a necessary evil, an ally on which they had to rely in order to provide themselves with an army capable of combating Antony's. Octavian was thus, on Cicero's recommendation, granted an extraordinary propraetorian magistracy and furthermore afforded a senatorial seat, a consular veto, and the right to stand for every magistracy ten years early (subsequently violated in his first consulship). By sheer coincidence, both consuls were killed at Mutina, and Octavian was left holding a bag of two consular armies and some assorted private troops, a force with which he was able to force a consular election in August which elected him--fully 23 years too early--for the consulship, alongside Q. Pedius. 

This was not normal. Classical Roman ideology held that although boys came of age around 15 or 16 they were not yet fully men until they had passed 30 or so. The *adulescens* was not, as in modern English, an adolescent, but he was still a man too young to be entrusted with independent action. In practice, at least for the *nobiles*, this generally meant that until the age of 30 or even older a young man was still considered not fully equal to his elders and certainly not fit for great responsibility. This is the time of life during which it was appropriate for a young aristocrat to enter into a military tribunate and learn the ropes of leadership and politics from his fully senatorial superiors. This has nothing to do with life expectancy. Average life expectancy in antiquity was reduced by premature death, either as an infant or due to disease or warfare at a young age. Provided one survived to adulthood the chances of reaching an advanced age were pretty good--Augustus himself lived to the age of 76. Instead, *adulescentes* were simply not considered mature or experienced enough to be entrusted with responsibilities. And indeed, as I have already pointed out, legal prohibitions prevented the entry of an *adulescens* into the *cursus honorum*--and thus, the senate, following his term as quaestor--before the appointed age for precisely this purpose. Octavian's case was not *wholly* unusual, however, if totally unprecedented in the speed with which Octavian sought the consulship. The *lex Vilia* had been passed in the wake of the Second Punic War, which saw a number of politicians rise exceptionally quickly through the political ranks. Scipio, for example, was elected to the consulship in 205, when by the *lex Vilia* twenty years later he would have been more than ten years early, and five years before in 210 had been granted proconsular *imperium* despite never having held praetorian or consular rank. Closer to Octavian's time (and, more importantly, following the establishment of minimum ages and Sulla's provision for mandatory quaestorships), Pompey held the consulship without having ever held any previous office or even having a senatorial seat. Pompey's career was arguably even more radical than Octavian's. Pompey had raised three Sullan legions from his own clients and dependents in 83, despite being a private citizen--this could only be allowed by the great instability and disregard for legal guidelines caused by the civil war. Pompey was granted his first triumph by Sulla in 79 (at 27 years old), despite being a private citizen, not being a senator, and never having held any elected office--technically speaking not altogether legal. Pompey's first elected office was the consulship, which he held in 70 at the age of 36. Both facts should have disqualified him for the consulship, but Pompey had enough influence at that point that the rules were bent--a not altogether unheard-of feature of the Republican state! But these cases were all anomalies, and the Republican *cursus honorum* was not *supposed* to function like this. Only crises, civil wars, and other instabilities granted these individuals the ability to enter into high-level political careers earlier than they were traditionally--indeed, legally--allowed.",0
"Children's culture in history can be difficult to study. Adult observers tend to reflect more formal types of play and practice; memoirs reflect the haze of memory and *desired* memory; it's sometimes difficult to tell whether schoolbook exercises represent an individual experience or a standard trope. In this case, fortunately, a look at the broader culture of children's wargaming in early modern Europe can help answer the question.

Children's play often imitates the adult world for two reasons: first, children mirror what they see; second, adults encourage it to help prepare children for the future. The encouragement of martial play among boys (only boys) in the late Middle Ages and early modern era is a good illustration of these. We have stories from hagiography of the future saint who stepped aside from the mock swordfighting to play at being a priest dispensing the Eucharist, and proverbs observing that when children play at swords, war is in the air. From the adult encouragement angle, with the rise of a visible toymaking industry from the fourteenth century, hobby-horses and swords were among the most popular items.

Although toy guns start to appear from the later half of the sixteenth century, it's really in the seventeenth that guns displace the swords of knighthood as boys' wargame toy of choice. This is in conjunction with the encouraged realism of militaristic play. Memoirs of noblemen raised at a royal court, like Louis Henri with Louis XIV, stress the popularity of fully organized ""armies"" of boys marching and drilling under their ""commander,"" the king. And this was in no way limited to the elite of the elite. Army-style drills were not just a major component of the all-around education of boys at grammar and secondary school--school sponsorship of these types of exercise actually served as a *marketing appeal*. The Lincoln Grammar School stressed the realism of its schoolyard play in opposition to ""childish games"": *its* students ""exercised in all their military postures, and in assaults and
defenses.""  And from the 17th century, assaults and defenses meant guns.

With princes and high nobles, it seems that the guns and cannons would be smaller, boy-sized versions, but real. Louis Henri describes learning how to shoot with a harquebus that then-Prince Louis gave him, and mentions how he and the other boys (which is to say, their fathers) lavished the future king with many and diverse shooting weapons.

But moving down the social ladder to the urban middle classes (presumably), actual toy guns were *exceedingly* popular among boys. Actual records are scanty, of course, but archaeological digs and trawls of the Thames (where worn out or unneeded toys might be disposed of, or make their way there through a sewer or settlement) turn up toy guns and cannons as among the most numerous survivals. And for the most part, these toys appear to have been designed for mock shooting, including packing with powder and pellets to fire.

But children playing with toy guns in the manner they were designed to be used by adults is not yet the same as children making the leap themselves. For *that*, we turn to disciplinary and legal records of situations where boys improvised. One student of the Bodwin School participated in wargaming exercises not just by carrying a dummy gun. He actually got a candle, hollowed it out, and packed it with powder. This isn't a mock stick, but it's pretty close.

There are plenty of tragic events where children did not realize a gun was loaded, went to play with it, and killed a playmate or bystander by accident. But one case from turn of the 18th century London demonstrates the realism that permeates surviving accounts of boys' wargaming. A twelve-year-old apprentice came across his master's gun and wanted to play with it. But he wasn't content to point and cock and click, like plenty of boys did in the cases where the gun proved to be loaded. No, for this boy, playing with guns started with inserting the stick and packing it with powder.

Some caution is of course warranted. These aren't cases of boys pointing sticks at each other and making exploding noises with their mouths. And surviving sources, as I've discussed, are going to be biased towards more realistic types of games. Nevertheless, the combination of being so steeped in militaristic ""play"" and the general tendency to learn adulting by observation led our two improvisers to want ""toy guns"" that mirrored the real thing at every step except the very last. It seems likely, therefore, that just like we can all remember a range of ways we ""played pretend"" with ""guns"" when we were little, 17th and 18th century boys also scaled up towards imitating the real thing with whatever they had.

Further Reading:

* Lois Schwoerer, *Gun Culture in Early Modern England*
* Mark Motley, *Becoming a French Aristocrat: The Education of the Court Nobility, 1580-1715*

",0
"In 1861, the British Empire sent all its available forces to hold onto Canada when they were considering whether to intervene.. All 11,000 of them. The British forces in Canada started with about 26,000 counting British Regulars and qualified militia units. They never made it to 40,000 troops to defend Canada in case of a war with the United States. They had no additional troops available unless they withdrew a large number of troops from India. The British Admiralty conceded that the United States manufacturing capability meant a loss of the Great Lakes as the United States could build a dominant fleet there with no real Canadian response.

The South had a rather large uphill climb to convince Great Britain to join, but never gave any large inducement to Great Britain that would offset the loss of food shipments and the United States statement that any such intervention would constitute a declaration of War between the United States and Great Britain. (With the probable invasion of Canada to immediately follow).   The CSA instructions to their diplomats Yancy, Rust, and Mann in early 1861 specified the various terms of treaties they could sign with Great Britain. Specifically stated as being not open for negotiation was the CSA signing any of the anti-Slave Trade treaties. Otherwise, the CSA would sign any treaties currently in effect between the US and Great Britain.. ie, status quo. There was no gain for the British to offset the risk. The United States, however, signed the Lyons-Seward Treaty of 1862 as part of an effort to ease tensions after the Trent Affair. That was a very solid diplomatic move to show Great Britain gaining something in its interests.

Then again, the South's insistence that it could win the war also meant that Great Britain had no reason to intervene to insure that end.

Fundamentally, the launching of the Monitor ended any real expectation of Great Britain militarily intervening in the war. The British had concentrated on a deep sea ironclad navy, so all of the ironclads under construction were designed for deep waters. The US coastal waters were notoriously dangerous to ships of the type being built by the British navy. The HMS Warrior, for instance, drew 27 feet. The USS Monitor drew 11 feet. During the Battle of Hampton Roads, the USS Congress (draft 22 feet), USS Cumberland (draft 21 feet), and USS Minnesota (draft 23 feet 10 inches) all ran aground during battle. That was an operational environment where the Monitor had no issues or concerns about running aground.  The HMS Warrior was also not fully armored, so if it did run aground, the Monitor, or others of its class, could easily maneuver into a position where it could fire into the unarmored stern and destroy the Warrior's steering mechanism.

The British Navy steam powered ships all drew 22, or more, feet. Admiral Milne commander of the British West Indies Squadron repeatedly stated in letters that he needed shallow drafted vessels in case of a war with the United States. He had no expectation of being able to close and engage on US naval ships operating in coastal waters with the deep water ships in his fleet. Along those lines, he indicated that the United States could attack his naval base in Halifax and his coaling station and he had no ships that could defeat an ironclad like the Monitor. The Battle of Sinope showed what happened when wooden ships went against explosive shells, so the British navy believed that its wooden ships would lose in a battle against US ironclads.

There was a flurry of letters published by naval experts, like Cowper Coles the inventor of the naval gun turret, in various newspapers talking about the Battle of Hampton Roads. The general perspective in those letters was that the British ironclads could win in a battle against the Monitor in a battle in open seas.. and that no such battle would ever happen in open seas. Note, that this was not ""British ironclads \*would\* win"". That was the key. The real power of the British navy was the perception of how powerful it was. The last thing they wanted was to get into a battle and lose or draw. They had to win decisively. They were concerned that other countries - South American countries like Peru and Argentina specifically - would then start building navies and challenging the British navy regionally before the British navy could build out its fleet of ironclads.

The other real item of concern was that the Monitor was built in a very short time and for relatively little money. The United States would be launching these in numbers the British could not keep up with.

The British navy was perfectly willing to sink Chinese junks, or bombard a Japanese fortress, but giving industrialized countries a formula for beating the British navy was not something they were willing to do.

That ignores the other variable that Russia fully intended to make good on its Crimean War losses by going to war against Great Britain and France while they were bogged down in the US Civil War, should Great Britain and France decide to intervene.",0
"The Spartans generally avoided battle whenever they could, even when directly challenged. Most of their history (especially in the Classical period) is defined by their attempts to keep the number of major engagements to a minimum, and to make sure as few full Spartan citizens as possible were involved.

The Greeks saw battle as a form of uncontrollable, unpredictable mass violence that was best avoided if alternatives were available. They tended to favour indirect methods (ambushes, surprise attacks, sieges) over pitched battle in open ground. Their approach may best be summed up as ""be brave, but don't be stupid."" This was further amplified in the case of the Spartans by the fact that Spartiate numbers were always dwindling, and Spartan citizen lives were irreplaceable. To ensure that recklessness wouldn't get the better of their citizens, the Spartans are said to have taught cunning and stealth from infancy, and to award higher status to victories attained by deception than to those attained by brute force.

The Persian Wars are a good example of their attitude. Both at Thermopylai and at Plataiai, the Spartans preferred to take up a strong position and wait for the enemy to attack than to march out and fight in the open.",0
"But that is for some exceptional folks. What about the average townspeople? Here it can be hard to get a broad sense, even if we're talking about the earlier generation, so I'm going to focus less on just throwing one or two people at you and instead look at broader trends. If we don't look at the *people themselves*, but rather the locales, we can suss out some very good insight here, and more importantly, insight that extends into the mid-century period! 

Let's take a look at Dodge City for instance. It earned a reputation as a violent den of iniquity during the 1870s, and although researchers such as Dykstra have shown that to be overblown, with a spate of violence concentrated in its first year and then quickly brought to heel, that is both a reflection of more recent scholarship, and in any case with little change that perception for many. Certainly the city boosters *did not like* their reputation. From the late 19th century through the early 20th century, Dodge City did everything it could to bury that history. They wanted to paint a picture of a peaceful community of law abiding citizens and economic opportunity, especially with the decline of the cattle trade there. Boot Hill was replaced by a schoolhouse - although locals called it ""Boot Hill School"", and barely a sight in the city would hint at the alleged history. 

Things changed though, and ironically it was in *reaction* to the growing media image that Dodge City came around to embracing her past. After two generations or so of silence, by the late 1920s there started to be a groundswell of interest in historical 'preservation' (recreation being more accurate) of Dodge City with an interest in leaning into that violent (and mythical) history. Far enough in the past to no longer be a liability, it instead was a potential tourist magnet! The ""Boot Hill School"" had already closed and was now owned privately, but the land was bought back in 1927 with the intention of using it for public commemoration of the 'Old' Dodge. Three years later, the publication of Lake's hagiographic biography of Wyatt Earp, brought with it mythologizing of his ""taming"" of the town, and the claim that ""*By the time Wyatt Earp reached [Dodge], some seventy or eighty argumentative visitors had been buried with their footgear in place—Dodge had lost accurate count*"", a several-fold exaggeration to say the least.

While the residents might have welcomed the additional attention given their new interest in tourist dollars, at the same time the result of Lake's book was many older residents opening up about their own memories, which still pushed back against the epidemic of violence of the popular image. While we're looking at the 1930s, rather than the 1950s, this *does* get to the heart of your question in offering insight into what those who lived through the period, but might not have been the headliners, saw in the portrayal in media produced many decades later. 

On the whole you see a balancing act as the residents attempted to offer just enough so as to support the image of just a little danger back there in the 'old days', but at the same time emphasizing that it was *the past* and also that it wasn't *too* violent. Britz and Nichols offer a very useful summary in their work on popular recreation of the frontier, from which I'll quote rather than summarize myself:

>The local press printed letters from and interviews with former residents eager to tell their stories. In Dodge City many of them viewed its early history as that of an adolescent community sowing its wild oats before growing up to become a vastly different place. Most of the recollections came from men who took pride in giving their impressions from a young male perspective. Often they dismissed the drinking, fighting, and pranks as natural boyish behavior. One man recalled that the town consisted mostly of saloons and dance halls with stacks of buffalo hides filling the vacant lots, but “no shooting and no rough house.” A boarding house operator remembered the Earps and Masterson as having been well behaved when they lived at her place, while another person remembered Dodge as “a town full of cowboys singing civil war songs.”

>A few mentioned less harmless actions. The Reverend George Durham, a former town minister, described an 1886 midnight burial of a murdered gambler on Boot Hill by some cowboys. One man remembered coming to town on a quiet day and seeing only one “little gunfight in which a gent was killed.” Another claimed to have known the famous gunmen, including both Masterson and the Earps. He depicted them as fearless and determined lawmen, any of whom “would have charged hell itself for the man he wanted.” The former mayor’s wife had a different view of the gunfighters. She remembered her husband leveling a gun at Masterson and ordering him to leave town.

It is important here to remember that the 'Wyatt Earp' narrative was also one of Dodge City having a brief period of wildness, but Earp putting it on the right path. Interestingly, he had something of a bad reputation in Tombstone as a problem-causer more than a problem-solver, at least until Lake's biography changed things, but Dodge City had generally always liked him, and the new picture of him roughly fit into the balance that Dodge City wanted to cultivate for their new embrace of historical tourism. Jump ahead to 1952 though, and the debut of *Gunsmoke*, and the show's portrayal of a violent, sinful town went too far. When the show debuted, the city's Chamber of Commerce was flooded with mail from people asking how accurate the show was, and aside from just *annoying* people with the bother, the business leaders of the city were broadly concerned that the show would hurt the reputation of the city in playing up its violent past too far. 

But as the show grew in popularity, and then made the jump to television where it became the top show in the country, the city learned to accept, and even love this popular portrayal. TV ratings equated tourist dollars, after all. Add in other popular programs like *Wyatt Earp* and *Bat Masterson*, and city leaders came to realize that while they might be wary of the image being broadcast, in the end they were receiving nearly an hour of free advertising a week. By 1960, they credited the show with creating 750 jobs in the city, and encouraging some 150,000 visitors per year, and eventually there was a full embrace of the show as part of the city's identity. In 1961, they made the cowboy Stetson part of the police uniform, and by 1967 they were a major voice in successful lobbying of the network not to end the show when they almost pulled the plug in 1967.

Other famous Western towns cashed in similarly. Tombstone and Deadwood, both of which had been in economic slumps, were especially fans of the renewed interest and embraced their place as tourist destinations thanks to rising Western media, and also perhaps embraced the *violence* a bit more quickly too. Tombstone added in reenactments of the famed shootout, and Deadwood was soon doing daily shows of the murder of 'Wild Bill' and the subsequent trial and hanging of Jack McCall. Deadwood also offers a slight window into those unicorns! While he didn't offer his thoughts on TV shows of the time, as far as I know, P.A. Gushurst, a 98 year old resident of Deadwood, claimed to be the last living witness of the murder and would share his memories with the crowd every day.

So to circle back to where we started, I'm conscious I've only kind of answered your question, but I do hope that the above gets to the *heart* of what you are asking. The American West as seen in popular media is a myth, which often does little to reflect the reality of the West. How people have responded to that media has varied greatly. The direct participants themselves were generally fans of their immortalization, and being cast as the heroes of one of the central legends of American history. But for most people who lived through the period, they saw the violent mythos as a detriment. By the 1930s though, a reevaluation started to take place as now removed from the period itself, Western locales like Dodge City cautiously moved toward acceptance of that portrayal in popular media, a move which included recollections by residents of the 'old days', and both positive and negative feedback on how that history was portrayed. By the 1950s, there were few surviving participants to offer insight into the contemporary crop of media, but we can look at the larger trends of how the cities themselves chose to continue their engagement with those myths, some such as Deadwood and Tombstone embracing them wholeheartedly, and Dodge City a bit more cautious in how, but eventually coming around as well.

2/2",0
"Well yes, there were always reporters for the [*Novum Eburicum Tempora*.](https://www.reddit.com/r/AskHistorians/comments/751bk6/on_youtube_i_can_watch_extremely_detailed_videos/do3ehgr/)

But seriously, it is worth just looking at the source material for this. The reconstruction of Teutoberg is more complicated than most in part because batlefield archaeology has actually played a role in shaping our understanding of it, so instead I will use Cannae as one of the most famous battles of the ancient world and one that tends to feature in these sorts of battlefield reconstruction. From Polybius' *Histories* 3.113-116:

>Next day it was Terentius' turn to take the command, and just after sunrise he began to move his forces out of both camps. Crossing the river with those from the larger camp he at once put them in order of battle, drawing up those from the other camp next to them in the same line, the whole army facing south. He stationed the Roman cavalry close to the river on the right wing and the foot next to them in the same line, placing the maniples closer together than was formerly the usage and making the depth of each many times exceed its front. The allied horse he drew up on his left wing, and in front of the whole force at some  p281 distance he placed his light-armed troops. The whole army, including the allies, numbered about eighty thousand foot and rather more than six thousand horse. Hannibal at the same time sent his slingers and pikemen over the river and stationed them in front, and leading the rest of his forces out of camp he crossed the stream in two places and drew them up opposite the enemy. On his left close to the river he placed his Spanish and Celtic horse facing the Roman cavalry, next these half his heavy-armed Africans, then the Spanish and Celtic infantry, and after them the other half of the Africans, and finally, on his right wing, his Numidian horse. After thus drawing up his whole army in a straight line, he took the central companies of the Spaniards and Celts and advanced with them, keeping rest of them in contact with these companies, but gradually falling off, so as to produce a crescent-shaped formation, the line of the flanking companies growing thinner as it was prolonged, his object being to employ the Africans as a reserve force and to begin the action with the Spaniards and Celts.

>The Africans were armed in the Roman fashion, Hannibal having equipped them with the choicest of the arms captured in the previous battles. The shields of the Spaniards and Celts were very similar, but they swords were entirely different, those of the Spaniards thrusting with as deadly effect as they cut, but the Gaulish sword being only able to slash and requiring a long sweep to do so. As they were drawn up in alternate companies, the Gauls naked and the Spaniards in short tunics bordered with purple, their national dress, they presented a strange and impressive appearance. The Carthaginian cavalry numbered about ten thousand, and their infantry, including the Celts, did not much exceed forty thousand. The Roman right wing was under the command of Aemilius, the left under that of Terentius, and the centre under the Consuls of the previous year, Marcus Atilius and Gnaeus Servilius. Hasdrubal commanded the Carthaginian left, Hanno the right, and Hannibal himself with his brother Mago the centre. Since the Roman army, as I said, faced south and the Carthaginians north, they were neither of them inconvenienced by the rising sun.

>The advanced guards were the first to come into action, and at first when only the light infantry were engaged neither side had the advantage; but when the Spanish and Celtic horse on the left wing came into collision with the Roman cavalry, the struggle that ensued was truly barbaric; for there were none of the normal wheeling evolutions, but having once met they dismounted and fought man to man. The Carthaginians finally got the upper hand, killed most of the enemy in the mellay, all the Romans fighting with desperate bravery, and began to drive the rest along the river, cutting them down mercilessly, and it was now that the heavy infantry on each side took the place of the light-armed troops and met. For a time the Spaniards  p285 and Celts kept their ranks and struggled bravely with the Romans, but soon, borne down by the weight of the legions, they gave way and fell back, breaking up the crescent. The Roman maniples, pursuing them furiously, easily penetrated the enemy's front, since the Celts were deployed in a thin line while they themselves had crowded up from the wings to the centre where the fighting was going on. For the centres and wings did not come into action simultaneously, but the centres first, as the Celts were drawn up in a crescent and a long way in advance of their wings, the convex face of the crescent being turned towards the enemy. The Romans, however, following up the Celts and pressing on to the centre and that part of the enemy's line which was giving way, progressed so far that they now had the heavy-armed Africans on both of their flanks. Hereupon the Africans on the right wing facing to the left and then beginning from the right charged upon the enemy's flank, while those on the left faced to the right and dressing by the left, did the same, the situation itself indicating to them how to act. The consequence was that, as Hannibal had designed, the Romans, straying too far in pursuit of the Celts, were caught between the two divisions of the enemy, and they now no longer kept their compact formation but turned singly or in companies to deal with the enemy who was falling on their flanks.

>Aemilius, though he had been on the right wing from the outset and had taken part in the  p287 cavalry action, was still safe and sound; but wishing to act up to what he had said in his address to the troops, and to be present himself at the fighting, and seeing that the decision of the battle lay mainly with the legions, he rode along to the centre of the whole line, where he not only threw himself personally into the combat and exchanged blows with the enemy but kept cheering on and exhorting his men. Hannibal, who had been in this part of the field since the commencement of the battle, did likewise. The Numidians meanwhile on the right wing, attacking the cavalry opposite them on the Roman left, neither gained any great advantage nor suffered any serious loss owing to their peculiar mode of fighting, but they kept the enemy's cavalry out of action by drawing them off and attacking them from all sides at once. Hasdrubal, having by this time cut up very nearly all the enemy's cavalry by the river, came up from the left to help the Numidians, and now the Roman allied horse, seeing that they were going to be charged by him, broke and fled. Hasdrubal at this juncture appears to have acted with great skill and prudence; for in view of the fact that the Numidians were very numerous and most efficient and formidable when in pursuit of a flying foe he left them to deal with the Roman cavalry and led his squadrons on to where the infantry were engaged with the object of supporting the Africans. Attacking the Roman legions in the rear and delivering repeated charges at various points all at once, he raised the spirits of the Africans and cowed and dismayed the Romans. It was here that  Lucius Aemilius fell in the thick of the fight after receiving several dreadful wounds, and of him we may say that if there ever was a man who did his duty by his country both all through his life and in these last times, it was he. The Romans as long as they could turn and present a front on every side to the enemy, held out, but as the outer ranks continued to fall, and the rest were gradually huddled in and surrounded, they finally all were killed where they stood, among them Marcus and Gnaeus, the Consuls of the preceding year, who had borne themselves in the battle like brave men worthy of Rome. While this murderous combat was going on, the Numidians following up the flying cavalry killed most of them and unseated others. A few escaped to Venusia, among them being the Consul Gaius Terentius, who disgraced himself by his flight and in his tenure of office had been most unprofitable to his country.

From this account you can get most of the details you will see in videos like the one you posted: the weak Carthaginian center that fell back to allow the double envelopment of the Roman lines, the cavalry action on the sides, the screening by the skirmishers, etc. What you *don't* see is the conceptualization of the army into neat ""blocks"" that tend to dominate [popular representations of battles.](https://upload.wikimedia.org/wikipedia/commons/f/f2/Battle_cannae_destruction.png) The Romans did organize their army in a way that could sort of allow that as the armies were dvided into named legions (~5000), cohorts (~600) centuries (~80) and conturbium (~10),^1 but it is very rare to see that reflected in battle literature unless a subdivision of the army did something remarkable (such as Cato's detachment during the Battle of Thermopylae). That is largely a  modern convention, and someone more familiar with the development of modern military theory can probably comment on that better than I can.

^1 These numbers are *very* approximate and vary across time and space, and the *conturbium* may not have been thought of as a tactical unit.",0
"
I agree with the core of this comment but am not completely comfortable with the categorization of neoconservatism forming from the rejection of “hippie culture and peace activism.” While those were certainly seen as threats, the thrust of government repression and violence (domestically and internationally) was aimed at radical left organizations particularly those advocating for self determination and racial justice rooted in anti-capitalism and anti-imperialism. The Black Panthers (who were used as part of the racist fear-mongering campaigns Reagan ran to win elections) certainly wouldn’t be classified as “hippie culture” and though they desired peace their philosophies of armed self defense and anti colonialism probably don’t look like the “peace activism” in popular imagination. This extended to other racial groups arguing for self determination as well as white labor organizers (who were a much bigger threat than “hippies”).

Beyond that I wanted to add that neoconservatism is a response to the inherent weaknesses of neoliberalism, namely the extreme commodification of every aspect of society, including culture, resulting in obvious contradictions and feelings of alienation among people. Consumerism, dismantled social safety nets, gentrification, privatization, unemployment, etc. weaken senses of kinship and community and result in crises of identity.

In addition to fostering jingoism and militarism, which create a sense of identity via nationhood, neoconservatism also responded to alienation by fostering what we see as “conservative ideals.” White Evangelical Christianity is a cornerstone of the rise of neoconservatism. Neocons solve alienation by uniting people through a fear of the “other” and an idyllic view of tradition (no matter how violent or exclusionary).

While neoliberalism has to pretend to be the vanguard of progress, opportunity, and meritocracy while simultaneously dealing with the inequality and discrimination its policies yield, neoconservatism gets to create social cohesion without that pretense.

Essentially, one can argue neoconservatism is a reaction and not its own philosophy which I think adds to your point about it being wholly compatible with neoliberalism.",0
"Before Adolf Hitler, there were several historical figures who were often used as benchmarks for evil or tyranny. Here are a few examples:

1. Genghis Khan: The Mongol conqueror Genghis Khan (1162-1227) led one of the largest and most brutal empires in history. He and his Mongol armies were responsible for the death and suffering of millions of people during their conquests across Asia and Europe. 2.",1
"Thanks, /r/Iphikrates. I've experienced this firsthand when I posted a question (on a lark) about Parsis being involved in the (18th maybe 19th century) Opium trade. And I got a damn good answer with great sources and leads as well. So I'm aware of the magic of this sub. I'd like that to happen to a lot more subjects, that's all.",0
"In short, yes. ""America First"" was a common slogan for the KKK and for white supremacists dating back *at least* to the 1920s.

[Here's an image of women in the Klan marching in the 20s during their resurgence bearing a sign with the 'America First' slogan](https://imgur.com/a/7rywRPF)  

[And here's a commemorative coin dated to the ""Third Movement"" (1954-1969) bearing the slogans - The Invisible Empire, America First, and Preserve Racial Purity.](https://imgur.com/a/u95zMdD).

The coin is discussed by D.E. Birdsell in his 1977 book, Ku Klux Klan Tokens and he mentions that he was unable to find them being officially made by the Klan as an organization, but it was undoubtedly made by an individual adherent to the group. The phrase ""America First"" is very deeply ingrained into the KKK's ideology, as are many other phrases meant to be generally agreeable to the population that have dark undertones.    

A 1921 congressional hearing about the Klan's activities also has them directly stating ""[The Klan] stands for America first — first in thought, first in affections, and first in the galaxy of nations. The Stars and Stripes forever above all other and every kind of government in the whole world.""  

The idea of returning America to a mythical past is also integral to the Klan starting in the second movement.  Many far-right movements have it as their designated goal, Hitler writes about the end result of Nazism being a return of the superior germanic people to their agrarian and peaceful past without the corruption of inferior and insidious others.  The Klan, starting in the '20s were highly focused on spreading this growing idea of the antebellum South as a paradise that has since been corrupted by the aftermath of the Civil War and the black population entering spaces they hadn't been before. *The Birth of a Nation* was a film highlighting the loss of the peaceful South to a violent and savage black population, only for the people to be saved by the heroic KKK, this was right at the beginning of the Second Movement and set the tone for their messaging.  Overall, their message was successful and the idea of the idyllic antebellum South was very prominent in American culture for a very long time.  I don't know of any specific examples of the Klan pushing the exact message of ""Make America Great Again"", but they've certainly pushed messaging that might be worded differently but is essentially the same in spirit. ",0
"Best that I can find, there wasn't much call for this. A *little*, which I'll touch on, but it seems to have been quite muted. Even by the time that the statue had arrived in the US, it had shifted from the red-brown copper it had shown off in Paris to a pure brown as the patination had begun (Thanks to /u/ducatimechanic [here](https://www.reddit.com/r/AskHistorians/comments/8w6pie/the_statue_of_liberty_is_now_green_but_it_used_to/e1u0s6v/), this is a [guide](https://i.imgur.com/35o8587.gif) to visualize the change). It was fully expected that patination would occur, and for that matter, *important* that it would. Although Bartholdi apparently had hoped it would end up in a bronzed color, the statue staying its natural copper would be awful for it, structurally, and in restoration work that has happened, it has been important not to disturb the patina, because it is a layer of protection against the elements, which helps prevent further corrosion. Removing it, or worse, maintaining a continual cleaning schedule to prevent it at all, would just mean the copper would just continue to deteriorate away! This was of great concern in the '80s when a massive restoration effort was underway, especially when a cleaner being used on the interior seeped out and started to dissolve the patina in places, which workers had to quickly work to stop. 

Checking through what literature I can find, the real issue concerning restoration has always been focused around who pays for it. The Statue of Liberty proved to be a somewhat awkward gift that left the US Government in a quandary, unsure quite what to do with it, and as such it fell into an administrative quagmire of competing jurisdictions. The National Lighthouse Board, the US Army, and the American Committee (the group which had fund-raised for the pedestal and operated ferries to visit), for various reasons, all had some level of responsibility, but also all insisted that the other groups were the ones who should be maintaining it. A 1890 bill was defeated in Congress that would have made is a public park maintained by the Federal government, and it wasn't until another decade had passed that Congress agreed to a small one time expenditure of $62,800, which was enough to do some interior painting and install the elevator, but hardly enough to anything more substantial, or long-lasting. 

Now, to be sure, there was a good deal of public concern about this, and a number of newspapers, starting within a few years of the statue's arrival, decried this, but it doesn't seem to be about the color. As already noted, the patina plays an important role in preservation, and the only articles I could find from the time which makes reference to any calls to strip the patina are focused on explaining why it would be a bad idea! There were *some* people saying it should happen, but one author, writing in 1910, makes it fairly clear it was a proposal that was ignored:

>The patina is the rust on bronze, which, however, does not, like the rust on iron, corrode the metal. It is green in color, the exact shade depending on the amount of alloy used with the copper to form the bronze. It is formed by the statue gathering from the atmosphere carbon and sulpher, and in the formation of nitrate copper crystals on the surface.

>""In the case of the Statue of Liberty on Bedloe's Island"" said Mr. [Gutzon] Borglum, ""the continuous washing of the wain and the spray keeps it clean. I should have gone to Washington, if necessary, if the proposal to take away its patina had been likely to be adopted a year or so ago. People said then that it was eating into the metal, and that in certain placed the bronze was rent away. All I can say is, if there are any thin places they are the result of flaws in the original casting.""

So again, as this suggests, there was clearly *some people* calling for the patina to be stripped, but from the start, experts were making clear that it was not needed, and a bad idea. An earlier article, from 1903, is even further in praise of the green, noting that of burgeoning hue at that point:

>Kindly nature has been spinning for her a fine cobwebby outer raiment or verdanlique, deeper in one place, lighter in another. Snows and stinging hail, fogs and rainstorms have been gently removing the repulsive newness of her bronze and streaking cheek and uplifted arm, draperies and crown with tender shades or green such as the cleverest bronze rounders try to produce with chemicals on their new 'castings, but never quite succeed in simulating.

The author further warns against any voices which might ""*shriek wildly for scrubbing brush and Putz-powder""*, not just for the writer's aesthetic tastes, but also again for the protective covering the patina offers. The editorial also calls for the appointment of a 'keeper of public monuments', specifically to ensure ignorance doesn't result in the destruction of monuments, not just 'Lady Liberty', a position which seems to have at least somewhat come about by 1910, as that seems to be the role Gutzon Borglum was assisting in by then. Additionally, as before, this suggests the existence of those who desired a de-greening, but again that despite their existence, it was a call which experts pushed back against.

One related thing I did find, which I would also make mention of, was a call not for a stripping down to the natural copper color, but instead to paint it! An article in 1906 makes mention of such a proposal, and again the author instead favors the ""marvelous harmony of blended colors"" from the ""varying shades of light green, delicate white, and a subtle dash of yellow"" which by that point have almost entirely overtaken the bronzed hues. The proposal had been written up in several papers, but at least as presented in this article, but was clearly not in an advanced stage of planning. Capt. George Burnell, in command on the island for the War Department, mentioned to the author the ""bushels of letters"", almost all of which were aghast at the proposal - which additionally helps to point to the acceptance of the green quite quickly - and additionally talks to copper experts who find the idea not only unnecessary, but quite ludicrous. Unfortunately I was unable to find the original announcement, but it is certainly clear enough that it all came to naught.

Anyways though, to continue with this more general over view of restoration, during the Wilson Administration, a Public-Private partnership with the *World* newspaper saw another $30,000 from the government, but again, it was a one time expenditure, and this time for installing flood-lights. Lights which, lacking maintenance funds, had mostly ceased working by 1930.

This funding morass continued through the 1930s, when it finally was moved to the jurisdiction of the National Park Service, as part of a larger push under the FDR administration, in step with the New Deal, to give 'Lady Liberty' her due as a symbol of the country. Unable to get the funds solely on its own, between 1937 and 1941 the NPS worked in conjunction with the PWA and WPA to give the statue its first real overhaul, a $1.5 million dollar makeover which involved clearing the entire island of buildings to start fresh, building a seawall, a new dock, walkways, and most importantly much needed maintenance of the internal iron structure - although not quite extensive enough - which was becoming quite rusted at points, thanks to the salty seawater surrounding it. Much was done, but of course the outbreak of war in late 1941 brought it to a halt. Work would resume again in 1946 to wrap up work unfinished in '41, but over the next decade or so, the real focus with funding was on the creation of an American Museum of Immigration, which would eventually open in 1972 after a long and fraught 'battle'.

By 1981, even the work done by the NPS decades earlier was showing its age, and spurred by an analysis by French architects, a renewed focus on a *thorough* overhaul of the statue, especially in light of the upcoming centennial, began to be planned, perhaps most famously with American Express donating a penny for every purchase made with their card to the endeavor, resulting in $1.7 million dollars. Closed to the public in 1984 for the work - and as noted above, careful not to disturb the green! - the work was completed in time for the ""Liberty Weekend"" centennial celebration in 1986, with President Reagan turning on the floodlights the night of July 3rd, and the statue officially reopened on the 5th.

Berenson, Edward. ""*The Statue of Liberty: A Transatlantic Story*"" New Haven: Yale University Press, 2012

""BRONZE MONUMENTS IN PERIL."" *New York Times*, May 18, 1903. 

""How Shall ""Miss Liberty's"" Toilet be made?"" *New York Times*, Jul 29, 1906.

""SPRING CLEANING FOR CITY STATUES."" *New York Times*, Mar 03, 1910.

ProQuest News and Newspapers Database, Various Searches

ETA: A few more articles to flesh things out.
",0
"While the exchange of portraits to facilitate marriage negotiations was indeed common among medieval nobles, the notion of sending nudes, as we understand it today, is not supported by historical evidence. The concept of sending explicit or erotic images was not a part of medieval courtship practices. During the medieval period, portraits were typically created as a means of representing individuals for various purposes, including marriage alliances.",1
">Most of the sexual abuse charges had continually been unfounded.

Can you provide a source for this claim?  It's been a while since I've read the relevant literature, but from what I remember the Congressional hearings on the subject were extraordinarily brutal in what they revealed went on at the compound.",0
"Honestly, credit to Dr. Williams. He made this material fuckin stick. It's always awesome to take a class with somebody who is passionate about the material. One of the only instances where I felt like the professor assigning his own book was the genuine best choice and not a money grab. He told us the first day that he got paid a bunch of money to be there, and he got paid a bunch of money for his speaking engagements, so we should pirate his book cause he was just there to educate and he already got paid for us. 10/10, radiates fuck the establishment energy while also working *so well* within it to educate others and continue his own research and work.",0
"To add a bit to this excellent answer, there was a feeling among early Americans of living in ""Columbia,"" the new land discovered by Columbus. Oct 12, 1792 was the first recorded celebration in the US which was celebrating Columbus' 300th anniversary of landing in the New World. It was organized by The Society of St. Tammany which would later become very influential in New York politics. Ironically, the Tammany name was sourced from a Delaware Chief and the Society saw themselves as preserving art and natural history of America and opposing aristocratic societies springing up (societies of this and that were all the rage and popping up everywhere at that time). Several years earlier (in 1786, the same year The Society of St Tammany was formed) the South Carolina Senate had voted to create a new capital. The name was decided in an 11-7 vote: ""Columbia"" would beat ""Washington"" as the name. In the early 1800s a similar action would happen in Ohio, the new town of Columbus swallowing the already existing Franklinton (which is still a community within Columbus, OH). Our new capital district would likewise honor the land by its pre-state name. Americans traditionally always saw him, and our shared connection to the land of Columbia, in a positive light.

It's also noteworthy that Columbus really wasn't that disgraced in the big picture. He was arrested and put in chains but largely for the actions of others, namely his brother. He had established a brutal approach but was absent himself when the worst of the violence occured, returning to a destroyed fort and then attempting to reestablish order. This was certainly a result of him establishing a system of oppression before leaving and he was officially in charge while the brutality was at its worst. At this point ships had already been dispatched to investigate what was happening with Hispaniola. *Note: this is a historical perspective and accordingly must exclude any personal opinions or presentation of ""modern morals"". I am not attempting to downplay his actions and their impacts on indigenous peoples by any means.*

After being arrested, Colombus himself refused to let the chains be removed on the voyage back. It was then he wrote a letter proclaiming his innocence which was delivered when he arrived in Spain. The result was the decision that he was an awful governor but excellent navigator and explorer, having likely discovered a new continent (based on the river delta he had seen which was in a fact the continent of S. America), helped fuel the spread of christianity (which intertwined heavily with his personal beliefs and later voyages), and had served the Queen and God dutifully, if not in action than certainly at least in spirit/intent. His argument and previous accomplishments were enough to earn his release and funding for a fourth voyage to search for the wealth of gold he was *obviously* so close to finding (which, of course, he never found). He wasn't quite absolved; he was prohibited from returning to Hispaniola and ordered only to engage in exploration for gold and the straight to India. His funding was also a total of four ships, while his replacement governor sailed with about 30. But it did enough that his name wouldn't be associated with the troublesome years of his governance but rather his earlier years as an explorer moving forward - the explorer was celebrated while the governor overlooked or forgotton (*The Christopher Columbus Encyclopedia*, Silvio Bedini).",0
"In a more recent context, the historian Kevin Gaines's 2006 book on African-Americans returning to Ghana during and after the civil rights movement deals with the reactions of West Africans. Saidiya Hartman's Lose Your Mother and Paulla Ebron's Performing Africa also consider West African responses to African-American heritage tourism and ""return to Africa"" narratives in the last fifty years--and there are other works by anthropologists and historians that do so. (I'll also recommend the novelist George Lamming's Pleasures of Exile, which includes some commentaries on being a West Indian resident for a time in Ghana and his observations about other Black expatriates and exiles.) Additionally, the historian James Campbell's Middle Passages is a great comprehensive history of African-American returns to West and Central Africa and various African reactions to those returns over time. 

I think in the recent era, one common summary of West African reactions to returns out of the African diaspora might be that they have been a mix of bemusement, appreciation, bewilderment, gratitude and occasional mild irritation. Since the 19th Century, some African-American returnees have been unsettled or surprised by the fact that they have been read as ""foreigners"", even as ""white foreigners"", rather than kin (Langston Hughes talks about this in his autobiography, and Hartman much more recently struggled to process the same reaction). But that is where all those feelings among West Africans come together--a pleasure that there are these travellers who so intensely desire a feeling of connection and who are often so flattering in the way they express that desire coupled with a modest amusement at how little they often seem to know about the place they've come to--and the irritation that can follow when the new arrivals clumsily elbow local people out of the way or try to tell them what their history and cultures really are or ought to be. (The scholar Henry Louis Gates Jr's first travel program on his African journeys created some modest annoyance for this reason, most potently when he picks a fight with one of his West African hosts about the legacy of the slave trade.) 

I think as Black heritage tourism has become more economically important, the sense of mild puzzlement at diasporic returnees has faded and more familiarity with returnees and their interests has developed. More recently, too, I think West African artists, scholars, writers, performers, etc. in Ghana, Senegal, Nigeria, etc. have developed more confidence in partnerships with diasporic collaborators, which has brought short-term and long-term returnees a closer sense of belonging and connection.",0
"In western culture, there is loads of imagery from the Bible--Babylon, Jezebel, serpents, the colors red and black. I want to focus, however, on two broader themes.

*Part I: Monsters*

We live in an age where you can buy a [cuddly Cthulhu](https://imgur.com/a/nFwc8jw)--a [cuddly *pink* Cthulhu](https://imgur.com/a/QEYuhuD)--a [cuddly Cthulhu *with a children's book*](https://imgur.com/a/DHrhSXb)--so I think we tend to lose sight of the, well, monstrosity of creature-type monsters (as opposed to the human variety).

But monsters were the code that the authors of apocalyptic tracts used to portray enemies and oppressors:

> Four great beasts, each different from the others, came up out of the sea.

> ...There before me was a fourth beast—terrifying and frightening and very powerful. It had large iron teeth; it crushed and devoured its victims and trampled underfoot whatever was left. It was different from all the former beasts, and it had ten horns.

> While I was thinking about the horns, there before me was another horn, a little one, which came up among them; and three of the first horns were uprooted before it. This horn had eyes like the eyes of a human being and a mouth that spoke boastfully.

> ...The fourth beast is a fourth kingdom that will appear on earth. It will be different from all the other kingdoms and will devour the whole earth, trampling it down and crushing it.

In Revelation, John does give us four ordinary people on horses. But he picks up the beast theme of Daniel 7 to describe Rome, this time:

> I saw a beast coming out of the sea. It had ten horns and seven heads, with ten crowns on its horns, and on each head a blasphemous name.

And more to the point for present purposes, the beast in Revelation doesn't just represent evil. It's also used to make otherwise-mundane imagery become evil:

> There I saw a woman sitting on a scarlet beast that was covered with blasphemous names and had seven heads and ten horns. The woman was dressed in purple and scarlet, and was glittering with gold, precious stones and pearls.

(This is, of course, also a case of Babylon being used to highlight Rome as evil.)

Medieval iconography is also keen on monsters illustrating evil. We're more used to the [Paradise Lost](https://imgur.com/a/EYC3RVj) Satan and his heirs (looking at you, *Lucifer*). For medieval Christians, demons and the devil were monsters. The nuns at the Rupertsberg [illuminated the Antichrist](https://www.medievalists.net/wp-content/uploads/2013/01/Hildegard_Bingen_Scivias_214v_Five_Ages_Antichrist.jpg), for example--the Antichrist, who most legends would make out to be a human being, yeah? We're also got plenty of [devilish monsters](http://ica.themorgan.org/icaimages/1/m153.019ra.jpg) on hand.

But devil iconography brings me to the second topic I want to look at--one I am not happy about, at all.

*Part 2: ""The Jews""*

Yes, the comparison here is to the 21st century use of [Nazi](https://youtu.be/EC9Pkz9dPMU?t=185) [imagery](https://www.youtube.com/watch?v=yys5iioLUNw), so this doubly or triply sucks.

But it's another case of associating something with The Jews to mark it as bad.

To be clear, I don't mean Jewish people/people who happen to be Jewish. (No, they just get to suffer the consequences.) I mean ""The Jews"": the racist European-American Christian *invention* of a cabal that...well, whatever they're doing, it's evil.

First, iconography.

To the 12th century, illustrations of Jewish men generally [denote them by hat](https://upload.wikimedia.org/wikipedia/commons/a/ad/GermanJews1.jpg) (from Herrad von Hohenburg's *Hortus deliciarum*, which sadly means Garden of Delights instead of Garden of Delicious Things). But as ideas of a Jewish ""race"" started to coalesce, Christian artists evolved the stereotype of the [""Jewish"" nose](https://imgur.com/a/IGCoELo).

By the late Middle Ages, [Satan and demons are depicted with ""Jewish"" iconographical features](https://imgur.com/a/YIUGo2J). Yes, mixed in with monstrosity. Association with The Jews makes the *devil* appear more evil--not the other way around.

Early modern witch hysteria gives us another example. In point of fact, witchcraft is usually an accusation levelled against Christians. But that doesn't stop artists from [adopting ""Jewish"" features to signal that a person is a witch](https://www.medievalists.net/wp-content/uploads/2012/08/Pagan-Traces-in-Medieval-and-Early-Modern-European-Witch-beliefs.jpg).

Second, conspiracy.

20th century American conservatism (20-year-rule, people) has a very, very strong anti-internationalist streak that draws on anti-Semitic associations at nearly every turn. The slight scuzziness you probably sense around the edges (or not so edges) of the phrase ""international banker,"" for example, doesn't go back to Banker being the profession that gives you the most money setting out in *Oregon Trail II*. 

For paleoconservatives' long-running fear about the US losing national sovereignty (no, really), they adopted The Jews as a quick and dirty way to express *just how evil* the people threatening to take over were. (The association of Jews and banking also goes back to the Middle Ages).

And in the early 20th century, at least, you had Jewish bankers...and then also, you had--in the words of conservative evangelical and fundamentalist Christians--""Jewish Communists."" Because ""The Jews"" were even used to make Communism seem more evil.

This didn't stop with World War II, either. Henry Ford might have distributed copies of *Protocols of the Elders of Zion* in the 1920s, but Mary Davison wrote *Profound Revolution* in 1960. And in the abominable *Left Behind* books, not only do they even turn their Rockefeller expy into a Jewish man, but they make him responsible not just for the new world order, but for their Antichrist.

Obviously there's a lot more to be said about the history of anti-Semitism, and Christian fantasies of Jewish association with evil and the devil. I've tried to concentrate here on examples of Jewish stereotypes used to make other things seem more evil, rather than just negative stereotypes in and of themselves.",0
"You mention that a number of former slaves became elected officials before the passage of Jim Crow laws.  Did these former slaves do anything significant with their public office?  Or were they too hindered by the exisiting white powers to get anything done?  

Also, what was the background of a typical former slave who was eleceted?  Did they do work while a slave that made them particularly suited for goverment jobs?  Maybe something like being an accountant for their master?",0
"As Hildegard of Bingen, author and abbess, preached in Cologne around 1163:

> [These people] will come with ashen faces, and, clothing themselves in sanctity, will ally with the great lords...They will walk about in black robes, with proper tonsure, and will appear to men as serene and peaceful in all their ways. Moreover, they do not love greed, and they do not have money.

> [...] And in their secret selves, they hold abstinence as so great a virtue that they can scarcely be reproved...Thus they appear in public as if they were filled with sanctity, and say with mocking words: 'Before now, all other people who wanted to remain chaste burned themselves up like roast fish. But no pollution of the flesh or lust dares to touch us, because we are saintly and filled with the Holy Spirit.' *(trans. Baird & Ehrman)*

Hildegard wrote text after text extolling those primary Christian ascetic virtues: charity, poverty, chastity. She lived during a time when religious leaders getting the ear of secular princes was a really big deal. (Medieval German emperors had a habit of appointing their own popes if they didn't like the one in Rome.) When Hildegard says ""proper tonsure,"" she means that this group of people conform in all ways to the appearance and values of Christian clergy. What she describes is...pretty much how she advises people to act.

...but...

> But the devil is within these men, revealing himself to them in the obscuring lightning, just as he was at the beginning of the world before his fall. And he makes himself like the prophets.

> ...When the full gamut of this error has been run, these people will everywhere persecute and exile the teachers and the wise ones who remain true to the [Christian] faith.

> Wake up!

Which is one very big and demonic asterisk.

Hildegard is acting here as apocalyptic prophet (how she was most famous in her own day), warning her audience to shape up or God would call down his wrath. This and some other medieval prophecies contain the most basic elements of dystopia and dystopian literature:

* a literary depiction of an ideal society realized but corrupted
* mixed with the notion of human improvement
* packaged as a warning to readers and listenerers

But if Hildegard's letter to the Cologne clergy seems kind of like an Epsilon Minus Semi-Dystopia, there's good reason. Our stereotypes of dystopia are linked to very modern developments: scientific engineering of human biology (eugenics, genetics, cybernetics) and societal engineering of human behavior (collectivism, behavior control). Some Cold War-era political philosophy even holds that dystopia is necessarily a police state: that the drive for societal and human perfection (utopia) will always end in mass, punishment-based control over behavior.

So I think of Hildegard's Cologne sermon and related medieval prophecies as the pre-history of dystopia, one defined above all by its religious nature, frustratingly insufficient worldbuilding, and message that ultimately, God is in control.

Thomas More usually gets credit for popularizing the genre of utopian literature in 1516, and 1600s England in particular produced a *lot* of literary utopias. But while plenty of these are written in a satirical vein, scholars trace the beginnings of the satire *of* utopia, as a concept, only to 1726 in Jonathan Swift's *Gulliver's Travels.* The key target here isn't politics so much as foolish human faith in science (a la the utopian writings of Francis Bacon):

> *The author permitted to see the grand academy of Lagado. The academy largely described. The arts wherein the professors employ themselves.*

> [The professor]'s employment, from his first coming into the academy, was an operation to reduce human excrement to its original food, by separating the several parts, removing the tincture which it receives from the gall, making the odour exhale, and scumming off the saliva.

Even with occasional bursts of satire in the realm of fiction plus some nonfictional treatises, people were still generally writing about idealized societies as the *ideal* through most of the 1700s and 1800s. But in the late 19th century, Gregory Claeys argues, two new hot-ticket strategies for achieving human perfection came to dominate literary portrayals of envisioned societies: eugenics and socialism. This could still be in an idealist mode, but often it was ambivalent: Ellis James Davis' *Pyrna: A Commune, or, Under the Ice* (1875), for example, presents a bubble society of perfect community, charity, and harmony that is achieved in part by euthanasia of unfit children. And sometimes it could be outright dystopian, as in the very subtly titled *Red England: A Tale of the Socialist Horror*, which combines fears of socialism with eugenics-related fears by stripping away citizens' control over their relationships and parenting

From the point of view of history of dystopian ideas, I think it's worth calling attention to the importance of child-raising and family issues in late 19th/early 20th century dystopian writing, which tends to play less of a role in the dystopian writing we read today (new or not). We see a little bit of this at the fringes of books like *Brave New World* (1932) with the Bokanovsky process and the scene with the Delta toddlers, but it's more of a peripheral horror-by-extrapolation. The world of a modern dystopian classic like *The Giver* (1993) is built on what looks like cozy, happily-chosen, functional families; in *Hunger Games*, family is what the state *can't* control.

So essentially, the length of the history of ""dystopia"" as an idea is as long as you want it to be--depending on how you define the word. Hildegard's sermons, the angelic pope prophecies, the prophecies of the last world emperor: later medieval Christianity envisioned more than its share of leaders who appeared virtuous but ended up as puppets through which Satan ruined the world. Or maybe the drive to perfect human existence through biology and behavior must always and only end in a surveillance and punishment-ruled totalitarian state. Whether or not we have always been at war with utopia is up to you to decide.",0
"Before Dante's Inferno, the concept of Hell varied across different cultures and religious beliefs. In ancient Mesopotamia, for example, the Sumerians believed in an underworld called Kur, which was a dark and gloomy place where the dead resided. The ancient Egyptians believed in a complex afterlife system, with different realms and judgment ceremonies determining one's fate. In Greco-Roman mythology, the concept of the Underworld was prevalent.",1
"  Another reason for Gillingham doubting Roger’s account is that a different chronicler credits Richard’s death to a completely different crossbowman. This account comes from another English historian: Bernard Itier. Bernard was based in the great abbey of St. Martial in Limoges from 1199 and has librarian there from 1204 until his death in 1225. This places him much closer to the centre of the action than Roger, even if he only took up office the year Richard died. Chalus Castle is only a few miles from Limoges and Bernard as librarian would have had access to all sorts of regional chronicles, annals, and other accounts to bolster his own knowledge. Somewhat confusingly, Bernard makes no mention of Richard’s death at Chalus in his entry for 1199 in the *St. Martial Chronicle*, only including the (presumed) name of the king along with a series of others who had died that year – no details or clear identifiers included. However, Bernard was also in the habit of adding notes to other books in his collection and it is in an addendum of sorts to the copy of the chronicle of Geoffrey de Vigeois that we find Bernard’s more detailed account of Richard’s death. In this account, in which Bernard identifies himself as the author, he provides a fairly standard description of how Richard I was shot by a crossbow at Chalus Castle and died later of infection. The interesting detail is he names the crossbowman who killed the king as one Peter Basil, not Bertrannus de Gurdon. Unfortunately, Bernard does not provide any further detail about Peter Basil or his eventual fate, but given Bernard’s close proximity to the location of Richard’s death it seems more likely that he was better informed about King Richard’s killer than Roger was. 

The final account I want to mention is that of Ralph of Coggeshall. Ralph’s account is arguably the most detailed version of the siege of Chalus and the death of King Richard I. According to Gillingham it was probably written before 1202 and the author claims to have vividly remembered meeting Richard I earlier in his life. Ralph tells us what time of day it was when Richard was shot, it was after lunch, as well as many specific details about the castle and the siege in general. Ralph’s account includes entirely leaves out the dramatic confrontation between Richard and his killer found in Roger. In fact, Ralph claims that the defenders did not even know Richard was personally among the besieging forces and had no reason to expect that the man they shot was the king. Making this famous regicide almost accidental (or as accidental as deliberately shooting a stranger can be). Ralph’s account includes no details about whoever shot Richard. Ralph does include plenty of gruesome detail about Richard’s wound. He describes how some of the iron from the crossbow bolt got embedded in Richard’s shoulder and his surgeons bled him and then tried to remove the last bits of iron but couldn’t find them. We are told of how the wound became infected and gangrenous and the various treatments Richard’s doctor tried in an attempt to battle an infection they had no way of curing. Finally, Ralph describes Richard’s taking of final confession at the hands of Milo, abbot of Le Pin, who also delivered Extreme Unction and closed the dead king’s eyes and mouth. Ralph’s account best captures the horror of watching the king die slowly and painfully while medieval medicine could do nothing to save him. It is the most emotional and intimate account of Richard’s death even if it is told in an impassionate tone.

So this is the most famous death by crossbow and we have major disagreements in our contemporary sources about who shot him, their possible motives for doing so (revenge or just being a good guard on watch), and the details of Richard’s final hours. Given all this confusion you can possibly see why even if Richard was killed by some kind of professional assassin we might not even know about it (although for the record he almost certainly wasn’t). 

For a case of more deliberate attempted assassination and regicide with a crossbow let us turn to the reign of Richard’s great-grandfather: Henry I. In February 1119, the Norman noble Eustace de Breteuil petitioned to King Henry I – King of England and Duke of Normandy – for the rights to the castle of Ivry – conveniently located in Normandy. Eustace made implied threats that he would join a rebellion against the king if he was not appeased, but Henry was unwilling to give him the castle. Instead, to appease Eustace King Henry arranged a hostage swap between Eustace and the current possessor of the castle, a Ralph-Harenc.

Eustace received Ralph-Harenc’s son while he in turn gave over his two daughters. Eustace, possibly under the influence of Amaury de Montfort – one of the nobles in rebellion against Henry I - blinded Ralph-Harenc’s son and sent him back to his father. The father, rightfully appalled and enraged by this mistreatment, petitioned to Henry I to allow him to mete out his own punishment on Eustace’s daughters. You see, the two daughters were Henry I’s granddaughters as Eustace was married to one of Henry’s illegitimate daughters so Ralph-Harenc wanted to be sure he wouldn’t displease the king by his action. Despite their blood relation, Henry I granted his permission and Ralph-Harenc had the two young women blinded and their noses cut off – a truly horrifying punishment for which they had done nothing to deserve. They were then returned to their parents who were understandably upset. 

Eustace proceeded to close his castles to the king in an act of rebellion and sent his wife, Juliana, along with a small force to defend the town of Breteuil should Henry I attack it. Henry I in response moved his own forces towards Breteuil and much to Juliana’s inconvenience the people of the town opened the gates to their duke. Juliana sealed herself into the citadel of the town and tried to await reinforcements. 

We are told by the chronicler Orderic Vitalis that during this time the “treacherous” daughter sought a conference with her father the king and when they met she fired upon him with a crossbow “but through God’s protection he escaped harm.” The details just say that she drew her crossbow and shot at him but it is not clear from the text what their meeting was. It is somewhat implied by the fact that Juliana was not captured afterward that they were not meeting in an enclosed space – and given that she had hidden the crossbow from sight (no small feat) the most likely explanation is that she met him from atop the battlements of her citadel. 

In any case, with her assassination attempt a failure and Henry I ordering his men to destroy the gate of the citadel Juliana was forced to flee. She lowered herself from the walls of the citadel – falling we are told into the ice-water filled moat – and escaped to reunite with her husband. After briefly having their lands revoked by the king due to their rebellion, Eustace and Juliana were pardoned by the king and allowed back into his good graces in the Autumn of that year. Eustace eventually died in 1136, the year after Henry I, and Juliana became a nun after his death.

This wasn’t as major an event as the death of Richard so we have far fewer accounts of it, which is simpler in one way but also means we’re less likely to spot places where Orderic might have bent the truth so bear that in mind. I think this case underpins part of the problem with using a crossbow as an assassin’s weapon: it was hard to get close enough to get a clean shot at an important person if you weren’t the person designated to be at the meeting. There’s no way Juliana was the best candidate for taking that shot, even if she almost certainly had some experience shooting crossbows to even consider this plan she wouldn’t have had professional level experience – she had other jobs to be doing than just practice shooting a crossbow just in case she ever needed to assassinate someone. However, Juliana was the person who could get into position to take the shot so it had to be her. Also, the time it would have taken her to pull the crossbow out (I presume she was on the battlements looking down on Henry since that’s the only way she could have hidden the weapon and also it explains why she wasn’t immediately captured) would have given Henry time to try and get away or behind someone.",0
"The initial expansion of the Mongols was ferocious and ferociously effective. Chroniclers from one end of Eurasia to the other record staggering death totals, the impossible numbers aiming for emphasis rather than accuracy. When Europeans sacked a city, it rebuilt its walls to be stronger the next time. When the Mongols sacked Samarkand, the survivors gave up and built a new city next to the ruins; when the Mongols sacked Otrar and Nishapur, they were never rebuilt at all. Sichuan province in China counted 2,500,000 households in its 1228 tax registration; in 1282, the Yuan dynasty's first census tallied only 500,000. The population loss across from Kiev to China was so precipitous and so slow to recover that the reforestation of newly empty land *lowered atmospheric CO2*.

An obvious reason for the scale of devastation of human lives was simply the size of the imperial conquest--swallowing nearly ten million square miles and countless peoples, polities, and empires. But smaller conquests, even by nomadic warriors like the initial Turks in the Near East, did not have a to-scale destructive impact. Historians have pointed to a number of factors contributing to the Mongols having the opportunity to exercise their ruthlessness so widely.

**Experience**

Steppe nomads like the Mongols didn't life in self-sufficient isolation; they traded and raided on the fringes of sedentary society. As John Fletcher argues, this meant they benefited from weak governments that ""let"" them get away with more. Stronger, more stable governments could muster up resistance or defense--which, rather than keeping the Mongols away, made them *fight* for what they wanted instead of negotiating or quietly stealing. Generations of this helped build a *skilled* warrior culture with knowledge of tactics against a strong government. And in their conquests, ""strength"" was not always a quality of their targets.

**Drafting**

One of the Mongols' most notable features was the forced incorporation of conquered peoples not just into an empire, but into the imperial army. New soldiers from places that had resisted would be dispersed to different thousands to prevent revolts, but serve the Mongols they still would. A solid top-down command hierarchy helped enforce standards at combat and strategic levels. And those who could not serve in the military might nevertheless serve military purposes. If they were lucky, they were the Chinese artisans instructing and constructing siege engines and incendiary artillery. If they were unlucky, they were prisoners-of-war brought to Urgench and Samarkand, and forced to haul stones underneath hails of defenders' arrows to the ditches around the city--with their bodies being just as useful to the Mongols for filling in the moat.

**Tactics**

The Mongols were excellent as individual soldiers and excellent as an army. One of their most notable features was a stunning success rate at triumphing over besieged cities--in the Near East and Europe, generally considered a rather difficult task. Sometimes this came through trickery--approaching the city at first in small parties, basically pretending to be the medieval equivalent of cattle rustlers squirreling away with a city's herds...only to draw pursuers, group by group, into a death trap. Other times, with the ""aid"" of prisoners-of-war as human shields, they simply drove directly at city walls to scale them. Siege technology acquired from China, as mentioned earlier, was used as well, but it doesn't seem to have been the crux of Mongol strategy.

**Speed**

This is the most interesting factor to me, because it helps explain how the utterly ruthless conquering Mongols could transform into *rulers*. In contrast to the example of the Turks I used above, Mongol expansion under Genghis Khan was *lightspeed*. Fletcher and David Morgan have argued that the Mongols adapted to the demands and cost/benefit analyses of urban/sedentary civilization...but this is a process that understandably takes time, and the devastating initial conquests were all over by then. So mores like ""leave the city intact because it's good for your trading network"" and ""peasants are useful as farmers to supply the battalion you have staged at the fortress with food"" were not yet developed as the Mongols stormed through Persia, China, Ruthenia. They were used to their old world divided into ""plunder"" and ""obstacles to plunder.""

**What About Numbers?**

Now, the question of statistics has also come up, and I've even offered one set in the introduction--the disappearance of *two million* households from the tax registration in Sichuan. But that already should give some pause before we rubber-stamp murdered by Mongols. We know from Central Asian chroniclers that the advancing Mongols produced hordes of very un-golden refugees--some fleeing in advance, some escaping with their lives. Perhaps this was also the case for some of those people. Or disease, or deportation (another strategy of the Mongols as rulers), or *it's the Middle Ages and numbers are weird.*

But medieval demographics being what they are--an estimate at *best*, and usually more like a guess--I wouldn't want to put a number on any death toll, and I don't see Fletcher, Morgan, Hugh Kennedy, Peter Jackson, Thomas Allsen, etc. going anywhere near one. In the end, it doesn't really tell us anything we don't already know about the Mongol conquest. And with the Mongols, we don't have modern political needs like the calculation of reparations owed by a still-extant government to the remnant nations they tried to obliterate, or an insistence against nationalists/white supremacists that a particular genocide occurred. 

Instead, we can tally up the awe-inspiring reach of the Mongol war machine--people *on horseback, armed with arrows* ravaging from Hungary to Acre to China. And we can read the terror in their targets' chronicles and letters through the endless tallying of atrocities real and rumored--catapulting the bodies of pestilence victims into the city walls to sicken and kill everyone left alive; rolling up a ruler in carpet and trampling him to death; wiping out entire cities because one general's son happened to be killed in battle; executing all the male inhabitants of Balkh to terrify Merv into surrendering without a fight...only to kill its women and children along with the men.

That, in the end, is a more useful metric of the Mongols' impact than a guesswork forty million or any figure. And it's far more fruitful as a basis to study why on Earth this monstrous, unstoppable behemoth was so heavily courted by Christians and Muslims as a potential ally, and how settled Mongol rule could eventually be such an important catalyst of intercontinental trade and travel.",0
"Sure! When I teach it, I usually start with the basic observation that *everything* - people, places, ideas, events, objects, languages, whatever - has a history. That includes concepts like “gender” and “sexuality”. But, just like many things from only a few decades ago seem different to us when we look at them in the past, so too do those subjects. 

I then use as an example Foucault’s observation that the idea of “homosexuality” is recent and, in some contexts, therefore doesn’t do justice to prior eras. I use as examples of this: Ancient Greek pederasty, Florentine sodomy courts, the (gender-bending) fashion of the 17th century, etc. The point of those examples isn’t so much to do a deep-dive as it is to just give students a sense of what queerness might be in the past and how it may not be described well within our modern conceptual schema for things. 

After that, I like to point out that sometimes our modern categories *could* work - say in the case of Angelo Poliziano and Pico della Mirandola - but that we need to be very intentional about what we mean, which is often not *so much* about a sexual practice as it is a complex relationship or identity. I point out that, in their case, it may even be more important to notice the question - “did they have a queer relationship?” - than find the answer, since the question marks where we ourselves (or others) are perceiving something queer. Sometimes we find a love whose name we dare not, or cannot, speak.

I then point out that, in some sense, history itself is a really queer enterprise. It’s an art that depends on our ability to form relationships with people of all sexual and gender preferences *across* gaps of time that often enough radically separate us. Whether we take this literally (I *actually* connect) or metaphorically (I *feel* connected but know that’s silly), it’s an experience we ALL have - not only in history, but in literature. It’s the feeling of “wow, this is old!” That happens, eg, when you visit a historical site.

Noting, then, that there’s A LOT more queer about history than people normally think, I conclude by suggesting that part of our role - if we take the job ethically - is to *actually* do justice for everything in the past that we consider. This means connecting with it, respecting it for what it is/does, and carrying those resonances forward in what we write. We have to do a good job of registering the queerness of the past in its own queer terms. That may mean being willing to see connections where there appear to be gaps (could they have been LGBTQ+?) OR being willing to let go of our own perspectives and vocabulary to better understand theirs. That process is important for treating the people of the past with the dignity they merit; not being willing to engage in it (whether out of a refusal to see the queerness OR an insistence that we find a genealogy for queers today) is the *real* act of erasure, since it opts to overwrite the individuals’ own voices, perspectives, and contexts rather than illuminate them.",0
"The sources on him are pretty scanty but the Greeks would erect monuments to victors inscribed with their victories, so it is likely that Pausanius, one of our sources for Leonidas, saw a monument honoring him. His account comes from a travel book describing the sights of Greece and he talks about Leonidas specifically in a section on Olympia ([6.13.4](http://www.theoi.com/Text/Pausanias6A.html)). 

As for fairness it would be crazy to think an athlete would blow the race. The Greeks took the Olympics (and the other panhellenic games) very seriously. They even had mechanisms for preventing false starts. Isthmia had a mechanism that held a rod at waist-height across all the runners and dropped the rod to start the race. If a runner was too eager, the rod would land on his foot.",0
"Firstly, schizophrenia is a confusing disorder to define, and not everybody reading this will have a very clear understanding of what it is. There's been enough things in the media, I think, so that people know the difference between schizophrenia and 'multiple personality disorder', but schizophrenia as currently defined in a diagnostic manual like the DSM-V has quite a wide array of symptoms; some may be surprised to discover that it is perfectly possible to have schizophrenia according to the DSM-V and *not* exhibit delusions and hallucinations. 

Additionally, there are controversies about the nature of schizophrenia reflected in the DSM-V definition - it looks to many practitioners that the difference between bipolar disorder, schizoaffective disorder, and schizophrenia is more of a spectrum than very clearly delineated separate disorders; each of them involve oscillations between positive and negative periods, to some extent, and each of them involves changes in behaviour and thinking which diverge from the normal. And it's worth reminding people that schizophrenia is not set in stone as an idea - as an idea, it's currently not much more than a common constellation of symptoms which are helpful in diagnosis because it suggests a method of treatment. I mean, recent research suggests that we don't necessarily know if conditions like schizophrenia are actually a singular disorder, or whether it's a set of similar conditions that manifest similarly; the media often reports interesting research on various neurological causes and correlates of schizophrenia, and of genetics correlating with schizophrenia in particular ways, but nothing has yet been found that is a slam dunk THE CAUSE of the condition.

With those caveats in mind, our current understanding of schizophrenia as a medical condition - like pretty much every psychological disorder - postdates the medieval period. The idea of a medical condition that was something like schizophrenia - 'dementia praecox' - dates from the rise of German scientific psychiatry in the late 19th century, and the term 'schizophrenia' was coined in 1908. No English speaker in 1318 would have known what a schizophrenia is, and the way that they would describe people who experienced delusions and hallucinations would have been - was in, as /u/sunagainstgold discusses in nice detail - a product of the way they saw the world.

It is, of course, implicit in the idea of a *paranoid delusion* that the delusion has to be compared with the normal way of seeing reality, and you don't have to be Michel Foucault for it to be blindingly obvious that the nature of society plays a major role in how we view reality, and thus what we class as delusions and what we class as very sensible behaviour and thinking. After all, to give an example, states in 2018 simply have the ability to access a lot of information about you that they did not have in 1998, thanks to big data, and so it starts to feel less delusive to believe that you're being watched (even if it's mostly just by algorithms trying to figure out how best to advertise to you).

And, essentially, even after psychiatrists had started using terms like 'dementia praecox' and 'schizophrenia', they often did not conceive of psychotic symptoms (i.e., things like delusions and hallucinations) in the same way as we do now, with an eye on the same constellations of symptoms as we do now. It's hard to tell whether, even in Freud's day in the early 20th century, the psychotic symptoms exhibited by a patient are due to what we now call schizophrenia (paranoid type), because Freud did not see schizophrenia through the same lens that we currently do, and he looks for different aspects of the symptoms than a modern psychologist or psychiatrist following the DSM-V diagnostic manual would. It is also the case that psychotic symptoms are caused by a whole range of things other than schizophrenia, from the ingestion of various chemicals (as you well know, you *hippies*), to the effect of medical disorders on the brain, to simply other psychological conditions that have psychotic symptoms as one of the symptoms.

What this means is that it is very clear that someone who insisted that they were a medieval knight in 2018 would obviously be deluded. But in 1318, they very well might have been a knight. Instead - if we assume that schizophrenia of the paranoid type is a unitary disorder (which we shouldn't, as I explain above) - the answer to your question is that the content of the delusions has not really been considered important to the definition of schizophrenia, and people have always found things to have delusions about which reflect the societies they live in. After all, the cameras of the late 19th century, when 'dementia praecox' was first discussed by the likes of Emil Kraepelin, were rather harder to hide than modern pinhole cameras, and, I mean, *The X-Files* hadn't yet been on TV at that point! The delusions of fin de siecle Europeans instead simply reflected fin de siecle European society. 

So, in one famous case of the time, Daniel Schreber, a German judge, wrote a 1903 book titled *Memoirs Of My Nervous Illness*, describing his experiences of dementia praecox and in asylums, which Freud wrote a paper analysing in 1911. To quote from a 2009 paper by Thomas McGlashan re-analysing this case, 

>The core of Schreber’s delusion was that he had a mission to redeem the world and to restore mankind to their lost state of bliss. In order for this to happen, he had to be transformed bodily into a woman so that, as God’s concubine, he could give birth to a new race of humanity. In his application to the courts for release from asylum, Schreber never disavowed these delusions nor did he hide his intentions to publish his experiences as memoirs.

As such, the *paranoid* delusions of the era inevitably reflect that era's social concerns. It would not surprise me at all if the *Protocols of the Elders of Zion* - like Schreber's book, originally published in 1903 - played a large role in the paranoid delusions of the era - because, well, it was a major paranoid delusion of the era for a lot of people who apparently didn't suffer from 'dementia praecox'.

Moving back to medieval times, we move back to a time before people conceived of behaviours as being 'paranoid delusions' indicative of having 'schizophrenia'. To the extent that we can call medieval behaviours 'psychotic symptoms' - something that the medieval people themselves would lumped into 'foolishness', as /u/sunagainstgold points out - those behaviours would have been expressed in profoundly different ways to how they're expressed now, because the world was profoundly different.

Or perhaps we can go one step further. It is possible that schizophrenia in the modern sense *simply didn't exist* in medieval times, because mental disorders are profoundly a product of a society. To the extent that our highly developed homo sapiens brains are evolved things, we have them because they help us interpret and navigate the world around us with precision and subtlety. A major part of the world around us that we need to interpret and navigate is social systems and beliefs and culture. It therefore, logically, is the case that if those social systems and beliefs and culture *change*, then the disorders that result from our interpreting and navigation systems being faulty will also change - our minds are equally a product of biology and society, being based on a biological entity - the brain - interacting with a society. So if society changes, our minds change. At a very basic level, the diagnosis of schizophrenia in the DSM-V requires that patients show 'impairment in one of the major areas of functioning for a significant period of time since the onset of the disturbance: work, interpersonal relations, or self-care.' But you can imagine ways in which psychotic symptoms might *not* cause impairment in functioning, and you can imagine societies which *don't* conflict with the neural systems that might be predisposed to schizophrenia in the modern world.

The classic example along these lines is, of course, hysteria. Freud's *Introductory Lectures On Psychoanalysis* talks about hysteria under the assumption that everyone - in an introductory lecture on psychoanalysis being published for a wider audience - already knows what hysteria is, in much the same way that everyone is assumed to know what depression is, because of all the awareness campaigns for depression and so forth. It was that common! The peculiar set of symptoms that seemed to characterise hysteria (the physicalisation of psychological distress, a certain sense of over-emotionality that is still seen in the layman's meaning of the word, etc, usually diagnosed in women) are way less common than they seem to have been in Freud's day. Nonetheless, hysteria is not a commonly discussed mental disorder in 2018 (when's the last time there were frenzied media stories about people with 'conversion disorder', which is what psychiatrists now call it?) and seems to be much less frequent than it was. If societal conditions in Freud's day played a role in the way that its disorder manifested, it seems likely that things like women's rights and a more sexually open society changed those conditions in a way that reduced its frequency. Schizophrenia and its paranoid delusions may also rely on the interaction of the brain with particular aspects of modern society - and therefore might not have occurred in medieval society, or might have manifested very differently. We don't know.",0
"I'll let someone who is more knowledgeable about Mediterranean slavery answer some of this, but I can tackle a few of the other portions of your questions.  But to be clear, my main area of knowledge is Britain.  So while a lot of this can apply across Europe, keep in mind that I'm mostly talking about Britain.

That being said, Britain wasn't a cultural outlier, and during the height of the medieval slave trade it was very much tied in with the continent so it can provide some insight.

**The Treatment of Muslim Slaves**

Muslims (or Saracens as they called them) were treated as an ""other"" by the Anglo Saxons, but the relationship was complicated.  On the one hand, you have Synods denouncing Saracen practices, but on the other hand, Anglo Saxon medicine included materials that could only be acquired from the Islamic world and you had pilgrims like Willibald travelling to the Caliphate and you even had corrupted Arabic script appearing on the coinage of King Offa of Mercia.

The relationship between the two groups was a weird one.

But that being said, for reasons that will become clear as we go on, it's unlikely that if there were Muslim slaves in Britain they would have been treated all that differently from other slaves.  So I'll just focus on slavery in general.

**Christian Slavery**

In your question you said that you heard that ""slavery mostly died out in medieval Europe among Christians.""

That is half true, but it obscures a very important fact.

Slavery prospered for quite a long time under Christianity.  It was practiced since the early days of the Conversion, and continued to be a very common practice through to about the 11th century.  In fact, entire post-conversion cities based their economy on it.

Many times the Scandinavians take the blame for this practice, but that isn't factually accurate.  Europeans were enslaving each other long before anyone came aviking.  What the Scandinavians did was allow the European slave traders to tap into a much larger market, thanks to their extensive trade routes.  And the West responded to these new markets with wild enthusiasm.  In Britain, as an example (as that's what I'm most familiar with) we see records of enslavement going in every possible direction.  Everyone was enslaving each other, or at least trying to.  And that includes the Scandinavian ""Vikings.""  The Irish records, in particular, speak of how they managed to defeat enslave Scandinavian settlements on the island.

So that's a long way of saying that this wasn't a ""pagan"" practice, and the spread of Christianity didn't lead to it dying out.  In fact, even 500 years after the conversion we still see slave trading cities in Britain.  Bristol, for example, continued to have a thriving slave market into the 11th century.

**Becoming a Slave in Christian Society**

Raiding and wars were a common way to be enslaved in Christian Europe, but not the only way.

Even medieval legal codes made it rather easy to find yourself enslaved.  For example, in Wessex, if you were a thief your punishment would be a 60 shilling fine.  No biggie. But if you brought the stolen goods home to your family... well, under the Laws of Ine you and your whole family would be enslaved.  Permanently.  This was a particularly brutal considering that a wife couldn't bar her husband from bringing things home. So if you were married to a thief, you were at tremendous risk.  It also lead to strange outcomes like infants being enslaved for their father's crimes, which was something that really enraged Archbishop Wulfstan.

You could also find yourself enslaved because of a bad harvest.  We see records of entire communities selling themselves into slavery to a wealthy landowner who just happened to have enough food to feed them  (and given the practice of Food Rent, I can virtually guarantee that they grew the food they were given in exchange for their freedom... which is an extra-special level of awful).  And again, these people were Christians.

Slavery was big business and everyone, including the Christians, wanted in on it.

**The Rights of Slaves in Christian Society**

Now, as for the rights of slaves.  They pretty much had none.  Eventually certain laws were enacted in England that regulated the practice... kind of.

For example, you could be ransomed out of slavery.  However, the slave owner had the sole choice of whether or not to ransom you home and at what price.  Furthermore, if your family couldn't or wouldn't pay your ransom then you were stuck.  You couldn't ransom yourself because, unlike slaves under systems like the Code Noir, you couldn't work for wages yourself.  You had no income, period.

As for your well being, that's rather ugly too.  For the first year of your enslavement you have some protections.  If you die during that first year, your family can demand a weregild (it means ""man price"" and is basically a wrongful death penalty).  The thinking goes that for that first year your family had an interest in your life because they might be able to ransom you back, but after that year passes, they no longer can claim a weregild.  Which means that the slave owner no longer has to worry about paying them an expensive penalty if he kills you.  So good luck.

And if your slave master is mistreating you... Tough luck. You don't have legal standing to raise a complaint to the authorities.  You're a slave.

Slaves could also be sold overseas, which was catastrophic because it meant you had pretty much no hope of ever being freed and getting home.  Early on this seems to have been a rare occurrence, perhaps to deal with particularly unpopular members of the community.  But with the rise of the scandinavian slave trading routes, suddenly there was a lot of money to be made by selling slaves overseas.  So rather than it being a rare occurrence, it become common.  It also made ransoming very difficult because now a small villager from an impoverished kingdom would have to compete on an open market with a vast trade network that included the frigging Byzantines.

**The Social Status of Slaves in Christian Society**

There was a heavy layer of judgment laid down upon slaves.  In the early days, if you read of any complaints regarding slavery it was about how terrible it was that Christans had Pagan masters.  They never complained about Christians owning Christians, or Christians owning Pagans.  The crime they were complaining about wasn't the slavery, it was the paganism.

Well, as paganism came to an end in western Europe, they had a new line they were drawing.  Worthy vs unworthy.  Archbishops lamented how there were slaves who were ""unforworhte"".  Basically, undeserving.  Which, naturally, indicates that there were deserving slaves.  And reading the letters and sermons you get a picture of who was undeserving of being enslaved.  They were virtuous, but more importantly they were *well born*.

It follows, therefore, that the people who deserved slavery were pretty much everyone else who found themselves in that situation.  So pretty much everyone who wasn't noble and completely innocent.

This means that in addition to having virtually no way to get out of being enslaved, and having no legal rights, and being completely at the mercy of your master...  you also have everyone, including even the Archbishop, clucking their tongues and talking about how you deserve your fate.

As you can imagine, there wasn't any social mobility available for slaves outside of manumission, and they were treated pretty badly.

And again, these were Christians doing this.",0
"This is a fascinating, but complicated question. It forces us to narrow down the inquiry to specific times and places, because it's impossible to generalize the entire medieval period - certainly a person might have *less work* at the height of the black plague but that might not mean *leisure time*, I'm sure you'll agree! - and the same must be said of the early modern period - however you may define it - and even post-industrialization, there were large variations in times and place.

On the whole, it *does* seem that free time or leisure for an average worker has been reduced, generally, from the medieval period to the modern day, but not in a strictly linear progression. There have been estimates that present the average hours of work done per year per average male worker in particular centuries. There are variations throughout the centuries, for instance the average hours put in in the 13th century are higher than those in the 14th, but the estimate given, assuming at least a 9.5 hour work day working 2/3s of the year across the whole medieval period in the UK is given as 2309 hours per year. From 1400-1600, the average is given as 1980 hours. 

There are of course exceptions to this model: James E. Thorold Rogers in [*Six Centuries of Work and Wages*](https://babel.hathitrust.org/cgi/pt?id=coo.31924002283079&view=1up&seq=1) argues that, at least in the 13th century, artisans worked a day likely no longer than 8 hours, with enough spare time that there's evidence that these artisans supplemented their income and subsistence with wage agricultural work, and even sold some of their produce to their artisan bosses. Later, in the 17th century, he puts the work day at between 12-14 hours. So again, this is likely not a simple linear progression.

However, the cumulative change is still striking: the average yearly hourly load number nearly *doubles* by 1840, to between 3100-3600 hours, based on a *70 hour* week. The average for the United States is comparable.

Why the enormous change?

You've alluded to it in your OP: industrialization. Agriculture, for centuries, had been the primary production economy in western Europe, but but the 1820s and roaring on until the end of the century, American and British manufacturers would take a larger and larger slice of the pie. This has been exhaustively studied, not simply because the hours of the workweek rose, but because the shift from an agricultural to a maufacturing economy changed nearly *everything* about your average worker's life. It changed the nature of their education, the food they ate, *how* they ate it, it changed the idea of the home, the idea of the family, the idea of holidays, the idea of their power in an economic relationship, it changed their politics, it changed their *militia requirements*, it changed ideas about sex and sexuality, about worship and entertainment and I am not exaggerating when I say it changed *everything*. One of the classic works that discusses this enormous multifacteted process is Charles Sellers' *The Market Revolution.* Sellers is not perfect and in some respects the work is dated, and it's cheerfully devoted to specifically *American* changes of the Industrial Revolution, but it's a good start. He sums up a vast amount of conflict thusly:

> Profound cultural differences arose from these contrasting modes of production. The market fostered individualism and competitive pursuit of wealth by open-ended production of commodity values that could be accumulated as money. But rural production of use values stopped once bodies were sheltered and clothed and bellies provided for. Surplus produce had no abstract or money value, and wealth could not be accumulated. Therefore the subsistence culture fostered family obligation, communal cooperation, and reproduction over generations of a modest comfort.

Part of what Sellers is loosely describing here is what economists call a backward-bending supply curve of labor; basically, when wages rise or needs are met, workers *stop working*. The thought of doing that today, of going to our boss and saying ""hey, I've done a lot of good work today, I'm going to head out"" at noon, even if we *have* done a day's worth of work, is profoundly alien to most of us. But this is a process that has been noted, and it was complained about by early capitalists and industrialists, and even has historical precedents in guild systems and others. Essentially, one of the major changes brought about by the Industrial Revolution was instituting a system where workers had no power to withhold their labor. It was a long, messy process the specifics of which are beyond the scope of the question, but while I may have implied above that this was done by greedy mustachioed factory barons plotting alongside bought-and-paid-for politicians, the truth was far more often that converging aspects of the new economy presented opportunities that capitalists (the definition of capitalist I'm using here is the open and intentional union of the *ownership class* with the political class, not simply the rich - capitalism is the union of political power with property power) were more than happy to take advantage of.

So what were these old freedoms that workers had? For one thing, holidays. But the erosion of the church-calendar year was already ongoing by the time floor bosses clocked their workers at the steel mill, *that* was a process jumpstarted by the Reformation and the chipping away of the dominance of the Catholic church. The medieval calendar year had, according to Francis and Joseph Gies, thirty or so holidays in the year. Some of those were multi-day, multi-stage holidays that allowed people to celebrate for days in a row. The conception of time implied by many medieval writings was one that counted time between holidays, or dated events to their proximity to a holiday. In the 16th century work of the ""poor knight"" Gotz von Berlichingen, for instance, he gives the date of the day that he lost his hand to a cannoball ""the day after the feast of St. Jacob,"" and remembers that it was a sunday, but he doesn't give a calendar date. 

Some of these holidays also had deliberate and tolerated subversions of rules, both of polite society and of political power. ""Topsy-turvy"" or ""world turned upside down"" themes reigned in holidays like All Fool's Day, and allowed poorer folks to prank or make jokes of their betters. But even a more casual relationship often ruled the notion of work. A 16th century curmudgeon, the Bishop of Durham James Pilkington, wrote of the work ethic he observed:

> The labouring man will take his rest long in the morning; a good piece of the day is spent afore he come at his work; then he must have his breakfast, though he have not earned it at his accustomed hour, or else there is grudging and murmuring; when the clock smiteth, he will cast down his burden in the midway, and whatsoever he is in hand with, he will leave it as it is, though many times it is marred afore he come again; he may not lose his meat, what danger soever the work is in. At noon he must have his sleeping time, then his bever in the afternoon, which spendeth a great part of the day; and when his hour cometh at night, at the first stroke of the clock he casteth down his tools, leaveth his work, in what need or case soever the work standeth.

Guild journeymen of the Holy Roman Empire in the same century had a tradition they called *Guter Montag* or Good Monday: their masters would allow, and sometimes even pay for, a bout of monday-afternoon drinking. Every monday. Post-industrial writers cite this as evidence of the poor work ethic and of the inherent laziness of the worker before the industrial economy forced them into stricter discipline, which is an idea of course that has a lot of problematic aspects to our relationship with work today.

We see in various parts of the economy methods by which workers challenged unfair or overbearing bosses. Wage laborers often simply left the job if they felt that they were not respected. Michael Behaim, an apprentice clerk in the 1520s, wrote to his guardian of his master taking advantage of him, and using him as a mere shop-sweeper, instead of properly teaching him the trade. His guardian eventually found him better employment elsewhere. Petty suits between apprentices or journeyman and their masters, and vice versa, were commonplace. Some wage laborers expected not only a day's pay, but midday breaks and a meal, and we have records of complaints when the meal is tawdry or the break is short, or even that their supplied tools are in poor condition, etc. Peasants were expected, sometimes, to work a number of days on their lord's property, and the demand for their proper recompense, in kind or pay, was loud and often belligerent.

Juliet Schor even argues that within the past two or three decades, the relationship between the worker and the workplace, the expectation of work and productivity, has changed even more, and that Americans are working more with less leisure than similar jobs had in the 60s or before.

To give a long, long story a short summation: yeah, it seems like modern folks work longer for less leisure than at many times and places in the past, but the more compelling observation, in my opinion, is the erosion of the ability to wield power in the workplace. Wage conditions and contract work in boilerplate leave little room for what a 16th century journeyman or even a 14th century farmer might expect as their just treatment, and the tools for hitting back today are curtailed, legally and culturally. But we should bear in mind that this is not universal, and work-conditions even just limited to a few centuries in Europe fluctuate widely.

Sources below.",0
"What do you think of when considering California in the 60s? The Summer of Love? Hippies in San Fransisco? Innovative, progressive musicians and filmmakers pouring out of Hollywood? Sex, Drugs, and Rock n' Roll? 


All of those things were happening in the 60s and early 70s in California, but it would be a mistake to think that these trends represented the majority opinion of Californians, never mind the majority opinion of Americans writ large. Consider that Ronald Reagan, running as a conservative Republican, won decisive victories to win (1966) and hold (1970) the governorship of California. Consider how Reagan achieved these victories - by exploiting the general public's fear of increasingly vocal challenges to the status quo on issues of gender, race, sexuality, and drug use.


A quote from Rick Perlstein's *Nixonland* about the race for governor and the moral panic surrounding the emerging 60's counterculture and civil rights movement:

*""The  pundits  little  noted  the  Reagan-friendly  culture  wars  roiling  beneath  the  surface  of  the  bourgeois 
utopia.  Only  recently,  the  drug  lysergic  acid  diethylamide  had  been  rhapsodized  as  a  therapeutic  miracle; 
its  acolytes  included  Cary  Grant.  Now  it  brought  headlines  like  “Girl,  5,  Eats  LSD  and  Goes  Wild”  and 
“Thrill  Drug  Warps  Mind,  Kills.”  Now  Time  reported  in  March  that  it  had  reached  “the  dormitories  of 
expensive  prep  schools”  and  “has  grown  into  an  alarming  problem  at  UCLA  and  on  the  UC  campus  at 
Berkeley.”  Senator  Robert  F.  Kennedy  changed  a  hearing  scheduled  on  mental  retardation  into  an  inquiry 
into  LSD  instead — one  of  three  going  on  concurrently.* 

*A  group  called  the  California  League  Enlisting  Action  Now  (CLEAN)  pushed  an  initiative 
forbidding  judges  from  dismissing  any  pornography  case.  Their  ads  called  pornographers  masters  of 
“Pavlov’s  conditioned  response,”  responsible  for  an  epidemic  of  “rape,  perversion,  and  venereal 
disease.”  Other  activists  went  to  war  on  a  textbook — Negro  historian  John  Hope  Franklin’s  Land  of  the 
Free,  which,  their  pamphlets  insisted,  “destroys  pride  in  America’s  past,  develops  a  guilt  complex,  mocks 
American  justice,  indoctrinates  toward  Communism,  is  hostile  to  religious  concepts,  overemphasizes 
Negro  participation  in  American  history,  projects  negative  thought  models,  criticizes  business  and  free 
enterprise,  plays  politics,  foments  class  hatred,  slants  and  distorts  facts,”  and  “promotes  propaganda  and 
poppycock.”  The  L.A.  County  Board  of  Supervisors  voted  “to  uphold  high  moral  standards”  by  censoring 
an  exhibition  by  an  artist  named  Ed  Keinholz,  who  said  he  displayed  his  dioramas  of  consumer  products 
and  mannequins  in  sexual  congress  and  babies  without  heads  to  comment  on  America’s  “sick  society.”* 

*In  the  Golden  State,  it  was  a  season  of  moral  panic;  and  as  so  often,  California  led  a  national  trend.""*

It is critically important to remember that the counter-culture of the 1960s was just that: a countercurrent to the predominance of social conservatism in American society in the 60s. The backlash to the radicalism of the 60's counterculture had just as, if not more important implications for the politics of the late 20th century as the counterculture itself did. Not only is there no contradiction between the conservatism of Baby Boomers and the youth radicalism of the 60s - there is a direct link between them. Consider another excerpt from ""Nixonland"":

*""Then  there  were  campuses  like  Berkeley — where,  late  in  1964,  a  police  car  rolled  onto  campus  to 
dismantle  a  recruitment  table  for  Mississippi  voter  registration  that  fell  afoul  of  campus  rules  about  where 
political  advocacy  was  permitted.  The  squad  car  was  promptly  trapped  on  the  main  campus  plaza  by 
hundreds  of  students,  who  started  climbing  up  on  its  roof  and  delivering  inspiring  speeches  about  the  right 
to  free  speech,  the  necessity  of  defying  illegitimate  authority,  the  soul-crushing  blindness  of  the 
bureaucrats.  Then  thousands  occupied  the  administration  building.  For  them  the  “Free  Speech  Movement” 
was  a  moment  of  moral  transcendence.  To  the  man  on  the  street — especially  the  man  on  the  street  never 
afforded  the  privilege  of  a  college  education — it  was  petulant  brattishness.  Then  came  the  “filthy  speech 
movement.”  That  started  when  a  couple  of  angry  kids  sat  on  the  Student  Union  steps  with  curses  scrawled 
on  placards.  A  few  score  kids  rallied  to  their  support.  But  by  1966,  these  few  score  kids  had  become 
Middle  America’s  synecdoche  for  “Berkeley.”  “All  the  most  vociferous  of  them  could  produce  was  four- 
letter  words,”  Illinois’s  Republican  Senate  candidate,  Charles  Percy,  told  eighteen  hundred  students  at  the 
University  of  Illinois  in  a  speech  on  the  New  Left’s  “general  uncleanliness.”  The  students  gave  him  a 
standing  ovation.*

*The  outrages,  all  of  them,  felt  linked:  the  filth,  the  crime,  “the  kids,”  the  Communists,  the 
imprecations  against  revealed  religion.  It  all  had  something  to  do  with  “liberalism.”  Pat  Brown  was  a 
“liberal.”  And  it  arrived  that  liberalism’s  enemy,  Ronald  Reagan,  wasn’t  doing  too  poorly  at  all.  He  was 
providing  a  political  outlet  for  all  the  outrages — outrages  that,  until  he  came  along,  hadn’t  seemed  like 
political  issues  at  all.""*


The majority of students on American college campuses in the 60s did not riot, did not protest, and were in many cases, as seen in the example of the University of Illinois, alienated and disgusted by the behavior and beliefs of their more radical classmates. And in the 60s, the vast majority of young people did not even go to college in the first place - Perlstein anthropomorphizes the ""man on the street"" perhaps a little more than I'd like but it is very much the case that the American public at large, including most Baby Boomers, were not fans of student radicalism. After the Kent State shootings in 1971, *""[a]  Gallup  poll  found  58  percent  blamed  the  Kent students  for  their  own  deaths.  Only  11  percent  blamed  the  National  Guard.""*


To come back to the question you asked, it is not the case that the conservatism of the Reagan and Thatcher administrations was enabled by former anti-establishment youth. Instead, the conservative wave of the late 70s-mid 90s in the United States was powered by people who had *always* been conservative, and indeed who had their formative political experiences in the 60s as people opposed to or afraid/hesitant of the social changes wrought by the 60s counterculture. None of this is to say that the 60s counterculture was unimportant or did not achieve long-term successes, but rather to demonstrate that the Boomers who voted for Reagan in the 1980s were *already* willing to vote for him in the 1960s, in large part because he was a crusader against the counter-culture.",0
"""Remember, all men would be tyrants if they could,"" wrote Abigail Adams in 1776. Thirty-five years later, she might well have added ""and they're not shy about trying."" Scholars today generally agree that the (recognized then as) unconstitutional theft of voting rights in 1807 was essentially a strategy in white male-led political parties' jockeying for power over the state. However, the specific assumptions and ideologies that made the strategy attractive and effective depended on racism, sexism, nationalism, classism, and good old-fashioned fear.

Jan Lewis suggests that the voting rights of New Jersey's black women, white women, black men, and foreign-born residents were initially recognized simply out of revolutionary Americans' commitment to the underly egalitarianism of republican ideals. Irwin Gertzog is more pragmatic, arguing that the added specificity of ""he or she"", first restricted to half the state (1790) and eventually extended to the whole (1797), was already a calculated power ploy. 

Either way, by 1798 it's clear that the major political parties in the state, the Federalists and the Republicans, recognized especially the voting power of women. Despite the implicit and explicit restrictions on which women could vote--enslaved black women could not; free black and white women who were married could not (because under the law of coverture, they could not own property independently)--contemporary observers were daunted by the scale of potential women voters. A 1791 newspaper article was titled ""The Humble Address of Ten Thousand Federal Maids""; in 1799, William Griffith similarly estimated, ""of widows and spinsters above
twenty one, there can not, I imagine be fewer than 10,000."" In a state where elections could be decided by hundreds or even tens of votes, 10,000 was a daunting number.

Furthermore, an idea was developing that we might consider a precursor to identity politics: the belief that groups like ""women"" and ""black men"" were homogenous blocs of voting interest. (But of course white men can have their own opinions.) On one hand, this meant that each party worked *furiously* to court The Woman Voter in hotly contested elections like the Essex County legislator race of 1797 (in which this all-powerful voting bloc amounted *seventy-five* women; I have not seen statistics on their race). On the other, it meant that both parties deeply feared the potential for women, black men, and immigrants to vote for the bad guys. 

And naturally, *both* parties developed a conviction that women, especially, would turn out for the other. Naturally, too, these parallel developments were both motivated by classism. Federalists worried that it was easier to get women to vote in (Republican-stronghold) towns than in (Federalist-stronghold) cities; Republicans worried that the women more likely to vote were the wealthier, conservative Federalists. They also seem to have simply assumed that black men and women were Federalists, period.

Or at least, the party leadership--that is, the men actively seeking to acquire and maintain power--did. A ballot initiative to strip the vote from everyone with at least 50 pounds of property and implement a white male taxpayer-based system failed *miserably* with the general population. There was a definite rift developing between the regular electorate and the power-holders, who had nervously watched the French Revolution tear down waves of political leaders and feared a similar outcome if more widespread participation in American politics.

The years between the 1800 election that so focused on winning women voters and 1806-07, when efforts to restrict suffrage kicked back up, witnessed several important developments. First, The Black Voter played the role of the desperately wooed in the 1804 election that The Woman Voter had in 1800, coupled with New Jersey's passage of a gradual emancipation law. 

Second, newspapers were filled with two-sided worry. Women voters were more likely to commit election fraud (generally accusations that enslaved women or free married women were sneaking in a vote); women were neglecting their proper duties to get involved in politics and society was falling apart; the initial constitutional recognition of women's voting wrights was wrong. BUT it was just that: a constitutional right, so they couldn't be deprived of it without a new constitution, and that wasn't happening. 

Judith Klinghoffer and Lois Elkis have argued that sexist ideology (they are not really concerned with race, unfortunately) had little to no role to play in these developments; the evidence does not seem to back that up. As with the editorials arguing that the 1776 constitution and 1790/97 amendments were wrong in extending voting rights to women, gender is not silent. But it's not the *driving* or causal factor. Rather, we can see how sexist and racist ideology played into the lust for political power and control, or rather, became the field on which it played out. The assumption that all women and all black people voted the same; the belief that losses were due to election fraud and of course it was women and black people committing the fraud.

(For the record: Gertzhog says it was actually mostly men, and there is evidence for this. A lot of the fraud involved keeping polls open days or weeks too long, or waiting until the ballots for an area dominated by the other party were counted and announced so they'd know how many to...scrounge up in order to win. In other words: tactics conceived and operated by party leadership, i.e. white men).

But while the initial recognition of voting rights and the first efforts at suppressing them had come from inter-party conflict, it was actually *intra*-party problems that ultimately killed suffrage for property-owning unmarried free black and white women and black men. Once again, the immediate cause was electoral politics and the relentless pursuit of power by white men; the means were racism and sexism.

In 1804, the Republican party split both geographically and ideologically into a ""moderate"" and ""liberal"" wing (with the Federalists occupying the other ""conservative"" pole). Among other things, the liberals actually wanted *universal* suffrage, not even connected to property ownership! But naturally, this rivalry played out as ""The Room Where It Happens."" New Jersey needed a new courthouse, and the dominating Republicans were at war with themselves whether to put it in the liberal north or the moderate south.

And a presidential election loomed.

The moderates offered a compromise: we'll let you put the courthouse in (Republican) Newark if you agree to restrict voting to white male citizen taxpayers. And this will restore party unity and *we won't support the Federalist candidate.* So in this case, we know exactly what the moderates said to the liberals to get them to sell black and white women's and black men's suffrage down the river: political power.

The liberal Republicans' acceptance of this bargain, thus, betrays their own racism and sexism. The desire to stay in power brought into conflict egalitarian republican ideals of equality and contemporary white patriarchy; white patriarchy won *in order for* their political power goals to succeed. Race and sex were not the cause, but were the tools.

Further Reading:

*  Irwin Gertzog, ""Female Suffrage in New Jersey, 1790-1807,"" *Women & Politics* 10 (1990)
* Judith Klinghoffer and Lois Elkis, ""'The Petticoat Electors': Women's Suffrage in New Jersey, 1776-1807,"" *Journal of the Early Republic* 12 (1992)
* Robert Dinkin, *Before Equal Suffrage: Women in Partisan Politics from Colonial Times to 1920* (1995)
* Jan Lewis, ""A Revolution for Whom? Women in the Era of the American Revolution,"" in *A Companion to American Women's History* (2005)",0
"I haven't seen the play, so I can't comment on it directly, but, yes, there was an outpouring of public grief upon the death of Alexander Hamilton, particularly in his adopted hometown of New York.

Hamilton is the founder of the *New York Post* (*New York Evening Post* at the time), and on the evening of June 13, 1804, the paper published a letter from the Rev. Benjamin Moore, the clergymen who attended Hamilton as he died. Moore began [his letter](https://founders.archives.gov/documents/Hamilton/01-26-02-0001-0268) by acknowledging that ""the public mind"" was ""extremely agitated by the melancholy fate of that great man, Alexander Hamilton"".

The following day, William Coleman, the editor of the *Post* and good friend of Hamilton's, couldn't really add much more, [writing](https://books.google.com/books?id=m8bmtJxvkCsC&pg=PA22): ""As soon as our feelings will permit, we shall deem it a duty to present a sketch of the character of our ever-to-be-lamented patron and best friend"". Instead of writing anything new, they just reprinted Moore's letter again.

Alexander Hamilton's father-in-law, who was bedridden in Albany with the gout, wrote to his newly-widowed daughter immediately, lamenting that he could not get down to New York due to his condition, but as soon as he was better he would. He instead invited her to come to Albany: ""I entreat you my beloved Child to come home as soon as you possibly can, with my dear Grand-children.""

James Cheetham, editor of the New York *American Citizen*, the Republican paper who had played a central role in goading the two men into the duel, was quick to distance themselves from what happened:

> ""Wrap[ped] up in himself—to appease his resentment, to gratify his ambition, [Burr] is capable of wading through the blood of his fellow citizens and of laughing at the lamentations of widows and orphans.""

The newspaper called Burr's actions ""predetermined hostility"". The *New York Evening Post* would say Hamilton was ""willfully and maliciously MURDERED by the hand of AARON BURR"" in an editorial a couple weeks later.

Ron Chernow's biography, *Alexander Hamilton*, probably gives the best account of the aftermath:

> ""When a handwritten notice of Hamilton’s death went up at the Tontine Coffee House [on Wall Street], the city was transfixed with horror. 'The feelings of the whole community are agonized beyond description,' Oliver Wolcott, Jr. [Hamilton's successor as U.S. Secretary of the Treasury], told his wife. New Yorkers of the era never forgot the extravagant spectacle of sadness, the pervasive grief. Even Burr’s friend Charles Biddle conceded that 'there was as much or more lamentation as when General Washington died.' As with Washington, this mass communal sorrow provoked reflections on the American Revolution, the Constitutional Convention, and the founding of the government. Unlike at Washington’s death, however, the sorrow was laced with shock and chagrin at the senselessness of Hamilton’s demise.""

Chernow writes that for the rest of its term that year, ""the New York Supreme Court draped its
bench in black fabric, while the Bank of New York building was also sheathed in black. For thirty days, New Yorkers wore black bands on their arms.""

Nathan Schachner's biography *Alexander Hamilton* gives a similar account:

> ""The city went into mourning, and the nation. The newspapers out-rivaled each other in expressions of sorrow. The clergy preached long sermons, with the duel as their text. Mass meetings were held, in New York, in Philadelphia, in Boston, in Albany. Church bells tolled...
>
> ""In New York City the merchants and citizens gathered at the Tontine Coffee House to mourn their loss. On the date of the funeral they closed all stores and marched in vast procession to the muffled beat of drums. The ships in the harbor half-masted their flags. The City Council attended in a body, and the members of the bar, party lines forgotten, decreed mourning for a period of six weeks. The mayor, members of congress, foreign ministers, the students of Columbia, the Cincinnati, Tammany and all the citizenry marched in line.""

Rev. Benjamin Moore led the service, which was held at Trinity Church on Broadway. U.S. Senator Gouverneur Morris, and fellow delegate to the U.S. Constitutional Convention, gave the eulogy and it was observed he was so broken up, the attendants weren't sure if he would be able to get through it.

In the second volume (*Alexander Hamilton: The National Adventure, 1788-1804*) of his lengthy two-volume biography, author Broadus Mitchell's account is largely the same as Chernow's and Schachner's. He writes that Aaron Burr spent eleven days housebound in his New York City home after the news broke, eventually fleeing with the help of some friends in the dark of night because ""it was feared that a mob would burn"" his house down. (Note: Chernow says it was only nine days. Burr snuck out two days before he fled to have a sexual liason.)

Mitchell writes that there was an outpouring of grief and memorializing in newspapers around the country. He quotes from Fisher Ames' eulogy in the *Boston Repository* as being one of the most ""thoughtful"" and ""perceptive"":

> ""[Hamilton] had not made himself dear to the passions of the multitude by condescending ... to become their instrument. ... it was by ... loving his country better than himself, preferring its interest to its favor, and serving it, when it was unwilling and unthankful, in a manner that nobody else could, that he rose, and the true popularity, the homage that is paid to virtue, followed him.""

But Mitchell does explain that the memorials weren't universal, led by the pro-Jeffersonian *New York Chronicle*:

> ""Of course, Burr found apologists too. The *N.Y. Chronicle* declared that when the public was candidly informed,
Burr would 'be justified by every disinterested ... man'. A Boston paper objected to a eulogy of Hamilton: 'An Oration! The Champion, the Goliath of party is dead and died like a fool! He ought to have the burial of an ass, and none to lament him...'""

So, while there *were* some newspapers that stuck to their anti-Hamilton guns, so to speak, that kind of sentiment was only expressed in a small minority of newspapers. 

In summary, yes, if the play shows this kind of outpouring, then they got it right. Considering the play is a loose adaptation of Chernow's book, they picked a good source. Some of the briefer biographies of Alexander Hamilton end abruptly with the circumstances of his death, with an epilogue of what happened to his wife, and to Aaron Burr. They don't say anything about the public reaction at all.



**SOURCES**:

Chernow, Ron. *Alexander Hamilton*, 2004.

Cray, Ed., ed., et. al. *American Datelines: Major News Stories from Colonial Times to the Present*, 2003.

Mitchell, Broadus, *Alexander Hamilton: The National Adventure, 1788-1804*, 1962.

Nolan, Charles J. *Aaron Burr and the American Literary Imagination*, 1980.

Schachner, Nathan. *Alexander Hamilton*, 1946.",0
"> He comes at it from the point of view of local collective memory in the later 19th century, the stories that Utah residents told about encountering the Baker-Fancher party in 1857. 

How do these authors address the problem that comes from trying to use ""local collective memory"" on an issue as hotly contested as this one, especially in a region where a powerful and centralized authority like the Mormon Church holds such control? The idea of some settlers boasting about how they helped kill Joseph Smith seems like a weirdly specific claim to me. You say that the wagon party was from Arkansas. Why would so many say that they killed Joseph Smith in Missouri? ",0
"Thanks, I got sucked in and spent longer than I should have ;-) 

I wouldn't say support for the Kaiser was a factor, except in a limited sense that some elements of the center right might have been old line conservatives not interested in Nazism. However, as we saw nationally from 1930 onwards, old line conservatives tended to fall in line with Nazis over time. In an electoral sense, you can see their voters shift away from other conservatives and to the Nazis. And then in the elite sense that those like Papen got too clever and thought they could use the Nazis for their own ends, thus the coup and then the eventual climax in appointing Hitler. 

So the real source of Prussian antifascism was the SPD. They controlled the state government for all but a short time from 1918-1932, and for most of the period had the biggest base and best turnout. They definitively saw themselves as the leader of the ""Weimar Coalition"" of democracy in opposition to both Nazism and Communism, and had good relations with the other centrist parties.

A lot of that description works for national politics as well, but the situation in Prussia was especially clear in this way.",0
"OK. So, I know a little bit about this at least as far as Britain is concerned. Up until the 1850's beards were usually associated with ""oriental"" or ""muslim"" types, brigands, savages and republican riff-raff. The military was another association, but the Brits were never too fond of their army.

By the mid-century, however, this changed. Growing sentiments of nationalism and patriotism made support for the army a key part of ""imperialist sentiment"", and (some would argue) post-industrial-revolution angst about gender roles made physical differences between the genders important.

Enter the beard. So, during the 50's and 60's more and more writers began talking about facial hair as manly, or writing about scruffy-faced heroes as the new images of imperialist, western, ""proper"" masculinty. Sometimes it was because of the frontier, other times, a link to medieval knightly ideals. In all cases, the beard was the sign of the ""true"" man.

However, it's important to remember that, for all intents and porpoises, this was just an *ideal*. Even Thomas Carlyle, one of the key factors in the formation of this new bearded masculinity in his writings, supposedly only grew a beard after he lost a bet and had his friend confiscate his blades. And even when facial hair was at the height of its popularity, some people still went around clean shaven, even when they were key political figures (like Archibald Primrose, PM 1894-1895 or Ernest Henry Shackleton, polar explorer). In some cases this clean-shaven face was associated with counter-culture movements (Oscar Wilde, the dandy to end all dandies, is of course an example here) but, well, it's just a grooming choice.

So, to conclude this, while some people might have considered you as more ""unmanly"" because you couldn't grow a beard, it wasn't that big of an issue beyond, well, people making fun of you for not being up to snuff with current fashion. Like, a balding 20something who is really into Twisted Sister in the prime of Hair Metal or a flat-chested teenage girl at prom. These are fashion choices, and popular as they might be, aren't some north-Korean standard everyone has to adhear to. Nobody would discriminate against you, but some would see you as less ""manly"" and make fun of you behind your back. 

Sorry if what's written above is messy or missplled, I have not had any sleep in a while and it's been four years since my paper about Victorian beards.

Further Reading:

•	Oldstone-Moore, Christopher , “The Beard Movement in Victorian Britain”, Victorian Studies 48:1 (Autumn 2005).

•	Shannon, Brent, “Refashioning Men: Fashion, Masculinity and the Cultivation of the Male Consumer Culture in Britain, 1860-1914”, Victorian Studies 46:4 (Summer 2004).

•	Walton ,Susan, “From Squalid Impropriety to Manly Respectability: The Revival of Beards, Moustaches and Martial Values in the 1850s in England”, Nineteenth Century Contexts: An Interdisciplinary Journal 30:3 (September 2008).",0
"Hong Kong cinema outlived the heyday of the kung fu movie - it was in a healthy state  for about a decade after the kung fu movie was largely gone. The Hong Kong martial arts wave began, as noted above, with the Shaw Brothers *One-Armed Swordsman* in 1967, and continued long past the end of the US kung fu craze into the mid '80s (with a shift from wuxia (typically centred on a swordsman/swordswoman) to kung fu as the dominant genre). In the '80s, the focus shifted from martial arts to more general action movies, crime, comedy, horror, and sex. The decline of the kung fu movie left Hong Kong cinema healthy.

Many things are blamed for the mid-'90s slump, but not kung fu or martial arts movies. What gets blamed? The Asian financial crisis, home video and piracy, penetration of Hollywood into the Asian market (not only taking market share away from local movies, but also generating desire for big-budget glossy movies). The growing mainland movie industry has been blamed for the lack of recovery.",0
"Their view of Scientology is surprisingly positive, and they point out that the conflict between new religions and government isn't a new thing. Even Christianity was at one point considered a crack-pot cult.

They're credulous about the Church's claims that they were unaware of what the GO was doing with Operation Snow White, and that the revelations lead to ""a complete reorganization of the church at the national, continental, and international level...."" After an internal investigation by the Church, 11 high level officials connected to OSW were required to resign. Some were even expelled entirely from the Church. In 1983 the Church decided that the GO was ""unsalvageable"" and disbanded it. This even lead to the relocation of Church headquarters.

All in all, the Church's claimed ignorance is believed by the authors, and they make a good point that the revelations lead to a dramatic change in the entire organization and personnel. 

Edit: Also, please bear in mind that the entire book isn't by Davis and Hankins. It's a collection of articles written by other authors. Most of my information is from *A Contemporary Ordered Religious Community: The Sea Organization* by J. Gordon Melton.",0
"Would it kill you to wait, I dunno, 5 or 6 years? You're emperor for life, your people love you, you should let them get busy and get your numbers back up. No need to keep going endlessly.",0
"**The End of Slavery in Medieval Europe**

What brought an end to the practice wasn't a Conversion, or a charismatic pastor, or a vision, or anything like that.

Most likely what brought an end to the practice of Slavery in Western Europe were two things:  Economics and bigotry.

When slavery was booming, Europe was fragmented.  What we think of as ""medieval kingdoms"" weren't born yet.  Instead, in their place, were a patchwork of smaller kingdoms and chiefdoms.  Europe was a battle royale as the wealthy and powerful vied for control.  That made Europe a slave rich territory because with every war came opportunities to enslave, and with so many rival societies nearby the opportunities were endless.

But then kingdoms began to expand and consolidate.  And suddenly, raiding those weirdo's over the hill for slaves wasn't a viable option because if you did that the King *who you both served* would get involved... and he'd punish you for enslaving his subjects.

So it was getting harder to make money, which meant that the powerful were losing interest in it.  Of course.

However, there were other people who weren't in the same situation.  People who could still get away with raiding for slaves, and were making quite a lot of money off the slave trade. 

And if there's one thing we've learned from history it is that the wealthy and powerful don't like it when other people are making money in markets that they can't access.  So this was bound to irritate them at a base level, but there was a secondary thing that seems to have really bothered them.  The people who were making money off slavery were people who were different from them... They were Muslims,  the Irish, the Scandinavians, etc.

Well, that simply wouldn't do, and suddenly we see this turn in the record.  Writers weren't simply concerned about whether someone was *unforworhte.*  Now the writers spoke of how slavery was something that was barbarous and practiced by base godless people who don't know Christ.

Even though, a generation earlier, there were plenty of Christians who were quite happy to engage in slave trading.  It wasn't the religion that changed, it was the economics of the situation.

But paraphrasing Dr. Ian Malcolm... Wealth, uh, finds a way.

The rich and powerful simply changed gears and instituted serfdom.

And serfdom was a hell of a hack because it has a lot in common with slavery but it cuts out the slave trading and slave markets.   Which were precisely the parts that the wealthy of western Europe were finding it hard to exploit lately.

What a coincidence.",0
"This is a pretty interesting question, not least because the job you're doing in London and the date you are considering emigration to India are going to make things a lot harder for you than you might imagine. Broadly speaking, you've picked almost the worst possible time to realise the ambitions that you have, and you would find it potentially very difficult to improve your lot in India in this period, much less make your fortune; odds are, in fact, that your attempt to ""better yourself"" would quite possibly kill you.

Why? 1850 falls at the tail end of the period during which the East India Company controlled India, but well before the one in which medical advances made it possible to control the diseases that European incomers almost inevitably contracted there. The EIC was a private chartered company of a very peculiar sort – it's charter, which it purchased from the British crown and was required to renew periodically, gave it a monopoly on the export of goods from India to Britain and permitted it to maintain its own armed forces and run its own foreign relations with Indian governments. These were the conditions in which, during the 18th century, it had indeed been possible for EIC employees – at least the senior ones – to literally make their fortunes on the subcontinent, if they were lucky enough and ruthless enough, and if they survived long enough to return to Britain to enjoy their almost certainly ill-gotten gains. This was because the EIC's activities in India were, in the previous century, well beyond the effective control of London, creating a situation in which a number of lucrative but technically illegal practices – beginning with conducting private trade on one's own behalf, rather than for the profit of the company, but moving all the way up, in the case of some Governors General, to deliberately initiating wars of conquest against Indian polities – were possible. If you chose to venture down this path, and were successful enough to benefit Company and country, as well as yourself, it was quite likely the actual actions you'd taken to make your money would be overlooked. The men who did enrich themselves in these circumstances, and did survive to return home, were known as ""nabobs,"" and were amongst the wealthiest people in the Britain of the 18th century.

By the middle of the 19th century, however, things had changed. For one thing, telegraphic contact was established between India and London, which meant the EIC representatives in India were placed on a much tighter leash; and the voyage out was shorter, too, in the steamship age – as little as six weeks stood between you and someone sent out from Britain to stop you. For another, the actions of the worst of the EIC's nabobs had already soured public opinion in Britain against their rapacious greed ... quite an achievement given the colonialist, imperialist, and supremacist attitudes of the time. And, finally, India in 1850 was very different from the subcontinent of a century earlier; most of its richest territories had already been seized by Company armies, and more or less stable relations had been established with the polities that remained at least nominally independent, so long as they recognised EIC suzerainty. All of this meant there were very few places you could travel to and begin acting rapaciously without being stopped. There were many fewer opportunities to exploit India's wealth by the date you specify.

In addition to all this, your own status as a factory worker would count heavily against you. It was certainly possible for private individuals to emigrate to India and try to make a living there outside the purview of the Company, but India was not an industrialised country – this was a deliberate EIC policy, since export of manufactured goods and finished textiles to India was one way the Company attempted to manage what would otherwise have been a considerable balance of payments issue – and so there would have been few of the jobs you were actually qualified to do available to you if you did make it out. If you had the right specialist skills, it might have been possible to find work in a government run factory; the EIC made its own munitions and built its own ships in India, for instance. But work of this sort was scarce, since India had plenty of manpower, and EIC economics depended quite heavily on exploiting a poorly-paid indigenous workforce where possible. In any case, factory work would not make your fortune. Similarly, while you would have been very welcome to join up to the Company's army and fight for it, the lowly start you got in life would almost have certainly have precluded you working your way up into the officer class, where pay and opportunities would eventually, after 15 or 20 years, render you tolerably affluent.

Finding work elsewhere would have been tough. There was, for obvious reasons, almost no demand for European servants in India. The few independent entrepreneurs that India boasted tended to be planters of various sorts – growing indigo, or coffee, for instance (opium was a government monopoly) – but to get a start there required considerable capital that it doesn't seem to me you'd have access to. Without a wealthy relative ready to back your fresh start, in short, or without adventure-novel-hero levels of good looks and good fortune, you'd be far more likely than not to find yourself returning to Britain a year or two later, as destitute as when you began your journey, and probably with your health badly undermined by the various tropical diseases you'd acquired during your stay on the subcontinent. Or you'd find yourself dying unmourned of alcoholism in some sweltering Indian town, another extremely common fate among the less fortunate expatriates in India at this time.

All this may sound like tough luck from your British factory worker's point of view, but of course we need to conclude by pointing out that that's to look at things from a very Eurocentric perspective. From the Indian one, the balances were just about to start redressing themselves – the rebellion of 1857 was only a few years away, and the earliest significant independence movements were not that far behind it. The days of freelance Brits looting and stealing their way across India were already over by then, and a very good thing too. The Company was still acting in this way of course, and the British government would continue to do so in its stead from 1858. But for an individual actor of the working class, 1850 was a better time to head for Australia, Canada or the United States than India.",0
"Neither is objectively better, but suited tomdifferent jobs. Straight swords are better for finding the gaps in armor and have easier angles to thrust through. Curved swords are better against lightly armored or unarmored opponents providing larger deeper cuts on slash. But even among these you have countless variations. Curved swords with weighted tips are some of the most powerful weapons and better at actually damaging armor than a standard straightsword. But larger claymores are meant to be able to dent and damage armor as much as pierce it. Larger straight swords would be better for cavalry rushes to pierce a line while a curved sword like a falchion or scimitar would be better to fight in a less mobile space from horseback. But then again the height and breed of the horse would matter. It ultimately depends on the situation and even wielder which would be 'better'.      
       
Note: please dont mistake these as answers to the orignial question. Im unsure of the armor types and fighting styles of the time period op mentioned and some of the general nuances mentioned here might not apply. Im lacking necessary information on the topic, so there definitely isnt enough relevant info in my comment for a solid answer.",0
"His first disputation (in Latin) was “De Jure Maurorum in Europa” (The rights of blacks in Europe) so it is known that he didn’t lose his identity and thought about people from his native continent.
 
However, from 1730 onwards, he devoted himself to study of Natural Sciences rather than focus on Ethics or race relations. His inaugural dissertation was then on “De Humanae Mentis Apatheia” (The Absence of Sensation and the Faculty of Sense in the Human Mind and their Presence in our Organic and Living Body) was a mix of philosophy and medicine and had nothing to do with racial relationships. If you are interested, he was a supporter of modified materialism. 

He returned to Halle in 1735 and taught a range of subjects, but specializing in exploring the relationship between the mind and body. This is indicated by the title of compilation of his lectures: “Tractatus de Arte Sobrie et Accurate Philosophandi”  (Treatise on the Art of Philosophising Soberly and Accurately). It had nothing to do with race relations.  He was known for chairing sessions defending doctoral theses that had themes close to his own which made a distinction between mind and our organic bodies. He was chairman of session where Johannes Theodosius Meiner defended his own thesis. 

In 1740, he moved to University of Jena but was unhappy due to prejudice there. Eventually, in 1746, he left Germany for motherland. Once he left Germany, voices against him became more vocal since he was no longer able to defend himself. In west-Africa, the Dutch found his first disputation and saw him as a danger against their colonial rule. He was arrested, taken to a Dutch coastal fortress, and died there in 1784. We don’t know about his quality of life within the fortress but it is likely that he was not jailed, just put under house arrest. Nevertheless, he produced no more works (or if he did, they were destroyed/unpublished). So, we don't know what happened to him there.

For further reading: 

* [Bemile, SEBASTIAN K. ""Anton Wilhelm Amo, from a Ghanaian Slave-Child to a German Professor and Philosopher."" University of Ghana, Legon, Ghana (2002).](http://www.afrst.illinois.edu/news/archive/seminar/documents/sem-bemile-2002.pdf)

Bernile would be of interest to you since he summarizes the main works of Amo and hence will allow you to learn what he was mostly interested in. 

* Lochner, Norbert. ""Anton Wilhelm Amo: A Ghana Scholar in Eighteenth Century Germany."" Transactions of the Historical Society of Ghana 3.3 (1958): 169-179.

* [Abraham, William. ""The life and times of Anton Wilhelm Amo."" Transactions of the Historical Society of Ghana 7 (1964): 60-81.]( https://www.jstor.org/stable/41405765?seq=1)",0
"There's a great book which discusses the banal and unsuspicious way that Hitler rose to power, written by a German in 1939, called *Defying Hitler*.  The author was a regular German, not a Jew or other persecuted minority, yet he writes clearly about the threat of fascism and how Hitler was often misjudged during his rise, in many cases due to the turbulent nature of recent German history.  Germans who had lived through the 1870-1932 period were used to enormous political changes, and when Hitler began agitating for power, they often took a cynical view of his true threat.  Discounting his rhetoric as empty demagoguery was very common.

The book is really chilling and is very effective at illustrating just how easy it is to delude oneself into hoping for the best and ignoring all the warning signs.  Here's an interesting passage I just flipped to, in which the author discusses the implications of Hitler's election to Reichschancellor in 1933 with his father on the night of the election:

> That evening I discussed the prospects of the new government with my father.  We agreed that it had a good chance of doing a lot of damage, but not much chance of surviving very long: a deeply reactionary government, with Hitler as its mouthpiece [the author had previously discussed how many intellectuals viewed Hitler as a clown].  Apart from this, it did not really differ much from the two governments that had succeeded Bruning's.  Even with the Nazis it would not have a majority in the Reichstag.  Of course that could always be dissolved, but the government had a clear majority of the population against it, in particular the working class which would probably go Communist, now that the Social Democrats had completely discredited themselves.  One could prohibit the Communists, but that would only make them more dangerous.  In the meantime the government would be likely to implement reactionary social and cultural measures, with some anti-Semitic additions to please Hitler.  That would not attract any of its opponents to its side.  Foreign policy would probably be a matter of banging the table.  There might be an attempt to rearm.  That would automatically add the outside world to the 60 percent of the home population who were against the government.  Besides, who were the people who had suddenly started voting Nazi in the last three years?  Misguided ignoramuses for the most part, victims of propaganda, a fluctuating mass that would fall apart at the first disappointment.  No, all things considered, this government was not cause for alarm.",0
">speculated the Jewish Defense League was behind the murder, but these are the most plausible.

Implausible as it may have been, why would that have been considered at all?",0
"This seems more Harari's interpretation of events than anything else. For one, it is really only Diaz del Castillo who talks about this. Cortes mentions the use incense in temples and the presence of priests at major meetings, but is silent on whether or not those priests came bearing burning copal to those meetings. It is Diaz del Castillo who seems obligated to mention that every meet-and-greet between the Spanish and the various nobility of Mesoamerica was preceded by the wafting of censers. This isn't particularly significant though, as Cortes and Diaz del Castillo often end up focusing on different things in their respective texts. 

The more significant aspect is that Diaz del Castillo seems to only mention the priests bearing incense at the start of meetings.  I could absolutely be missing some passage where he mentions being constantly accompanied by priests bearing censers, but the fumigation seems to have been a greeting ritual. Here's just a few examples from shortly after the Spanish arrived on the coast:

> when these people arrived and came before our Captain they first kissed the earth and then fumigated him and all the soldiers who were standing around him, with incense they brought in braziers of pottery. Cortes received them affectionately and seated them near himself... (p.124)

> We were thus waiting when Tendile returned accompanied by many Indians, and after having paid their respects in the usual manner by fumigating Cortes and the rest of us with incense, he presented ten loads of fine rich feather cloth... (p. 127)

> we reached the buildings, and the fat Cacique came out to receive us in the court... and he made deep obeisance to Cortes and fumigated him, as is their custom, and Cortes embraced him and we were lodged in fine and large apartments that held us all... (p. 142)

> We went halfway through the town without meeting a single Indian to speak to, at which we were very much surprised, for they had fled in fear that very day when they had seen us climbing up to their houses. When we had reach the top of the fortress in the plaza near by where they had their *cues* and great idol houses, we saw fifteen Indians awaiting us all clad in good mantles, and each one with a brazier in his hand containing incense, and they came to where Cortes was standing and fumigated him and all the soldiers who were standing near and with deep obeisances they asked pardon for not coming out to meet us, and assured us that were welcome and asked us to rest. (p. 144)^1

So there's a multitude of episodes where the Spanish were *greeted* by incense-bearing priests, but nothing about being *accompanied* by those priests, particularly not a full-time retinue of perfumery. We also have instances of the ritual fumigation being done, as in the last quote, by people who were meeting the Spanish for the first time, and therefore could not have been aware of whatever level of body odor they had going on at that time. Finally, this is all occurring after the Spanish had been joined by both Geronimo de Aguilar, who had been living among a Maya group for several years at this point, and Malinalli/Malinche/Dona Marina, a Tabascan Nahua woman. Both of them would be aware if the fumigation was somehow unusual or unique to meeting the Spanish.

To accept Harari's premise we would have to believe all of these groups somehow improvised a ritual, which was then somehow immediately standardized across various groups to such an extent that an outsider like Diaz del Castillo took it as a normal custom. In addition, we would have to accept that neither Aguilar nor Malinalli would have remarked upon this as strange, and this despite the typical view that the latter guided Cortes on matters of Mesoamerican politics and society.

The more plausible explanation is that the fumigation ritual was a pre-existing and widespread practice. Particularly given how incense-loving Mesoamerica was, this makes much more sense. We have copal as a major trade and tribute good, with the *Codex Mendoza* documenting the Tlachco region as being required to provide 400 baskets of refined incense and 8000 balls of unrefined copal every 80 days.^2 Smith notes that the long-handled ""frying pan"" incense burners are ubiquitous both in [archaeological excavations and contemporary imagery](https://imgur.com/cCc3s17), writing

> This image is so common and standardized that in many cases it may have been an icon for magico-religious activity rather than a depiction of incense offerings in a specific setting... It is clear that the long-handled censer, used by professional priests, was an important component of Aztec public religion.^3

Finally, and ironically, in a meeting between indigenous priests and itinerant Spanish, the most odoriferous individuals might not have been who Harari presumes. While Mesoamericans in general, and Nahuas in specific, were noted for their assiduous attention to personal hygiene, the priesthood could be a notable exception. These were persons deliberately set apart from society, their persona cultivated to be otherworldly, a conduit of divine energy, living as much in the spiritual realm as in the physical. As such they often underwent long periods of fasting, abstaining from bathing, regularly engaging in autosacrifice, and famously never cutting or brushing their hair.

Diaz del Castillo remarks on the wild and shocking appearance of the priests, describing them thus:

> The priests wore black cloaks like cassocks and long gowns reaching to their feet, and some had hoods like those worn by canons, and others had smaller hoods like those worn by Dominicans, and they wore their hair very long, down to the waist, with some even reaching down to the feet, covered with blood and so matted together that it could not be separated, and their ears were cut to pieces by way of sacrifice, and they stank like sulphur, and they had another bad smell like carrion... (p. 163)

These priests might go weeks without bathing as part of their penance and rituals, and spent their days covered in the blood of themselves and of sacrifices. Perhaps a more wholistic view of the smells of Mesoamerica at the time of Spanish arrival would argue against burning braziers of copal being improvised to guard against Spanish stench, and instead being long associated with counteracting the otherworldly odor of native priests.
_____

^(1 All quotes from Maudslay's 1924 abridgment of his 1908 translation, *Bernal Diaz Del Castillo: The Discovery and Conquest of Mexico 1517-1521)

^(2 Berdan and Anawalt 1997 *The Essential Codex Mendoza*, p. 76)

^(3 Smith ME, 2002 ""Domestic Ritual at Aztec Provincial Sites in Morelos"" in *Domestic Ritual in Ancient Mesoamerica*, ed. Plunket)

____

 PS - I also object to Harari stating that ""it was the first time the Aztecs encountered a completely unknown people."" Aside from the fact that the Aztecs were always encountering new cultures and people as they expanded from semi-nomads from the arid north to lords and masters of almost all of central Mesoamerica, Cortes' arrival was not even the first time Spanish had landed on the Gulf Coast. There had been two previous expeditions: Cordoba led a disastrous venture around the Yucatan, and Grijalva had pushed even further up the coast, encountering Aztec officials which first gave the Spanish word of the great state of Mexico. Cortes explicitly stated that he was following the route of Grijalva, and Diaz del Castillo was himself a veteran of that earlier expedition.",0
"I'm not familiar with any sources that indicate saltpeter was historically believed to be used for that purpose on naval vessels (and certainly none that indicate it actually *was* used). However, the broader belief that saltpeter is an anaphrodisiac stems back to at least the Middle Ages, when it was considered a possible remedy to being dosed by a love potion.

From Rich and Jacobs, *Saltpeter: A Folkloric Adjustment to Acculturation Stress*:

> The earliest known references to saltpeter and its supposed anti
sexual qualities occurred during the Middle Ages and the Renaissance when learned medical doctors and chemists were spending a great deal of their time writing dissertations on the use of love potions. Gifford notes the proliferation of literature beginning in the middle 16th century dealing with the magical preparation of sexual stimulants. With the wide dissemination of aphrodisiacal knowledge there also developed, of course, the need to issue prescriptions for counteracting these love potions. While some authorities, such as Marsburg professor of chemistry, John Hartman, advised that concoctions of holy water and antimony be taken orally, others advised 'the popular use of mineral baths of alum, antimony, arsenic,salt, sulphur, vitriol, and nitre (saltpeter) as ""an infallible cure for the victims of philtres.""

That article also has some interesting statistics on the prevalence of this myth, along with a great deal of discussion and speculation on why it is so common within the military. I would recommend you check it out if interested in more information here.

----

Rich GW, Jacobs DF. Saltpeter: A Folkloric Adjustment to Acculturation Stress. Western Folklore. 1973 Jul 1;32(3):164-79.",0
"Kim MacQuarrie's ""Last Days of the Inca"" and John Hemming's ""Conquest of the Inca"" are both excellent, they were my favorite and most colorful explorations of the matter when I was reading about the Inca for work and also the most in depth. Both explore in differing ways the role that native allies played in helping the Spanish conquer the Inca, although it is not truly their focus.

/u/Pachamcamac is probably more qualified to speak on Andean resources and he wrote an excellent comment with detailed resources that deserves more attention [elsehwere in the thread,](https://www.reddit.com/r/AskHistorians/comments/affur9/machu_picchu_was_never_discovered_by_the_spanish/edzsrbg) you may find it insightful. I am not an expert on the Inca, I had to become an amateur one for a work project.

The cliff notes would be that, more than the military exploits, the Spanish did a masterful job of exploiting weaknesses at the foundation of the Incan empire. Those weaknesses grew from its explosive rise from a kingdom under Pacha Kuti (there are myriad spellings of his name) to an empire in the way which almost all empires are built, through the conquest of other peoples. The Inca maintained their empire through a system of (forced) marriage and, from at least some historian's perspective, continued, ritualized terror of their conquered peoples. Their name for their ruler, ""Sapa Inca"" itself hints at the deeply familial aspect of its government - Inca (or Inka) is the familial name of the supreme royal family of Cuzco. Calling it the Incan empire as compared to the Cuzcan empire would be similar to calling the Roman empire the ""Julio-Claudian"" empire - . Sapa Inca, the name for the ruler, just means ""the only Inca"", i.e. the patriarch of the Inca family.

The Inca were excellent conquerors but, as Omar Bradley quote fabulously puts it, ""Amateurs discuss strategy. Professionals talk logistics."" More than anything, the Inca's ability to construct roads in the rugged Andes to support and transport their large armies was immensely important to their conquest of their neighbors, who were often at similar or in some cases (from what I recall reading) arguably more advanced states of development, such as [the Moche](https://en.wikipedia.org/wiki/Moche_culture) (those same roads were invaluable to the Spanish in time).

After conquest, the Inca imposed taxes, which included their religion upon native peoples. This religion, which, to the extent I have read, was in many regards not dissimilar from neighboring cultures in its religious/military/political aspects or utility, but became something of a sticking point with their conquered people, and I've read different commentaries arguing that it wasn't simply imposed as a religion, but became a tool of state sponsored terrorism to keep conquered peoples pacified.

The Incan taxation system was unusual (not for the Americas but in most other cultures) in that, along with ordinary tribute/taxes (jewels, silver, livestock, etc), it included a human taxation. Human taxation has actually been pretty standard throughout time and actually existed in other cultures as well, it took the form of mandatory work time: at a certain time in the year, communities were expected to  send workers to maintain the roads, bridges, and forts that were so essential to the Incan military - just, to give a sense of how amazing these roads were, [the Incan ability to transmit information via the Chasquis](https://en.wikipedia.org/wiki/Chasqui) is mind boggling in a society that had no horses and was in many parts mountainous, and that messenger system was a product of the roads, waystations, and forts that the Incan military supported through tax tribute and built by those mandatory labor taxes. But there was another kind of human taxation, a kind of state terror program in which the children of foreign polities like the Moche linked above, were sent to Cuzco to be ritually killed.

The Inca do not seem to have practiced human sacrifice to the scale of the Aztecs, and there is some indication that certain Sapas outlawed the practice at various times, but they [archaeological work shows they definitely did practice it in what they called Capacocha](https://en.wikipedia.org/wiki/Capacocha) consistently over time, even if not continuously, and it was aligned with the Inca Sapa - if he fell sick, or if one died, there would be sacrifices, and people knew where their children were being sent. And what a strong reminder of who your conqueror is - ""the king fell sick, we are gonna need a few more of your kids to replace the ones you just sent us we had to kill to bring him back to health, the empire appreciates it"". What is interesting, you'll note from that wikipedia link, is that A. the translation is ""royal obligation"", i.e. a tax, B. most of the children came from outlying (more recently conquered) territories.

The Spaniards were appalled at this, as many people today would be. One of the (many) ways the Spaniards persuaded natives to aid them was by promising to lower taxes, which included ending the sacrifices. Now, what is much less clear is if the natives understood this in the way the Spaniards meant, which was ""by forcefully converting everyone to Catholicism as subjects of the Spanish Crown"" or more as ""we will help you conquer your Inca oppressors and now they will have to give you tribute"", because, as I mentioned, human sacrifice was pervasive in pre-Colombian America and it was part of many local different religions, and people don't generally participate in religion without believing it. I saw little evidence in my reading that any historians think the Andean people saw the practice as bad per se, only that it was not great to be the one footing the bill with your children - quite the opposite would be my interpretation. The resistance to Catholocism and some of the fascinating fusions of pre-Colombian ritual and catholocism, such as the parading of the saints in a similar manner to how the mummies of Sapa Incas were paraded, indicates to me the answer is that people believed this religion and didn't exclusively see it as a political tool, even if it was also used as one. I think the difficulty Spanish priests had in filtering pagan aspects is a clear indication that people were not angry about the religion itself and probably expected to continue the practice, simply by reversing the roles of who gives Capacocha.

Something MacQuarrie emphasizes in ""Last Days of the Inca"" is a pretty good microcosm of the questions around ""to what extent was taxation and human sacrifice a thing the natives were concerned with in helping the Spaniards?"" It's the question of the truth of post-conquest writing by the descendants of native people: those who were literate were often mestizos who found themselves targets for suspicion as native sympathizers by Spanish governors/anyone with an [encomienda](https://en.m.wikipedia.org/wiki/Encomienda), but as the children of Spaniards they also were often in positions where they stood to be quite powerful and wealthy if they could prove their loyalty to Spain. Some writers were very denigrating of native religion even while speaking very highly of other truly impressive achievements such as the engineering feats of Ollantaytambo, which could not have been completed without the system of taxation that also happened to include human sacrifice.

Today (in America at least) I think it’s fair to say we are much more open in our discussions of the cost of colonization to the native people, but I think a misconception has arisen that people at the time universally thought what they were doing was just. Undeniably some/many did, but one of the tragedies of the post-conquest restructuring is that there were Spanish contemporaries who, if not initially, eventually recognized the cruelties Pizarro, the conquistadors, and the [Encomienda system](https://en.m.wikipedia.org/wiki/Encomienda) were inflicting on the natives in the effort to stamp out any trace of human sacrifice or paganism. Those complaints were taken seriously by the Spanish crown initially and were simply ignored in the colonized places because, well, Latin America is very far removed from Spain, oversight was limited, and the profits of killing so many indigenous people were so immense that, in some of the same sense that we today look the other way toward labor exploitation in poor countries, people were willing to look aside at how awful the replacement system was for the natives. [Bartolome de las Casas](https://en.wikipedia.org/wiki/Bartolom%C3%A9_de_las_Casas) comes to mind as a complex person who generally agitated for change. I cannot recall which resource or whose account it was I was reading, but one of the grievances against the Spanish crown in the post-Inca environment by later Peruvian writers was that the Spanish had secured native aid on the promise of reduced taxation, only for those natives to find that once Pizarro had settled into power, even though they had stopped the ritual kililng as a form of taxation, far more were dying of the commercial labor tax being exacted to work as slaves or in royal silver mines.",0
"I'm not a historian by any stretch of the word, nor do I have any African heritage, but I believe I have three specific skills that will help me to adequately answer the two main questions being asked. First, I took a history of Hip-Hop course in college-- it was eye opening and engaging! Second, I dated a girl who was obsessed with Kate Moss. Third, as a major in sociology and globalization, I am always interested in the ""how"" and ""why"" certain cultural institutions exist. As Hip-Hop and ""Pop Culture"" Model/Fashion trends are the two worlds colliding, let's fix those lenses in to place and examine the topic closer.


I'm going to do my best to stay focused to these topics exclusively, but at the root of this all there is a much larger history of African Diaspora and its ever-evolving cultural impact not only on people of African descent who were moved out of Africa via slave-trade, but also its impact on the cultures where those larger African populations now reside. As I go along, there are a few more areas of sociological, cultural and historical interest that I'll mention while attempting not to get too distracted. I'll include some texts that might help you learn more about these areas at the end.


To begin at the nearest beginning, Hip-Hop as we know it today evolved from a cultural out-pour in the late 1970's and early 1980's from African Americans in highly urban environments. The culture of hip-hop manifested itself through four main avenues: graffiti or tagging (visual representation); b-boys or break boys and b-girls (dance); DJs or Disc Jockeys (audio); MCs or Masters of Ceremony (verbal.) Again, to stay focused, we'll fast-forward to specifically DJs and MCs.


Within these dense metropolitan areas, street hangouts and block parties (which were already a common event) began to occur in more modern formats. Although it grew up from humble/varied gatherings, what we look back at today is generally this formula: a DJ (or several) would set up an area (or stage, if available) to play music while friends and neighbors gathered to dance and socialize. The MC (a much less significant role in the beginnings) was there merely to introduce the DJ or entice the crowd-- a hype man, if you will. However, as events went on, gatherings would produce sometimes multiple MCs and they were much more active in livening the crowd by engaging them through call-and-response.


Within the realm of African tribal musical styles exists a very prevalent ""call-and-response"" music. This is a style that persisted in the U.S. from field songs sung by African American slaves (an individual or few leading a chorus, and then the rest responding with the refrain) to gospel choirs and Baptist churches (the preacher giving gospel and then pausing to allow the congregation to respond aloud with affirmations) up to the topic at hand- MCs. This call-and-response from multiple MCs with the crowd and even with each other, helped develop what we refer to as ""Rapping"" today.


Cut to Sir Mix-a-lot, a rapper recording ""Baby Got Back"" in 1991 and releasing it in 1992. The focus of the song is not just, as mentioned, a preference for bigger butts, but is additionally a cultural value statement. With Hip-Hop (and Rapping) rising in popularity enough to gain a U.S. national and global spotlight (Baby Got Back was the #2 selling song in the U.S. for 1992, spent five weeks as the number one song on the Billboard Hot 100 and received a nomination by many musical awards ceremonies, including the Grammy's, as best Hip-Hop song of the year), it became a platform to really vocalize and amplify the repressed cultural expression and representation that this particular generation of African Americans were experiencing.


Again, without diving off the deep-end, there's a wealth of history and exploration if you're interested in learning about other Hip-Hop artists/regions and their varied messages (no surprises: police brutality and discrimination were definitely on the radar as a hot topic.)


The opening vocalist, Amelia Rivas (Dorsey), had commented later that the song was mostly just a challenge to the then current (predominantly white) cultural preference toward dubiously skinny models as the ideal aesthetic-- Which is what we will pivot toward now.


Heroin, AIDS, and Grunge music. (I'm told a strong opener/segue can help maintain an audience for longer.)


Flashback, again, to the 1970's and 80' when the AIDS epidemic began to gain enough traction that major news outlets were covering the story. Articles or exposes would make reference to homosexuality (less relevant to our current topic) and heroin, despite the fact that those were not the only causes of the spread. Again, a lot to unpack on this topic as well, but we'll keep moving forward. Regardless of media coverage accuracy, needle sharing was indeed point of contraction for HIV (the precursor to AIDS.) This simultaneously led to a larger public awareness of heroin use in the U.S. and an (even larger) stigma with its relation to HIV/AIDS. Additionally, heroin's presence in some popular songs of the 1970's, 80's and early 90's (Neil Young, The Velvet Underground, The Beatles, Red Hot Chili Peppers, Nirvana, etc.) gave it both a dangerous, but enticing allure. In an effort to steer away from the HIV stigma, heroin use began to be snorted instead of injected. This method of use was more in line with the cocaine heyday of the 1980's and subsequently became more approachable by upper and middle class drug consumers.


Then, in the early/mid 1980's, a grunge scene developed, heavily in Seattle, Washington. As time went on, Grunge music (think Nirvana and Pearl Jam) and accompanying counter-culture grunge aesthetic of the 1980's had gone more mainstream by the late 80's and early 90's. Coincidentally (or not?), Nirvana and Sir Mix-a-lot are both from Seattle, which is the unofficial birthplace of the grunge movement. This rise of popularity is relevant, because some of the main ""looks"" of grunge were pale faces, dark sunken eyes, and underweight figures. To the untrained eye, the grunge look was very much the look of a heroin addict.


The fashion industry separately, simultaneously embraced these grunge-y traits and a very angular, bone-forward look known in the industry as ""heroin chic."" One of the top models to exemplify this look was none other than the aforementioned Kate Moss, although Gia Carangi would be a more foundational example from the early to mid 1980's.


This gaunt, emaciated aesthetic was gaining popularity in the late 1980's and peaked in the early 1990's, only fading in the mid-90's to be replaced by a slightly fuller figure (think Gisele Bundchen, the informal ""new look of fashion"") by the late 90's.


In summary, (1) pop culture and magazines in the 90's did indeed focus on being skinny, bony and waifish because of the hype that the grunge counter-culture movement brought alongside the fashion industry's heroin chic-centric models. (2) Both of these microcosms were predominantly white in creation, ownership and perpetuation, and Sir Mix-a-lot's song ""Baby Got Back""- being rooted heavily in African American culture and expression- was indeed a response to them.


I hope this helps at all and does some justice to the subject matters at hand. It was very rushed, so, again, I'm sorry for not going in to more detail on some of the tangential details.


If you're interested in general African Diaspora and its effects on global cultures, consider looking at: 
The African Diaspora: A History Through Culture by Patrick Manning or African Diaspora Identities: Negotiating Culture in Transnational Migration by John A. Arthur


If you're interested in Hip-Hop specifically, consider:
Black Noise: Rap Music and Black Culture in Contemporary America by Tricia Rose (or other books by Tricia Rose) or Can't Stop Won't Stop: A History of the Hip-Hop Generation by Jeff Chang


Edit: Typos DX

Edit edit: Typos, grammar, formatting, clarification & content requests",0
"In 1919, H. P. Lovecraft had a dream, which he described in letters to two friends (*Letters to Alfred Galpin* 87-88, *Letters to Rheinhart Kleiner* 188-189), and formed the basis of entry #25 in his Commonplace Book, a sort of journal where Lovecraft included plots. It was as follows:

> Man visits museum of antiquities—asks that it accept a bas-relief he has just made—old and learned curator laughs and says he cannot accept anything so modern. Man says that 

>> ‘dreams are older than brooding Egypt or the contemplative Sphinx or garden-girdled Babylonia’

> and that he had fashioned the sculpture in his dreams. Curator bids him shew his product, and when he does so curator shews horror. Asks who the man may be. He tells modern name. “No—before that” says curator. Man does not remember except in dreams. Then curator offers high price, but man fears he means to destroy sculpture. Asks fabulous price—curator will consult directors.

> Add good development and describe nature of bas-relief. [Cthulhu]

Steve J. Mariconda notes in ""The Emergence of Cthulhu"" (*Lovecraft Studies* #15, 1987), that on the same day Lovecraft hatched the plot, he had been reading Lord Dunsany - which will be important later.

From this basic idea H. P. Lovecraft outlined [""The Call of Cthulhu""](http://www.hplovecraft.com/writings/texts/fiction/cc.aspx) April 12-13, 1925, and actually wrote the story a year later in August or September 1926 (S. T. Joshi, *I Am Providence* 2.600-601, 636-638).

There were several significant influences on ""The Call of Cthulhu"" - the plot element of creating the bas-relief in dreams is evident from Guy de Maupassant's [""The Horla""](http://www.gutenberg.org/ebooks/10775) (1887); Theosophy, with its cosmic cycles and strange entities that came down to the stars:

> Old Castro remembered bits of hideous legend that paled the speculations of theosophists and made man and the world seem recent and transient indeed. There had been aeons when other Things ruled on the earth, and They had had great cities.

...and A. Merritt's [The Moon Pool](http://www.gutenberg.org/ebooks/765) (1918), which is set in the strange ruins of Nan Matol in Ponape.

> Remains of Them, he said the deathless Chinamen had told him, were still to be found as Cyclopean stones on islands in the Pacific.

But the most immediate influence is the artificial mythology of British fantaisiste Lord Dunsany, which began with [The Gods of Pegāna](http://www.gutenberg.org/ebooks/8395) (1905); there is actually a specific reference in ""The Call of Cthulhu"" to one of Dunsany's stories:

> The Thing cannot be described—there is no language for such abysms of shrieking and immemorial lunacy, such eldritch contradictions of all matter, force, and cosmic order. A mountain walked or stumbled. God! What wonder that across the earth a great architect went mad, and poor Wilcox raved with fever in that telepathic instant? The Thing of the idols, the green, sticky spawn of the stars, had awaked to claim his own.

This is a reference to Act III of Dunsany's [The Gods of the Mountain]():

> MAN You were all green, Master, all green in the gloaming, all of rock again as you used to be in the mountains. Master, we can bear to see you in flesh like men, but when we see rock walking it is terrible, it is terrible.

> AGMAR That is how we appeared to you?

> MAN Yes, Master. Rock should not walk. When children see it they do not understand. Rock should not walk in the evening.

The idea of gods sleeping for a time, seeming dead, appears to be taken from Dunsany's [""A Shop in Go-By Street""](http://www.gutenberg.org/cache/epub/11440/pg11440-images.html) from *Tales of Three Hemispheres*:

> At that he moved heavily and slowly in way-worn carpet slippers, panting as he went, to the back part of his shop, and I went with him. This was a dingy lumber-room full of idols: the near end was dingy and dark but at the far end was a blue cærulean glow in which stars seemed to be shining and the heads of the idols glowed. ""This,"" said the fat old man in carpet slippers, ""is the heaven of the gods who sleep."" I asked him what gods slept and he mentioned names that I had never heard as well as names that I knew. ""All those,"" he said, ""that are not worshipped now are asleep.""

> ""Then does Time not kill the gods?"" I said to him and he answered, ""No. But for three or four thousand years a god is worshipped and for three or four he sleeps. Only Time is wakeful always.""

> ""But they that teach us of new gods""—I said to him, ""are they not new?""

> ""They hear the old ones stirring in their sleep being about to wake, because the dawn is breaking and the priests crow. These are the happy prophets: unhappy are they that hear some old god speak while he sleeps still being deep in slumber, and prophesy and prophesy and no dawn comes, they are those that men stone saying, 'Prophesy where this stone shall hit you, and this.'""

The actual physical appearance of Cthulhu:

> If I say that my somewhat extravagant imagination yielded simultaneous pictures of an octopus, a dragon, and a human caricature, I shall not be unfaithful to the spirit of the thing. A pulpy, tentacled head surmounted a grotesque and scaly body with rudimentary wings; but it was the general outline of the whole which made it most shockingly frightful.

A number of suggestions have been put forth by fans and Lovecraft scholars over the years; Phillip A Schreffler in *The Lovecraft Companion* suggested Lord Tennyson's [""The Kraken""](https://www.poets.org/poetsorg/poem/kraken) (1830); Robert M. Price poo-poo'd the idea in the introduction to *The Cthulhu Cycle* (1996), and suggested the tentacle'd nature of Cthulhu owed something to M. R. Jame's [""Count Magnus""](http://www.thin-ghost.org/items/show/135) (1904), and further notes the close similarities between Cthulhu as an aquatic giant in the last part of ""The Call of Cthulhu"" and the depiction of Dagon in Lovecraft's [earlier eponymous tale](http://www.hplovecraft.com/writings/texts/fiction/d.aspx).

So the *idea* of Cthulhu was percolating in Lovecraft for some time; he borrowed portions of concepts from other writers - artificial mythology, sleeping gods and mountainous size from Dunsany; the alien origin and creepy cults from Theosophy (this is actually made more explicit in the text); the telepathic dream-sendings from Dunsany and de Mausauppant - the octopus/dragon mixture is a little hard to pin down, since Lovecraft never went into specifics about his influences on that in his letters, although he did provide [a sketch](https://lovecraftzine.com/2012/03/23/cthulhu-drawn-by-h-p-lovecraft-himself/) of the idol. But tentacles were not unfamiliar in weird fiction in the period; in addition to M. R. James' ""Count Magnus"" I would also add Arthur Machen's [""Novel of the Black Seal""](http://gutenberg.net.au/ebooks06/0601861h.html) (1895) and the alien invaders from H. G. Wells' [The War of the Worlds](http://www.gutenberg.org/ebooks/36) (1897-1898), both of which Lovecraft had read.

After ""The Call of Cthulhu"" was written, Lovecraft developed the idea further, positing Cthulhu as the leader of an extraterrestrial race of ""Cthulhu-spawn"" in [At the Mountains of Madness](http://www.hplovecraft.com/writings/texts/fiction/mm.aspx) and a Great Priest and cousin to the Old Ones in [The Dunwich Horror](http://www.hplovecraft.com/writings/texts/fiction/dh.aspx). You can read more about that in Price's ""Cthulhu Elsewhere in Lovecraft"" in *H. P. Lovecraft and the Cthulhu Mythos*.",0
">> The later Middle Ages did have knowledge of and practice rudimentary cataract surgery

>***WOAH,*** hold on a second! You can't just drop that and keep going...

Cataract surgery was performed even prior to medieval times, though the real early stuff wasn't cataract *extraction* just yet.  The earliest record of a cataract being operated on may be from Ancient Egypt.  The Code of Hammurabi (approx. 1754 BC) even has mentions of a sliding scale of different prices for ophthalmological procedures.  But if I understand correctly, those procedures were the ""couching"" technique, where the cloudy lens was just pushed out of the way, to the bottom of the eye.

Cataract extraction surgery was done later.  There have been tools found dated to the 2nd century AD that were likely for this use, and the first written records of this procedure comes from 10th century Persian and Iraqi texts, one of which mentions a 2nd century source.  (They cut a hole and sucked them out with a hollow needle.)

What we usually think of now as cataract surgery--extracapsular cataract extraction--was first done on April 8, 1747 by French ophthalmologist Jacques Daviel.

----

F.J. Ascaso and V. Huerva (February 7th 2013). The History of Cataract Surgery, Cataract Surgery Farhan Zaidi, IntechOpen, DOI: 10.5772/19243. Available from: https://www.intechopen.com/books/cataract-surgery/the-history-of-cataract-surgery",0
"I'm sure /u/qhapaqocha is going to come in and hit this out of the park, but I'll make a first pass at it, if that's alright. I can think of four ways women could attain positions of high rank within the Inca system of government.

The first and probably most obvious is to marry the Emperor, the Sapa Inca. His official wife —who was almost always one of his sisters or half-sisters (not step-sisters, thank you to u/MooseFlyer for catching my mistake)— held the position of Coya, Empress, which came with a good deal of religious veneration in addition to the soft power that comes from having ready access to the most powerful man in the Inca world. Below the Coya there were other wives and concubines —both related and unrelated to the emperor— and they had as much personal power and authority as their personal relationship to the emperor and their status as mother of various princes and princesses determined. Imperial favourites and mothers to prominent royals were women of great influence.

The second position that immediately comes to mind is to be the head of a Panaca. The Panaca is an institution unique to the Inca. Emperors were mummified upon their death, and that mummy continued to hold a court in its palace, ruling over much of its former estate, forever. The trouble with a mummified emperor, of course, is that it doesn't talk much. How do you know what it wants to do with all that wealth? Well, a cult of royal courtiers formed around the mummy and interpreted its wishes. Any Inca could join a Panaca —although only one Panaca, and their powers and influences did vary— and this became an avenue for both men and women of unimportant birth or low reputation to align themselves with an organization whose power might almost rival that of the current Sapa Inca. The inner circle around a mummy was often almost entirely female, and though we do not know quite how leadership within the Panaca's courtiers was decided, it may well be that the fact that men had other career opportunities outside the cult dedicated to a mummified emperor saw ambitious women commit themselves more fully to climbing up through the ranks to wield real power over the wealth of the deceased emperors. 

Third, Chosen Women were beautiful girls taken from their parents by the Inca State and put into special houses until they were married to either the Emperor, one of the Emperor's favourites, or to Inti, the Sun God who was the patron god of the Inca.* Now a Chosen Woman had little power, it is true, but a Bride of the Sun who went on to become the leader of a House of Chosen Women —especially the large ones in Cuzco and on the Island of the Sun in Lake Titicaca (then Lake Chucuito)— might be equated to a Mother Superior or an Abbess of a particularly powerful religious institution. What happened within the walls of her domain were her business, and her House of Chosen Women had enormous influence on what happened in the surrounding area.

Finally, the Inca were polytheists, and they were very open to the gods and cults of the people they absorbed into their empire, provided the superiority of the Inca pantheon was acknowledged. In that spirit, female religious leaders and especially women with oracular powers were given as much power within Tahuantinsuyu as they would have held before their worshipers were amalgamated into the Empire. The Inca were also willing to creating new female religious figures of enormous power. Probably the best example of this happened when Mama Ocllo —Coya to Topa Cusi Yupanki and mother of Huanya Capac— died. Huayna Capac adored his mother, and so he commissioned the creation of a new state cult in her honour, including a young woman to be known as Mama Ocllo's Voice, who had the ability to speak on his mother's behalf. Years later during the great revolt when the northern third of the Empire (roughly Ecuador) revolted, several units of pure-blooded Inca were embarrassed during a siege and then publicly berated by Huayna Capac. Now the rest of the Inca military were soldiers fulfilling their tax labour quota, but pure-blooded Inca —Pacuyoks, Ear-Plug Men— considered themselves to be volunteers. They decided to 'unvolunteer' and march home to Cuzco, and only the timely intervention of Mama Ocllo's Voice heaping them with praise and promising them riches turned the deserters around. 

To circle back to your reference to Yzma, while none of these are a perfect fit for her, I think from context we can establish she's not married to the Emperor, and if she was running a Panaca we would see her in close proximity to a mummy and speaking on its behalf. If she was a Bride of Inti running a House of Chosen Women, she wouldn't have Kronk as a henchman, because Houses of Chosen Women were exclusively female domains. Yzma —to the extent that Disney cares about such things— almost certainly was some kind of witch or religious figure. She clearly has magical abilities and the freedom and power to go where she wants, when she wants, doing what she wants.

---

As a footnote, let me explain that asterisk I put after Inti the sun god who was the patron of the Inca. There is a lot we don't know about the Inca religion. There is a theory that I like the sound of that suggests Inti was not actually the Imperial patron in the Inca pantheon. He was more the patron god of their empire-building. As Tahuantinsuyu grew, the Inca built sun temples on heights overlooking their conquered peoples to let the new citizens know who was boss, but Inti as an Empire Builder and Huanacauri as their war idol were probably much more cults of conquest than of true reverence to the Inca. Viracocha, the Creator God and head of the Inca pantheon, was probably the true patron of the Inca, but they did not create many new temples to honour him during the course of their empire-building. They did maintain and glorify the places where others worshiped him, though.

Edit: Minor edits for clarity.

Edit 2: Half-sisters, not step-sisters.",0
"*Rubs hands together gleefully* I can do that! 

First, the work of education historian Johann Neem is really interesting as he focuses on the connection between public education and democracy. His book, [Democracy's Schools: The Rise of Public Education in America](https://jhupbooks.press.jhu.edu/title/democracys-schools) is a solid overview. It also speaks to how the idea of ""indoctrination"" is exceptionally squishy and in the eye of the beholder.

[Wolf at the Schoolhouse Door](https://www.wolfattheschoolhousedoor.com/) and [Schoolhouse Burning](https://www.derekwblack.com/schoolhouse-burning-public-education-and-the-assault-on-american-democracy-1) are two good new releases that both use the history of public education to help the reader better understand current events and efforts to dismantle public education and why so many are pushing for privatization or dismantling of public education. 

At the same time, it's helpful to understand that while the Founders advocated for public education for white children, there were Black educators pushing for public - i.e. taxpayer-funded - education for Black children which is why Kabria Baumgartner's, [In Pursuit of Knowledge: Black Women and Educational Activism in Antebellum America](https://nyupress.org/9781479823116/in-pursuit-of-knowledge/) is such an essential read. 

I could keep going but will limit myself to two more. First, is [Classroom Wars: Language, Sex, and the Making of Modern Political Culture](https://nataliapetrzela.com/books/classroom-wars/) by Natalia Mehlman Petrzela which gets into modern history about ""culture wars"" in the classroom. Also, [Adam Laats](https://adamlaats.net/about/) focuses on the impact of conservatives and conservatism. I've only read his [first book](https://www.amazon.com/Fundamentalism-Education-Scopes-Era-Americas/dp/1137021012/ref=pd_rhf_dp_p_img_6?_encoding=UTF8&psc=1&refRID=2NT5RWNYEZNB5DDEMPY4) but he's done a bunch of shorter writing that's helpful. 

OK. One more. Other historians have raised concerns about his historiography but Joel Spring's [American Schools](https://www.thriftbooks.com/w/the-american-school-1642---2004_joel-spring/9753407/#edition=2818938&idiq=4745833) is a solid overview of the impact of racism and ablism on public education.  

Happy reading!",0
"Juanita Brooks, *The Mountain Meadows Massacre*, and Will Bigley, *Blood of the Prophets* both discuss this. Since historical memory is our framework here, Bigley's section might interest you. He comes at it from the point of view of local collective memory in the later 19th century, the stories that Utah residents told about encountering the Baker-Fancher party in 1857. For example:

> [I]n 1882 Charles Wdlden provided a virtual catalog of the sins of the
emigrants. He recalled seeing about twenty wagons drive into Lower Town and stop
near the gristmill. Willden heard the men ''taking in a loud excited and boisterous manner
profaning and threatening to do bodily harm and kill some of the citizens."" They
boasted they had helped to kill Joseph Smith and other Mormons in Nauvoo and
Missouri ""and that By G--- they would lull some more yet."" 

or

> Seventeen-year-old Ed Parry was working on an irrigation 
ditch when the Fancher wagons passed through Cedar. Parry told how two mounted
men from the party approached a group of Mormons. ""[The man] riding a large gray
mare seemed to be spokesman. He tried to get some of the crowd to buy him a gallon
of whiskey, but none would do it. . . . [He] became abusive, swore at us all, said that
he had the gun that killed Old Joe Smith, and that his company would go on to California
and get an army and come back and wipe out every ---- ---- Mormon.""",0
"Thank you so much for this answer! It’s very informative. And it’s completely understandable, of course you wouldn’t expect the state to murder you and your children.

It’s a shame that there feel to be large parts of the story we aren’t taught about the holocaust which would better serve to protect society against this happening again. We don’t know enough about the little steps and the manipulation that led normal people to commit such crimes.

If anyone’s interested in that I’ve started reading [this book](https://www.goodreads.com/book/show/33917107-on-tyranny) which is fantastic",0
"The short answer is: it's complicated.  The German informed narrative downplayed Soviet... everything.  They talk about the winter and the numbers and Hitler meddling and craft the narrative that the Soviets didn't win, but rather the Germans lost.  You get Manstein talking about the ""asiatic hordes"" and even in Clark we see the comparison to an army of ants overwhelming an elephant etc etc.  

That said, as much as the Germans devalued the Soviets as just a human wave monster, we are only finding out just how much things like:

drunkenness, desertion, ethnic conflict, defection, and morale crises infected the Red Army.  With the opening of the archives, we know that the Soviets hit lower lows than we used to understand, and boy do newer veteran testimonies admit that, but we also have a better understanding of how the Soviet Red Army changed, adapted, and won the war, rather than allowing the Germans to lose it.

Does that make sense?  It is complicated because you get historians like Glantz and House (Western) who are adamant that the Soviets made meaningful changes to their doctrine and warfare and how they actively won the war, and then you get Russian historians like V. V. Beshanov who go into excruciating detail of how poorly led the Red Army was until 1943 (arguably 1944).  Now, with the new historiography, we see higher highs and lower lows.",0
"I'd like to add a bit of a broader perspective to the question, piggybacking a bit on u/DBHT14's answer. I think it's important to contextualize the size and scope of the United States Army in the 1820s, because it was, as an institution and as a part of American culture, *much* different than it is today.

Apart from the uniforms (which are somewhat outside the scope of the question), it's likely to be the *size* of the army that would be most shocking to a modern person. After the Treaty of Ghent, which ended the War of 1812, the congress of the United States restricted the size of the army to its pre-war levels to 10,000 officers and men. That was its *authorized strength*, the legal limit to the size of the army. In doing so, the army disbanded its dragoon regiments,  collapsed its four rifle regiments into a single rifle regiment, but retained its corps of artillery. The infantry, at a wartime strength of fourty-four regiments, was brought down to eight, mostly by merging partially manned and wartime expansion regiments into newly reconstituted ""fresh"" regiments. The paper strength of the infantry was just over six thousand men *total*, with an additional rifle regiment with a paper strength of around 800 men. The small additional cavalry (a single regiment) and the corps of artillery would fill in the rest of the slots.

The problem was that the wartime strength of the regular army was around 30,000 by the end of the war, meaning that more than 20,000 men, mostly short-term members of the volunteer regiments raised for the war, would be expelled, along with 1,700 officers.

This may seem, living in the post-World War II event horizon, somewhat odd. But this happened after the War for Independence, as well, and it would happen again after the Mexican War and *again* after the Civil War (I talk a bit about this [here](https://www.reddit.com/r/AskHistorians/comments/a64f5v/why_did_ranks_such_as_those_of_general_custer/ebrpkt3/) ), and it should be understood as a reflection of the political ideas surrounding a standing army. To be short and blunt, standing armies - and soldiers, by extension - were viewed as pernicious outgrowths of tyrannical power. Or, conversely, they were viewed as manicured footpaths going directly from a well-maintained republic to a brutal tyranny. In either case, *armies* were viewed as moral wastes, as expensive and excessive embodiments of unrestrained power, as tools of political malefactors, and more or less turned every problem faced by a growing republic as one that needed *force* to solve. The lure and the supposed ease of using force was something that American politicians (and enlightenment thinkers in general) traced in the dramatic fall of the Roman Empire, and more recently, saw unfold in Revolutionary France. Massively reducing the size of the army after it had fought a war was an expected and a welcomed result of living in a society that was structured to maintain freedom for its citizens. Many modern writers tend to view this through a lens of disbelief or chagrin, but if we put our feet into the shoes of a citizen of the United States in the 1820s, this was neither surprising nor distressing, but a mark of a well-ordered society.

Those with military inclinations, like Poe himself, could find their place within local militia regiments. Poe, then 15, joined up with a local honor guard that formed and marched in the parade that welcomed Lafeyette to his home town in 1824. Local militias were meant to be reflections of the local reputation network, and tended to have ""elites"" - either men with money and means, or men with experience - elected as leaders, with the rest working to gain military (as well as social) skills for the betterment of their community, as opposed to the army, which was so easy to be misused for the benefit of a few alienated politicians. These were, at least, the *ideals*, but the model of the interested local militia and the aloof, mercenary regular army was breaking down even by the early 1820s. Again, more details and information [here](https://www.reddit.com/r/AskHistorians/comments/amq2zc/what_was_the_internal_rankingorganization_of/efo6zp9/?context=3) and [here](https://www.reddit.com/r/AskHistorians/comments/a2i1gi/how_organized_was_the_early_us_militia/eb04lk5/?context=3).

By January 1817, the army maintained a surprising portion of its manpower, with the secretary of war reporting that there were 10,024 officers and men in the United States Army, but even this was short lived, as in 1821, the size of the army was reduced, again, to a *total strength* of six thousand. Throughout the 1820s the army had difficulty maintaining even its paper strength. The artillery, which Poe joined, had a paper strength of 2,180 and in March, 1825, totalled 1,921. It was *this* army that Poe joined - a small, mistrusted force scattered across dozens of tiny forts and flung into border skirmishes (the Blackhawk War, the First Seminole War, etc) as they happened.

Life at the forts also would have had an effect on Poe's swift rise to Sergeant Major. Ranks and Posts were different things; artillery companies did not have First Sergeant positions, which were *company* appointments, but they did have post positions, like *ordinance sergeant*, which carried higher pay, but were limited in their authority to their job on the post. A sergeant major is a rough equivalent to first sergeant.

Poe's experience can further be qualified by the fact that, as a native-born American from a somewhat wealthy family, he *stuck out like a bleeding wound* in the enlisted ranks. Given the inherent suspicion of the American polity toward armies and soldiers, wealthy young men joining the army in peacetime were exceedingly rare. Most new recruits were immigrants, many of whom may not have spoken english, desertion was rampant, and discipline problems omnipresent. Which in a year of Poe's enlistment, the army not only experienced mutiny at West Point, but had a soldier murder another, *and* an attempt to murder an officer in a short lived Christmas Mutiny at Fort Mackinac, in Michigan. If he served consistently even for a short time, he'd very likely stick out to his officers, and justify his quick prominence within the tiny post life.

I could go on and on, but I hope this helped a bit. Happy to answer follow-ups!

_________________

All the numbers came from Gregory Urwins *The United States Infantry, An Illustrated History*.",0
"Hmmm. Truthfully, I wasn't familiar with this particular claim concerning royal tombs. However, there *is* precedent for some controversy over the (likely) foreign origins of the Japanese royal line.

**Note:** A lot of this is owed to remarks made by Emperor Akihito on December 23rd, 2001, which unfortunately falls within our sub's 20-year limit. Further complicating things is the *context* in which he made these remarks, which is how I found out about them in the first place. North Korea's my usual specialty and you'd think this would be a little out of the way, but soccer's one of my hobby fields, and ...

**Q:** Who jointly hosted the 2002 World Cup? 

**A:** South Korea and Japan. 

**Q:** Was it a long, difficult, and expensive process made even harder and more diplomatically nightmarish by Japan's colonization of Korea between 1910 and 1945 and its repressive and exploitative treatment of Koreans during World War II? 

**A:** Does the Pope shit in the woods? 

Anyway, I'll try to keep to historical concerns. Japan isn't my field and I hope you'll get someone with a better handle on this, but maybe it'll be a little helpful regardless.

**The short answer is yes: Japan's imperial family is likely part Korean in origin:** Akihito gave a press conference on his 68th birthday in December 2001 and said:

>*(Korea) contributed greatly to Japan's subsequent development ... I, on my part, feel a certain kinship with Korea, given the fact that it is recorded in the ""Chronicles of Japan"" that the mother of Emperor Kanmu was of the line of King Muryeong of Paekje.*

**The ""Chronicles of Japan"" that the emperor was referring to is a series of works well-known to East Asia scholars.** In the early 8th century, the Japanese imperial family commissioned a history of the nation and its own bloodline. Some of what resulted, particularly in the first volume -- e.g., collections of folk songs, stories about the creation of the world and the rise of the gods, the divine rights of the imperial family, a suspiciously convenient set of excuses for military failures, and some fudged dates to make the family look older than it actually was -- isn't really what we'd expect out of history books today. It doesn't matter. Honestly, it's stellar work for its time, and it's an invaluable window into the period and a foundation text if you're serious about Japanese history and culture.

**The second volume, the *Nihon Shoki* (日本書紀) was completed in ~720 CE.** It describes how King Muryeong of Paekje sent his second son, Junda, to Japan as a diplomatic envoy with tribute in 505 CE. Paekje was one of three Korean kingdoms in this period (known, with impeccable logic, as the Three Kingdoms period), located on the southwestern portion of the peninsula. Paekje was a regional naval power with extensive trade links to both Japan and China, and Muryeong already had ties to the Japanese throne. His uncle had served Emperor Yuryaku, and Muryeong himself is recorded by the *Nihon Shoki* as having been born on an island in Japan. Anyway, Muryeong's son Junda remained in Japan for the rest of his life, leaving children when he died in 513 CE.

**Roughly two centuries after Junda's death, his great-great-great-great-great-great granddaughter (1), Yamato no Niigasa, was born in 720.** She became the consort of Prince Shirakabe (later Emperor Konin) at what we would consider to be a horrifyingly early age. In 736-7, she gave birth to Prince Yamabe, who became Emperor Kanmu in 781 after his father abdicated. It was actually during Kanmu's reign in 797 that the third volume of imperial history, the *Shoku Nihongi* (続日本紀) was completed, recording the aforementioned.

So you don't actually have to go digging anywhere to find evidence that the Japanese imperial family is partly descended from a Korean bloodline. *It's literally there in the court records*.

**But are the imperial records accurate?** Always a good question. When examining the histories, scholars generally distinguish between: a). the material that's pretty baldly intended to shore up the legitimacy of the imperial family (e.g., its descent from the goddess Amaterasu, and records of the earliest emperors, whose existence no one's been able to verify), and: b). the material that they can confirm with other sources or that appears to have been an apolitical record of events at court. For example, portions of the *Nihon Shoki* record when refugees from Silla (the Korean kingdom on the southeastern portion of the peninsula) arrived and were given land, and when members of the royal family travel to the hot springs. We know that infighting between the Korean kingdoms sent people to Japan, and the *Nihon Shoki* references a number of Korean place names that the Japanese imperial court is otherwise unlikely to have known. And while we can't necessarily verify, say, a trip to an inn built around hot springs, we know that that was (and remains) a normal thing in Japanese culture. Interestingly, the [oldest inn in Japan](https://en.wikipedia.org/wiki/Nishiyama_Onsen_Keiunkan) dates to 705, before the *Nihon Shoki* was completed.

As an aside, if you've been keeping up with the news, you'll likely have seen reports that Emperor Akihito has abdicated (as of today, as a matter of fact). 

**(1)** I really hope I got that right and apologize if I didn't. I had to resort to counting on my fingers multiple times. Genealogy is not my strong suit.",0
"There would certainly be a problem if I had written North Sea culture! The plurality is expressed to avoid conflation, and I hope I made it clear how nearly impossible it is to generalize because folklore changes over time and space and cannot be easily pinned down.

That said, when it comes to the pre-modern diverse traditions associated with social supernatural beings of nature more ties the region together than separates it - something that seems unlikely given the extremely complex linguistic and historical legacies one encounters - and with all the other cultural differences that encounters now and at any point in history. This peculiar body of fairy/elf/pixy/hidden folk, etc. traditions share a great deal while each having their own expressions. This has been observed as discussed by many authors, but we can go back to Elisabeth Hartmann (1912-2005) and her Die Trollvorstellungen in den Sagen und Märchen der Skandinavischen Völker – The Troll Beliefs in the Legends and Folktales of the Scandinavian Folk, to see the idea developed as early as 1936 (this under the direction of Carl Wilhelm von Sydow (1878-1952) and Sven Liljeblad (1899-2000)). More recently, the work of Bo Almqvist (1931-2013) explores the subject with all its nuance as do I (I hope with appropriate nuance) in my recent book, [The Folklore of Cornwall: The Oral Tradition of a Celtic Nation](https://www.amazon.com/Folklore-Cornwall-Tradition-Celtic-Nation/dp/0859894703) (2018).

You are quite right to point out that the ""Church"" was not monolithic. At the same time, it is not always possible to express all appropriate nuance in the brevity of a reddit answer - even when an answer as I provided cannot be accused of being brief! Let us say, however, that with the few Anglo-Saxon sources that actual exist where elves are mentioned, one senses an attempt to put elves ""in their place"" - as part of the realm of evil as expressed by the Beowulf poet. No doubt it was not the only effort and others felt more kindly toward the elves - a very nice Irish example that you cite. Perhaps we see some degree of kindness toward elves as expressed in the name that inspired OP with the question that set all of this off!

I purposefully did not mention the Old Norse sources because they introduce so many other issues. If I had mentioned them, I would not have made the mistake to suggest that they were early medieval - a period during which we know next to nothing about Scandinavian traditions. When I did mention the Scandinavian-speaking world, it was only in the context of pre-modern traditions and the attempt to stretch them backwards to a former time. And I hope I gave proper due to the problems inherent in such an effort.

As for the witch trials - if I assumed incorrectly that our readers would not understand that I was writing about much later events, my apologies. I dropped that example in to demonstrate that Christian efforts to cast our poor elves and their kindred into Hell was ongoing. It is good that you remind the readers that my example was not early medieval or Anglo-Saxon! It is one of the problems with using too few bits of information to reach back in time to understand the Anglo-Saxon period, which in itself has so little offer on the subject. As I indicated, there are inherent problems with using later sources to understand a time many centuries before, but we are forced to use what we have given the paucity of Anglo-Saxon information.

A great recent book on Anglo-Saxon elves is Alaric Hall, Elves in Anglo-Saxon England: Matters of Belief, Health, Gender and Identity, 2009.

A book that includes information on how church officials equated associations with elves with demons during the witch trials can be found in Lizanne Henderson and Cowan, Edward J., Scottish Fairy Belief: A History (East Lothian, Scotland: Tuckwell Press, 2001).

Reidar Th. Christiansen also wrote on the topic of the Northern European elf traditions, but I find inspiration in Almqvist's great work, Viking Ale: Studies on Folklore Contacts between the Northern and the Western Worlds, edited by Éilís Ní Dhuibhne and Séamas Ó Catháin (Aberystwyth: Boethius, 1991). But let's give Christiansen his due - see his ‘Some Notes on the Fairies and the Fairy Faith’, Béaloideas, 39/41 (1971–1973), pp. 95–111.

I hope this provides some clarification. Thanks to you for your efforts to add nuance to a topic that deserves it. I would not want to irritate the elves with too many generalizations. They can be quite vindictive if not treated properly!

edit: readers may be interested in [""Nazis, Trolls, and the Grateful Dead,""](https://www.academia.edu/38773351/Nazis_Trolls_and_the_Grateful_Dead) a brief article I wrote about von Sydow, Liljeblad, and Hartmann - their student who produced the benchmark study on Northern European trolls and the Celtic fringe counterparts.",0
"Part 2: With this in mind, as a young, eligible bachelor you are far more likely to be courting your betrothed’s father than her directly! The formal process for asking for a woman’s hand in marriage was to ask her male guardian (*kyrios*, usually the father if he is alive) and enter into a contract of sorts with witnesses to confirm the agreement. The bride herself would probably not have been consulted. This contract is termed the *engye*, literally a “pledge” by handshake. Outside of some vase painting depicting this important moment in the marriage contract, we have little evidence for how an ordinary Greek might attempt to convince a father to approve of the marriage. Among the aristocracy in archaic Greece *hedna* or “bridewealth” was offered – gifts given by the groom to the bride’s father when the marriage was agreed (see for example Hector and Andromache in the Illiad, 22.472). Over time this shifted to the father offering the groom a dowry, often a sum of money. Blundell (1998, pp. 67-9) argues that by providing money in the form of a dowry to the groom it ensured his daughter was provided for whilst ensuring that the father’s most important property - his land, was not touched and could be passed on to his sons. It also protected the woman outside of the home, as if the marriage ended in divorce the groom would be expected to repay the dowry, thus acting almost as an insurance against the groom mistreating his wife as if he did the marriage could be annulled and the dowry had to be paid back. 

How the father determined whether the groom was worthy of marrying his daughter would likely be the mirror of the man’s desires: is he from a good family? Can he provide for my daughter? Does he show good character? Perhaps the best (and funniest) example we have of this comes from the marriage of Agariste referred to in Herodotus. Agariste was the daughter of Cleisthenes, the tyrant of Sicyon. According to Herodotus in the late 7th century BC he held a contest to determine who would have his daughter’s hand in marriage. Suitors from across the Greek world came to Sicyon and competed in athletic games: chariot racing, wrestling, archery and other tests such as discussions and interviews. In the end an Athenian called Hippocleides was looking like the best candidate. However on the day Cleisthenes was to make his decision, Hippocleides got drunk at the banquet, got onto a table and started dancing by balancing on his head and waving his legs in the air (ancient breakdancing!?). Cleisthenes was so disgusted by Hippocleides behaviour that he marched up to the table, and said to him “son of Tisandrus, you have danced away your marriage”... Hippocleides snorted back “Hippocleides doesn’t care!” - according to Herodotus (6.128) “Hippocleides doesn’t care” became a proverb (similar to “who gives a shit!”). 

So as it seems one rule of courtship from the Ancient World still persists today: don’t get drunk and make a fool out of yourself in front of your father-in-law! 

Hope this has helped! 

 

Bibliography 

Blundell, S. (1995) *Women in Ancient Greece* pp. 67-69. 

Kraut, R. (2008) “Plato on Love”, in *The Oxford Handbook of Plato*  

Konstan, D. (2018) *In the Orbit of Love: Affection in Ancient Greece and Rome* 

Loven, L. L., Strómberg, A. (2010) *Ancient Marriage in Myth and Reality* 

Oakley, J., Sinos, R. (1993) *The Wedding in Ancient Athens*

Edit: some typos fixed as I had to keep coming back to this whilst doing other work.",0
"No, in medieval Europe,  there were no restrictions on common soldiers killing a king or any other aristocrats. If soldiers were able to identify a particularly important individual, there would probably have been a mad scramble to capture him rather than kill him outright, both for political purposes and for the potential for ransom profits. This happened to the French king Jean II at Poitiers, who was taken prisoner in a furious melee where other high-ranking French aristocrats, who ordinarily would have made excellent ransoms, were killed (possibly by accident as English troops swarmed to grab Jean specifically). When kings were killed outright in combat, it does not seem to have been a particular mark of controversy. John the Blind, King of Bohemia, died at Crecy but his killer was not specifically identified in accounts of the battle and the story of his death is conveyed merely as an illustration of John's bravery and valiant nature. Kings who fought did so in the full knowledge that they were at risk of being killed or captured at the battlefield.

English military contracts from the Hundred Years War suggest that there was no stigma attached to a common soldier capturing important aristocrats; they often contained clauses saying that any particular important prisoners (like a prince or something) would be handed over to the king in exchange for a cash payment. This worked to both parties'  advantage: common soldiers would probably not be be financially able to house and feed a prisoner (especially a royal prisoner) during the period in which the ransom was negotiated and paid. If there was any limitation on killing aristocrats, it would be that troops wanted to maximize their profit by taking nobles alive for ransom. This is most apparent in the hesitation of some English troops to kill their prisoners at the Battle of Agincourt. This incident is often taken as some kind of commentary on class relations, but in reality is just a reflection of soldiers looking out for their bottom line. ",0
"**TL;DR**

Because that's their goal, and it works.

**Silver Spring Monkeys**

First up, the Silver Spring case is an interesting starting point for PETA, because Edward Taub's work with the monkeys turned out to be groundbreaking research into neuroplasticity, and led to the development of [constraint-induced movement therapy](https://web.archive.org/web/20100118063829/http://www.strokeassociation.org/presenter.jhtml?identifier=3029931), which is used to help stroke victims recover. Additionally, PETA's undercover investigator, Alex Pacheco, [waited until Taub was on vacation to ""document the lab"" (jump down to Silver Spring Monkeys),](https://web.archive.org/web/20090710103241/http://dels.nas.edu/ilar_n/ilarjournal/40_1/40_1Roots.shtml) giving Taub a stronger defense that Pacheco intentionally trashed the lab before documenting it. This defense was more plausible because the state prosecutor joined PETA, the monkeys were housed with PETA's president by the state, and the monkeys mysteriously disappeared from her house and returned with no explanation.

And true to form, PETA didn't care about the research, didn't care about the goals, didn't care about anything other than the welfare of the monkeys under experiment. Whether you agree with PETA (that intentionally removing sensation from limbs was unethical) or Taub (that the research was important work for people who have nerve damage), the Silver Spring case was a complete harbinger of PETA's all-or-nothing tactics and unwillingness to accept nuance.

**ALF**

Not long after Silver Spring, the first American Animal Liberation Front cell was formed. PETA, ostensabily, is non-violent. ALF claimed to start out non-violent, but is considered a terror group by the FBI, and had ties both to the Earth Liberation Front (ELF) and PETA. PETA gave money to ALF, acted as the spokesgroup for the front, and there has been documented cross-pollination between the two groups. [The ties between PETA, ALF, and ELF are covered in this Senate committee hearing report.](https://www.govinfo.gov/content/pkg/CHRG-109shrg32209/html/CHRG-109shrg32209.htm)

Note: PETA's president, Ingrid Newkirk also wrote in ALF's ""newsletter"". The ties were never hidden, nor meant to be hidden.

**Ill-considered publicity stunts**

Let's first start that their reputation for Ill-considered publicity stunts is well earned, and comes from the fact that it's the point of the group. From [interviews given to the New Yorker in 2003](http://www.michaelspecter.com/2003/04/the-extremist/), PETA's president Newkirk invited the reporter into their weekly war council:

>Each week, Newkirk holds a kind of war council: she gathers two dozen of  her top strategists around a square table in the second-floor  conference room to plot their next moves, and while I was in Norfolk she  invited me to join them. Jason Baker, who runs the peta operation in  Hong Kong (there are also offices in England, Germany, Holland, and  India), presented a slide of a new advertisement he was preparing for  the Asian market to publicize the plight of elephants. It is a picture  of a naked woman, shackled and in chains. (The woman, Imogen Bailey, was  recently voted Australia's sexiest model.) ""We are going to put whip  marks on her back,'' Baker explained to approving mutters, ""and, if it  works visually, tears in her eyes."" Newkirk stared at the picture for a  minute and then shook her head. ""She looks like she's pouting,'' she  said. ""It's too sexy. We need to make her look terrified."" Baker promised to take care of it.

Remember - this is Newkirk's firsthand account of her own thought processes. The ""ill conceived publicity stunts"" happen because PETA morphed into a publicity stunt generator. Between Silver Springs and our 20 year cutoff in 2001, PETA's growing budget wasn't spent on investigation, it started being spent more on PR than investigation ([as seen in the oldest IRS Form 990 I could find, from 2004](https://www.scribd.com/doc/206381561/peta9902004?secret_password=1l58duc1kcbc0ur12it4#fullscreen&from_embed)).

Maybe someone in later comments can add more specifics pre-2001 to some of the publicity stunts, but a notable 2001 stunt was lamenting orphaned animals in the wake of 9/11 and demanding that Rudy Giuliani put together an animal-focused task force.

**Impact**

PETA's impact in animal rights cuts many ways:

Animal Cruelty Enforcement: their activism has raised the profile of animal rights and inspired many other mainstream animal-focused groups that are more normal. They've inspired new animal cruelty laws, increased enforcement of those laws. For all the focus on ""PETA runs stupid campaigns that anger everyone"", Animal Cruelty is simply an issue that was seen differently and more importantly in 2000 than in 1980, and is seen as more important now than in 2000.

Anti-Animal Investigation Enforcement: Their tactics (such as infiltrating laboratories and big ag sites) also have led to acts such as the [Animal Enterprise Protection Act in 1984](https://www.govtrack.us/congress/bills/102/s544/text) to allow the DoJ to target animal rights groups for infiltrating and damaging agricultural, and efforts by research and agricultural issues (into the last 20 years) to criminalize other activities. Ag gag laws actually help PETA, because to people who care about animals, it makes Big Ag look worse just by pushing for them.

Consultation and Improvement: As PETA is a PR engine, companies quickly realized that they needed to combat PR with PR, but also to actually improve. For example, the agriculture industry began hiring consultants such as Temple Grandin to make their operations less abusive. Research into agricultural and animal bioethics increased through the period (leading to dedicated research groups at major universities now, such as the [Candace Croney Research Group at Purdue University](https://vet.purdue.edu/discovery/croney/)). In 2000, McDonalds [became the first fast food company to enforce supplier standards around treatment of hens](https://www.washingtonpost.com/archive/politics/2000/08/23/mcdonalds-tells-farmers-to-treat-chickens-better/e03b291a-d563-4321-b14e-57ec73338952/) (a practice that has since accelerated and become more industry standard).",0
"While it's not Nazi political leadership, Otto Hahn, Werner Heisenberg, Carl Friedrich von Weizsäcker, Kurt Diebner, Karl Wirtz, Eriche Bagge, and Walther Gerlach, that is to say the scientific leadership of the German nuclear program and several geniuses in their own right, were taken into custody by the allies. Hoping to acquire intelligence from them, their conversations were ~~superstitiously~~ surreptitiously recorded, including their response to the Hiroshima bombing. [The transcript](http://germanhistorydocs.ghi-dc.org/pdf/eng/English101.pdf) gives us a rare, reliable glimpse of historical figures' candid reactions. It's actually quite readable (abridged to only 12 pages) so you may want to just read it instead of my disjointed summary.

Though the German scientists were not on track to weaponize their research in 1945, they were still among the world's top nuclear scientists, allowing them a uniquely knowledgeable perspective on the attack. Some like Hahn had even made discoveries that enabled the bomb. The most direct answer to your question as it is phrased though comes from Weizsacker and Wirtz:

>WEIZSÄCKER: If we had started this business soon enough we could have got somewhere.
If they were able to complete it in the summer of 1945, we might have had the luck to
complete it in the winter 1944/45.
>WIRTZ: The result would have been that we would have obliterated London but would still  
not have conquered the world, and then they would have dropped them on us.
>WEIZSÄCKER: I don't think we ought to make excuses now because we did not succeed, but we must admit that we didn't want to succeed. If we had put the same energy into it as the Americans and had wanted it as they did, it is quite certain that we would not have succeeded as they would have smashed up the factories.  
>DIEBNER: Of course they were watching us all the time.  
>WEIZSÄCKER: One can say it might have been a much greater tragedy for the world if Germany had had the uranium bomb. Just imagine, if we had destroyed LONDON with uranium bombs it would not have ended the war, and when the war did end, it is still doubtful whether it would have been a good thing.

So while the scientists were relieved not to have been targets, they were particularly relieved that Germany had not used nukes. It should be noted that though the victims were Japanese, not German, the scientists were horrified all the same:

>WIRTZ: I'm glad we didn't have it.   
>WEIZSÄCKER: I think it's dreadful of the Americans to have done it. I think it is madness on their part.  
>HEISENBERG: One can't say that. One could equally well say ""That's the quickest way of
ending the war.”  
>HAHN: That's what consoles me.

Gerlach though felt the opposite, regretting that they had not worked harder and predicting how atom bombs would be used for all history since Nagasaki, more as threats than as weapons:

>GERLACH: No. We never worked on the bomb. I didn't believe that it would go so quickly. But I did think that we should do everything to make the sources of energy and exploit the possibilities for the future. When the first result, that the concentration was very increased with the cube method, appeared, I spoke to SPEER's right hand man, as SPEER was not available at the time, an Oberst GEIST first, and later SAUCKEL at WEIMAR asked me: ""What do you want to do with these things?"", I replied: ""In my opinion the politician who is in possession of such an engine can achieve anything he wants"". About ten days or a fortnight before the final capitulation, GEIST replied: ""Unfortunately we have not got such a politician"".  
>HAHN: I am thankful that we were not the first to drop the uranium bomb.  
>GERLACH: You cannot prevent its development. I was afraid to think of the bomb, but I did think of it as a thing of the future, and that the man who could threaten the use of the bomb would be able to achieve anything.

So among the nuclear scientists (except for Gerlach), there was a sense of relief. But not a relief that Germany was spared--relief that the bomb was not used on England. A relief that, in a way, they could claim to be on the right side of history. Weizsacker sums it up:

>History will record that the Americans and the English made a bomb, and that at the same time the Germans, under the Hitler regime, produced a workable engine. In other words, the peaceful development of the uranium engine was made in Germany under the Hitler regime, whereas the Americans and the English developed this ghastly weapon of war.",0
"(2 / 4)

**JAMES MOORE WAYNE**

Of all eight members of the court, Wayne may have been the least publicly explicit on what he believed. However, his actions make it pretty evident what he did believe. For one thing, he had a son—a West Point graduate—who joined the Confederacy, who Wayne gave his blessing to, but he himself never defected. Wayne did understand there were risks involved whatever he chose to do—he had his land in Georgia confiscated by the Confederate government, as it turns out, though he did work behind the scenes to get some of it into the possession of his Confederate son. 

Still, he remained loyal. He not only kept his seat throughout the war and beyond, but of the pre-war Democrats who served during the Civil War, he actually turned out to be the most reliable ""War Democrat"" of any of the justices from slave states (and probably of all the Democrats aside from Grier). Upon the first session of the Supreme Court after Fort Sumter, in August 1861, [the *New York Times* wrote](https://www.nytimes.com/1861/08/06/archives/judge-wayne-of-the-united-states-supreme-court.html) of him upon his return:

> ""[Judge Wayne] is in Washington, prepared for the discharge of his judicial trusts, regarding himself as a citizen of the United States, and ready to defend and uphold the authority of the National Government, though his State is among the foremost in the rebellion that would overthrow it.""

As reprinted in the book *James Moore Wayne, Southern Unionist* by Alexander A. Lawrence, Wayne's son would later recount their conversation when he told his father of his decision to defect to the Confederacy:

> ""[A]t the same time that he approved of my resignation as a military man and my return to my native State, he told me that he could not resign.""

There were surely words exchanged on the topic of secession and the Southern cause during this conversation, but Wayne's son did not write about them. But in a later private letter, Wayne himself gave his reasons for remaining on the court:

> ""...[C]ases might come before the Court in which it would be necessary for the South to be represented, and if the Southern judges abandoned their positions there could be no judicial voice in behalf of the South and her Constitutional rights....I expect to be misunderstood and misjudged but I shall leave posterity to do me justice.""

Ironically, though, as Lawrence writes: ""less than any member of the Supreme Court Wayne was to articulate the Southern viewpoint during the war"".
 
In a review of Lawrence's book for the *Fordham Law Review*, George Gordon Battle characterized Wayne's decision and Civil War jurisprudence this way:

> ""In considering the reasons for his action it must be remembered that Mr. Justice Wayne had been from his early days a devoted adherent of the principles and of the party of Andrew
Jackson...Undoubtedly the resolute determination with which President Jackson proclaimed and maintained that the Federal Union was indivisible and that it must be preserved, must have
had a very great effect on the mind of James Moore Wayne during his early and more impressionable years. He became himself a devoted adherent of the Union; and he was equally positive in his opposition to the doctrine that the States had the right to secede.""

Like Catron, there is little doubt that Wayne believed that not only was secession and war unconstitutional and illegal, but he appeared to believe it was an act of treason. He would sign on with the majority in the Supreme Court's 1863 *Prize Cases* decision, which used the words ""traitors"" and ""treason"" several times to describe the Confederacy's actions, such as in this passage:

> ""They [Confederates] have cast off their allegiance and made war on their Government, and are nonetheless enemies because they are traitors.""

Though, like many of the time, Wayne did believe that people were free to make their decision on the issue from the extra-legal standpoint of revolution, which leads into what Campbell decided to do.

**JOHN ARCHIBALD CAMPBELL**

Campbell is the justice who resigned, but even he didn't appear to believe that secession was an actual Constitutional or legal remedy. On November 26, 1860, he wrote a letter to his former law partner and current brother-in-law, which was published in his hometown paper the *Mobile Daily Mercury* on December 22, 1860. In the letter, Campbell characterized secession as a ""coups d'etat"" or illegal overthrow of government:

> ""...I have too much confidence in the constitution of my country to suppose that it does not afford a sufficient remedy in case of his [Lincoln's] wickedness. It is our pride and glory that for all the evils of government, there are constitutional modes of redress for every citizen. I am wholly unwilling to dishonor them before the civilized world, by any *coups d'etat* or insurrections against their authority except as a last resort.""

In other words, secession was not a legal act, but an extra-legal act of revolution/rebellion. He believed it could be justified in the same terms that James Madison had described it a generation earlier during the Nullification Crisis - that a minority may exercise such a right if the ""usurpation and abuses"" of law render ""passive obedience and non-resistance a greater evil than resistance and revolution"". But as Madison said, revolution comes with the risk of losing, the result of which is going to be trials for treason and other legal repercussions for the illegal act of rebellion.

As of December 1860, Campbell didn't think resistance was justified. By Inauguration Day, he had already prepared his resignation letter, but he delayed delivering it because, he believed ""if a little time be given"", peace could still be achieved. He hoped that ""PEACE, PEACE, PEACE will be the first and the last thought of every responsible person in both sections"" of the country. 

After Fort Sumter, he decided to resign. But it was still with the implication that he was doing so to *abandon* the rule of law, not to uphold it, since it no longer ""afforded a sufficient remedy"" against Lincoln's ""wickedness"". 

Even so, as Robert Saunders recounts in his book *John Archibald Campbell: Southern Moderate, 1811–1889*, Campbell wasn't exactly sympathetic of the Confederate government, either, as Campbell himself wrote. But after his role as peacemaker failed colossally, one friend wrote that Campbell was  humiliated and embarrassed, which is what led to his resignation. Famed Civil War-era diarist Mary Chestnut wrote: ""A resigned judge of the Supreme Court of the United States!! Resigned—and for a cause that he is hardly more than half in sympathy with. His is one of the hardest cases.""

> *Did everyone manage to put aside the politics and war in order to work together from the bench?*

**SUPREME COURT AS PEACE NEGOTIATORS**

This goes back to what I mentioned about both Samuel Nelson and John A. Campbell. During the Secession Crisis, before Fort Sumter, Campbell believed, with Nelson's urging, that he was more useful staying on the court than leaving it, with the two men in the best position to broker a peace negotiation. With the Lincoln Administration refusing to recognize or meet with any Confederate leadership, Nelson and Campbell took it upon themselves to act as intermediaries, lobbying the new Secretary of State under Lincoln, William Seward, to get Lincoln to withdraw troops from Fort Sumter as the crisis there escalated. Seward gave them some encouraging answers at first, so, when the troops weren't withdrawn, and then there was a battle, and then Lincoln called for troops from the states to end the rebellion, Campbell felt betrayed. The abuses and usurpations evidently, to him, justified a revolution against the U.S. Constitution. On April 26, 1861, he handed in his resignation letter.

Side note that Campbell went on to serve as the Confederacy's Assistant Secretary of War, and would be one of the peace negotiators at the failed Hampton Roads Conference in 1865 (when the Confederacy was already on the brink of losing the war).

So to answer the question, yes, there was *some* effort to put politics aside at the beginning of the conflict, though outside of Nelson and Campbell, none of the others were particularly pro-active. Taney was actually a bit privately antagonistic. He began preparing, in private, some opinions on issues he thought might come before the court. In one surviving fragment, he appeared to be preparing a full-on, direct opinion supporting the legality of slavery, referencing ""free state aggression"" in it, and, as Don E. Fehrenbacher writes in his article ""Roger B. Taney and the Sectional Crisis"", he also consistently used a derogatory term to describe the Republican Party: ""Taney used the phrase 'black republicanism' throughout the passage, but then went back and struck out the adjective in every instance"".",0
"Wow! I can finally provide some help with a question on /r/AskHistorians as a lawyer who does some labor, employment, and wage work, and as a big fan of the show. I can at least give some background on what we know about the show and about the wages Al earned while joking with Griff and [taking jabs at the rotund customers.](https://www.youtube.com/watch?v=IRCHBi0DRFg&t=126s)

Let's rock:

Some basics on this for those who don't know: The show ran from 1987 to 1997 on Fox, and was (along with The Simpsons) the first hit for the fledgling network.

First, the income analysis. We can look at this in two ways: How much did shoe salesmen earn around that time, and how much Al Bundy actually earned.

We actually know a great deal about how much Al earned. Al earned a base salary plus commission at the store. We know from ""My Mom, the Mom"" (S03E12) Al earns that way, because he states he earns a 10% commission on each sale. I would say this really tells us how great of a salesman he is, considering how many customers he can insult and still earn those bonuses. 

Even better, we actually know Al's base salary! In ['Tis Time to Smell the Roses, S07E23](https://www.imdb.com/title/tt0642200/plotsummary?ref_=tt_ov_pl), Al is offered ""a year's salary"" for an early retirement. How much? $12,000. At 40 hours a week that breaks down to about $5.77/hour. Or $231/week. Of course, Peg spent Al's retirement bonus in a single day, as she is known to do, and Al returned to work the very next day. :-(

How realistic was that for retail employees in general during that time? I found data from [1993 Chicago](https://babel.hathitrust.org/cgi/pt?id=coo.31924071730059&view=image&seq=238), showing that retail clerks at that time had a mean weekly salary of $278. So, when you add in Al's commissions, it seems entirely realistic!

Just to add in general: The minimum wage of Illinois in 1991 increased to $4.25/hour. So, again, Al's compensation on the show is very realistic. Jefferson approves!

Now, the matter of the family living arrangements. We know that the Bundy family lives in a ""Chicago suburb"". The actual exterior shot of the Bundy house is taken from [641 Castlewood Ln, in Deerfield, Illinois](https://www.complex.com/style/2014/09/the-real-world-locations-of-iconic-tv-homes/married-with-children). That home sold in 1998, a year after the show went off the air, for $320,000. What's more, we know from 1990 Census data that average home costs for Deerfield, Illinois, were between $1400-$1500 per month for homeowners with a mortgage. So, unless scoring 4 touchdowns in a single game at Polk High came with a big cash bonus (and BTW that fact is extremely relevant at all times), Al wasn't mortgaging a home in Deerfield (using 30% monthly income as the ""affordability"" figure like most banks).

Uh oh. Not looking good we would realistically see The Dodge parked in that driveway. Historical home values from the county clerk's office suggest that was not a huge sudden increase, either. 

So we know Al couldn't swing that particular house, but what about in general? The median home price in 1990 Illinois was $80,100 based on the 1990 Census. But Al didn't BUY the house in 1990. He bought the house sometime before 1987. 

Assuming Kelly was a child and Bud was a toddler when they bought the house, which would make sense, they could have purchased it around 1980. In 1980, the average home price in Illinois was $50,004, again using [Census data from 1980](https://www.census.gov/hhes/www/housing/census/historic/values.html). [Freddie Mac data](http://www.freddiemac.com/pmms/pmms30.html) says the average 30-year fixed mortgage rate was 13.74% that year (oof). That makes the mortgage payment $466, figuring Al scraped up a 20% down payment. We can reasonably estimate $500 with taxes and insurance.

So now, in terms of a median home price and the Bundy family's likely situation, the show makes some sense. In 1987-1997, Al would maybe be able to ""afford"" that median house he purchased in 1980, as in, make payments, but (especially if he drove most customers away with his fat jokes, and had less commission), it would be a real struggle. The struggle often portrayed by Al's frustration on the show. And why shouldn't he be frustrated? All he wants is to sit on the couch and possibly read the occasional issue of Big Uns (or potentially the special issue with 120 pages - that's 240 ""Uns""!).

Pointedly, then, the show's realisticness in terms of their home and arrangements might depend on whether Al drew any income as the founder and President of the National Organization of Men Against Amazonian Masterhood.

Further reading? Kelly says reading is for girls who aren't hot. Instead, I suggest watching the show, which is available on Hulu. Grandmaster B approves.",0
"By and large, hairlessness was the ideal standard. Now for various reason, some sculptures don't follow that standard. Considering the sheer amount of sculptures and the various reason for their creation, this isn't that surprising. You're bound to find at least some examples of things that deviate from whatever ideal existed at the time. 

The point that was being made by other users was that the Ancient Greeks preferred hairlessness, thus they did not sculpt pubic hair in female statues. However, this is a flawed conclusion because we know that the Greeks and Romans *did sometimes* sculpt pubic hair, and that aside, female genitals were not sculpted *at all*. As the OP said, they're like barbie dolls, there's nothing there -- in stark contrast to the male statues.",0
"To give a brief answer, Europeans often sent colored prints of their coat of arts or chosen design to merchants in China who would coordinate with porcelain manufacturers. Prior to the Treaty of Nanking in 1842, the ""Old China Trade"" was centered in Guangzhou, whereas porcelain production was centered 500 miles north in Jingdezhen. There's a series of illustrations of [porcelain manufacture and commerce](https://ocw.mit.edu/ans7870/21f/21f.027/rise_fall_canton_04/cw_gal_03_thumb.html) in Jingdezhen and Guangzhou, c. 1820. M.I.T. has an amazing [website](https://ocw.mit.edu/ans7870/21f/21f.027/rise_fall_canton_01/index.html) dedicated to the history of the China Trade.

To learn more about the China Trade, particularly the American perspective, I recommend *To the Farthest Gulf* by Dorothy Schurman Hawes, which is a relatively short but useful overview. Eric Jay Dolin wrote a popular history a few years ago which is good for beginners interested in the subject. The truly best sources are exhibition catalogues from various exhibitions of Chinese Export Porcelain, but those can be difficult to track down. 

Lastly, a clear illustration of this process is the ""mistake plate"" in the collection of the Peabody Essex Museum in Salem, Massachusetts, which has the greatest collection of China Trade-related materials in the United States. The Dobree family wrote the colors in rather than coloring the prints and got a plate with the word red written on a section painted green, etc. You can see pictures of it and read about it at the bottom of this [blog post](http://connected.pem.org/ask-a-curator/). 

",0
"In 1980, archaeologists working at Herculaneum (just down the coast from Pompeii) discovered a series of ""boat sheds,"" low vaulted structures arrayed along the ancient beach. During the eruption of Vesuvius, nearly 300 people, unwilling or unable to escape, had sheltered in the sheds, hoping for rescue from a passing boat. Death found them there in the middle of the night, when a wave of superheated gas rolled over the city. Pyroclastic flows soon covered the remains with up to 60 feet of volcanic debris. As archaeologists began to remove the rubble, they found the bones of the refugees in superb condition, and turned them over to a physical anthropologist for close study. The Herculaneans in the boat sheds, it appeared, had come from all walks of life. There was a wealthy man, with the gym-toned muscles and soft hands of a leisurely life. There was a Roman soldier, bones seamed with old injuries. And there were two women who, to judge from their badly scarred pelvic joints, had earned their living as prostitutes. One of these women, in her late forties, had clearly borne four or five children. 

Most prostitutes were slaves or freedwomen; those who were freeborn came from the ranks of the poor. Unless they happened to become the mistress of a prominent man (typically their owner), they had little hope of escaping a dangerous and degrading career. Like actors and gladiators, they were regarded as the dregs of society, and were banned from marrying any freeborn Roman. From the reign of Caligula onward, to add financial strain to social exclusion, they were subject to a special tax. 

Few working prostitutes had the resources to raise a family. Even if they were free, or were enslaved but allowed to keep a portion of their earnings, they only earned (to judge from the going rates at Pompeii) between 2 and 16 asses per customer (by way of comparison, a loaf of bread cost 2 asses). And few pimps, we can imagine, would have allowed prostitutes to keep children around the brothel. 

Pregnancy, in other words, was probably seen as a disaster. Prostitutes used more or less ineffective contraceptives. When these failed, as they invariably did, most carried their children to term, since abortions (herbal, very occasionally surgical) were highly dangerous. Occasionally - if we can believe Procopius' slanderous stories about the courtesan-turned-empress Theodora - babies were given to their fathers. Typically, however, the child's fate would depend on the mother's status. If she was a slave, her children were automatically her master's property, and had a reasonable chance of being raised in his household. If she was free, her pimp would probably force her to either give the child to a friend or expose it. 

Exposure was probably the fate of many children born to prostitutes. The child would be swaddled, carried to a public place (often a temple gate or a dungheap outside the gates), and left there. Mothers hoped that their children would be adopted by childless passers-by. Often, however, abandoned babies were picked up by slave traders. Many would be raised as prostitutes.",0
"Put simply, the salience of cultural issues. Cultural issues were not an issue for farmers then the way they are today.

Before getting into that, we have to clarify three terms which your question runs together. Progressivism, socialism, and populism were very different and had different roots and political bases. Today, they sometimes get lumped together as being generally on the political left, but this obscures important differences that must be understood to answer your question.

The Populist or People's Party was organized around the economic distress of rural farmers who felt exploited by railroads, banks, and a political system more concerned to protect the explosively growing industrial economy than the relatively sleepy agricultural sector. Populists originated much of the language used today to rail against wealthy special interests with corrupt influence over politicians. 

For them, these interests weren't primarily the rich so much as railroads, which would exploit their natural monopoly on local transport to extort farmers to bring their produce to market, and Eastern bankers, whose policies, such as the gold standard, made it hard for farmers to get credit (which they needed to buy seed to plant every season). The populists wanted the government to regulate or seize ownership of the railroads and wanted to break the centralized control of the financial system to free up credit, such as through bimetallism. They were also in favor of political reforms such as the direct election of Senators because they wanted to make representatives more accountable to the people.

When you say that farmers supported ""socialism and progressive populism,"" what that really meant was, in the first place, populism. Of the three movements, populism gained a mass following first (peaking in the 1890s), and would serve as the seedbed for farmer political activity through the New Deal. Though the political mobilization behind a left agenda that this original movement sparked explains some farmers later supported the Progressives and Socialists, these latter movements also had very different political bases and aims. 

The Progressivism was in the main a middle class movement. It aimed, above all, at the reform of government by purging it of corruption and making it more directly accountable to the people, as well as more professional and scientific. It also sought to regulate capitalism, but not to abolish it. The issue of reform linked the populists and the Progressives, but the Progressives were much more concerned with urban governance and industrial issues like safety and sanitation than were rural farmers. 

The Socialists had their main support in urban centers, among the industrial working class. There were substantial efforts at cross-worker solidarity, between farmers and industrial workers, but these never entirely coalesced. The attraction of socialism to farmers is not hard to grasp; the populists' desire for government ownership of railroads and other agricultural services, among other farmer-supportive policies, were entirely consistent with socialist ideology. Scared of big government, farmers were not. But farmers were less invested in some of the socialist movement's central efforts, which were oriented toward confrontations with capitalist factory owners in strikes and efforts at collective bargaining. 

Farmers were a mainstay of FDR's New Deal, which included massive government action to support farmers financially and to improve agricultural infrastructure, in the spirit of, if not entirely along the lines advocated by, the Populist Party. 

So when did this all change? Like so much of American politics, in the 1960s, with the Civil Rights movement. The Civil Rights movement kicked off what political scientists call a realignment of the parties and their coalitions. The reason is that the Democratic Party became the party of civil rights for black Americans. Before this time, a lot of farmers were Democrats, as part of the New Deal coalition, and a lot of these farmers were Southerners. The national party's backing of civil rights alienated many white Southerners, including farmers. Race, which had always been important but usually not in determining *elections*, became electorally explosive. Where people stood on race would come to be an important part of where you stood, and who you stood with, in politics overall. This is the origin of what we today often euphemistically call ""cultural issues."" 

In the years following civil rights, a new politics would emerge, midwifed by people like Nixon and Reagan, which divided Americans along cultural lines. Were you on the side of the forces that were upsetting traditional American life? The dope-smoking, free-love hippies? The frightening and ungrateful Black Panthers? The student radicals disrespecting the troops and the flag? Did you support black advancement, even if it meant affirmative action? Did you support women's liberation? New sexual mores? Divorce? Abortion? All these new ""cultural"" questions served to displace the economic dimension of political competition which had defined the Populist era. Now, farmers found themselves on the opposite side of these questions from those who supported left policies. 

And that brings us to today. Farmers are today a very small group, but they are generally heavily rural, and rural Americans are culturally identified with the party that opposes most of the things the Populist, Progressive, and Socialist parties stood for. This identification was the outcome of the civil rights realignment, which raised the salience of race as an issue and made possible its exploitation to divide Americans along cultural lines. 

Sources: 

Hofstadter, The Age of Reform

Goebel, A Government by the People: Direct Democracy in America, 1890-1940

McCarty, Poole, and Rosenthal, Polarized America",0
Also follow up question. At what point were the nazi atrocities we’ll known amongst common people around the world? Did the news spread quickly after the camps were discovered? Or did the news disseminate slowly over time to the common person?,0
"---

**Part 2b - Modernity and the Rise of Skyscraping Statues**

---

&nbsp;


Curiously enough, I have not been able to find that many academic writings on the tallest Buddhist related statues in the world. I imagine there are a number of writings in Chinese and Japanese, but my language level with both are still too low to be able to read heavy scholarly texts. What I can say is that the relatively recent phenomena of enormous Buddhist statues are a result of two things, engineering and economics.

Japan can continue to illustrate these points. Following the devastation of World War II, the nation went about a great rebuilding process that often addressed Buddhist sites. Modern materials and engineering opened up opportunities for new forms of architecture, construction, sculpturing, and expression. This also corresponds to the way practice of religion changed in 20th century Japan, the fluctuating and diminishing number of followers, and shifting aesthetic and ideological attitudes of temple leaders, financial backers, and the lay community.

The Ofuna Kannonji is a prime example of this, completed in 1960, four years before the Tokyo Olympics just as the economy was picking up more speed. Rather than traditional wood or bronze, it is made of reinforced concrete. Kannon here represents compassion and protection, and is immensely popular in Mahayanan countries. And of course, tourism is a factor too, with it's prime location in Kamakura. It can be seen even at night, with lights shining on it, again, creating a sacred luminous glow.

Takamura Koun can be credited with the boom of modern statue building. In his contemporary time, he would be inspired by projects such as the State of Liberty. At that time, large figurine statues of national heroes, leaders, and characters would also be common in Europe and America. Meiji Japan, taking great steps to model itself on various western traits, surely would be inspired with statue making too. Takamura, would had done projects such as a Saigo Takamori commemoration in Ueno Park, had planned for a temporary ""amusement"" structure in a vacant Tokyo lot, with a 14m/46ft tall Kannon statue. Another plaster replica of a Kobe Nofuku Daibutsu in the San Francisco Panama-Pacific International Exposition inspired concrete. A once famous 24m/79ft high Daibutsu was constructed in the hot-springs resort town Beppu in 1928, supposedly as a tourist site and as an act of devotion. One wealthy Okamoto Eizaburo was scolded by his mother as too selfish, and convinced him to pay the entire project and even become a jodo priest! Here we can see the work of charitable devotion and merit again, albeit with the greater economic incentive of tourism. Of course, historical sites like in Bamyan and Luoyang also garnered quite a crowd, it can not be understated the significant increase in travel and leisure, especially with a booming modernizing Japan. Interestingly, the original intentions for the aforementioned Ofuna Kannonji were not for economic tourism or personal devotion, but rather altruism and compassion. The members of the project could not help but strive for fame and popularity, and this modern project at 25.4m/83.3ft high would greatly overshadow the 11.5m/37.5 high historic Kamakura Buddha. However, these sorts of statues also served as signs of peace, something important to consider with Japan's defeat and devastation in the Second World War.

China, on the other hand, is another matter. The socialist period under Mao saw heavy repression of Buddhism, temples, and statues. It is only rather recently in the past couple of decades that we have seen a resurgence of Chinese Buddhism, albeit with the special administration of government involvement. Taiwanese Buddhism has flourished quite well, however, this is a matter for another discussion.

It seems that with Reform and Opening Up, and China's tremendous economic growth, the government saw fit to sanction official construction projects of Buddhist related temples and statues. One example is in Wuxi, Jiangsu, where a 88m/289ft tall statue was erected in 1997. The relationship between state and religion here involves, once again, economic tourism and donation transactions. And again, the monastic community becomes the sacred, representing the Buddha and Dharma, therefore holding the authority to espouse doctrine and dictate practice. Lay Chinese Buddhists then must make offerings to benefit and sustain the monks, which then allows them to reap the benefit themselves through this reciprocal cycle. Naturally, affluent individuals in an affluent economy can sponsor great projects. This has come to the case that certain sites and statues may have plaques that list the names of donors who have generously given 1000 or more yuan. Thus, these individuals reap good merit and are memorialized themselves in giving up their parts for the sake of Buddhism.

In short, large statues of Buddhist figures are a continuation of historical trends. We only see colossal statues now because of affluent economies in respective countries and modern engineering to support it.

&nbsp;

---

**Sources**

* Smith, Huston, and Philip Novak. 2003. Buddhism: a concise introduction. New York: HarperSanFrancisco.
* Kuwayama, Shoshin. ""Chinese Records on Bamiyan: Translation and Commentary."" East and West 55, no. 1/4 (2005): 139-61. www.jstor.org/stable/29757642.
* Higuchi, Takayasu, and Gina Barnes. ""Bamiyan: Buddhist Cave Temples in Afghanistan."" World Archaeology 27, no. 2 (1995): 282-302. www.jstor.org/stable/125086.
* Chiu, Angela S. ""Buddha Images as Objects of Donation: Intention, Wishes, and Economic Value in Inscriptions on the Bases of Images."" In The Buddha in Lanna: Art, Lineage, Power, and Place in Northern Thailand, 130-59. Honolulu: University of Hawai'i Press, 2017. www.jstor.org/stable/j.ctvvn7g7.12.
* ""Maha-parinibbana Sutta: Last Days of the Buddha"" (DN 16), translated from the Pali by Sister Vajira & Francis Story. Access to Insight (BCBS Edition), 30 November 2013, http://www.accesstoinsight.org/tipitaka/dn/dn.16.1-6.vaji.html
* ""Early Buddhist Architecture in China."" In Traditional Chinese Architecture: Twelve Essays, edited by Steinhardt Nancy S., by Xinian Fu and Harrer Alexandra, 79-96. Princeton; Oxford: Princeton University Press, 2017. doi:10.2307/j.ctt21668kt.11.
* Rhie, Marylin M. ""Some Aspects of the Relation of 5th-Century Chinese Buddha Images with Sculpture from N. India, Pakistan, Afghanistan and Central Asia."" East and West 26, no. 3/4 (1976): 439-61. www.jstor.org/stable/29756322.
* Graham, Patricia J. ""Buddhist Sites of Worship, 1945–2005."" In Faith and Power in Japanese Buddhist Art, 1600–2005, 226-50. University of Hawai'i Press, 2007. www.jstor.org/stable/j.ctt6wr0x0.16.
* Graham, Patricia J. ""Buddhist Institutions after an Era of Persecution, 1868–1945."" In Faith and Power in Japanese Buddhist Art, 1600–2005, 177-98. University of Hawai'i Press, 2007. www.jstor.org/stable/j.ctt6wr0x0.14.
* Zenryū, Tsukamoto, and Hirano Umeyo. ""Buddhism in the Asuka-Nara Period."" The Eastern Buddhist, NEW SERIES, 7, no. 1 (1974): 19-36. www.jstor.org/stable/44361382.
* Andreasen, Esben, Zhang Lin, 张琳, Qu Jun Zhong, 蘧俊忠, Xiang Xue, 向學, Shen Zhi, and 慎知. ""Chinese Buddhism Today: Impressions."" The Eastern Buddhist, 42, no. 1 (2011): 151-73. www.jstor.org/stable/44362437.
* WANG, MICHELLE C. ""Early Chinese Buddhist Sculptures as Animate Bodies and Living Presences."" Ars Orientalis 46 (2016): 13-38. www.jstor.org/stable/26350430.

**Recommended Related/Further Readings (Academic and Nonacademic)**

* Coomaraswamy, Ananda K. ""The Origin of the Buddha Image."" The Art Bulletin 9, no. 4 (1927): 287-329. doi:10.2307/3046550.
* Lopez, Donald S. ""From Stone to Flesh: The Case of the Buddha."" In Things: Religion and the Question of Materiality, edited by Houtman Dick and Meyer Birgit, 77-89. New York: Fordham University Press, 2012.  doi:10.2307/j.ctt1c5chw4.9.
* Chiu, Angela S. ""Buddha Images and Place: Materializing the Buddha’s Agency in the Landscape."" In The Buddha in Lanna: Art, Lineage, Power, and Place in Northern Thailand, 69-97. Honolulu: University of Hawai'i Press, 2017.  www.jstor.org/stable/j.ctvvn7g7.10.
* James, Jean M. ""Some Iconographic Problems in Early Daoist-Buddhist Sculptures in China."" Archives of Asian Art 42 (1989): 71-76. www.jstor.org/stable/20111195.
* KIELY, JAN, and J. Brooks Jessup, eds. Recovering Buddhism in Modern China. New York: Columbia University Press, 2016. doi:10.7312/kiel17276.
* Tetsurō, Watsuji, Steve Bein, and Thomas P. Kasulis. ""Criticism of Art."" In Purifying Zen: Watsuji Tetsurō's Shamon Dōgen, 82-84. University of Hawai'i Press, 2011. www.jstor.org/stable/j.ctt6wqgvw.14.
* https://bitterwinter.org/all-outdoor-buddhist-statues-must-go/
* https://bitterwinter.org/ccp-exterminating-buddha-by-destroying-large-statues/
* https://bitterwinter.org/eliminating-buddhism-by-razing-statues-of-deities/
* https://apnews.com/5a9c70110c524804856897a091e79036
* https://www.reddit.com/r/Buddhism/comments/eaqbla/eliminating_buddhism_by_razing_statues_of_deities/
* https://www.reddit.com/r/Buddhism/comments/colb44/the_ccp_demolishing_and_covering_buddhist_statues/
* https://www.reddit.com/r/Buddhism/comments/er0des/why_statues_is_important_in_buddhism/
* https://www.reddit.com/r/Buddhism/comments/2gjxw9/til_that_buddha_dominates_the_list_of_worlds/

Just some final things to note, I'd be *very careful and skeptical* with news articles listed from bitterwinter. Find multiple sources, and always consider the bias from the author/site. Not that I don't trust or distrust these news sites, but, they are only something to read/consider outside of a proper academic context. The r/Buddhism threads are just various discussions in relation to large Buddhist statues. Please note that our subreddit only represents a small minority of Buddhists, with certain geographical and cultural limitations as well.",0
"There have been very few Presidents who served in public office after the Presidency. JQA is *by far* the most famous of them, due not only to the mere length of his tenure in the house, but also due to the tenacious reputation that he earned while serving, where he became of the leading voices of abolition within Congress, fighting against the Gag Rule, the annexation of Texas, and participating in the famous legal case concerning the slave ship *Amistad*. He was the only former President to go to the House after, but not to Congress, joined in that by Andrew Johnson, although roughly opposite in terms of honor, Johnson serving a mere few months in the office in 1875 before dying in July of that year, and leaving no legacy to speak of.

There were a few others who had notable post-presidential political careers in highest level of government though. The next most obvious would be William Howard Taft, who was appointed as the Chief Justice of the Supreme Court by the next Republican, President Harding, in 1921, 9 years after he had failed to win reelection. It is often said that this had always been his true ambition anyways, and it is generally agreed that not only did he *enjoy* his time on the bench a great deal more than in the White House, but also that he was much better at it too, establishing himself as an able and forceful leader of the Court through the 1920s, generally seen as consistently conservative in how he kept the court directed, as well as being a strong advocate for legal reform from Congress, resulting in the creation of the Conference of Senior Circuit Judges and the passage of the Judges' Bill.

The final former President to highlight would be John Tyler, who I'm going to focus on here because conversely he is the most obscure on this count! After the Presidency he returned home to Virginia, and he did attempt to keep his reputation burnished, but didn't seek major office. With the decline of the Whig party, he began to turn toward the Democrats. In 1860, with deteriorating national situation, he attended the 1860 Democratic Convention, and although he didn't campaign for it, ""entertained the delusion"" that he might perhaps be offered up their Presidential candidate, under the impression that he would provide a unity candidate for the entire south to coalesce around. The odds of this were, in fact, nil, and he in the end became a supporter of Breckenridge, the more hardline candidate, announcing his impression of the situation to be ""*live or die, survive or perish.*""

Nevertheless, he wished not to see the Union perish, even if he feared it might be impossible to avoid, and as the wave of secession began, he was a supporter of the Crittenden Compromise, which had hoped to over a means to ensure slavery had stronger protections to alleviate concerns over Lincoln's election, but of course in the end failed. He continued to try to position himself as a force for compromise, offering to head a conference of the six slave and free states closest to the border, but this too didn't prevent the march to war, even if serving in the Virginia delegation gave him further chance to feel like he was trying. He saw a few other honors in the period though, being picked by Virginia to head a delegation to Pres. Buchanan to discuss the crisis, as well as to later meet with Lincoln on the eve of his inauguration.

His return to public service continued further with his selection to attend the Virginia convention for secession, where he was considered one of the most honored members, and by that point had shifted to being pro-secession, and by the vote on the 17th, had become one of the vocal proponents, and afterwards gave a public speech comparing their coming struggle to that of their revolutionary forefathers

With secession a done deal, he finally returned to public office, standing for election to the Confederate Congress, and being chosen by Charles City County to represent them in the House. Congress wouldn't meet until February, 1862, so he spent the intervening months negotiating the official terms for Virginia's entry into the Confederacy, as well as the agreement to move the capital of the wannabe nation to Richmond, from Montgomery. He traveled to Richmond to begin his new position... and died a month before the Congress convened, passing away on January 18th, 1862. In the United States, his passing received perhaps the least notice of any former president, the traitor's death going without comment from Lincoln and the government, while in Virginia, a 150-carriage funeral procession and great mourning accompanied his passing.

**Sources**

Burns, Kevin J. “Chief Justice as Chief Executive: Taft’s Judicial Statesmanship.” *Journal of Supreme Court History* 43, no. 1 (March 2018): 47–68

Crapol, Edward P.. *John Tyler, the Accidental President*. Chapel Hill: University of North Carolina Press, 2012. 

Waldstreicher, David, ed. *A Companion to John Adams and John Quincy Adams*. Chichester: John Wiley & Sons, Incorporated, 2013.

ETA: Formatting and some clarity",0
"TL:DR at the bottom

I am taken aback that a week has passed, and no answer has been placed so I’ll try to do it as it also interests me.
There is an interesting essay from a Mexican historian Jose Manuel Ruelas [1] that dives deep into this topic.

As you mention, before 1970 Mexico and USA’s gun culture were quite similar. The Article 10 of the Mexican Constitution, inspired by the Second Amendment, was created in 1857 and has stayed active ever since, with a few modifications throughout the years.

We have to take into consideration the time in which the Article 10 was written: 
The US-Mexican war of 1848 stripped the nation of 55% of its territory [2] and the recent Ayutla’s Revolt (1854) overthrew Santa Anna, the president/dictator responsible for the loss of territory.
It was critical to the Liberal government to strip the army of its power or risk a coup to reinstall a conservative ruler. Between 1856 and 1857, the army was ordered to reduce to 10,000 men and locally sourced National Guard similar to the US model was implemented. 
These changes were mandated in the Constitution of 1857, as well as the order to limit the political, economic and cultural power of the catholic church. As expected, the conservative party revolted and the Reform War broke out in 1858. [3]
Bear in mind that the times between the two constitutions of 1857 and 1917 saw plenty of armed conflicts: Wars in the northern border, the second Franco-Mexican War, Wars in Central America, and the Mexican revolution.  

Weapons and conflict was just part of life.

The key to your question as why this changed is found during the cold war.
As the world attention focused on Vietnam, Mexican working class went on strike constantly due to the poor working conditions during the 60s: Oil and Railway Workers strikes that were suppressed by the military.  Doctors were arrested or fired when did it too. The key players, though, were the students.

A student protest in 1966 at the University of Michoacan due to a increase in bus fare was repressed under the accusation that they were “Communist and professional agitators”, one student was shot dead. Things went south from there.
As the government wanted to show off the “Mexican Miracle” to the world, the Mexico City Olympic Games were organized. To raise the capital, new taxes were implemented. As usual, people disliked the move and plenty of protest were held. “We don’t want the Olympics, we want revolution” was the motto. 
The IOC warned the Mexican government that if things continued unstable, the games would be relocated to Los Angeles. President Diaz Ordaz ordered the army to repress as much as needed. 
protests scalated until October 2nd 1968, at the Three Cultures Plaza, the Mexican army opened fire against the unarmed civilians. Marking one of the most controversial and bloodiest events in modern mexican history, the Tlatelolco’s Massacre. [4] 

The ruling party, PRI, kept information shut, and in parallel, started training a paramilitary group called Los Halcones that was used to suppress violently the continuous student protests. But on 1971, they carried out the Corpus Christi massacre on July 10th, killing officially 120 protesters, including a 14 year old.
The backlash from society expected by the government was not of lesser extent. So by October, the president Echeverria passed a gun control reform to cut out weapon supplies and to “ensure the tranquility in the country, to prevent any more blood baths” [1 pg 9] but by this moment, the damage was done and people were not willing to hand over their guns. 

Many left wing groups rose against the government. The most infamous, September 23rd Communist League launched guerrilla attacks throughout the Mexican territory:
-The Communist League killed Eugenio Garza Sada, famous Mexican Entrepreneur, during a kidnap attempt. 
-Kidnapped the English Consul in Guadalajara.
-Raised the workers in Culiacan to assault the weapons cache of the Secretary of Agriculture. Were brutally repressed by the military.
Other violent revolts and movements sprouted in the north and mostly in the south. The southern states of Guerrero and Chiapas stayed as the main area for left wing armed government resistance up to 1994. [5]
During this two decades, there were still some private owned armories that had to abide by the Federal Law of Weapons and Explosives of 1972, but as a consequence of the Zapatist Revolution of 1994. The creation of the Arms and Ammunition Directorate ensured the monopoly of legal weapons commercialization in Mexico.
 
TL;DR: before the 70s Mexico had similar gun culture as USA, following a few armored revolts between the 60s and 70s backed by communist groups and fearing retaliation from the government ordered massacres of Tlatelolco and Corpus Christi (el Halconazo), a gun reform was passed on 1972 but not properly enforced. As many of the weapons in circulation went to the black market. After the Chiapas Zapatist Revolt of 1994, the last privately owned gunshops in Mexico were shut down.

References
[1] [Armas de fuego en Mexico](https://ief.jalisco.gob.mx/sites/ief.jalisco.gob.mx/files/armas_de_fuego_en_mexico_ruelas_gonzalez_jose_manuel.pdf)
[2] [US-Mexican War 1846-1848](https://www.despertaferro-ediciones.com/2020/intervencion-guerra-mexico-estados-unidos-1846-1848/)
[3][Fuerzas armadas durante la Guerra de reforma](http://www.scielo.org.mx/pdf/sh/v10n19/1665-4420-sh-10-19-36.pdf)
[4] [Tlatelolco’s Massacre](https://en.wikipedia.org/wiki/Tlatelolco_massacre)
[5] [Mexican Dirty War]( https://en.wikipedia.org/wiki/Mexican_Dirty_War)",0
"Was it love at first sight? Did Eros sting you with love’s deadly poison? Have Aphrodite’s fair charms brought together star-crossed lovers? 

Probably not – in Ancient Greece romantic relationships outside of marriage as we might envisage them were rare, and for those that did marry love came a close-second to marriage’s main purpose, crassly summarised by Menander: “for the *aroto* (ploughing) of legitimate children” (Perikeiromene, 1013-14). 

Since others have discussed other romantic ventures that were available outside of marriage, and it’s such a massive topic to cover, I thought I might focus on how conventional relationships surrounding marriage would work, and how someone could go from an admirer to husband following from what you asked in the question. 

Let’s start by clearing up any misconceptions we might have based on our own values surrounding marriage in the modern world. Thankfully there is a wealth of evidence of marriage practises and courting rituals in Ancient Greece. For the Greeks as much as us marriage was an important moment in one’s life, and so we have marriages described in authors ranging from Homer and Herodotus down to the playwrights such as Aristophanes and Euripides, and all things in between. We also have an invaluable trove of visual depictions of marriage ceremonies and courtships on vases, though many of these present stylised and symbolic depictions rather than literal ones. For the Greeks, marriage was an important ritual that constituted the creation of an *oikos*, the family unit central to Greek society through which citizenship and legitimate heirs were produced. Strangely, there was no actual word in ancient Greece for the “institution” of marriage in itself. The verb *gamein* was often substituted, which could refer to marriage or sexual unions in general. A brilliant summary of this ancient distinction is offered by Oakley and Sinos (1993) “the wedding was, in essence, a celebration of a sexual union that was sanctioned by the community”. 

With this in mind, we should perhaps ask what our “type” might be. Of course our modern sensibilities may like to imagine shared interests in the Olympic games, or going to the Theatre, or our shared love of all things Spartan (because what says romance better than some *melas zomos*, black-blood soup) – not forgetting of course basic romantic chemistry. Though we cannot completely discount these things, your question probably gave away the best criteria a lower-class Greek would take into consideration when finding a suitable partner: a “girl of similar class on the other side of the polis/village to me”. The main criteria would likely be that your betrothed is a citizen of the city you are from (let’s say for sake of ease that we are Athenian, since most of our evidence inevitably leads us to Athens). Citizenship was a vital component of Ancient Greek society, certain legal and political privileges were only afforded to citizens, and in many cities (including Athens after Pericles’ citizenship reforms in the 450s BC) *both* parents would have to be Athenian in order for their children to be considered for citizenship. Love is nice, but for the Greeks citizenship would be more important. Property was also an important consideration, and in some cities (Athens included) cousins might marry to ensure that family estates remained in the family. But of course rules are not always clear-cut! And in true irony the Athenian statesman Pericles was notoriously ridiculed for hitching himself to a foreign lover after divorcing his first wife (whom he was closely related to, though the details aren’t clear). Plutarch says that “he legally bestowed her upon another man, with her own consent, and himself took Aspasia, and loved her exceedingly. Twice a day, as they say, on going out and on coming in from the market-place, he would salute her with a loving kiss.” (*Pericles, 24*). Pericles own experiences highlight that marriage and relationships in the ancient world could be messy and complicated at times, and that we should not assume that the often idealised view of marriage we have in many other contemporary sources were applicable to every marriage and relationship. However Pericles relationship was not received well by his contemporaries: the comedians ridiculed Aspasia “As his Hera, Aspasia was born, the child of Unnatural Lust, A prostitute past shaming."" (Cratinus as quoted by Plutarch), and Pericles own son with Aspasia was considered illegitimate under Pericles’ own citizenship laws… 

So likely as a young Greek man you would avoid the social stigmas attached to wedding outside of your own community, the benefits enjoyed by citizens were too important to lose for your children. 

You also mentioned that we are envisaging an “adolescent” or “young” Greek man. The age of marriage in Greek society is a bit unclear as the sources do advise different ages – girls seemed to marry once they reached sexual maturity in their mid-teens; Plato and Hesiod suggest men marry in their late twenties or even older. Aristotle suggests 37 was an ideal age: “Women should marry when they are about eighteen years of age, and men at seven and thirty; then they are in the prime of life, and the decline in the powers of both will coincide.” (Aristotle, *Politics*, 7.16). How far this age-gap difference could be is open to interpretation, Menander (*Aspis*, 266-7) scolds an older man for marrying a young girl, advising him to “let her find a groom of her own age”. So certainly couples of a similar age would marry, but it was not uncommon for older men to marry girls even in their early to mid-teens. This age disparity again highlights what I already mentioned: that marriage is primarily about producing children: for girls in Ancient Greece once they had reached sexual maturity then they would be “suitable” for marriage.  

This also of course brings up another big issue with regards to Greek marriage – did the bride have any say in who they married? We could look at someone like Penelope in the *Odyssey*, who famously delays and tricks her suitors to avoid marrying any of them, to suggest some female agency in their choice of partner, but this of course is obscured in the realm of legend and myth. In all likelihood the bride would probably have little-to-no say in who they married, which was far more likely decided by their father or nearest male relative who acted as their “guardian”. Courtship or acting as boyfriend/girlfriend before marriage would be rare among “respectable” circles, as chastity was considered an important virtue of women in Ancient Greece. *proteleia* or prenuptial sacrifices and rituals would propitiate the goddess Artemis (who represented virginity) whilst entrusting the bride to Aphrodite, the goddess of sexual love (Diodorus, 5.73.2) – the idea being that the wedding night consummation was expected to be when the bride lost her virginity. Love poetry and other benign forms of wooing your betrothed did exist (perhaps most famously in Sappho), but to the best of my knowledge none of these are about a groom writing emphatically about his would-be bride!

With regards to physical/sexual attraction or love between spouses etc. – it is a huge subject and something that could be written about extensively here (take for example Plato’s *Symposium* where his speakers spend a whole evening talking about it!) It is a bit out of my own knowledge and so hopefully someone with more expertise can expand on this point for you. But perhaps unsurprisingly, the Ancient Greeks could find themselves stuck in a loveless marriage, or they could be very much in love! Plenty of ancient writers talk affectionately about their wives or present happy relationships, conversely many mythological stories and comedic/tragic plays centre on wayward spouses, adultery and all other issues that can still affect marriages to this day. What these stories attest to is the breadth of experience that people could experience in their marriages, some were successful, others not. 

Crucially however, physical and sexual pleasure was not just consigned to the marriage once a man and woman married. The statesman Demosthenes said that “We keep hetaerae for the sake of pleasure, females slaves for our daily care and wives to give us legitimate children and to be the guardians of our households.” (*Apollodorus Against Neaera*, III, 122). In this case hetaerae would be best defined as prostitutes or mistresses frequented by the men. This point reiterates the primary purpose of marriage, as for the purpose of having children. Though many couples would have been in happily in love, sexual pleasure and other more intimate needs were not a primary driving purpose when *seeking* a marriage, as these could be separated from the marriage and sought externally (it’s perhaps no surprise that most of Plato’s discussion in the *Symposium* centres on male homosexual desire). Indeed, if we were to take the stories from Greek theatre as gospel, we might imagine women desiring extramarital affairs as much as the men, Aristophanes’ Lysistrata comes to mind.",0
"Greetings. Not to discourage further responses, but do check this older answer while you wait : 

[Since most of our physical and visual perceptions of hell come from Dante or later works, what did earlier medieval European Christians associate hell with in a visual or physical sense?](https://www.reddit.com/r/AskHistorians/comments/5z13di/since_most_of_our_physical_and_visual_perceptions/deurwwj/) by u/sunagainstgold 

Hope this helps.",0
"I researched this event extensively for my first novel and I'm happy to share what I found.  
  
The program you're talking about was *The Adventures of Superman* which aired from 1940 through 1951 and was sponsored by Kellogg's Pep Cereal (that's going to be important in this story later). Keen comic book historians will note that this show began airing a mere two years after Superman's first comic book appearance in 1938 (Action Comics #1). This shows just how quickly Superman, and the whole concept of superheroes, had risen in popularity (in the wake of Superman, you had Batman, the Human Torch, Namor, and many lesser known debuts before the '30s ended). By the mid-40s, the entire nation was obsessed with superheroes and, of that group, Superman was king. 
  
Of course, one of the biggest reasons for Superman's rapid rise of popularity (and superheroes in general) was WWII. In every iteration (comic book, newspaper strip, radio show), Superman battled Nazis and Axis powers and bolstered the morale of American boys and girls nervous about the fates of their fathers and brothers overseas. It was the perfect combination, a villain so powerful and evil it could only be met by a hero with superpowers.  
  
And then the War ended in 1945 and the creators of these superhero stories were faced with a dilemma. Whom would the most powerful beings in the world face now that Hitler was gone?  
  
Enter human rights activist and author Stetson Kennedy. Kennedy wanted to expose the inner workings of the Ku Klux Klan to the world and so infiltrated the Klan and recorded everything he witnessed. (It should be noted that the Klan during the '40s is considered the Second Klan, and was different from the Klan of the '60s or the Klan of the 1800s. The nation was greatly divided in their opinions about the Klan and many associated the Klan with fascism) Kennedy took all of his notes exposing illegal activity on the part of the Klan to the police, however the local police refused to do anything about it, reportedly because the Klan had infiltrated the police force.  
  
And so you had a group of writers looking for a new villain comparable to the Nazis and an activist wanting to expose a group many saw as fascists in America. It was a match made in heaven.  
  
""The Clan of the Fiery Cross"" was a fifteen part series that aired in  1946. In the story, an Asian-american kid named Tommy Lee needs Superman's help as he and his family are facing opposition from a mysterious group that wears hoods and terrorizes the minority community. In the series, the writers used the information given to them by Kennedy, including code words, strategies, etc. and built a villain that was recognizable to their audience. And the audience loved it.  
  
And the Klan did not. The Klan petitioned Kellogg's to order the writers to stop the series or drop the show all together. But the reaction of the public was so unbelievably positive, Kellogg's denied the claim. This would prove to be a fatal blow to the Second Klan. The entire organization fell apart on a national level and, from the '50s through the '60s, any Klans that rose up were local groups with no true national leadership. And when the Klan rose up, they faced a reality that America now viewed them as comic book status villains instead of the freedom fighters they had hoped to portray themselves as.  
  
One trivia item of note, you talked about how Superman had as association with ""all-American"" values, and you probably think of the phrase ""Truth, Justice, and the American way."" The tag on, ""The American way,"" wasn't included in the radio show and only became part of the Superman vernacular until much later.  
  
Sources:
[Listen to the radio series on Youtube](https://youtu.be/1ol8Gmi57DI)  
[Read a summary of the story on Superman Homepage](https://www.supermanhomepage.com/radio/radio.php?topic=radio-reviews/070146-fierycross)  
[A brief article on Mental Floss](http://mentalfloss.com/article/23157/how-superman-defeated-ku-klux-klan)  
*The Klan Unmasked* by Stetson Kennedy  
*Superman: The Complete History: The Life and Times of the Man of Steel* by Les Daniels",0
">*In order to be known as a serial murderer, two conditions are necessary: first the murderer must be able to carry out his murder and _not_ be identified and apprehended. Second, he must later be identified and apprehended, if not already dead. So you have a paradoxical situation-- a serial killer can operate freely when the ability to detect him is low, but we'll only know of him when we can actually detect him.*
    

I think that it’s worth to mention that there’s also a third condition: serial killers take well over a month to murder either three or more victims whilst after each murder refraining themselves from doing so, or resisting the urge to do so, for a remarkable length of time (Holmes & Holmes, 1998; Petherick, 2005; Flowers, 2012; Schechter, 2012).",0
"The awareness among Africans regarding the destinations of slave ships varied depending on the specific circumstances and regions involved in the transatlantic slave trade. It is important to note that Africa is a vast and diverse continent with numerous societies and cultures, each with its own distinct experiences and understandings of the slave trade. In some cases, African communities along the coast, particularly those involved in the trade, had direct knowledge of the destinations of slave ships.",1
"A lump sum payment would still be made, just calculated to the date of death. The hypothetical wife could have one of many avocations -- being an ordinary or able seaman was not an upper-class job, so many women would have had ordinary jobs of the working class at the time.",0
"You're looking for the as (plural asses), not the ass. The as was the Roman copper coin and their equivalent to the penny as the lowest form of currency. By changing the as-denarius ratio the Romans just were saying you needed to trade in 16 asses per denarius, not 12.",0
"2/2

Similar to the Illuminati fraternity, these secret societies were often ways for men to gather in lodges, temples, or homes (after they were sworn to secrecy, of course) and discuss and debate the issues of their time. Given that strong central governments are often afraid of losing power, they are historically hesitant to allow academics, scholars, politically different (or radicals) to gather and discuss ideas, one can see how the mythos of the Illuminati or the Freemasons were able to flourish. This is why groups like the Masons and the Illuminati (a group philosophically based on the ideas of rationality and the enlightenment) were banned or targeted. The mythos and conspiracies did not just develop out of public speculation, however. The public was often egged on by authors. Referencing Bogdan again, he states

>""They \[anti-Masonic writings\] culminated in the extremely popular works of Augustin Barruel’s *Memoirs Illustrating the History of Jacobinism* (1797) and John Robinson’s *Proofs of a Conspiracy* (1798)  who both claimed that a masonic-inspired organisation, the Illuminati, had not only caused the French Revolution but continued to conspire against the European states. The popularity of these two books and similar works that repeated the polemical narrative directed against secret societies (such as Seth Payson’s Proofs of the Real Existence, and Dangerous Tendency, Of Illuminism, 1802) led to widespread popular suspicion against secret societies and their hidden political agendas. In the USA, this trend culminated in the creation of an Anti-Masonic Party in 1828, which was actually the third major party to emerge in that country after the Democrats and the National Republicans. The party had its immediate origins in the so-called “Morgan affair,” with the disappearance in 1826 of the former Mason William Morgan (1774-ca. 1826) in upstate New York.""^(5)

So, a combination of anti-secret society writings, publications, government persecution, public misunderstandings, and how these groups presented themselves all contributed to the mythos and conspiracies that surround these groups. This leads all the way into modern times, including conspiracy theories centered around anti-Zionism and the establishment of a 'new world order'. They have their roots in the same anti-intellectual, fear-based motives that compelled groups like the Catholic Church or central governments to condemn them in the past. Discussions of religious openness, liberalism, democracy, new ideas, and the like were not always a welcome discourse. Throw in that many prominent men in public society (the Founding Fathers, for example) were part of these intellectual/professional/business-oriented groups, it is easier to see why some of these conspiracy theories can appear to be valid (if one doesn't dig much deeper).

Hopefully, another contributor/historian can add some additional background information because my interest/where the rabbit hole of Free Mason history began for me did not come from its origins or its European history

I started reading about the Free Masons when I was studying the US Civil War, interestingly enough. I came across some readings about a group called the Red Strings (Heroes of America) during the war that existed primarily in the South (but also in West Virginia and were anti-Confederate. They were an underground group that worked to protect Unionists, dissenters, deserters, pro-Union Southerns, escaped Black slaves, and the like.  So, I started researching other 'secret societies' during the war and found an interesting overlap with Masons, the Antebellum South, the war, and during Reconstruction. Most of my reading came from the establishment of the first African Lodge in 1784. Historian Stephen writes, ""In 1775 the former slave Prince Hall and fourteen other black Bostonians were re-  buffed by the white Masons of colonial Massachusetts when they sought permission to organize a lodge (Masonry's basic governance unit). The group then asked for - and received - permission from a lodge formed by some of the British soldiers occupying the city. After the war, Hall petitioned for a charter - the authority necessary to make men  into Masons - from the Grand Lodge of England, which enrolled the Boston group on  its list of subordinates as African Lodge No. 459 in 1784.""^(6)  While many lodges maintained the racist structure of the time, some lodges (and specifically African Lodges) served as way for many formerly enslaved and free African American men to meet, discuss, deliberate, and vote on issues away from the eyes and ears of white men (many of whom were hostile to the idea of total equality). These lodges would go on to be known as Prince Hall Masons.  


So, in conclusion, (this ended up way longer than I thought it would be) it isn't necessarily what have Freemasons done as a collective group (which wouldn't really be possible given the various offshoots we've seen between Prince Hall Masons, different groups within and outside of Europe and America). The question really is what have significant individuals done in history who also happened to be Freemasons or part of the Old Fellows or Skull and Bones, etc. It's not an international web, it is more of an overlap of social, political, and class involvement in societies that happen to attract those types of individuals. 

Quoted Sources:

1. Mary Greer. ""The Secret Subscribers to C. P. E. Bach's Oratorio Die Israeliten in Der Wüste: The Masonic Connection."" *Bach* 47, no. 2 (2016): 77-94.
2. Bogdan, Henrik. ""Is It True That Secret Societies Are Trying to Control the World?"" In *Hermes Explains: Thirty Questions about Western Esotericism*, edited by Hanegraaff Wouter J., Forshaw Peter J., and Pasi Marco, 39-46. Amsterdam: Amsterdam University Press, 2019.
3. Bogdan, 42.
4. Greer, 78.
5. Bogdan, 45.
6. Kantrowitz, Stephen. ""Intended for the Better Government of Man"": The Political History of African American Freemasonry in the Era of Emancipation."" *The Journal of American History* 96, no. 4 (2010): 1001-026.

Other Sources:not quoted, but used:York, Neil L. ""Freemasons and the American Revolution."" *The Historian* 55, no. 2 (1993): 315-30. Accessed January 4, 2021. [http://www.jstor.org/stable/24449525](http://www.jstor.org/stable/24449525).

Hackett, David G. *That Religion in Which All Men Agree: Freemasonry in American Culture*. Berkeley; Los Angeles; London: University of California Press, 2014. Accessed January 4, 2021. [http://www.jstor.org/stable/10.1525/j.ctt5hjj61](http://www.jstor.org/stable/10.1525/j.ctt5hjj61).

**The Red Strings:**

Johnston, James J. *The Arkansas Historical Quarterly* 61, no. 1 (2002): 107-09. doi:10.2307/40031047.[https://www.ncpedia.org/heroes-america](https://www.ncpedia.org/heroes-america)

**African American's and Masonry**LaRoche, Cheryl Janifer. ""Faith and Fraternity."" In *Free Black Communities and the Underground Railroad: The Geography of Resistance*, 145-55. Urbana; Chicago; Springfield: University of Illinois Press, 2014.

Sesay, Chernoh M. ""Respectability and Representation: Black Freemasonry, Race, and Early Free Black Leadership."" In *Black Knowledges/Black Struggles: Essays in Critical Epistemology*, edited by Ambroise Jason R. and Broeck Sabine, 44-67. Liverpool: Liverpool University Press, 2015.",0
"This question is an excellent one, but it veers into notions of sociology a bit, so I apologize in advance if this answer is a bit narrow. I'll chip in with a brief summary of an outstanding historical investigation called 'The Great Arizona Orphan Abduction' by historian Linda Gordon. 

In sum, Gordon makes a very good argument for this sort of watering down of racial ideals into simply ""white"" and ""other"" as a product of American western expansion in the Reconstruction period (late-19th century and very early 20th). Her book tells the story of a group of orphans who were sent west from New York City to Arizona in 1904. These orphans were taken in by a Catholic organization that housed, schooled, and fed what were classically thought of as street urchins that spanned any number of nationalities. These kids represented a mixed bag of Italian, Irish, Dutch, German, Russian, etc. heritages, and were the product of broken, disintegrated, or lost families in many cases. 

This is where it gets interesting, though. In New York City, these kids were viewed as undesirable for any number of reasons, not the least of which because of their respective ""races."" In short, the Catholic charity that looked after these kids couldn't give them away (literally...nobody would take them). An idea was hatched to clean these kids up and send them west, where good, Catholic families that applied and were properly screened could adopt them. Out west, these kids could be a boon to families who had lost their kids in the journey west, or just due to the sometimes harsh conditions out there. 

Gordon's book details a 1904 expedition of children sent to an Arizona mining town called Clifton/Morenci (the towns were combined) where a number of generous, charitable Mexican families went through the proper channels to apply for and adopt these kids for a number of entirely respectable reasons (because these families had lost kids of their own, because they saw it as their Christian duty, etc.). It is important to note that these kids weren't just given away willy-nilly: the families that adopted them went through the proper channels, as did the organization that saw to their relocation. 

None of the white families in Clifton/Morenci had shown any interest in adopting these kids before the children arrived in town, but a very interesting thing happened once they did. When the white residents of the mining community saw these white kids get off the train and go to live with the Mexican families, they LOST. THEIR. MINDS. Something akin to a lynch mob formed that evening, and the white residents went house to house, armed, and took the white children out of the Mexican homes. At one point, this white mob held the priests and nuns responsible for the adoption placements at gunpoint, and demanded the names of all the families that had taken custody of these ""white"" children. 

Sadly, the courts upheld this action as entirely legal and justifiable, since (according to the courts) these white adults were acting in the best interests of the children. Yep, the courts sided with an armed mob of kidnappers because it thought that Mexicans getting custody of white children was so offensive and dangerous an act, that armed abduction was necessary to rectify the situation. 

Gordon uses this incident to illustrate just how flexible and malleable notions of race truly are, and to illustrate how these notions were bent and reformed in the United States at the turn of the 19th/20th century. In New York, these children had been Irish, German, Italian, etc. Once out west, where whiteness was threatened by Mexicans, Native Americans, or Chinese, these kids simply became ""white."" So one could, by extension, argue that in the United States, the default ""white"" category developed as a defense mechanism for European transplants who saw an opportunity to reframe the debate on race once they were out west, where one's country of origin mattered less than if one was not Mexican, Native, or Chinese. This is a simplification of both Gordon's work, and the discussion on ethnicity studies in American history, but beginning with 'The Great Arizona Orphan Abduction' and digging into Gordon's sources might be a good place to start if one is looking to do a deep-dive on this subject.  

**EDIT** - I really appreciate the gold! There's been a lot of great follow-up answers here that more comprehensively outline the broader history of ethnicity studies in not just the U.S., but in Europe as well. For the people asking for more information about orphan trains, the legal ramifications of the 1904 incident, or ""whiteness"" in European culture, I'd recommend digging into Gordon's sources, or even just having a look at what u/FoucaultMeMichel wrote below. ",0
"Thanks for the great answer!  Follow up regarding the the ""trial"" route:

>the assembly will likely render a judgement based on the evidence presented, the witnesses (both in the sense of actual witnesses and character judges/patrons), and taking your own history into account.

Modern murder trials in the West can take years to resolve -- how slowly did the ""wheels of justice"" grind back then?  Would the accusation and subsequent judgment be rendered during the same meeting or shortly afterward?  Or would this drag out over several weeks/months as witnesses were summoned, you decided whether or not to pursue the ordeal, etc.?",0
"Medieval Europeans had a few go-to ways to defraud spice buyers, especially when it came to the use of spice as medicine. And they were eager practitioners--a little *too* eager. In fifteenth-century England alone, multiple doctors and apothecaries were executed for counterfeiting coins as well. (All in a day's work, I suppose.)

Spice fraudsters had a few go-to tricks, accounting for the use of spices as both medicine and flavoring:

* Sell something that wasn't the claimed spice at all

* Invent a new kind of spice (""St. Paul's grace"" is my favorite)

* Sell impure spices by weight, although this became less a fraud and more just something you dealt with

* Mix in stuff to add weight--often just adding water (sorry, I know that's kinda dull)

* Mix in sand or sawdust. Bakers even did this to sell *bread* by weight! Mixing in cobwebs was another bakers' trick, which, ew.

Unsurprisingly, medieval literature satirized fraudsters; medieval apothecaries complained bitterly about them and accused every medical practitioner who wasn't an apothecary of being such a fraudster; and medieval merchants spent a good bit of time trying to come up with ways to thwart fraudsters.

Pharmaceutical guides like the [*Liber de simplici medicina*](http://blog.wellcomelibrary.org/2017/02/a-medieval-medical-bestseller-the-circa-instans/) (usually known as the *Circa instans*), a 12th century guide to medical recipes and such, often covered some ways to detect fraud. This is one area where the substance in question being a spice could help, because they *generally* have taste. (Tutty is chimney scrapings and ambergis is dried whale vomit, more or less, so remember that tasting *good* is not necessarily the idea.)

As far as selling spices in a 'raw' or impure state--you know ""garbled speech""? (I am, um, *fairly certain* that you are aware of garbled speech right now.) A ""garbler"" in late medieval England was someone responsible for sorting out impurities. (I have no idea what that actually entailed, sorry. I just read about this stuff.)

But my favorite stories are the invented spices, and my favorite inventors are the *pauliani* of 15C-16C Italy.

Which is to say:

*Renaissance snake-handlers.*

The *pauliani* would arrive at the gates of a city, with a banner displaying a snake and...well, at least one big bad snake with them. And they (the pauliani, not the snakes, although you could draw some metaphors here) were good enough at convincing people to buy their snakebite cure--""St. Paul's grace""--to make the *pauliani* A Thing that a whole lot of writers liked to complain about.

I realize that Just Add Water is not quite the exciting answer you were looking for in terms of cutting spices. But hopefully snake-handling can make up for that, a bit.

~~

Further Reading:

* Paul Freedman, *Out of the East: Spices and the Medieval Imagination* (2008) is a delightful and readable book about the late medieval spice trade from a western European point of view, especially why spices *mattered* so much to people.

* Sara Butler, ""Medicine on Trial: Regulating the Health Professions in Late Medieval England,"" *Florilegium* 28 (2011) deals with various forms of fraud or accused fraud among late medieval medical practitioners, including when it came to medicines. (I don't have the article in front of me right now, but I *think* this is the one that talks about magical fraudsters--people who sold text-based healing amulets that didn't say what the seller claimed. This is either kind of hilarious, or if it was a case of an illiterate seller as intermediary, actually really sad.)",0
"The 1939 census put the German population including Austria and the Sudetenland at 69 million people. If 45 million people were in Nazi-run organisations then that's around 65% of the population.

However, this shouldn't be taken as a measure of guilt or support for the regime - part of the Nazi consolidation of power was replacing old organisations with their own eg. the Hitler Youth, which replaced other banned youth groups. Just because a child was in Hitler Youth didn't necessarily mean that they were supportive of the Nazis.",0
"There is a spectacular book on exactly this subject, and if it truly interest you I suggest you get it. The name is ""Superman Versus the Ku Klux Klan"" by Richard Bower. Most of the information I can give here stems from that source, along with some common historical knowledge one can find at sources such as Supermanhomepage.com. That site is a great source for information on the caped crusader, but I always ensure any statement I make from an internet source (that is not a .org, .gov, or .edu) is triple verified; which is to say I must be able to find matching stories from at least 3 non-wiki web sources. The main three I used for this were the one stated above, Otrcat.com, and from listening to some of the actual program, which is also available on  otrcat, an other places.

Also, in a sidenote, the kkk arc was one piece of a broader agenda the radio program was pushing around this time in the form of 4 arcs (called together ""the unity house"" arcs.) That promoted American values of freedom, and equality for all Americans in a time where post war disgruntlment was rearing its ugly head.

I will answer the second half of your question first. By 1946, the year ""Clan of the Fiery Cross"" first aired, Superman the radio series had already been running for  six years, five days a week. He was very well known, had been chopping down Nazis, Italian Fascists, and anti-Semitism for years to this point. Some of the most highly rated episodes were in the story arcs called ""The New German Weapon."",  And ""German Submarine Menace."" the radio hour was many families evening program as they heard all the men who caused fear get trounced by this All-American Superman. The famous ""It's a bird, It's a plane"" line had it's beginnings in the radio show, his weakness to Kryptonite was created to give his voice actor, Bud Collyer, the ability to take time off in 1943; the persona of a reedy voiced Clark Kent changing into the baritone Superman was created by the voice actor even!  Superman was by this point definitely the superhero we envision today, mostly thanks to this program itself. 

Now to backstory of the story arc.

 By 1946 Superman was back to fighting supervillains and people were losing interest. The war was over, Superman defeated all the real evil, what was left to listen to? Luckily for the producers a southern journalist by the name of Stetson Kennedy was looking for an outlet to do real good, and just so happened to be infiltrating the KKK under a false name (that of a dead uncle who had been a member). 
By this point, Stetson had turned over the information to authorities, even got the klan sued by the irs for $685,000 in tax dues in April 1944, which caused them to legally disband by June of 1944 (https://www.encyclopediavirginia.org/Ku_Klux_Klan_in_Virginia#its5). But he was frustrated, and fearful, as local authorities were either too afraid to act or worse were supporters/members of the klan, so he turned to the one national name he knew could stop them: Superman.

Stetson approached the creators and gave them what he said was inside information; handshakes, codewords, meeting places. He helped the writers make the storyline creating a thinly veiled interpretation called ""the clan of the fiery cross,"" and from there the rest is, well, history.


Which brings us to the point of how this was taken, both In the South and elsewhere.

The Klan did not, as you could expect, take this well. Especially the end, where it is revealed that the leadership is nothing but shills spreading hate they don't believe to make money selling robes to rubes. This, along with the codewords and other aspects of the organization presented made the group lose its mystique, and soon enrollment began to plummet. 
There were calls for boycotts, by the klan, on the serials cereal sponsor Kellogg's PEP (this is the era of sponsored programs. like the original soap operas presented by soap brands aimed at housewives). But, that only further weakened The group, as it gave substance to the idea that the odd codewords and other aspects said on the program hit a little to close to reality for them to feel comfortable. 
The story was such a hit nationally that Kellogg was not intimidated, and stood by their political stance. It was a huge uptick in listeners compared to the previous arcs. The show went on to have mostly good ratings and continued support until it's end in 1951.

I hope this gave you everything you were looking for.

",0
" What a great question! Though it is one traditionally outside of my wheelhouse in food history, I just finished up some writing on issues of ancient gender roles and sexuality, so I’m going to do my best to answer this by looking at the limited context of how scholars have discussed this in the context of ancient Greece and Rome. 

Some added context to my answer: I am a lesbian. I do not study LGBT history, but I certainly understand what’s at stake in the ways people represent and discuss sex and sexuality in the past and the ancient world. The thing is, there’s no one “right” way to address there, and there is certainly no scholarly consensus on when you can or cannot apply an anachronistic label like “gay” onto a historic actor. Historians use anachronistic terms all the time: there are several works, for example, on colonialism and imperialism in Ancient Rome, but neither of those words existed in Ancient Rome. I’d argue (and this is an argument, and there are scholars out there who disagree) that this is perfectly permissible. It is fine to define and explain the use of anachronistic terms for descriptive reasons. For this reason, I follow the lead of scholars who think it’s okay to use words like “gay” to describe actions and acts, as “homosexual” has the unintended impact of medicalizing your subject, which I think it worse than using an anachronistic term (and, for that matter, homosexual is an anachronistic term). 

To really score the sense of debate on the topic, I’m going to quickly address the work of two scholars who have written on the idea of whether or not a history of sexuality exists, and how scholars should treat that. David Halperin, a well regarded classicist who studies ancient sexuality, argued in his “Is There a History of Sexuality?” that there is not, because sexuality as we understand it (as an essential characteristic of the self) is a relatively recent invention. Non-normative sexual relationships did form in the ancient world especially, where men had sex with other men. There was a whole complicated typology of terminology. An classicist (whose name I don’t bring up because he is in federal prison on child porn charges) wrote a relevant article “The Teratogenic grid,” that explains the different roman terms attached to different active and passive sexual positions with respect to different orifices. Was a Cinadeus, a man who was the passive participant in anal sex, gay, because he had gay sex? Halperin views such a scholarly imposition as anachronistic in part because of the lack of any organized self-concept attached to it. I think it’s relevant here to mention that Halperin himself is a gay man, and has been openly gay for his career. Some contemporary audiences might view this as a form of erasure, but I think it’s clearly more complicated than that. 

Obviously, there are many scholars who disagree with Halperin, and the contemporary consensus is far more nuanced, thought many scholar especially of ancient sex and sexuality I have found are still touchy about calling any ancient figure “gay” “Lesbian” or “bisexual,” because the sources we have simply don’t illuminate much about their own self concept. John Boswell disagrees with the essential idea that something (ie, being gay) only exists when society gives a name or explanation to that identity, and identifies places in ancient texts where preference, and not simply action, are expressed. And on a more basic and theoretical level, many gay people (myself included) would tell you that we would be able to comprehend our experience and our difference even in the absence of the descriptive language of today’s LGBT movement. 

This seems to be where most scholars today settle, and who tend to avoid the erasure trap you bring up. Though it is still easier to speak in abstracts here, about ideas of sex and sexuality, versus about individual people. I tend to think biography is a different beast entirely, thought many people tend to synonymize it with all types of history. Even scholars who embrace these expansive conceptual ideas might hesitate to apply them when writing about the life of an ancient figure whose life is only recorded through rumor and hearsay. By all means, describe their actions, describe what we do know, but historians writing scholarship has an obligation to their sources and to the truth. Do not use euphemisms like friendship where your subject used words like “love,” but seek to understand that in context. Write about affairs, romances, sexual relationships, etc, with honesty and the best description you have, but scholarly arguments are rarely about whether someone was gay or not. That detail should not be the focus, or the only part, or a biography.  

An interesting a case study that I think is illuminating here: was Sappho a lesbian? Can we call Sappho a lesbian? I find that when I see people discussing this and historian erasure, Sappho’s name appears fairly often, supposedly as one of few examples of women who loved other women in the ancient world. Also, both our terms “lesbian” and “sapphic” come from her. Certainly, scholars must be engaging in erasure to deny her homosexuality? Obviously it is far more complicated than this, and scholars react in different ways inside and outside of the academy. We have almost no information about Sappho. We have a handful of fragments, some of which express love and affection for other women. But the Brothers poem, some of the longest fragments we have, discusses nothing of the sort. Sappho’s “husband” whose name basically translates to Dick Allcock of man island, doesn’t poke any meaningful holes in this theory but. Again. We know so little about her that it’s difficult to say anything about her with certainty. 

But that lack of certainty does not mean we should look away from the question of get on people’s case when they call her a lesbian or when queer women say they ID with her poetry. This eidolon article Re-queering Sappho suggests that academic rigor can live alongside a more easy going identification. Seeing yourself reflected in the past can be a good thing— and while its important to understand those limitations falter, I don’t think its historic to allow scholars to loosen these restrictions in public writing, as many scholars seem to. 

So to answer the question, I think the best way I can think to solve this problem is simply distinguish between academic scholarship, where language must be more precise, and yes, constrained, and public scholarly work or other artistic engagements with a figure. I think there is room for both, and I think it’s important to distinguish between scholarly engagement with ancient sexuality and erasure because we don’t use the terminology people want. These issues are fraught, and lots of well meaning scholars have taken different perspectives, some of which I find more limiting. These questions are more difficult, especially in the ancient world, than people realize, given the proliferation of memes about how gay the greeks were etc, . There’s a lot more to say about this, but hopefully some people with more expertise than I can help fill in the holes here.",0
"This is usually a question that students of archaeology, rather than history, stumble upon sooner rather than later. In my case the cat question was related to the ['mother goddess' at Catal Höyük](http://www.catalhoyuk.com/node/736) and similar figures from different eras. The question went along the lines of ""Was there really a wide-scale worship of big-breasted mothergodesses or were these figures just early porn?"". The short snippet on the linked site opens up what I'll try to elaborate on in this post - namely how archaeologists and historians to a lesser degree come up with their theories. My field of study used to be Near Eastern Archaeology, so I won't be able to go into the specifics of egyptian cat cults, but I'll try to give a small overview as to how archaeolgists end up with certain interpretations.

As a basis, we have to keep in mind that we're building our knowledge on hundreds of years of research which has been in a state of flux. Speaking in general, archaeologists, and historians to some degree, only establish theories. We find things and create theories that can change over time. A consensus might or might not be reached and might possibly change when new information comes to light, be it due to new digs, new texts or even new technologies used to analyze previously discarded evidence.

A prime example would be Winckelmann's Studies (1760's) on Greek and Roman statues, which were based on the idea that the state we found them in (unpainted, perfectly white) was their intended state. When he published his interpretation, it became a popular opinion that pure white statues were the epitome of beauty. In his opinion, colors found on statues were signs of barbarian abnormalties and not the intended way the artists made them. His publication [""The History of Art in Antiquity""](https://archive.org/details/historyofartofan0000winc) in turn influenced a lot of neo-classical art, which is why many people still view pure white marble statues as peak beauty. Nowadays, we know that most statues and buildings were painted - some even rather gaudy for our own tastes ([examples](https://buntegoetter.liebieghaus.de/en/)). It still took a rather long time for the consensus to switch, or rather to reach the public and not be limited to scientific circles.

In a similar, more recent case, when Klaus Schmidt started to publish his findings from Göbekli Tepe ([2001](https://www.persee.fr/doc/paleo_0153-9345_2000_num_26_1_4697) for the preliminary reports of the first few seasons), he suggested that they were purely sacred sites with only temporary inhabitants. Quasi a pre-historical Mekka where hunter-gatherers gathered occasionally. This had rather large implications for (local) history, as it would mean that before hunter-gatherer socities (permanently) settled in the area, they had already started to create permanent places with sacred (thus the interpretation as shrines) or social functions. Schmidt later revised this and suggested that there might have been some permanent personel on site. The most famous counter to Schmidt's early theories came in 2011 by [Edward Banning](https://www.journals.uchicago.edu/doi/10.1086/661207) who suggested we're basing our opinion on incomplete research and maybe we're just looking at symbol-rich houses. And even 20 years after Schmidts first publication, there's no ""100%"" answer as to what exactly Göbekli Tepe was. There's some general consensus based on the found architectual remains as well as small finds or rather the general lack of certain small finds that would be indicative of permanent settlement. But due to the nature of archaeology (you can only dig so much), it's enterily possible that we're one dig season away from scrapping all that, though the focus of the dig has shifted since Schmidt's death in 2014, imo for the worse, towards a more small-scale approach so we'll probably never really know.

And this is where your question comes in again. For most of prehistory and early history, we're basing our opinions on material finds. Ideally, we cross-reference in the same time or shortly after (for Göbekli Tepe Schmidt's ""Sie Bauten die Ersten Tempel"" from 2009 does exactly that), try to slot it into overarching developments that have been established (like this [series](https://www.mprl-series.mpg.de/studies/3/index.html) on architectual history \[there's more books labled studies [4](https://www.mprl-series.mpg.de/studies/4/index.html) and [5](https://www.mprl-series.mpg.de/studies/5/index.html)\] by the Max Planck institute, in german) and then to make educated guesses based on this. But they more often then not remain guesses - even in times were we have written records, it's not very often that we get explicit texts on what something was intended for but that's another can of worms. These guesses are, in the most optimal cases, backed by evidence and the later in time we get, the more kinds of records we can use to back our guesses and the more sure we can be that we're correct. So we can't 100% rule out that cat-worship in ancient egypt was all an elaborate, wide-scale hoax. But we can make a pretty good guess that this wasn't the case. If we ever find evidence of it being a hoax, we can adjust the theory, much like Schmidt (or the team at Catal Höyük) did to a certain degree or how Winkelmanns ideas have been challenged and adjusted.

On a very much less serious note, David Macauly created a great book in 1979 called Motel of the Mysteries, where life in North America got wiped out in 1984 and archaeologists hundreds of years later are interpreting bedrooms as burial chambers and toilets as sacred urns. The book plays on the stereotypical idea that whatever archaeologists find, it's always a burial site, a temple or palace - which is something that used to happen a lot in older digs. It's a good example why we need to adjust our theories, as it's ok to interpret a large building as a palace but if you know there's generally only one or maybe a few palaces and you find 28 of the same kind of building, it gets a bit tricky to justify ""Palace A-Z"" in your publication.

/e: Reddit formatting is still a bit of a pain.",0
"There's not an answer here yet so forgive me if this breaks rules.

 When did the view of Islam in general turn to homophobia?  

Also this answer was difficult to read.... It seems like these relationships would fall into what modern society would consider pedophilia. This leads me to consider my next question: 

Do we have an understanding of how often these older males had consensual vs non consensual relationships with these beardless boys?",0
"After reading about how philosophers questioned t'he myths, I've got a question that I'm glad to ask on the subreddit rather than here if It hasn't been asked before. And it's about the origin of these myths. Who wrote them? Where they oral (and local) traditions? When did the ancient greek religion originate and how? I've always got the perception that it was always there, and that they always believed in their gods. It's a sensation I have about many ancient civilitzation (excepte, maybe, Rome). But religion certainly came from somewhere and evolved from something, right?",0
"u/Mikedash has written about a specific peasant political unit, I'm going to answer more with reference to the specific scenario of the scene; England in... somewhere in the Middle Ages. Asking how accurate something from a Monty Python film is perhaps a fool's game, but there is a kernel of truth to the scene. The answer to 'so who is your lord?' could sometimes be 'we don't have a lord!'

Self governing settlements, known as communes, were a thing in the Middle Ages, and actually a very important part of the political landscape. Communes were given a charter and permitted to do what they liked in exchange for regular lump sums of money, though the specific arrangements might vary from town to town. The villagers or townsfolk would be bound by oath to each other; to protect each others' interest come what may. Initially only the cities and larger towns were given the status of communes, and you can read about why kings might be willing to forfeit control in an answer by u/WelfOnTheShelf [here](https://old.reddit.com/r/AskHistorians/comments/gwzh4u/im_a_medieval_european_king_and_a_town_of/). If you want to know how communes could come into being by a variety of methods, [I've written about that here](https://old.reddit.com/r/AskHistorians/comments/ake1ls/the_anarchist_peter_kropotkin_posited_that_there/).

But onto the specific scenario in the scene. The lord has died heir-less, and nobody else has come along. That in itself is rather unusual since there would often be some relative somewhere, but it wasn't unheard of for small villages to be left lordless during periods of war (crusades often had a fatality rate of over 50% so... lots of dead lords there). Another issue was villages left to their own devices because the local lord just couldn't be bothered running the place. Some villages did indeed seek the status of a commune, and got it. We know this from charters in which villages pay tax not in goods per household, but in a collective sum of money or goods of equivalent value. These are not always referred to as communes, and would not necessarily have had the legal status of one, but were communes *de facto*. The idea that a wondering king might come across a village in which the lord has been absent and the villages have gone 'sod this, we can take care of ourselves' is not as absurd as the scene makes out. An actual medieval king would not have been so confused by the back-talking peasant. The 1381 Peasant's Revolt began when a meeting to resolve a tax dispute in such an autonomous rural village turned violent. 

The more interesting aspect to me about the scene is the peasant going on about scimitars and watery bints. There were plenty of peasants who thought the monarchical government was fundamentally silly. During the Peasant's Revolt of 1381, some literature derided the idea of monarchy and feudalism entirely, pointing out that God did not make Adam a duke. One of the radical leaders, Wat Tyler, called for complete dismantelling of the political landscape and instead for all land and property to be held in commons, and for that is often labelled a proto-marxist. During the baronial revolution in 1258, in which a council of barons seized control of England from Henry III, peasants generally supported the barons. When the new government and Henry III came to blows, royalist forces were occasionally met with the kind of sass dolled out by the Python characters. Only it wasn't as funny because 'sass' quickly evolved to 'swords drawn'. I've written about the peasants and their political views during the revolution, and how they might stick it to their lords, [here](https://old.reddit.com/r/AskHistorians/comments/hth4mq/saturday_showcase_july_18_2020/fyh5ufo/). 

So to answer the core question of 'Is it true that some small scale medieval settlements could be considered communes, collectives and autonomous, with sovereign and/or noble authority being absent?', the answer is sort of! There were many situations in which it was just easier to let villages do their own thing. But that came at a price - specifically lump sums of cash. They could be politically autonomous, but still answered to someone economically, which reminds me that I need to pay this month's rent :/ 

P.S.

> the feudal pyramid? 

Why we do we still teach that in schools? It's as if the curriculum hasn't been updated in 50 years. Fortunately, our FAQ section is updated regularly, including [the one about that god forsaken pyramid.](https://www.reddit.com/r/AskHistorians/wiki/middleages#wiki_feudalism)",0
"Further Reading:

This is actually a topic where there are some books that hit the triumvirate of happiness: generally good historically, interesting to read, and affordable on Amazon. I'd recommend:

* Michael Bronski, *A Queer History of the United States*
* Lillian Faderman, *Odd Girls and Twilight Lovers: A History of Lesbian Life in 20th Century America* (this is older, now, and I have some problems with how it handles race and class, but it's well grounded in its sources, and both educational and entertaining)
* Scott Herring, *Queering the Underworld: Slumming, Literature, and the Undoing of Gay and Lesbian History*

So that's where I'd start. :)",0
"I love this question because it starts out seemingly negative (how do you lose a city?!) but ends up someplace positive: people can be amazingly resourceful.

Have you seen a building torn down or imploded today so something else can be built in its place? The building becomes first a pile of rubble, then a giant hole. We haul away the rubble that was the building and its foundation. Lots of it ends up in landfills. The useful parts--wires are the big one cited--might be sorted and sold as scrap.

Premodern building demolition in Europe was all about reusing what was reuseable, of course. One famous example is the piecemealing of Roman stone structures in early medieval Britain. Notre-Dame de Paris has Roman-era stone pieces in part of its foundation, and possibly some Carolingian as well. And of course, as I talk about in this cool [earlier answer](https://www.reddit.com/r/AskHistorians/comments/7uweo1/the_city_of_rome_had_a_population_of_over_one/dto1631/) the medieval city of Rome itself was largely built into or out of old Roman buildings!

But lacking a real-life Merlin to engineer exotic contraptions that could haul heavy building materials from Ireland to Stonehenge, medieval people tended to skip that ""gaping hole"" stage. It was logistically just plain easier to salvage what could be salvaged and then build the ground *up* to smooth over ruins to the point you could build a new structure. 

In fact, this could become part of the building itself. In one type of construction, the ground was leveled off by packing in dirt and rubble, and the stone walls were constructed right on top of it. Otherwise, a building might be constructed around stone-and-mortar piers. A foundation trench would be dug, the pier constructed up to ground level, and the space around it packed in. From both France (Cluny) and England (London), there is evidence that in some places the use of piers was practiced in religious building in the early Middle Ages and then adopted for secular construction from the twelfth century or so.

(John Schofield's excellent article ""The Construction of Medieval and Tudor Houses in London"" is available online, for anyone interested in understandable explanations of medieval construction beyond lists of basic materials. The [PDF](https://www.arct.cam.ac.uk/Downloads/chs/vol7/article1.pdf) is 41MB; caveat clickor.)

So essentially, there's not really an intermediary stage *if* a particular space is going to be continuously inhabited. For truly abandoned sites, like the cities wasted by the Mongols that didn't re-attracted residents until new settlers showed up centuries later, probably one could witness ruins half-buried in dirt deposited by wind and water. /u/kookingpot has a fabulous and fascinating post on [geoarchaeology](https://www.reddit.com/r/AskHistorians/comments/3bz6zp/why_did_the_romans_build_on_top_of_things/) that discusses the natural burial of old cities as well as human-caused.

As to the lack of memory? If the buildings were also demolished in the Middle Ages or the early early modern era, it doesn't surprise me at all that there's no modern memory or record of what stood there. That's just not the kind of thing recorded in city archives. We might know from tax records or a parish register or a will that someone was a shoemaker, or a draper of secondhand fabric. We don't have a street address or the deed to a building.

And medieval (and early modern) people were pretty used to rebuilding. Medieval peasants, according to Barbara Hanawalt, built and rebuilt houses fairly frequently. In cities, fires frequently gave people no choice but to rebuild. Fear of fire was *rampant* in the Middle Ages; in handbooks for priests to help them instruct people in not sinning, arson is right next to murder as the two worst sins of Wrath. When the *libri mechanorum* gets going as a genre of literature in the 15th-16th centuries, featuring wondrous extrapolations of existing technology into helpful new machines, firefighting engines are among the most prominent. ([Here's one idea from 1594](http://www.people.vcu.edu/~mjmurphy/history_of_science/besson_theatre/besson_fire_engine.jpg)). That's to say: medieval people's experience of everyday architecture was that it was necessarily *transient*.

Which always makes me wonder what medieval pilgrims to a splendor like [Sainte-Chapelle](https://blogs-images.forbes.com/ceciliarodriguez/files/2015/08/0827_FL-saint-chapelle-stained-glass_2000x1125-1152x648.jpg) thought. Did they believe it would last forever? Or did they see it crumbling into decay like, they believed, all matter in a fallen world ultimately must?",0
"This was hotly contested!! There were a lot of ""official"" theological descriptions of Heaven, and even more ""unofficial"" traditions of imagining Heaven, in 18th- and 19th-century America. And race and segregation were right in the center of these debates! 

In the early colonial period, Protestants in America and Europe wondered whether race would even exist in Heaven. They mostly agreed that Heaven was a kind of temporary holding tank for souls, which would receive new, perfected bodies at the end of history. Questions about race played out in theological disputes over bodily resurrection. For example, in a printed sermon from 1636, radical Protestant minister Martin Day describes his English and American audiences clamoring for answers:

 ""in what kind of stature they shall rise in? What colour shall they have? What imployment shall they be raised for? Whether a childe shall rise as a childe? Whether an old man shall rise in his old age? Whether crooked and deformed men shall rise crooked and deformed?"" 

In eighteenth-century America, these debates intensified. They converged with new scientific ideas about race (as a fixed biological reality), and with new Southern Protestant theology and political philosophy. Many white Americans (North and South) began debating whether the biological fixedness of race extended to spiritual realities. There's a great snapshot of how this played out in Samuel Sewall's diary entry for April 3, 1711. Sewall was having dinner with his fellow justices of the Massachusetts Superior Court, and the conversation turned to ""Negroes"" in heaven. Sewall argued that Heaven was populated by disembodied souls, and when the bodily Resurrection took place, they ""should be white."" John Bolt found this ""absurd,"" because race was a temporary, physical thing: the resurrected body would be ""perfectly translucent... void of all color."" For Bolt, the radical Protestant ideals of spiritual equality meant that racial difference was a temporary, earthly thing. But for Sewall, and many other colonial elites, blackness was a burden and a curse. Just as a blind man would be given sight in Heaven, black people would become white. Or, as African-American poet Phillis Wheatley put it: ""Negroes, black as Cain/ May be refined and join th' angelic train."" 

Puritan preacher, gentleman-scientist, and part-time ghost hunter Cotton Mather articulated the normative view for eighteenth-century Northerners: Heaven was a place for souls awaiting resurrection. The souls were transparent and their resurrected bodies would be ""luminous"": raceless, genderless, clothed in white. At the same time, Mather and other preachers had no problem using racialized metaphors about sin and hierarchy. Blackness was ""loathsome,"" sinfulness created a ""savage wilderness-condition"" in the individual's soul, etc. And, just like Wheatley, these ministers essentially saw blackness as disability, and disability as both a temporary suffering and a spiritual degradation. Heaven would perfect everyone, given them bodies that weren't literally white-skinned, but had all the dignity and safety that whiteness conferred on earth. 

After the Second Great Awakening, though, Southern Protestantism began charting a different course. Race science and theological ideas about polygenesis created vicious debates about whether the races were spiritually different (essentially different). Most mainstream religious leaders argued that racial differences were natural, biological, and definitional for time on earth, but the afterlife would have different rules and different forms. Such rules were certainly *not* familiar extensions of life on earth. Heaven was a fantastic, alien place-- at least, when Heaven was described to white elites. 

 Southern ministers and theologians tended to switch up descriptions of Heaven depending on their audience. When addressing the slaveholding elite, ministers emphasized Heaven's hierarchical nature. These ministers rejected popular Northern descriptions of Heaven as a happy home. Instead, they drew imagery from John's Revelation. They describe Heaven as a huge city or sometimes a fantastic plantation, a place of peace and luxury made possible by God's unchallenged sovereign rule. In many Presbyterian adaptations, Heaven is literally a golden tiered city with God (unchanging, rigid, all-powerful) sitting at its apex and radiating pure white light. Spirits in Heaven were described as whirlwinds, crystals, and diamonds. The new resurrected bodies would not necessarily look human (but more angelic, in the old school eyes-wings-fire-and-terror model). But individuals could recognize people they'd known and greet them. Scholarship that looks at correspondence between white slaveholding women, and Confederate soldiers' letters describing heaven, finds almost no mention of black people in Heaven, because servants won't be needed there. Instead, White Southern Heaven is a place of stability, order, peace, nobility, and worship. 

Southern ministers used apocalyptic imagery because they wanted the slaveholding class to do two things: allow their slaves to adopt Christianity (and not go to hell), and be better masters. But, when the same ministers were giving sermons to enslaved people, they often added descriptions of segregation in Heaven. (One minister famously told his enslaved audience that there would be a dividing wall separating blacks from whites in heaven, echoing the dividing walls of the Jewish Temple). White ministers trying to get black converts would also describe Heaven as a place of family reunion, but not of racial equality. The white version of heaven for black audiences was a place where scars were healed, families came back together, but black people still worked in God's kitchen.

Segregated Heaven did not gain much traction among enslaved blacks. (Also it outraged Northern white ministers, who described Heaven as a happy household of God and all his post-racial, genderless children). Against visions of White Southern Heaven and Segregated Heaven, enslaved blacks created their own version of Heaven. They embraced white ministers' promises of family reunion, singing, ""When we all meet in Heaven, There is no parting there."" But enslaved blacks mocked the idea that ""when \[whites\] go to Heaven the colored folk would be dar to wait on em."" They defined Heaven in terms of freedom, rest, community, and justice. Heaven had ""no auction blocks, no slave drivers, no traders, no whips."" God's justice would condemn all cruel masters to Hell, where they would eternally suffer the violence they'd inflicted on others. And Jesus himself would welcome slaves to a huge celebration of singing, shouting, dancing, and feasting. Completely rejecting the view of God as a benevolent sovereign upholding Heavenly order, enslaved blacks imagined dancing with Jesus and arguing with God about earthly suffering: 

""When I get to heaven, gwin be at ease  
Me and my God gonna do as we please  
Gonna chatter with the Father  
Argue with the Son  
Tell him bout the world I just come from."" 

Black Heaven contained good white people (e.g. not slaveholders), and excluded wicked blacks who had lied, stolen, betrayed fellow slaves, or engaged in evil witchcraft. These wicked people would be trapped with their masters and mistresses in Hell. 

**tl;dr: White Northern ministers (and novelists, playwrights, etc) imagined Heaven as a post-racial utopia where everyone was essentially white. White Southern ministers imagined Heaven as a peaceful, authoritative city ruled by God. Black people were segregated in another part of Heaven, worked in the kitchen, or just weren't part of the picture. Enslaved Blacks imagined Heaven as a giant party that centered on Black experiences but included some whites too.**",0
"At the time, Jefferson and Madison's party was called the Republican Party. During the Second Party System, Democratic becomes the more dominant term in part because a constituency within the party, the National Republicans, largely bolt to form the Whigs. They're a going, if unlucky, concern until the early 1850s when first their southern wing and then the northern collapse over the question of slavery in the territories. The same issue splits the Democrats, with some of the disaffected joining the Whigs' antislavery wing in a new party which they call Republican because they're going to save the Republic from the despotism of the Slave Power. The ex-Dems in the GOP coalition -usually on its right- are usually called Democratic-Republicans. Just the term we've agreed to use for Jefferson's party *to limit confusion*.

I'm being a little unfair there, though. Antebellum people are usually dealing with just the Second Party System and its aftermath and Early Republic people tend to stop around the War of 1812, so most historians aren't going to have to wrangle two groups of Democratic-Republicans, both of whom are actually Republicans, but not *those* Republicans, except when they *are* those Republicans at the same time. And the Jeffersonian-era political leadership are all dead by the early 1850s so we don't have to finagle multiple crossovers. ",0
"The cartoonish bomb with a black sphere and a short wick is a visual representation that has its roots in early 20th-century comic strips and animated cartoons. It emerged as a way to depict explosives in a lighthearted and easily recognizable manner. One of the earliest instances of this bomb portrayal can be found in the comic strip ""The Katzenjammer Kids,"" created by Rudolph Dirks in the late 19th century.",1
"The classification of the dandelion as a weed is primarily a result of its ability to thrive in various environments and reproduce prolifically. While the dandelion (Taraxacum officinale) is indeed a versatile plant with edible leaves, roots, and flowers, it can quickly spread and dominate lawns, gardens, and agricultural fields. The perception of the dandelion as a weed can be traced back to cultural and historical factors.",1
"A better way to look at it is that the idea of ""Conservatism"" was articulated as a response to Liberalism. Conservatives historically speaking believe in tradition and order. They believe things are the way they are for a reason, and that one should be skeptical of change. In most European societies this would be articulated as the defense of traditional rights and privileges for the aristocracy, a skepticism or outright opposition to constitutions and parliaments, and support for the powers of the King. 

Both of these existed in a gray area. There were some very conservative democrats who wanted suffrage only extended to the very wealthy and wanted extensive powers reserved for the monarch. Conservatives believed that everyone had a place in society and that God determined that place. Changing that natural order caused chaos and upheaval, the prime example being the Terror of the French Revolution. This is very simplified, but essentially Conservatism is the position that is elucidated and made explicit by Liberalism's rise by those who defended the old ways. As politics changed, in time free markets and capitalism became merged with Conservatism as old landed hierarchies were subsumed into the new capitalist order, and classical liberals followed, in opposition to the Social Liberals and Socialists.",0
">As an aside, I love Kurosawa's movies so I was glad to see this question. One of my favorite things about his work is his dedication to representing the old lifestyle as accurately as possible as a framework for his stories, even going so far as to rent out preserved ""museum towns"" that have been kept unchanged for centuries, and filming his entire movies in them.

Except the village in the Seven Samurai was [100% open-set reconstruction](https://books.google.co.jp/books?id=oIx-AAAAIAAJ&q=%E4%B8%83%E4%BA%BA%E3%81%AE%E4%BE%8D+%E3%82%AA%E3%83%BC%E3%83%97%E3%83%B3%E3%82%BB%E3%83%83%E3%83%88&dq=%E4%B8%83%E4%BA%BA%E3%81%AE%E4%BE%8D+%E3%82%AA%E3%83%BC%E3%83%97%E3%83%B3%E3%82%BB%E3%83%83%E3%83%88&hl=ja&sa=X&ved=2ahUKEwjS5p6wuYLsAhUTHaYKHTc0A2cQ6AEwAHoECAAQAg), Akira Kurosawa is not known for being historically accurate (peasants didn't eat white rice at the time and the samurai focus too much on sword skills for example, and considering a character accurately and effectively shot a bow in heavy rain, apparently realism wasn't a huge priority either), and if anything farmhouses were usually built entirely on leveled, slightly-elevated foundation platform to keep out rain water. The thatched roof of the farmhouses are also not nearly thick enough or at steep-enough angle, especially for an area that demonstrably gets heavy rain. Understandable for a movie set that doesn't need to last, not a historical or reconstructed farmhouse or museum town.

Even the premise of the movie is improbable (as good as the movie is). Sengoku villages were perfectly capable of defending themselves. Hineno estate then with two villages totaling to 2~300 peasants once somehow fought off a marauding force of a thousand, even if the writer was exaggerating by a factor of 5 or 10 that's still pretty damn impressive. Marauding *villagers* killed Akechi Mitsuhide after his defeat at Yamazaki and forced Tokugawa Ieyasu's and 34 samurai following him to buy local protection while travelling through Iga. The village in the movie supposedly had 23 families, meaning it would be around the size of one of Hineno's villages. If the opposing forces were so strong that the village couldn't take care of it (as was often the case and happened often at Hineno), seven samurai would do nothing and more likely the villagers would either plead to the authorities or run for the hills.",0
"In short, no. And even things like the Generalplan Ost are less clear in what they were intended to be than people often assume but more on that below.

The thing about post-war plans is that they tend to be extremely tentative and uncertain until the point the war is actually over and even then they can change in some rather crucial details. Just as a quick look at the Allies: The first sketching of plans began in earnest in 1943 after first territories in North Africa had been liberated and many things weren't actively settled until Yalta and Potsdam in 1945 and even then substantive changes remained, f.ex. Yugoslavia becoming a socialist country. Plans for how the occupation of Germany would be handled concretely weren't finalized by the time the first German city, Aachen, was liberated by the Western Allies. So, post-war plans by the Nazis at any point in time are to be taken with a huge grain of salt and were subject to constant change.

Generalplan Ost is actually the best example of this. I've written about it before [here](https://www.reddit.com/r/AskHistorians/comments/6eqm99/why_is_the_generalplan_ost_considered_different/) and the crucial point is that the GPO was never fully implemented, only in very small parts. Because of rising, cost, effort, and the course of the war, planning for the GPO stopped and all started projects in connection to the GPO came to a halt after 1942, when the Nazis decided instead to use their available resources and manpower for the war and the Holocaust (which had been a crucial part of the GPO). What was implemented was very piecemeal and compared to the grand vision of the plans, comparatively small scale.

The same holds true for other plans, some of which never really made it out of the planning phase. Here the example of the Madagascar Plan is very fitting. I've written some more about that [here](https://www.reddit.com/r/AskHistorians/comments/5hkvcr/what_were_the_nazis_planning_to_do_with_the/) and with this example, the gist once again is that the so-called DIII proposal is genocidal in its intentions but only every existed in a very, very rough outline that never made it past a draft phase due to the fact that it wasn't feasible under existing conditions – Allied control of the seas – and that it was never fully endorsed by the entire political establishment of the Third Reich, which due to the political structure of Hitler's Germany had a tendency to be at each other's throat the entire time of its existence.

This last point is important because there only existed one agency charged with developing policy towards Africa: The NSDAP Office of Colonial Policy (Koloniapolitisches Amt der NSDAP – KPA). The KPA had been founded in 1934 and drawn together Nazis who had been involved in colonial policy during the period of Imperial Germany prior to WWI and charged with the mission of developing plans on how to bring former German colonies back under German control. It's head was Franz Ritter von Epp, an old Freikorps member and early Nationalsocialist.

Epp was rewarded this way for his continuing loyalty to the Nazi movement. A former soldier and successful Freikorps leader he might have been but a shrewd political operator he was not. Within the power structure of the Third Reich all his attempts to re-make the KPA as an official state agency rather than a mere party office failed and in 1938 the German Foreign Office won a significant political victory over Epp by taking charge of all efforts to win back the German colonies. Henceforth Epp and the KPA were charged with developing plans for Africa in general.

For the next two years, Epp and his fellow Nazi colonial administrators without a colonial empire sat around and dreamed up megalomaniac plans for how a new German colonial Empire in Africa would look like. Most of these plans however were either so bad or so unfeasible that they apparently never made it to the phase of being written down coherently and therefore there is little record of them whatsoever.

Only in 1940 with Axis success in North Africa did the KPA have its fifteen minutes of fame. What they essentially did was dig up old drafts from WWI Germany for what would happen after a German victory in Africa and slightly re-tooled them. Effectively, they planned for a large central African German colony encompassing the majority of British colonies in the area and containing Tanganjika, half of Nigeria, Uganda, Kenya, Nordrhodesia and the Kongo with South Africa as a German puppet state. attached to it. They drew up racial laws more modeled along Nazi lines for these territories that were closely oriented also on those in South Africa and Rhodesia. Their plan also included some presence by the SS.

However, this plan – and this must be emphasized – was really, really rudimentary and while the KPA did already start preparing future colonial administrators for their role, it lacks the detail that – say – even the GPO had. This is all laid down in a document that doesn't encompass more than a good 30 pages of typewriter script, which isn't all that much.

And Epp's fifteen minutes would be over soon anyways. Going on into 1941 and 1942 losses in North Africa squashed dreams of a Nazi colonial empire and in 1943 the KPA and all Nazi planning for Africa was officially ended by Hitler and Bormann with all resources going into planning for the East.

Epp's plan did include very harsh racial laws that certainly could be seen as a preparatory stage to genocide or in as much as South African and Rhodesian policy edge on forms of cultural and other genocide but it did not include such concrete measures like the GPO, the Hungerplan or the Holocaust. Like so many other things it was a Nazi fever dream fantasized up by people who saw themselves as the future white supremacist overlords over the world and who thought that they could colonize Africa on a 30 page memo.

Edit: There still is an interesting history to write about Nazi war criminals fleeing to Africa though as just a few crumbs of information I dug up for [this answer](https://www.reddit.com/r/AskHistorians/comments/9b0q48/why_would_nazi_doctor_horst_schumann_receive/) reveal though.

Sources:

* Alexandre Kum'a N'Dumbe: Was wollte Hitler in Afrika? NS-Planungen für eine faschistische Neugestaltung Afrikas, 1993.

* Sönke Neitzel: „Mittelafrika“. Zum Stellenwert eines Schlagwortes in der deutschen Weltpolitik des Hochimperialismus. In: Wolfgang Elz (Hrsg.): Internationale Beziehungen im 19. und 20. Jahrhundert. ",0
"With much thanks to /u/Pytheastic ([go upvote!](https://www.reddit.com/r/AskHistorians/comments/bxi6a9/how_did_joan_of_arc_an_illiterate_16_year_old/eq77j97/)), I'd like to write an answer more focused on this specific question. :)

The simple answer is that Joan had the support of the king, but that's pretty much running a shell company on my part. If I had to sum things up, I would say ""religion and prophecy,"" but that also is not very interesting in and of itself.

**Joan as Holy Woman**

From Joan's own testimony at her trial, it is easily apparent that she was deeply immersed in the religious culture of her time. The saints most important to her are the most popular ones, she's right with the new trend in angels, she's sold on the rising importance of devotion to the Holy Name of Jesus.

This is important because Joan fits firmly in the context of the early 15th century as a holy woman and prophet. Her visions and auditions anchor her in a tradition going back to the mid-12th century of women who used the message that they spoke and acted based on direct revelation from God. 

In Joan's time, *some* people are starting to question the validity of holy women's claims. The initial questions themselves, though in some ways the culmination of a longer trend, are highly political as a result of the Avignon papacy and (especially) the Great Schism (ca1378-1415). In other words, they are very much tied to ecclesiastical politics.

On the ground level, what we find is much more ongoing confidence in women's revelations. 14th century saints and visionaries Catherine of Siena and especially Birgitta of Sweden are *all the rage*. People even start attributing to Birgitta texts that she didn't write; she's that famous and popular even among the literate classes. Birgitta (and Pseudo-Birgitta) becomes especially well-known for two things that transcend the literacy barrier: prophecy and a set of prayers.

Not everyone, but a whole lot of people, took Joan absolutely seriously as conveying divine messages directly. In very particular, Charles VII was raised in an environment where his parents firmly believed in the prophetic powers of holy women. Charles VI had given audience to Jeanne-Marie de Maillé; and Isabeau, to Marie Robine (a peasant, by the way). 

And this was, of course, the key issue at her initial and nullification (rehabilitation) trials: were Joan's fake or real; demonic or divine...according to the political beliefs of the judge or witness. For a demonstration, turn to no less a contemporary authority than French ""theologian &c"" Jean Gerson (uh...he was Really Important; roll with it), who is infamously on the record as opposing the legitimacy of holy women...but wrote dramatically in support of Joan.

**Joan and Wonders**

Kelly DeVries, who is basically *the* authority on Joan as a soldier and commander, stresses the importance of religion in the accounts of Joan's contemporary supporters as well as her own (*Joan of Arc: A Military Leader*, but especially ""A Woman as a Leader of Men"" in *Fresh Verdicts on Joan of Arc*, which is, well, about this question's precise topic). He's right, although his account is based on Joan's full career, including her victories. Which, again, is a liiiittle bit of a cop-out.

I want to go back to the 15th century mindset again, to look at the overall supernatural cosmology of the era. Well into the early modern era, there's no real divide between what we would call ""religion"" and ""magic."" (Indeed, ""religion"" doesn't even have our meaning until the 15th century.) As with belief in revelations from God, people live in a world of wonders and miracles and saints and supernatural creatures. But as seen with growing concern with witchcraft and questions about holy women's sincerity, the boundaries are just starting to be sketched out by some people.

This is especially apparent in Joan's case. The wonders associated with her don't really have a division in what she relates about other people's support of her. They *do* have a divide in the mindset of her interrogators--and, because Joan is frakking *awesome*, she knows exactly what they're doing and keeps pace. (Seriously, read Dan Hobbins' translation *The Trial of Joan of Arc*. She's great.)

A big one is Joan's knowledge of and then the discovery of ""her sword"" in a church dedicated to one of France's most important saints. The finding of a blessed object has major precursors in the Middle Ages, especially associated with the Crusades. In the 15th century, that was more important than ever. The *physical reality* of objects was critical to how people saw the world and religion in a way it wasn't earlier. Second, the cult of relics and saints was, you guessed it, critical in a way it hadn't been earlier. (Think of Mark Twain's remark about there being more shards of the Holy Cross in the world--in the 19th century still!--than there could have been in the actual cross.)

According to Joan herself, people also told stories about a prophecy they associated with her and a tree/forest near her home in Domremy. But in her own words, what people said to her about this was just linked to her performing wonders. This probably included a miraculous power to heal, which was also heavily tied to holy women/living saints in late Middle Ages. (There are stories about men, women, and children all trying to touch Joan, which seems suspiciously, I don't know, *biblical*. And yes, at a time when there was much more preaching of the Bible directly.) 

The tree was associated with fairies and local children performed May Crowning-type playing/ritual activities around it, although Joan insists she never believed any of it and never engaged in a lot of the behaviors her judges asked her about. Of course, they lie WILDLY when they write up the articles of condemnation. On one hand, they say Joan admitted to various things when she categorically had not. On the other, though, they exaggerate the various behaviors and beliefs they had asked her about earlier. And, unsurprisingly, they exaggerate according to particular patterns that align with the question of fake or real, demonic or divine.

So people associated Joan with the general performance of miracles and wonders.

**Hans Böhm**

Okay, obviously a man, obviously German, and not obviously a few decades after Joan. However, Böhm is a crucial parallel for a few reasons. Even venturing further into the very slowly increasing fear of witches, Böhm--a shepherd from Baden-Württemberg--essentially launched an entire revolt against unjust conditions based on his own prophecies and visions of Mary.

People were ready to heed prophecy that called to them--and did.

**Conclusion**

Joan was awesome; she promoted her awesomeness; she did so in a way that grew out of the religious culture in which she lived and believed.",0
"Filippo Brunelleschi, the artist who left the most visible mark on Florence out of any of them. Gaze out at the Florentine skyline and what's the first thing you see? The Dome of the Cathedral of Santa Maria del Fiore, of course. Filippo Brunelleschi invented engineering techniques on the fly to construct such a massive structure, like the use of an iron chain wrapped around the breadth of the structure to protect against hoop failure or the use of a smaller dome instead of the larger super dome to support it from within. His ingenuity cannot be understated. I'm only being somewhat hyperbolic when I say that it was like making a Dyson Sphere at the time. It is a blessing from God that Brunelleschi happened to lose a competition to Lorenzo Ghiberti to smelt the twin doors to the Santa Maria del Fiore, because if you would believe the biographer Vasari (don't, by the way) Brunelleschi was so offended by his defeat that he swore off sculpting forever and stomped off to Rome, with his friend Donatello by the way, where he would immerse himself in Roman architecture and become an architect. He would win the real prize after he was awarded the commission to construct the Dome of the Florence Cathedral, his eventual Magnum Opus, which would put him firmly in the same breath as the likes of Leonardo or Michelangelo, although he first had to feign illness for a few weeks to kick his rival Ghiberti off of the joint-commission that they were both awarded.  ",0
"While this is probably further back then when your professor was talking, the Classical Athenians did have the belief/stereotype that women were sex-obsessed. Aristophanes’ comedy, *Lysistrata*, is probably the most blatant demonstration of this belief. This comedy is about the notion that women could stop the Peloponnesian War by withholding sex from their husbands. Sure, the husbands are ultimately very much affected by this, but the beginning of the comedy very much plays around with the idea that women are sex-crazed. At one point, one of the characters proclaims that she would rather cut herself in half or walk through fire rather than give up sex. As Lysistrata proclaims ‘Oh what a low and horny race we are!’ while her Spartan counterpart acknowledges that ‘it’s difficult for females to sleep alone without the hard-on’. A play with a similar vibe would be Aristophanes’ ‘Ecclesiazusae, often translated simply as ‘Assemblywomen’, or more flamboyantly, as ‘Sexual Congress’. Essentially women seize control of the male-only voting process and vote themselves in as the rulers of the city. They institute what could be described as, in a very loose sense, a sexual communism. Essentially, before a young lover can meet with his girl, he must first ‘assist’ his community by ‘taking on’ charity cases first (such as old or ugly women). 

Anne Carson’s chapter in ‘*Before Sexuality. The Construction of the Erotic Experience in the Ancient Greek World’* also discusses this stereotype in a bit more detail (a few relevant pages are previewable through Google Books). Essentially in archaic poetry, the woman was seen as ‘roasting her man’ with her insatiable sexual appetite. A fearsome creature indeed - lacking the ‘self-moderation’ of men, and incapable of restraining their desires. Women are characterised by classical philosophical texts as akin to mares in heat, unable to be stopped and unable to stop themselves.   


Dover also summarises this point well - ‘Just as it was thought masculine to resist and endure, it was thought feminine to yield to fear, desire and impulse… It seems to have been believe not only that women enjoy sexual intercourse more intensely than men, but also that experience of intercourse put the woman more under the man’s power than it put him under hers, and that if not segregated and guarded women would be insatiably promiscuous’. (Dover’s chapter in *Sexuality and Gender in the Classical World: Readings and Sources* *-* p. 25) 

Here Dover touches on a few things – the myth that women enjoyed sex more than men comes from an old story where the prophet, Tiresias, was turned into a woman by the gods for 7 years. And through ‘scientific experimentation’ he indeed discovered that women found the experience more enjoyable. (this is from admittedly fragmentary evidence from Hesiod (fr. 275?), and a later commentary on Homer attributed to Eustathius) 

Secondly, it skirts around the ‘Seclusion Theory’ that women were primarily secluded within their households for the majority of their lives, possibly in part because they are unable to control themselves, so they must be controlled instead. However, this is a *very* contested and complex debate which has been the subject of scholarly interest since at least 1923 (or arguably since the 18th century). I cannot summarise this theory well here – but see *Sex and Difference in Ancient Greece and Rome* (2003) for a more detailed, though admittedly somewhat outdated now, overview of the subject. A more up-to-date but shorter look would be Skinner’s chapter in *A Companion to Greek and Roman Sexualities* (2013, ed. T. K. Hubbard). 

Essentially, there was a stereotype in Athenian literature and poetry that women were unable to restrain their desires. So possibly not exactly more ‘obsessed with sex’ than men, but less able to control said obsession. Accordingly, men should act as their guardians and ensure that their honour (and the honour of their male family members) remained intact. Certainly the reality was more complex than this, and whether such a belief was held by the average Athenian citizen is surely up for debate. However, there was at least the literary trope in Athens that women were unable to control their sexual desires.",0
"huianxin gives a treatment of sushi restaurants through the 1970s and 80s. I'll focus on sushi purchased in grocery stores, as well as looking at the context in which sushi is used in The Breakfast Club.

The Breakfast Club character Bender spends much of the movie harassing Claire about her personal life. Food analogies are used in this harassment a few times. In the first example, Bender asks another boy if he [slips Claire ""the hot beef injection""](https://www.youtube.com/watch?v=5HZ1iHCX3vU) When Claire takes out her sushi lunch and explains that it has raw fish, Bender rejoins [""You won't accept a guys tongue in your mouth and you're gonna eat that?""](https://www.youtube.com/watch?v=u3mupIlFIYQ) While prissy isn't used in the movie, Claire does declare, ""I'm not that pristine!"" This prompts Bender to ask, ""Are you a virgin?"" and to [continue with a variety of questions about her personal life](https://www.youtube.com/watch?v=S5IHNcpa7p0).

In large part, the reason for Claire to eat sushi was to provide further avenue for Bender's harassment, to provide another food analogy to sex. Molly Ringwald [has written about the treatment of women in John Hughes' films](https://www.newyorker.com/culture/personal-history/what-about-the-breakfast-club-molly-ringwald-metoo-john-hughes-pretty-in-pink): ""It’s hard for me to understand how John was able to write with so much sensitivity, and also have such a glaring blind spot."" ""Bender sexually harasses Claire throughout the film. When he’s not sexualizing her, he takes out his rage on her with vicious contempt.""

Beyond its usefulness as movie innuendo, was sushi takeout in the 1980s rare? From grocery stores the answer is certainly yes. In the late 1980s, America groceries were only starting to offer prepared sushi. In 1988 a supermarket trade magazine declared ""several of the most innovative supermarkets \[offer\] catering... remarkable for breadth: from entire dinners, to tempting snack trays--goat cheese and croissants, for example--to fresh sushi, to Chinese food."" In Houston Texas in 1991, supermarket chain Fiesta Mart reported that 6 of their locations were experimenting by offering sushi bars, pizzas and quiche. A few years later Houston based Randall's Food Markets reported selling sushi in 6 of their 72 locations.

Often this sushi was (and still is) provided by a partner company. Today's largest provider of grocery store sushi, Advanced Fresh Concepts, was founded only in 1986. In 1995 their reach was only 200 locations in 13 states. Included in their partners were Schucks, Dierberg's, Kroger, Simon David, Tom Thumb, Randall's, H-E-B, Vons, King Scoopers, Jensen's Finest Foods, Lucky Stores, Hughes, Sam's Club and Costco. At this time, grocery store sushi bars were called ""the exception"". Grocers commented that while ""the margins are very good... the problem is that the volume is not very high."" Others opined that the concept worked in ""an area with a higher disposable income.""

Indeed this points to trends still visible in sushi consumption today. Roughly 1/3 of Americans have never eaten sushi. Americans who are younger and those with higher education are more likely to buy it. It is much more likely to be bought by consumers on large shopping trips where 20+ items are purchased. Individuals on the coasts are much more likely to eat sushi, with 80% of individuals on the west coast reporting they have eaten sushi compared to 55% of Midwesterners. (The Breakfast Club takes place in a fictitious midwestern suburb).

My favorite indication of how rare take-out sushi was in the 1980s is how different Claire's meal is to a standard take away sushi lunch today. Sushi to go most often arrives in plastic serving plates, with soy sauce in individual packets. In the movie, Claire pulls out a wooden sushi board and pours soy sauce from a ceramic container! That's fancy.

**References**

* Riell, H. (1995). If you knew sushi. Supermarket Business, 50(6), 119.
* Hughes, B. (1988). The growing cachet in catering. Supermarket Business, 43(7), 5.
* Duff, M. (1991). Houston foodservice shootout. Supermarket Business, 46(5), 125+.
* Ringwald, M. (2018). What About “The Breakfast Club”. The New Yorker.
* https://snapshot.numerator.com/brand/store\_made\_sushi
* https://www.keltonglobal.com/recognition/32-of-americans-never-tried-sushi-pei-wei-survey-says/
* House, J. (2018). Sushi in the United States, 1945–1970. *Food and Foodways*, *26*(1), 40-62.",0
"European Muslims fought to keep the adhān in the late Middle Ages, even to the extent of changing the call. Some changes were major, some were minor, and some were a matter of life or death.

The adhān was a major point of contention as Iberian Christians invaded and conquered more and more of al-Andalus in the late Middle Ages. Muslim communities negotiated hard to maintain as much of the call to prayer as Christians governments would allow.

But while Christian sources often express much anger and fear over one specific line in particular--""I bear witness that Muhammad is the Messenger of God""--this is in the context of restrictions on the adhān overall. Laws permitting the other lines of the call while excluding that one don't seem to follow.

In fact, actual differences in the *wording* were present but don't seem to have been important. Remie Constable, who wrote probably one of the most important on the matter of ritualized religious noise in the Middle Ages, just notes, ""The language of the medieval mosque call was transcribed in a Catalan
summary of Muslim law, probably dating to the late fourteenth century,
and it differs little from modern versions"" (understanding that we are dealing with a third-hand account here) and ""there
could be variations on the muezzin’s text, the basic words of the adhān
begin by praising God and naming Muhammad as the Prophet of God.""

**Instead, the differences in the call to prayer in Christian-ruled areas transcended words.**

Sometimes it was banned altogether--including with the penalty of *death*. James II in Valencia in the early 14th century laid this down, and it definitely did not work out in practice the way he/the bishop he was trying to placate had intended. It also did not work out for Martin of Aragon in 1403, whose nephew promptly overturned it. This leniency did not work out for Ferdinand I when Alfonso V overturned *that*...which did not work out so well for Alfonso when John II allowed it again. Words and all.

That last ruling was significant.

**Where an adhān in some form was still permitted, the most prominent change was the requirement to substitute music for words**--usually horns; sometimes horns and drums. (This is mirrored by Christians in some Muslim-ruled areas, like Tunis, being permitted to ring their own churchs' bells). A change like this one really does drive home that some Christians feared the loud ring of Muhammad's name more than a widespread reminder of Muslims' presence and piety.

The other major strike against the ritual was to forbid muezzins from calling others to prayer from a minnaret or other tower. They might be limited to within the door [frame] of the mosque, or simply ""in a low voice and in a low
place,"" like the Muslims of Jativa in 1357.

It was not exactly fun to be a Muslim in Christian-controlled Iberia, especially towards the end of the Middle Ages (a.k.a. closer to the time they would be exiled from their homes altogether, and permanently). But Muslims negotiated *hard* to maintain their most important religious practices, including ones that necessarily intruded on Christian life. Not always, but sometimes, they did win--but often only with significant changes to the ritual overall.

~~

Further Reading:

* Olivia Remie Constable, ""Regulating Religious Noise: Te Council of Vienne,
the Mosque Call and Muslim Pilgrimage in the
Late Medieval Mediterranean World,"" *Medieval Encounters* 16 (2010)
* Kathryn Miller, *Guardians of Islam: Religious Authority and Muslim Communities of Late Medieval Spain* (2008)",0
"Textually, we have an incredibly rich trove of accounts of visitors to Rome. On one hand, the collection is perhaps not quite as interesting as we might want: the Venn diagram of ""people who were both literate and whose writings are likely to have survived"" and ""people with an awareness of a basic history of the Roman Empire and its decline"" is basically the first circle inside the second, especially from the mid-11th century on. On the other hand, their shared knowledge of and appreciation for ancient Rome offers a good basis for comparison of different perspectives.

Benjamin of Tudela is a good place to start for an important reason: in the face of Rome's role at the heart of medieval Christianity, Benjamin was Jewish! He came from Navarre in Iberia, and his meandering travel account catalogues the Jewish communities he traveled among around the Mediterranean. You can read his full account of Rome and Roman Jews [here](https://www.gutenberg.org/files/14981/14981-h/14981-h.htm#bpage_5) (Cntl/Cmd+F for ""Rome"" is easiest if the link target doesn't work), but to excerpt a few bits:

> There are many wonderful structures in the city, different from any others in the world. **Including both its inhabited and ruined parts,** Rome is about twenty-four miles in circumference. In the midst thereof there are eighty palaces belonging to eighty kings who lived there, each called Imperator, commencing from King Tarquinius down to Nero and Tiberius, who lived at the time of Jesus the Nazarene, ending with Pepin, who freed the land of Sepharad from Islam, and was father of Charlemagne.

> There is a palace outside Rome (said to be of Titus). The Consul and his 300 Senators treated him with disfavour, because he failed to take Jerusalem till after three years, though they had bidden him to capture it within two.

> In Rome is also the palace of Vespasianus, a great and very strong building; also the Colosseum...There were battles fought here in olden times, and in the palace more than 100,000 men were slain, and there their bones remain piled up to the present day. The king caused to be engraved a representation of the battle and of the forces on either side facing one another, both warriors and horses, all in marble, to exhibit to the world the war of the days of old.

> In Rome there is a cave which runs underground, and catacombs...In the church of St. John in the Lateran there are two bronze columns taken from the Temple, the handiwork of King Solomon, each column being engraved ""Solomon the son of David."" The Jews of Rome told me that every year upon the 9th of Ab they found the columns exuding moisture like water.

Although Benjamin observes that some of Rome is standing/inhabited and some is ruins, he does not distinguish which is which in his description (nor does that distinction allow for, as we will see, inhabited ruins). However, he is keenly aware of the history of the ancient Roman buildings and those who lived in them. Those stories--what Rome *was*--matter more than what they *are*. He takes note of great buildings, natural features, and smaller monuments. I also think the detail about the columns of the Temple seized and appropriated into a Christian church are fascinating and significant. Especially in recounting the miracle story of the local Jewish community, Benjamin shows that Rome could have a sacred geography for *non*-Christians--something I, at least, am not used to thinking of.

Notably absent from Benjamin's record, on the other hand, is commentary on the *fall* of Rome. For this, believe it or not, we have to turn to Christian writers. In their stylings, a very real admiration for classical antiquity aligns with the medieval Christian theology of history that saw a ""world grown old,"" decaying towards apocalypse and only ever renewable by God. 11th-12th century cleric Hildebert of Lavardin, eventually archbishop of Tours, wrote two famous poems *de Roma* which both celebrate and mourn the ancient city as he found it at the very end of the 11th century. Here's an excerpt from one:

> The city now is fallen; I can find

> No worthier epitaph than “this was Rome.”

> Yet neither the flight of years, nor flame nor sword

> Could fully wipe away its loveliness

> […] Bring wealth, new marble, and the help of gods

> Let craftsmen’s hands be active in their work—

> Yet shall these standing walls no equal find,

> Nor can these ruins even be restored

> The care of men once built so great a Rome

> The care of gods could not dissolve its stones

> Divinities admire their faces carved,

> And wish themselves the equal of these forms

> Nature could not make gods as fair of face

> As man created images of gods

With Hildebert, praises of Rome move into a more emotional register, but also a more intellectual one rather than practical/geographical. His words are grounded in ancient Rome's buildings and especially its art but he evokes the splendor of a lost civilization rather than the immediate materiality of buildings rooted in history. It's also significant that Hildebert's praise, while overtly of the artistic qualities of ancient Roman art, is actually directed at the *human artists*. He elevates the abilities of humans of old especially compared to present ones, whose skills and vision could never possibly measure up.

A few years later, the English traveler known as Master Gregory famously followed Hildebert's footsteps to Rome. Gregory actually knew one of Hildebert's poems--he quotes it in his own little travel guide-like account!--but takes his commentary a step further.

> The sight of the whole city is, I think, most wonderful, where there is such a multitude of towers, so great a number of palaces, as none can count. When I first saw the city from far off, I was overwhelmed and remembered Caesar's view of it, when having conquered the Gauls and crossed the Alps, he exclaimed *substantial quote from Caesar*...

> This beauty passing understanding I long admired, and I thanked God who...yet has magnified there the works of man with immeasurable beauty. For even if Rome falls into complete ruin, nothing that is intact can be compared with it. As has been said [by Hildebert, in fact]:

> *Nothing can equal Rome, Rome even in ruins*

> *Your ruins speak aloud your former greatness*

> **The ruin of Rome shows clearly, I think, that all temporal things are near their end, especially when the worldly center of all things, Rome, daily languishes and decays.**

Rome as the ""worldly"" center of the word is one of those little noteworthy turns of phrase. In medieval Christian *sacred* geography, the center of the world was Jerusalem. Here, though, Gregory focuses on the human component in Christian world/salvation history--and he is even more explicit than Hildebert about the decline of the present from earlier greatness.

We're used to a ""decline and fall of Rome"" narrative as Christians supposedly ruining the great rationality/progress/technology of pagan/philosophical Rome. Medieval Christians actually took part in the view of a Roman golden age compared to their own; for them, however, the rise of Christianity was less a *cause* of decline than an inevitable step towards ultimate divine redemption.

Gregory relates one more detail I want to highlight here: he tells us that many of the statues from the days of pagan Roman glory were dismantled by (very important) Pope Gregory I! We can agree on one hand it's quite noteable that he's repeating a story about Christians actively opposing the preservation of pagan art, and not very approvingly. On the other, this Gregory projects the 'desecration' onto another Gregory several centuries in the past.

In fact, the appropriation and remixing of ancient Rome into a Christian city was *ongoing* throughout the Middle Ages. Even in the later 15th century, with ""Renaissance"" adulation of classical antiquity building to a fever pitch, prelates in Rome were still plunder the Colosseum and deserted palaces for stones for their own lavish building projects!

This brings us to the last thing I want to talk about: archaeological evidence for ""what people thought"" of Roman ruins, evidence that perhaps helps us get beyond the view of the absolute elite of the elite of high medieval society. The Colosseum is the famous example here, since it enjoyed many afterlives throughout the Middle Ages. Most famously, it eventually became a little neighborhood for artisans! Quarrymen and blacksmiths set up residence, even building the occasional shop for horseshoes and other goods. Eventually, a monastery was constructed in and around part of it. And all the while, tantalizing blocks of stone were usurped for building projects elsewhere.

Visitors to Rome saw ""ruins"" and ""desertion,"" and the ashes of of past splendor. People who lived in Rome may well have seen that, too. But they also saw promise for the present and the future: what could be out of what had been.",0
"Since no one else has chimed in, did a quick review of the Nuremberg lit to confirm my recollection was correct, and the answer is pretty much no.

There were a few things going on.  First, as /u/restricteddata or others can discuss in far better detail than I, the actual details of what the bombings did were fairly closely held for years beyond immediate death tolls and (to an extent) surface damage.  About the last people in the world who were privy to the more gruesome aspects to it were those held at Nuremberg and elsewhere.

By and large, this led to a general view of atomic weapons by most strategic thinkers for a few years after the war as merely a much more powerful and compact version of the strategic bombing that had already defined World War II.  The Nazi leadership certainly subscribed to this, and had no problems comparing the Allied raids carried out by Spaatz and Harris' air forces to their own crimes, far lesser in their minds:

>Frank to Rosenburg: ""They are trying to pin the murder of 2,000 Jews a day in Auschwitz on Kaltenbrunner—but what about the 30,000 people who were killed in the bombing attacks on Hamburg in a few hours?—They were also mostly women and children.—And how about the 80,000 deaths from the atomic bombing in Japan?—Is that justice too?""

Where the atomic bomb came up more often was in the grand view of politics the leadership continued to assess from afar, where most of their interest was in what the development of the bomb would do to US-Soviet relations:

>Ribbentrop had his first good laugh in several weeks at the reviewer's comment that Bullitt is in favor of using the atomic bomb ""to scare the pants off Russia."" As Ribbentrop translated the comment that Russia was only waiting to develop its own atomic bomb to attack us, Goering replied expansively, ""Why, of course, every child in the street knows that— anybody who has the slightest political knowledge ... I give them about 5 years' time.""

>They did not talk about it very much in my presence, but from their smug grins, it was easy to see that they received no small delight from this sign of tension between America and Russia.

This was also - give or take - the relative position of the few people in Nazi leadership who had some idea about their own atomic weapon development, which was that with a focus on plutonium, it wasn't likely to be something that might affect the outcome of the war, but it had a good chance if it succeeded of determining the balance of power after it.

Of those at Nuremberg, only Speer was a member of that group from the spring of 1942 onwards when he transferred out Heisenberg and his group from the Education Ministry to Goering's (relative) oversight and kept some tabs on what they were doing.  He had a far less sanguine view of what might happen down the road,

>Speer turned from his defense to warn the world of the destruction we might expect in a future war: radio-controlled rockets, aircraft flying at supersonic speed, and atomic bombs, destroying everything within reach; chemical and bacteriological warfare snuffing out all life that remained.

However, even in the extensive interviews with Sereny and his extensive writings, we don't really see him expressing much of a feeling that Germany had dodged a bullet.  It's more that he felt the world faced a much heavier caliber one than it had ever dealt with before.

All this said, there may have been those outside the higher leadership that expressed opinions on it later (or were recorded at Trent Park), but as best as we can tell, the bottom line is that the highest leadership didn't really consider it much worse than had already happened to them anyway.

Sources:
GM Gilbert, *Nuremberg Diary*
Goldensohn, *Nuremberg Interviews*
Sereny, *Albert Speer: His Battle with Truth*
Rose, *Heisenberg and the Nazi Atomic Bomb Project*",0
">It's possible to reliably strike human sized targets at 100 yards or more with primitive archery tackle

It isn't.

I'll speak as someone who has dabbled in traditional archery and coach modern archery. Others have provided some historical accounts of notables being struck down by projectiles. There is largely a lack of historical evidence of a notable figure being *purposefully* hit by an arrow at long distance.

**What exactly is a ""sniper""?**

In modern usage, a sniper refers to person armed with a precision weapon (typically a rifle) capable of engaging targets at extreme distances. There's also the extended understanding of a sniper in a military sense, being a scout and reconnaissance asset, as compared to a marksman, but that's a different discussion. In either case, a sniper is expected to engage targets beyond the normal combat range of other combatants (riflemen, machine guns, etc.)

At this point we have to appreciate the ranges that we can expect to see snipers being used as compared to other troops. Modern infantry combat takes place up to 200m with modern rifles. Snipers can expect to engage targets over 1000m away, with the record distances over 2000m (the longest confirmed kill being 3,540m) typically using high-calibre ammunition (such as .50) not used by regular infantry. In security contexts, marksmen (or police ""snipers"") would be expected to cover several hundred metres.

What's crucial to understand here the myth of archery accuracy. ""Long range"" for a bow, or a ""bowshot"", is around 150m, and this is a point where archers don't aim for specific targets but shoot indirectly (i.e. the ""volley"" that most are familiar with).

Modern snipers can function because they can, with extreme skill, training and with no small amount of arithmetic, guarantee that a bullet will hit a specific target hundreds of metres away. This is beyond what a person with a bow can do.

**How far can an archer shoot accurately?**

A modern competitive archer, using modern equipment (carbon arrows, sights, stabilisers, either recurve or compound), can hit a \~12cm spot at 70m (the size of the 'gold' 9/10 ring) about **95%** of the time.

These are the world's *best* competitive athletes shooting in a sport that prioritises precision and consistency above all else, shooting in ideal conditions with impractical equipment. A more typical ""elite"" athlete might achieve about an 80% hit rate on that spot, while the average amateur club-level archer might be closer to 30-50%. The hit rate on the target will still be fairly high - you don't expect to *miss* the target once you've trained for the distance, but it's difficult to guarantee a hit on the centre unless you've honed your shooting skill over years.

(For those wondering, I'm basing these figures on world, national and club events and records. The sighted recurve men's record at 70m is 353/360, which means the archer hit the 9 ring seven times as opposed to the 10 ring).

In contrast, the Finals of the traditional division of the 2020 Conquest Cup, featuring Turkish archery shooting at a *puta* target \[a human-sized pear-shape target\] at 50m counting *only hits*, the winner achieved **five out of nine** hits; runner-up being **three out of nine**. Having shot with archers who are serious about this competition, five hits is *good*, seven is exceptional.

A 55% hit rate at 50m with somewhat equivalent equipment from historical periods isn't exactly ""reliable"".

Hunters, who need that single shot for a clean kill, typically shoot at much shorter distances, often under 30 metres. Compound shooters can engage further, but apart exceptional hunters such as Howard Hill, most traditional archers would not be confident shooting beyond 30 metres. Too many things can go wrong on release to guarantee the arrow hitting the kill zone.

On a side-note, battlefield archery was also known to be very short range for maximum effect. The archers at Agincourt, while capable of shooting over a hundred metres, mostly engaged the French knights at 25 metres or less. At this distance, this would be a ""certain hit"" most of the time with a war bow. On the other side of the world, Gao Ying's *The Way of Archery* \- a Ming military manual - leans towards the reality that an archer should engage at close distance. To paraphrase, the idea is that they estimate their ""certain hit"" distance, and half that for the distance they should shoot.

The notion that historical archers, battlefield or otherwise, would be able to pick out a single target 100 metres away, is a romanticised ideal worthy of legend rather than a realistic application of a bowman. Realistically, the distances that a bow can accurately hit a target is so short, you may as well use a dagger.

**Finally, so what if an archer could hit at that distance?**

Arrows aren't bullets. A sniper can bring down a target with a high degree of lethality with the ammunition we have available, even more so if you're referring to .50 rounds. A shot to the head will likely kill, a shot to the torso will most likely cause a mortal wound. Arrows seldom kill instantly, the biggest threat being their removal and subsequent infection long after. Poison could work, but very few cultures made widespread use of poison, let alone for assassination purposes on arrow-heads.

**In conclusion,** you don't get archers to snipe high-profile targets or provide a security detail. The biggest threat isn't an arrow, but a dagger.",0
"Ancel Keys is the major contributor toward the low-fat culture we have today. He conducted an epidemiological study dubbed “the five nations” study. Some cherry-picking of data resulted in some correlations that incriminated saturated fats. Actual scientists at the time thought he was ridiculous and that his science was bad, but Ancel was charismatic and received support from grain lobbies.

US already had this awkward “grain is healthy” propaganda— Kellog being a big contributor to that narrative. Meat safety was also lagging behind grain— there are a number of factors here I am sure.

Anyway— it’s all very slimy and manipulative and fat is delicious.",0
"During the turn of the century in large American cities like Chicago, cases of missing children were unfortunately not uncommon. While it is difficult to say exactly what happened to your Great-Grandfather's sister without more specific information, I can provide some insight into the circumstances surrounding child abductions during that time. In the late 19th and early 20th centuries, child abductions often involved cases of kidnapping for ransom or forced labor, although sexual exploitation was also a concern.",1
"The change in Nathan Bedford Forrest's views towards the end of his life is indeed a topic of debate among historians. Forrest, a Confederate general during the American Civil War and a prominent figure in the early Ku Klux Klan, did make public statements advocating for racial harmony in the years following the war. Some historians believe that Forrest's change of heart was genuine, pointing to his efforts to promote unity and reconciliation between white and black communities in Memphis, Tennessee.",1
"Okay, this is a hard question, because it is a very loaded one. You assume that current Turkish state does not see itself as the continuation of Ottoman Empire, but it does. I think an easier question to answer would be ""why and how Turkey denies the Armenian genocide"". 

There are different levels of genocide denial in Turkey, I will explain some of them and source them.

1st level: ""There was no killings, therefore there was no genocide."" This is NOT the most popular version of the denial in Turkey. Most Turks accept that there were killings. Defenders of this version are commonly the Neo-Ottomanists in Turkey. Their argument is that Armenians were called as ""sadik millet"" or ""millet-i sadika"" (""loyal nation"") in the empire, then they betrayed Ottomans and sided with Russians. Started gangs and mass killed Turkish people and Ottomans had to exile them. This version of history totally dismisses all the evidence of mass killings, by using conspiracy theories. As an example of this theory you can look at an example, which defends that Armenian genocide was invented to slander Ottoman empire:

Ermeni Sorunu: Büyük Oyun (Armenian Problem: Grand Game) by Adem Suad

2nd level: According to this, there were mass killings on both sides, but this was not a genocide. This is by far the most commonly believed version in Turkey. They mix truth with conspiracy theories and say that accepting genocide will have serious consequences for Turkey so we should not do that. They describe Ottoman Empire at the time as weak, they say that as the empire could not protect its citizens, it had to use force against Armenians. As Armenian gangs terrorized Turkish villages, Turkish people started arming themselves and attacked Armenians for revenge and Ottoman Empire could not stop them. 

There are many different versions but they are all around denying responsibility in some manner. They say that for it to be counted as genocide, it has to be systematic, government has to round up all Armenians and kill all of them. As Ottoman Empire exiled the Armenians and some of them died on the road due to climate, starving etc. it is not a genocide. Founder of Turkey Mustafa Kemal Ataturk himself described the events by using the word ""fecaat"" (""disaster""). But he would not see it as a genocide as well. The defenders of this theory overlook events where government killed people just because they were Armenians as particular mistakes. They say that ""there is no written general plans of genocide"".

Some examples of Turkish historians, politicians and generals defending this theory:

Sürgünden Soykırıma: Ermeni İddaları (""From Exile to Genocide: Armenian Claims) by Yusuf Halaçoğlu (historian)

Ermeni Suçlamaları ve Gerçekler (""Armenian Accusations and Truths"") by İlker Başbuğ (ex army chief of Turkey)

Ermeni Sorununda Strateji ve Siyaset (Strategy and Politics in Armenian Problem) by Doğu Perinçek (far-right politician)

3rd level: It was a genocide, but it was committed by the Kurds. This is a last resort defense and has some truth to it, but again only partly. Ottoman Empire armed Muslim Kurds against Christians during 1890-1908, they are named as Hamidiye corps. After 1908, Ottoman Empire continued to support Kurdish aşirets (tribes? clans?) against Christians, especially Armenians. So some people claim that genocide happened, but it was the Kurds. Again, dismissing how Armenians in other parts of the empire were killed as well.

Source:

Hamidiye Alayları Ağrı Kürt Direnişi Ve Zilan Katliamı (""Hamidiye Corps, Agri Kurdish Resistance and Zilan Massacre"") by Kemal Süphandağ.


All of these are very controversial. Especially in Turkey. But I hope I gave you some idea at least. Sorry if I made mistakes, English is not my native language.

Edit: There are also many other versions that I did not list. For example some people claim that it was the will of Germany and not the Ottoman Empire. A source for that:

Ermeni Tehcirinde Almanya Etkisi (""German Influence at Armenian Exile"") by Bülent Keçik",0
"I'm going to have a crack at answering this question. It's late and there's a lot to discuss, so forgive my typos and organization. I've also been working on this for many hours, and it's already 2 in the morning, so I'm afraid I'll have to continue my response some hours after I've posted this initial one. I hope this is okay, it already partially answers and illuminates some of the things brought up in the question.

&nbsp;


^^edit ^^1: ^^I ^^accidentally ^^deleted ^^two ^^paragraphs ^^of ^^followup. ^^I'll ^^need ^^to ^^rewrite ^^them, ^^and ^^retrace ^^some ^^sources. ^^I've ^^also ^^included ^^additional ^^information ^^regarding ^^Japan. ^^Will ^^then ^^discuss ^^modern ^^statues, ^^and ^^finally, ^^criticism ^^and ^^issues.

^^edit ^^2: ^^I ^^have ^^a ^^third ^^comment ^^discussing ^^modern ^^statues, ^^which ^^is ^^what ^^op ^^is ^^referencing. ^^There ^^are ^^less ^^readings ^^available ^^than ^^with ^^pre-modern ^^statues, ^^but ^^I've ^^tried ^^to ^^shed ^^some ^^light ^^on ^^Chinese ^^and ^^Japanese ^^projects. ^^I'm ^^reaching ^^the ^^limit ^^of ^^my ^^ability ^^through ^^fatigue, ^^if ^^possible ^^I'll ^^have ^^one ^^more ^^section ^^on ^^the ^^criticism ^^of ^^these ^^""ostentatious"" ^^art.

^^edit ^^3: ^^It's ^^5:20 ^^am, ^^I ^^will ^^polish ^^up ^^my ^^grammar, ^^writing, ^^and ^^formatting ^^when ^^I've ^^gotten ^^some ^^rest. ^^There ^^may ^^also ^^be ^^some ^^small ^^technical ^^errors ^^to ^^fix. ^^For ^^now ^^I've ^^addressed ^^most ^^of ^^what ^^I've ^^wanted ^^to ^^say, ^^if ^^there's ^^anything ^^else ^^that ^^comes ^^to ^^mind ^^I ^^may ^^add ^^it. ^^Cheers ^^all.

^^edit ^^4: ^^Fixed ^^some ^^things.

&nbsp;



---

**Part 1 - Background in Understanding Buddhism and its Visual Aesthetics**

---

&nbsp;


Firstly, I find the question somewhat problematic in discussing Buddhism and its ideals. Without going into too much detail and losing focus on the main question at hand, it's crucial to discuss how popular western understanding of Buddhism is heavily built upon the work of Buddhist propagation in the 20th century. In the 1900's onwards, Buddhism would begin to truly take hold in Europe and America as a practice, where previously knowledge of the religion (and I do say religion, this is a wholly separate and complicated argument and matter which we will not explore not) mostly came from explorers, missionaries, historians, and philosophers. Theravadin Buddhism occupied the most interest in the first half of the 20th century, accessible due to colonial presence in Theravadin countries. With Japan's defeat in World War II, Zen came to be truly globalized. Tibetan traditions also have a notable presence, greatly due to the political situation of the Chinese invasion of Tibet and the Dalai Lama's subsequent flight and advocacy work. A second wave of immigration from Southeast Asia can be credit from -political, economic, and social instability, prompting growing communities of Vietnamese Mahayanan and Southeast Theravadin traditions.

Why do I mention all this? Because the practice of Buddhism in the west is noticeably different than in Asia. Whereas Western Buddhism heavily focuses on and specializes in meditation and contemplation, the various kinds of Buddhism in Asia treat the religion on a much more intimate, everyday, and cultural level. In the same manner a churchgoer interacts with the community through practice, ritual, holidays, etc, the Buddhist in Asia does not only focus on the dharma, but the relationship with the laity and the monastic. Historically, the monks were the vanguard of the tradition, and they served the community by offering the spiritual, moral, ethical, ritual, educational guidance. Laypersons were observers and offered whatever was necessary to sustain the monastery. The differences in proportion of layperson to monk in the west and east must also be highlighted. Fewer ordained and qualified teachers in the west means the entire practice of Buddhism needed to change, and the monk-layperson relationship drastically adapted to new conditions.

As such, there have been efforts to alter, deemphasis, and deminish the religious and worship aspects of Buddhism, to be compatible with spiritual trends and social circumstances of the mid to late 20th century. We must understand that Christianity's overwhelming historical presence in Europe and America has heavily impacted society and culture. Alternative options to the dominant norm are always sought after, and Buddhism is one of them. This was especially the case in the 20th century, with the growth of New Religion Movements and a tendency to look towards Asia as a guide for spirituality. This further shapes how Buddhism is to be shared and taught with a different kind of audience. (This is why we also see differences today with American Buddhism and Ethnic Buddhism, the kind practiced by ethnic and immigrant communities.) Meditation and personal insight as a means to achieve liberation and spiritual cultivation is a major focus accessible to the laity in America and Europe. It is not just something reserved for the dedicated monks, the common layperson can participate too. And this is compatible with western conditions, as Buddhism offers a path that isn't in servitude towards a God or higher being, but rather, the personal self. This is why we might mainly focus on or understand the dharma and its ideas as concepts such as the paramitas and the Four Noble Truths. It is also critical to distinguish *detachment* from *asceticism*, which is something discussed in the Middle Way and cautioned by the Buddha. Asceticism is an extreme, not encouraged in Buddhism. However, there are historical instances of intensive devotional acts, but this cannot be considered the norm. The lifestyle of the monastic is certainly rigid, restrained, and intense, but ascetic is somewhat of a misleading term.

To return to my main point here, Buddhism in the east and west are engaged and understood differently. We may understand parts of the dharma as removing attachment, reliance, craving, towards material and impermanent things. These elements, of course, aren't neglected in Asia, but the Buddha and other Buddhist figures represent other ideas of wisdom, salvation, guidance, and meaning. This is why you may find gilded figurines and statues in Tibet and Southeast Asia. Why the offering of merit is so crucial to Burmese society. Why temple goers regularly prey and offer incense in China. Simply put, aesthetics, images, and physical, consumable, and tangible things help one to understand and engage with Buddhism. This is effected by, effects, and intermingles with culture of whichever society Buddhism is in. Ostentatious art is just a natural result of this.

&nbsp;


---

**Part 2a - Answering the Main Question, On Statues and Grandiosity**

---

&nbsp;


Okay, so now that we somewhat understand why some arts and aesthetics of Buddhism may be fanciful and extravagant, why are Buddhist statues so darn big?

We must explore this by dividing time between the pre-modern and modern world. Firstly, we can consider the ancient world and earlier Buddhism. I will try to create a narrative from three examples, Central Asia, China, and Japan, each relying on the preceding civilization with the transmission of Buddhism and its art and practices.

Buddhism originated in the Northern India/Nepal region, and made its way south and west. Buddhism would take an immense hold in Gandhara/Central Asia, up until the great social changes of Islamic conquest. As such, Buddhism spread through the Silk Road, and would be impacted by other ideas with it. Alexander's conquests brought the importance of Hellenistic culture to Persia, Central Asia, India, and China. This is significant as the Greco-Roman valuing of the human figure would dramatically influence art in the aforementioned regions. There is some debate in art history regarding the significance and impact of cultures on one another, though. Large statues of human-like and animal figures existed in Egypt and the Achaemenid Empire. Subjects can vary, be it royalty, rulers, deities, or other mythological figures. Nevertheless, it could be fair to suggest that Hellenistic, Egyptian, and Persian aesthetic values had some impact on Central Asia.

This is why the Bamyan Statues, destroyed in 2001 by the Taliban, are/were such important examples of early Buddhist art. Previously, depictions of the Buddha and other Buddhist figures in the Gandharan region might take form in stupas and statues, but those would not exceed 10m/33ft in height. The larger Buddha in Bamiyan was 53m/174ft in height, while the smaller was 35m/115ft. Not only that, the famous monk-traveler Xuanzang describes them as to be decorated with precious stones and gold. These large statues would be an imposing and awe-inspiring site, one highly praised by residents and visitors alike. It would also represent the devotion of the people, the power of the ruler, and the admiration and respect followers would have towards the Buddha.",0
"In ancient Rome, including Pompeii, the options available to a pregnant woman, particularly one engaged in sex work, would have depended on various factors such as social status, personal circumstances, and individual choices. However, it's important to note that historical records may not provide specific information about the experiences of individual prostitutes in Pompeii during this time. That being said, let's explore some general possibilities for a pregnant sex worker in Pompeii around 20 CE:

1.",1
"Like in so many other cases of deportation, they would have been told that they would be resettled to somewhere in ""the East"" as it was called by Nazi German officials with only a vague sense of what that meant concretly.


People, including the Jewish population, knew about concentration camps. As early as 1933 aft er the founding of the Dachau camp, Concentration Camps and the Concentration Camp system were referenced often in decrees issued by state agencies as well as by the news media, which hailed the development as a bold and necessary step (Himmler giving the press conference for the opening of Dachau). Here are some examples: [Münchner Neueste Nachrichten reporting the opening of Dachau](http://www.vulture-bookz.de/imagebank/Dokumente/images/1933-03-21~Bekanntmachung_KL_Dachau.jpg); a newspaper from Brandenburg [reporting about prisoner transfers to Oranienburg](http://www.stiftung-bg.de/kz-oranienburg/images/597.jpg); the Sindelfinger Zeitung [reporting arrests of communists and their transfer to the Heuberg Concentration Camp in March 1933](https://image.jimcdn.com/app/cms/image/transf/dimension=512x1024:format=gif/path/s190e0533b578352d/image/i7b28571eeff9a93d/version/1383406510/image.gif). Similarly, people were aware during the war that the Soviets and especially the Jewish Soviets were treated very badly and even murdered. We have documents such as diaries and letters starting in 1941 that talk about relatives returning from the front or writing from the front about the Einsatzgruppen murdering Jewish commissars, Soviet PoWs starving to death and so forth.

Similarly, with the deportations from Germany starting in 1941, there was a certain awareness that the conditions in ""the East"" were not very good for those deported and that people died. However, the first deportations of German Jews in the winter of 41 mostly went to Ghettos in Minsk and the Baltics where the previous inhabitants – the Jews of these areas – had been murdered prior to their arrival in order to make space for the German Jews. Many of these deported Jews died to the intentionally poor conditions but with the German Jews, the policy of killing them on arrival only came in 1942. One indication that Auschwitz as a destination did not necessarily hold the horror it would, say, two years later when German spouuses of German Jews took to the streets in the Rosenstraße protests knowing that deportation meant death, comes from a letter by Clara Grunewald – German schjool teacher and pioneer of the Montessori paedagogical system. In August 1942 Grunewald wrote to her friend Clotilde Schewnck zu Schweinsberg: ""The people went to Auschwitz near Krakau, that's a transit camp. And if you mean Hetta Meyer [another friend of theirs who had been deported]? She went to Theresienstadt. YOu can tell her relatives that it isn't that bad. The old and sick are well taken care of there."" They weren't. Hetta Meyer died after a couple of weeks in Theresienstadt while Clara Grunewald was also deported to Auschwitz where she was murdered in 1943.

And while many German Jews lacked concrete knowledge of the horror that awaited during and after deportation, others had a stronger awareness that it meant nothing doog when compared to Grunewald. Adolf Guttentag, a medical doctor fromn Breslau writes in his diary prior to deportation to Theresienstadt in September of 1942 twrites about his elderly neighbour of 87 who said he was not going to bow and therefore killed himself. He also writes about considering killing himself together with his wife and supplying his elderly mother with enough sleepping medication to do the same – something he would do barely a month later shortly before his final deportation.

A similar awareness of the situation can be gleaned from a document in the files of the Düsseldorf Gestpo from October 1942 concerning the case of Anne Baum. Baum had married the Jewish company owner Mayimilian Baum in 1917 and had had several children with him. While their youngest son had already been evacuated from Germany on one of the so-called Kindertransporte to Great Britain, their duaghter Irmgard, born in 1919, had not left Germany and was regarded as a Jew by Nazi German authorities. And while Maximilian Baum remained spared from deportation due to his ""Ariian"" spouse, their daughter did not and was deported to the Izbica Ghetto in April 1942, where she was murdered. In October 1942, Anne lodged severl inquiries with her local Gestapo in Düsseldorf concerning location and fate of her daughter.

In the protocal recorded by Hermann Waldbillig, Waldbillig writes that the Gestapo will not give her any information concerning the location of her daughter, that further inquiries with any sort of official burecratic institution is senseless and that her daughter will not be returned to Germnany. In her original letter, Anne Baum had mentioned severl things she had heard about those resettled:

> When shopping in the bakery of Mr. Ehrmann I overheard a nurse and civilian talk about how the Jews in the east had to dig their own graves and then were killed with a shot to the nape of the neck. Other Jews had been placed in an ambulance that was then filled with gas in order to kill them. [...] I am convinced that those rumors are not true and I demand to know about my daughters fate in order to disprove such vicious propaganda.

All of these things Anne Baum had heard were true. But Waldbillig warned her to never repeat those rumors for that would consititue a crime.

The fact that while there was knowledge of what happened to deported Jews floati9ng aropund Germany in 1942, people still went to assigned place in order to be concentrated, hearded into a train and sent off to an uncertain and very dangerous fate has to do with the fact that after all, it is very hard to shatter a person's relationship to state authority. When organizing these deportations, the letters and declarations people received were steeped in legalistic police language and splattered with official seals and signatures. It came from the police after all. While there were people who took active steps to avoid deportation – not going to the synagogue, staying with friends and so forth – there were also people whose faith that while the state they lived in might discriminate against them, they would not outright murder them and their families was strong. In the modern world we are conditioned to follow laws and directives and to asusme good faith on behalf of ""our"" state, ""our"" country, ""our"" government. Hence, people – often despite existing knowledge – would bow to faith, surerender to the police and go and do what they were told – up to wlaking into the gas chamber on arrival still clingin to the hope that it really was only a shower.

At the same time the Nazi regime used all this in order to make everything seem less threatening and more legitimate by couching it in the symbols and language of odfficial state buisness – from letters of deportation sent by the police to giving everyone a ticket on their journey to ""the East"".

It's these things coupled with assurance from people in unfiforms, people who had the air of being officials that lead to so many deportations running so smoothly.",0
"#Part 1

While I generally agree with /u/retarredroof that being more specific will yield a better chance to analyze a situation, I think we can paint a broad, yet somewhat accurate, picture by looking at several different instances. To start this picture, we can look back on several past answers I've written for previous questions.

* [Is there a recognized ""Native American"" philosophy, similar to how Western thought and Eastern thought is generalized?](https://www.reddit.com/r/AskHistorians/comments/8do4hf/is_there_a_recognized_native_american_philosophy/)

This answer explains that for Indigenous Peoples, we most certainly had our own understandings of philosophy and what it meant to lead a good life. This will ultimately differ from Tribe to Tribe, but it provides a foundation for understanding that while Tribes certainly do not reinforce the ""Noble Savage"" myth,^1 which is essentially what your question is getting at, there was intellectual thought and reasoning behind model conduct that individuals of Tribes were to maintain for their collective identity.

* [Was sovereignty a part of Native American political thought?](https://www.reddit.com/r/AskHistorians/comments/6setwq/was_sovereignty_a_part_of_native_american/)

In a similar matter, I discuss here how politics was also a very ingrained concept for many Indigenous Peoples. This is to say that life was not really ""simple"" in the sense that people just ate, hunted, and slept. Nations were maintained and doing so often required high levels of organization and structure in order to logistically provide for these needs and protect their land holdings.

* [Were Native North Americans egalitarian?](https://www.reddit.com/r/AskHistorians/comments/57utud/were_native_north_americans_egalitarian/d8v5ulw/)

This answer probably has the most applicability to your question. It describes that for many Tribes, particularly my own, society overall was more egalitarian, which I think many exaggerate for being ""idyllic,"" though I also wouldn't say that isn't too far from the truth.

There are two major areas we can look at to further inform our conclusion on this matter, to access if Tribes were ""idyllic,"" or at least if they happened to be different enough that we can see varying degrees of overall prosperity. These are culture and practicality. For the first, the last link addresses this:

>What often determined how egalitarian a society was is the societal structure and resources. Many Native American communities were hunter-gatherer societies. Some were farmers. Some had a combination. However, most were communal. This caused them to have a more egalitarian society than what we see today because the ""wealth,"" so to speak, was distributed more evenly because life was organized around kinship ties and reciprocity for the well being of the whole community. This lifestyle was rooted in both cultural value as well as economic value.

>In terms of cultural value, sharing, gift-giving, and trading were all highly valued because what was given was expected to be repaid in some way later in the future (hence the ""reciprocity""). This was because resources were often limited and the gaining of resources required community effort, more than what one person could provide.

>This leads into what we could consider the economic value. Because many hunter-gatherers were nomadic or semi-nomadic, they were not able to keep vast reserves of food on hand. They needed to carry what they needed. This means that they could not sustain a population beyond a certain size. Thus, many native societies were balanced in number of births and deaths. And when there isn't a surplus of food, you are more dependent on others of your community who have also gathered enough food. This indicates that not one person was in charge of all the food. It was all shared and ""owned"" by the community. Unlike in agricultural and industrial societies, structures where mass production of resources can be carried out by a few, hunter-gatherers had to share everything in order for everyone to survive. This meant that resources, particularly food and shelter, were distributed equally. Individual wealth and prestige might differ, but not so much as to offset the balance of the society.

Beyond this, let's examine some key areas to better inform this position.

##Government

One of the most well known examples of Indigenous Government that existed prior to the United States and even colonization by European powers is that of the the Haudenosaunee, or the Iroquois Confederacy. The origin stories of their own nations give a glimpse into the social environment it was born out of and also exemplify Indigenous values that standards of the Old Ways were based off, though they were not always followed.

>The Haudenosaunee ... are an ancient people of North America. Our tradition states that our people originated in the northwestern woodlands of North America ... Our existence in these lands has not been one of absolute peace and tranquility. We have had to work hard to develop the civilization we enjoy. There was a time when or lands were torn by conflict and death. There were times when certain individuals attempted to establish themselves as rulers of the people through exploitation and repression.

>We emerged from those times to establish a strong democratic and spiritual Way of Life. The confederate state of the Haudenosaunee became the embodiment of democratic principles that continue to guide our peoples today. The Haudenosaunee became the first ""United Nations,"" established on a firm foundation of peace, harmony and respect.

>Within the Haudenosaunee, all member nations are equal, regardless of size. Within their national territories the member nations are autonomous, but all adhere to the central principles of democracy that we agreed to at the formation of the Confederacy (Akwesasne Notes, 2005, p. 26). . .

>All of this political activity is set in the roots of an ancient tradition of the spirituality of our peoples. This cosmology places the Haudenosaunee in a balanced, familiar relationship with the universe and the Earth ... This philosophy taught us to treat the Natural World with great care. Our institutions, practices, and technologies were developed with a careful eye to their potential for disturbing the delicate balance in which we live (p. 27).

The Haudenosaunee explain here that for them, before the formation of their government, they experienced many of the same problems we encountered after colonization and into our modern day. Wars, fighting, killings, and injustice did exist in various forms. These were carried out at times by Natives against Natives. **However...** The occurrence of these actions are regularly filtered through a Western lens. As the Haudenosaunee describe, they, as a people, were not governed by carnal instincts that put them at odds with others and with the Natural World. Rather, they sought to cooperate with their environment, recognizing their role in it and the balance they should maintain.

When forming their government, they based it off what would become known as The Great Law of Peace. This law, which is their constitution, provided the model conduct for their citizens to emulate. This law was provided by a man called the Peacemaker.

>The first principle that the Peacemaker set forth was indisputable to those who heard his words. He said that it has come to pass that in this land humans beings are seen to abuse on another ... From that initial explanation--that the Giver of Life (later addressed as the Great Creator) did not intend that human beings abuse one another--he proposed that human societies must form governments that will serve to prevent the abuse of human beings by other human beings and that will ensure peace among nations and peoples (p. 32). . .

>Peace was to be defined not as the simple absence of war or strife, but as the active striving of humans for the purpose of establishing universal justice. Peace was defined as the product of a society that strives to establish concepts that correlate to the English words power, reason, and righteousness (p. 33).

Here, we see a good blend of practicality *and* Culture. In order to sustain the very principles of their society, they needed to conduct themselves in accordance with this law. It is important to note that this law was not written down. It is an Oral Tradition that is [enforced by internalization of the law,](https://www.reddit.com/r/AskHistorians/comments/80eg1h/monday_methods_the_we_and_the_i_individualism/) which fosters a strong constitution in individuals of their nation. This observance is what was noticed by the colonial powers and their misunderstandings led them to conclude that their primitiveness is what resulted in their supposed nobility rather than a complex spiritual and philosophical lifestyle in which their society was dependent upon. This, of course, was encouraged due to their collective nature. They worked more on a collectivist mentality rather than an individualistic one, the latter being more common in the Western World today and which is the lens many non-Indigenous researchers regularly peer through. Their collectivistic nature is then demonstrated in the rights established for citizens of their nation: all leaders are elected democratically, councils have the power to nominate and advise elections, all citizens have full rights, all citizens can travel freely, women had all the rights as men (women even were in charged of selecting leaders), immigrants were provided protection for under their laws, individual nations of the Confederacy maintained their autonomy though separate national boundaries had been abolished. For individual internalizing this law, it can be summed up in this phrase:

>A society was socialized to the ideology that, if an injustice occurs, it is their moral duty to defend the oppressed again their oppressors (p. 38).",0
"For further reading: 

Gary Scott Smith, *Heaven in the American Imagination.* Oxford UP, 2011. 

Kathryn Gin Lum, *Damned Nation: Hell in America from the Revolution to Reconstruction.* Oxford UP, 2014. 

Fay Botham, *Almighty God Created the Races: Christianity, Interracial Marriage, and American Law.* UNC Chapel Hill, 2009. 

Paul Harvey, *Bounds of Their Habitation: Race and Religion in American History.* Rowman & Littlefield, 2016.",0
"That is kind of where you get into my original point about oral history being different. 

It's easy to think about history is being just a chronographical set of facts.  A happened and then B happened and then C happened. 

However, a truly impartial chronicling of facts is a very rare thing. As often as not if you have a mere list of facts there is some non-historical purpose. Historians might make good use of church baptism records or tax collection records, or a shipping manifest but those creating them were not intending to write history.    When people actually intend to write history, usually there's political motives involved.

Part of what a historian does is ask questions like "" who was telling the story?,"" "" why was he telling a story?"" And  ""why did he choose to tell this particular story?"" 

Bringing this back around. All of the stories that appear in the Book of Judges purport on their face to be historical accounts.   But consider the time and place they were told, retold, and subsequently written down.  The Israelite people are in Babylonian captivity.   The a temple that was the center of their religion has been destroyed, and their people have ceased to have an independent political entity.  

A group of students are sitting around an elder or a scholar and he is telling them the stories of their people, as the stories were told to him when he was a student. The person telling them though stories is a man, and most likely a man of it least some influence in society.  What stories does he tell and why?   When he finishes the story, what's the lesson?",0
"All right, y'all, here's what's up:

There are over 100 comments in this thread. *Yes*, they have all been removed barring a follow-up that is closely related to OP's question. *Yes*, they all violate AskHistorians' rules in some way. Here is what you are missing:



Category | Tally
--------|-----
Mod comment | 3
If you know so much, why don't you answer? | 2
But I [circle one] want/need/deserve an answer | 21
Question is unanswerable | 10
Stop ruining reddit | 5
Follow-up questions | 14
*Unique* follow-up questions | 1
1-5 word answer | 15
Joke | 12
Bestiality joke | 3
Monty Python joke | 2
People who think the 17th-19th century are the Middle Ages | 5
Blatant misogyny | 4
RemindMe! messages | 4
Block quotes/links | 5
Evo psych speculation | 6

The more people complain about the number of removed comments, the more removed comments there are going to be.

You've all been warned, now twice, about shitposting in AskHistorians. We've banned ~~five~~ 11 people based on this thread already. [Don't be number ~~six~~ 12.](https://giant.gfycat.com/BountifulAmpleAffenpinscher.webm)",0
"It depends when in the century you look. **Caffeine** was ubiquitous throughout the century in tea and coffee. **Nitrous oxide** was most popular at the start of the century, thanks to Humphry Davy and his ilk. 

**Opium**, like caffeine, was present throughout the century, but it reached its peak in the middle of the century before tapering off under public scorn. Refined opium, as **morphine**, was used as medicine and becomes more popular after the development of the hypodermic needle. The same is true with **codeine**; both morphine and codeine were developed as chemical science develops during the century.

**Cocaine** was the miracle drug of the 1880s but shunned by the 1890s. **Heroin** was the miracle drug of the 1890s (it was synthesized in 1874, but it didn't become widespread until well afterward) but shunned after the turn of the century. 

**Cannabis** was around throughout the century, but like opium, it was shunned by all but the lower classes because of its racial connotations, and it remained a minority taste. Drug varietals of cannabis were closely linked to India, and given India's status as a British colony, it was not seen as suitable for white British society.

**Ether** became popular about the same time as nitrous oxide, and it was one of the most popular drugs of 19th century Britain. As Jay writes, ""ether was in some ways the cannabis of its day: a tool of hedonistic and often deliberately irresponsible abandon, a spur to social 'frolics' and outlaw behaviour, a passport to a subculture beyond the pale.""

**Psilocybin** wasn't identified until the 20th century as a separate drug, but it does show up in the 19th century via warnings against eating the Liberty Cap mushroom, of which botanist James Sowerby, writing in 1803, said, ""nearly proved fatal to a poor family in London, who were so indiscreet as to stew a quantity for breakfast.""

Imagine that trip. 

It's worth noting that *Alice in Wonderland*'s mushroom-related fantasies likely came from early 19th century descriptions of the effects of the Fly Agaric, described in travelogues from Russia. H.G. Wells also enjoyed his magic mushroom trips, with *The Purple Pileus* of 1897, and the psychedelic fungi of *First Men in the Moon*.

**Mescaline** shows up at the very end of the century, and only in tiny, tiny amounts worth noting only as a curiosity.

And if we're being complete, we can't forget our friend **alcohol,** the most abused substance in Britain during the 19th century, the 20th century and the 21st century, as well as **tobacco**, the second-most-popular choice.",0
"If you are in Turkey, as /u/flying_shadow already linked, you could do far worse than checking out our FAQ which has some content on this. If you are looking for published sources, I'm not positive what the availability would be within Turkey, so you might not be able to find all of these - I know some, but not all, are available as eBooks - but I can recommend some literature for a more in-depth dive into the topic:

*[""They Can Live in the Desert but Nowhere Else"": A History of the Armenian Genocide](https://amzn.to/32T7gpI)* by Ronald Grigor Suny is what I usually recommend for a lighter, basic introduction to the topic. It isn't as in-depth as some other works, but it provides a good overview, especially if you want something that doesn't feel too academic.

*[The Armenian Genocide: A Complete History](https://amzn.to/3g2SAb6)* by Raymond Kévorkian is then the deeper complement if you *do* want that heavier, academic treatment, providing a very thorough study of the topic.

*[The Armenian Genocide: Evidence From the German Foreign Office Archives, 1915-1916](https://amzn.to/30LNqd4)*, edited by Wolfgang Gust, is probably the best primary source collection available on the topic, providing ample material that was reported on by German officials present in the Ottoman Empire at the time as they were then allied with the Turks.

*[A Shameful Act: The Armenian Genocide and the Question of Turkish Responsibility](https://amzn.to/32Q50iJ)* and *[The Young Turks' Crime Against Humanity: The Armenian Genocide and Ethnic Cleansing in the Ottoman Empire](https://amzn.to/3jF9PkQ)* by Taner Akçam are also worth mentioning. He isn't my first choice to recommend, but Akçam definitely has some power in being a Turkish scholar who studies the genocide.

*[America and the Armenian Genocide of 1915](https://amzn.to/2CN1hI5)* by Jay Winter isn't a collection of primary sources, but it similarly provides a look at an outsiders view. American diplomats were present in the country as well, and likewise are important witnesses for our understanding of what was going on at the time. *[The Burning Tigris: The Armenian Genocide and America's Response](https://amzn.to/30I9PrU)* by Peter Balakian covers similar ground but from a more popular history approach.

If you want to really get to the niche topics, *[The Armenian Genocide: Cultural and Ethical Legacies](https://amzn.to/30HG0HT)* edited by Richard G. Hovannisian is an edited volume with essays tackling various aspects of the genocide in detail. It isn't a full history, but rather a way to learn about various details.

*[In God's Name: Genocide and Religion in the Twentieth Century](https://amzn.to/2EaIFCc)*, edited by Omar Bartov, has a very interesting essay by Ara Sarafian, ""The Absorption of Armenian Women and Children into Muslim Households As a Structural Component of the Armenian Genocide"" which looks at aspects of the genocide beyond the killing, and how women and children were forced to convert and assimilate into Turkish households.

There is also *[Survivors: An Oral History of the Armenian Genocide](https://amzn.to/3f07JZr)* by Donald E. Miller & Lorna Touryan Miller if you want something with a more personal voice of the victims themselves.

*[Children of Armenia: A Forgotten Genocide and the Century-long Struggle for Justice](https://amzn.to/2EgNWZ8)* by Michael Bobelian is much less academic, but I found to be an interesting, and heartfelt, look at the meaning of the Genocide especially within the diaspora community

Hope that helps!",0
"There is absolutely a sex and gender element in witchcraft.  Karlsen's *The Devil in the Shape of a Woman* is another great book that really dives into the gender aspect of witch hunts.  Gasser's *Vexed with Devils* pairs well with it by discussing masculinity and witchcraft.

&#x200B;

Witch was a gender neutral term- anyone could be a witch.  A lot of accusations and defenses suggest gender conformity or non-conformity to make arguments.  The defense of Rebecca Nurse mentions her children since adhering to Puritan standards of womanhood, ie producing children and raising them in the faith, was a sign of virtue.  Accusations against Sarah Good referred to her unwomanly cursing.  George Burroughs was described a 'a puny man' and needed the devil to gift him physical strength to strike against his masculinity.

&#x200B;

However, the Salem Witch Trials are a multi-stage process.  It takes time to build up accusations where the Boston elite are potential targets.  First, its the usual suspects- Sarah Good, a beggar; Sarah Osborne, reputed for promiscuity; and Tituba, an enslaved woman, most likely a Carib Indian woman.  Gender, status, reputation, and race were markers of a person's spiritual standing and fortitude.  Eve was tempted first then Adam which led to this mindset that women were the easier sex for the devil to ensnare- gender could cause suspicion.  Status, reputation, and race were markers of spiritual success or failing- a good Puritan would be white, upstanding and blessed with fortune in life and God would punish souls destined for damnation with poverty, they'd commit sins in their lifetime, and non-whites were seen as an 'other' cursed by God with limited potential for salvation.

&#x200B;

Next came some of the less likely suspects- Martha Corey and Rebecca Nurse, both church members.  However, Corey's reputation was imperfect and her acceptance into the church was controversial.  If one church member can be a witch, then any can, even the model Puritan woman Rebecca Nurse.

&#x200B;

Accusations continued and finally Elizabeth Procter is named.  Then John Procter, the first man.  A month into accusations and the only way to credibly accuse a man is to build it out of the accusation of his wife.  Satan tempted women who in turn brought their husbands to the devil.  Its a process to accuse men, but once one man is accused, more men are.  The accusations are roughly 75% women and 25% men during the Salem Witch Trials.  

&#x200B;

Five men hanged of the 19 executed, so the executions roughly line up with these percentages too.  I just quickly ran through my list and counted \~46 men accused of the 177 on my list.  Most of those men are accused along with family members, and while women are named alongside family, there are more cases where women are named outside of their marriages or fathers or sons than men are accused separately from their wives, daughters, and mothers.  Its not exclusive, but it is so deeply gendered and so strongly biased against women.",0
"There are other Abrahamic religions that have formed historically as sects from these three, or as relatively new religious movements that are partly inspired by them (Baha’i), and others that may or may not be recognised as part of these religions depending on whom you ask (the Mormons and Jehovah’s Witnesses certainly self-identify as Christian, while many major ‘traditional’ churches do not see them as such). 

So as not to get into the very controversial cases of religions founded in modern times, let’s restrict this to those that were founded in ancient and medieval times, but still exist. I’ll try to give an overview of three major examples and why they fit the criteria, which I’ll use as ‘having historically largely developed from an established Abrahamic religion’, and *possibly* ‘according the figure of Abraham special status as a prophet or similar authority or precursor.’ 


**Samaritanism** is probably the most ancient ‘fourth’ Abrahamic religion, older than Christianity and Islam, and can be seen as ‘alternate timeline’ Judaism, or Judaism’s sister religion. They’re most well known through the parable of the Good Samaritan, but there are about a thousand still left in both Israel and the West Bank. They have their own version of the Torah, with subtle differences and in a slightly different ‘Samaritan’ Hebrew. From Jesus’ time they were seen by many Jews as ‘fake Hebrews’ who were really Gentiles from the Assyrian Empire, and their centre of worship was Mount Gerizim rather than the Temple in Jerusalem, a fact the Jews generally saw as unacceptable. It seems they are the one group with a real claim to continuity with any of the Northern Kingdom (which fell to the Assyrians before the southern ‘Jewish’ kingdom of Judah fell to Babylon), or the ‘Lost Ten Tribes’ of Israel (by tradition, the tribe of Ephraim, son of Joseph), appearing to descend largely from Israelite men and women from elsewhere in the Middle East, possibly brought in by the Assyrians, so in a sense it appears both narratives had a grain of truth. The post-diasporic Jewish convention of recognising the female line adds some new ‘legitimacy’ that was lacking in Jesus’ patrilineal time. They are thus essentially the small exception of Hebrews today who are not Jews. They are certainly Abrahamic. 

The **Mandaeans** of southern Iraq (many of the remaining few tens of thousands scattered by the recent wars) are essentially, according to a numbers of scholars, the last true remaining branch of ancient Gnosticism, from the first few centuries AD. The consensus has shifted over the last century (especially since the discovery of the Nag Hammadi library) from seeing the Gnostics as pre-Christian philosophical schools that incorporated Christian elements as it became socially and politically wise to do so, to early Christian-based sects that incorporated pre-Christian elements. The origins and even original beliefs of the Mandaeans in particular are obscure, and are still varied, but from the early Islamic era we have records of them proclaiming their reverence for John the Baptist, and some see Jesus as a ‘usurper’ of John’s role of major prophet. Baptism is a major part of their ritual life, and their beliefs include aspects of cosmology, the nature of God, and eschatology that are similar to other Gnostic traditions. There is a lot of debate among secular scholars about the degree to which they can be claimed to have basis in Christianity (internally, Mandaeans would not like this idea), but there are undeniably ultimately key Jewish elements in the religion. They do not, however, recognise the prophethood of Abraham or Moses either (though they recognise both John the Baptist and others who are key in Judaism), so calling them ‘Abrahamic’ might be a thorny semantic issue. It is reasonable from the available evidence, and what we can now deduce about the history of Gnosticism, that they are *based* on Abrahamic religion. 

The **Druze** are another, far more numerous group, numbering over a million. They may or may not fit the criteria I specified above, since not everyone agrees on their status as a separate religion and they were founded much later (around 1000 AD), in Egypt, at the time of the Ismaili Shia Fatimids, and accounts differ very strongly: the Fatimid Caliph al-Hakim was particularly repressive of non-Muslims and Muslims outside his Ismaili Shi’i sect, and the most commonly recognised founder of the Druze, Hamza ibn-Ali ibn-Ahmad, for reasons complex and unknown, but possibly in a misguided attempt to placate the Caliph, declared al-Hakim to be divine. This resulted in his execution. Other versions are that he claimed no such thing, and the significance of the exact sequence of events, as well as exactly what Hamza ibn-Ali ibn-Ahmad believed, are sensitive and highly disputed. Regardless, the Druze community eventually shifted to Mount Lebanon, eventually coming to rule much of the area. They also incorporate mystical elements sometimes seen as Gnostic or Sufi in basis, as well as such unusual beliefs among the Abrahamic religions as reincarnation. Most Muslims do not recognise them as Muslims, and about half of Druze (from memory; will check the source of this stat) consider Druze to be a separate religion from Islam, though with a great deal of intersection. There are of course many other esoteric sects with a pedigree that are regarded with varying levels of suspicion by most members of the major religion, but Druze has a more widely recognised separate status, and this is recognised politically in Lebanon. Much of the religion is still clearly grounded in Islam, and it developed within an Islamic context, its earliest adherents being Muslims, and they do see Abraham as a significant figure in a similar way - so ‘Abrahamic’ applies.",0
"



A follow up question, if you could help with that:

What would happen if I owned a semi-destroyed building but could not (or would not) repair it for some other reason than those you mentioned.  Like, say I was a citizen in good standing with no ties whatsoever to the Nazi party but after the war I had no money and no surviving tenants or family but was left with five stories of rubble on my property.

Would the property be taxed and eventually seized?  

I have no idea why this subject is suddenly so fascinating to me.  Thanks for answering OPs question!",0
"While legal importation of slaves was banned in 1808, smugglers continued bringing slaves into the country illegally. The final known slave ship was the *Clotilda*, which arrived in Mobile, AL, on July 9, 1860. The captain reported that he burned her down after delivery. The US government tried him the next year on charges of importing slaves but he was acquitted.",0
"The murder of Tupac Shakur on September 7, 1996, and the subsequent speculation surrounding the involvement of the Notorious B.I.G. (also known as Biggie Smalls) have been subjects of ongoing debate and investigation. It is important to note that no definitive evidence has conclusively proven Biggie's involvement in Tupac's death.",1
"Great question. The way that the admiralty would budget for the year is to decide on what number of ships would need to be manned, then use a formula that allocated a given number of men for a ship, to form a fairly decent budget (there's a reason these were called Naval Estimates). Then, each ship's captain or each fleet's admiral would have to find men for his own ship. Pursers (the warrant officers in charge of supplies and victualling) would be allocated a fixed rate per man for provisions and slops (clothes) and would make their own money by being frugal with what they bought. (This is one of the reasons that carrying men on the books who weren't on the ship was considered a fairly serious crime -- corrupt pursers could claim allowances for men who weren't there.)

In terms of making a claim on wages, seafaring men of the period would usually write wills during the course of a voyage -- if one could be found, the wife would have a claim; otherwise she would be at the mercy of the Admiralty to prove that she had some sort of a claim on his wages. The navy did track seamen who had been injured, for purposes of payments from the [Chatham Chest](https://www.reddit.com/r/AskHistorians/comments/4sdmh3/napoleon_bio_has_quote_from_a_medical_officer_on/d58n38z/), but the men or their agents would have to self-apply for that payment.",0
"Would you settle for...*a combination of factors*?

;)

You're right, though--the Mongols are one of those areas of history where popular interest significantly outpaces research. We can find contributing factors in the primary source record, the practicalities of archaeology, the state of secondary scholarship, and the long stranglehold of ""classical"" education and adulation in the West.

I'm going to write this from a western, primarily Anglophone perspective, although it will become clear that the specifically Western factors involved affect Near Eastern and Asian scholars as well.

In terms of primary sources: internal to the Mongols, the source record is extremely weak and late. The oldest surviving textual source, known today under the excellent title *The Secret History of the Mongols*, was written sometime after 1228. Even then, our copy is in Middle Mongolian, but recorded in and for a Chinese context in the early 14th century. Igor de Rachewitz argued in 1965 that the *Secret History* was an original for the Mongols, not just the sole survivor. Genghis Khan had ordered the adoption of Uighur script for the writing of the Mongolian language, reinforcing our understanding of earlier Mongol illiteracy; the text itself seems to be a collection of oral genres mashed together rather than reflecting any established written genre from surrounding civilizations--and definitely not reflecting the internal historiography that Mongols under the Yuan dynasty would produce.

Later on, even with Kublai Khan's advisors convincing him that the history of the Mongols starting from Genghis' sons needed to be written (the *Secret History* covered to that point), the segmented Mongol Empire had entire swathes that just...shunned an internal historiography. Thus, even with the Mongols developing an elite literate class within the Golden Horde or the Chagatai Khanate, they seem to have preferred to maintain *oral* histories rather than written ones.

The other body of Mongol Empire writing that survives is diplomatic letters especially to the West, which exist in Latin translation. But the utility of this corpus for informing us about the Mongols themselves is limited by its purpose, especially since Latin Europe had a vested interest in seeing the Mongols as potential converts to Christianity and allies against Muslims.

Then there are the external written sources from the people who interacted with and reacted to the Mongols. There are lots of these!

...And they are written in medieval versions of Chinese, Persian, Arabic, Turkish, Russian, Armenian, Georgian, Old French, Italian, and Latin.

To really get a grip on the Mongols, a scholar would need a handle on a rather breathtaking number of languages, *or* access to reliable translations. This is why ""how the West reacted to the Mongols"", as Peter Jackson points out, has been a rather popular line of scholarship since the 1820s...in the West.

Which leads us to the secondary scholarship problem: it's dispersed among even more languages than the primary sources. Bits and pieces of analysis of the Mongol Empire have absolutely been done! Are ongoing! And often impossible to access for the majority of scholars.

Archaeology, which is ever more important even to historical fields with a strong textual record, poses challenges for tackling the Mongol Empire as well. The Empire was *big*, y'all. There's not as much ""knowing where to look"" as there is when, say, walking through Rome. And much of the space the Empire held--including actively held, with outposts and castles--is not very densely populated today. So in England, for example, you've got another medieval village discovered when the city goes to build a parking lot or an apartment building. That kind of serendipity is less likely to occur for archaeologists searching out Mongol material culture.

And finally for present purposes, I'll talk about the overarching, many-tentacled influence of the Western inheritance of classical (Greco-Roman) culture. I was originally going to make a joke about how if you're comparing density of historical scholarship on a subject, ancient Rome is about as useful a metric as the tanks of World War II. But the fact is--that is *not* a given, that is an active, ongoing choice of European/American historiography. Even with ""barbarian"" appropriation and alteration of *Romanitas* in the early Middle Ages, ""Rome"" as an idea held a certain allure, a prestige that everyone wanted to tie their own polities into. In the 14th century, early humanist writers lament how the world has fallen into a dark age *since the high point of Rome*; the Renaissance of the 15th-16th centuries was a group of white men wanting to *bring back Rome*. Rome has been at the center of historiography and the west in more ways than just the immense debt we owe to Edward Gibbon--the whole idea of ""western civilization"" derives from a culture repossession of 'classical' heritage. Into the 20th century, learning Latin and Greek was the basics of an advanced education; ""classics"" is not only its own field of study, but monopolizes the label ""Classic"" for Greece and Rome above the classical traditions of every other society on the planet.

So for Westerners, learning Latin and Greek in the course of a formal education was for so long a necessity because of our belief in the importance of classical heritage...but that also made it much easier to study Greece and Rome because people could read the sources!

And the linguistic imperialism of the west continues today with the surpassing importance of English in historiography. Historians trained in western graduate schools must know English, French, German, and then (or possibly substituting for French/German) the languages relevant to their own research. What if we required historians to learn Arabic or Farsi and Chinese or Japanese, and then allowed the *substition* of French or German for one if their research demanded it?

In the case of Mongol research, that would connect Anglophone scholars with the scholarly community that *doesn't* produce writing in English, French, or German. It would provide a basis for tackling a body of sources that is immense when you combine languages.

And it is almost definitely not going to happen. What I said above about how Europeans, and European-descended North America, viewing Greece and Rome as the ""foundation of western civilization"" since, well, since Greece and Rome? This invents a continuity, a line from them to us (I am American) today, which has major ramifications for scholars having to defend our existence as scholars, not just the conclusion of our research. There are, they say, two reasons to study the Middle Ages (slash Renaissance slash European history etc). One, because it's the foundations of the *modern* world and lets us see how we got here today. Two, because the Middle Ages are f*cking weird. 

If you take a sweeping, people-focused historical approach, as history academia to its credit is starting to/trying to do, you absolutely find the ""foundations of western civilization"" in a web *with* other civilizations; it's not a story that can be told in isolation. But the mainstream idea of ""foundations"" tends to be intellectual, scientific, probably less so religious today. Lawyers drop Latin terms into everyday prose because the roots of our legal traditions are Roman; you're reading this in a font that derives from the 15th-16th century humanists believing that the Carolingian miniscule handwriting their classical sources were written in was actually Roman handwriting.

So ultimately, it's easy to say ""the Mongols didn't write much."" It's easy to say, ""the Mongols are hard to find in the dirt."" But look--the Mongols are a medieval civilization, and if there is anything medievalists do better than you, it is drawing entire books and decades-long scholarly controversies out of the doodle marks in the margins of one manuscript, or the discovery of a single coin. 

We absolutely could do better. But long, long patterns of Western cultural chauvinism have shaped a scholarly environment where research on the Mongols is compartmentalized in isolated linguistic/geographic communities. And we're a long ways from fully reconciling those divides.",0
"> The first week in power wasn't that important. 

But did he do anything at all during his first couple of days as Chancellor or right after he was given autocratic powers? With all due respect, OP didn't ask for an explanation about how Hitler rose to power.",0
Damn! That was quite a ride. Thank you so much for such a great answer!,0
"In his first week in power, Adolf Hitler and the Nazi Party took a series of significant actions to solidify their control over Germany. Here are some key events that occurred during that period:

1. January 30, 1933: Hitler was appointed as Chancellor of Germany by President Paul von Hindenburg. This marked the beginning of his official political power. 2.",1
"In 166 CE, a group of men claiming to be an embassy from the Roman emperor Marcus Aurelius arrived in the Chinese court at Luoyang. They weren't really an embassy - more than likely, they were just some marooned merchants from Alexandria - but they had managed the impressive feat of traveling the length of Asia. If some masochistic impulse had impelled them to make the journey overland, they could have followed the lacing branches of the Silk Road the whole way from Rome's eastern frontiers to the western marches of Han China. And over the five thousand miles of that wholly unnecessary journey, they would have crossed only three borders: from the Roman Empire to the Parthian, from the Parthian to the Kushan, and from the Kushan to the Chinese.

By the beginning of the third century CE, the political structure of Eurasia had been more or less stable for a long time. The border between the Romans and Parthians had shifted little over the years, though the Romans had managed to claw away bits of Mesopotamia and Armenia. The Parthian realm had gone through a few cycles of fragmentation and consolidation; but it retained more or less the same borders it had defended since the great Roman campaigns of the Late Republic. The Kushans - as far as we can tell - controlled much the same parts of northern India and central Asia that they had for centuries. And the Han Dynasty, though riven by rebellion and gradually receding from its conquests in the Tarim Basin, still governed virtually all of its traditional domains.

A half-century later, the world had changed. The Roman Empire was still there, but torn by civil war and battered by invasions. The Parthians were gone, replaced by the Sassanid Dynasty. The Kushan Empire had withdrawn to India. And China was divided between three kingdoms.

It is tempting to connect these stories of decline, and there certainly were ties between them (as we'll see shortly). But it would be misleading to assume that they had the same root causes.

To begin with the Romans: the half-century between the assassination of Severus Alexander (235) and the accession of Diocletian (284) is sometimes called the military anarchy. Whatever you call it, it was an unpleasant time to be Roman. Domestically, the chief problem was a crisis of imperial legitimacy. Emperor after emperor was proclaimed by the legions, none lasting long enough to establish a dynasty, each compelled to placate the troops with currency-ruining raises and bonuses. The domestic crisis was worsened, and largely driven, by developments beyond the frontier. To the north, unprecedentedly large and well-organized tribal confederations swept over the northern frontiers. And to the east, the shambolic Parthians were replaced by the aggressive and expansionist Sassanid Dynasty, whose kings Ardashir and Shapur led massive raids into Roman Syria.

The Parthians had never had a strongly unified realm. Part of the problem was the decentralized nature of their imperial administration, which seems to have devolved a great deal of power to local dynasts. External pressures were another perennial weakness. From the reign of Augustus to their third-century demise, the Parthians were Rome's punching bag. Their capital was repeatedly occupied by Roman troops, and thier dynastic politics were chronically complicated by Roman interference. The nomads on their northeastern frontier were an additional long-term hemorrhage on the treasury. In the end, however, the Parthian Empire was brought down from within. Ardashir, the founder of the Sassanid Dynasty, raised his rebellion in the wake of a protracted war between the Parthian king and the Romans, and seems to have taken advantage of the ruling dynasty's exhaustion. A battle was fought, the Parthian king fell, and his dynasty died with him.

We know less about the Kushans than we do about their neighbors. Their decline in the mid-third century, however, can be blamed largely on the Sassanids, who conquered Kushan territories west of the Indus River. After these campaigns, the Kushans withdrew to the southeast, where their control would endure until the rise of the Gupta Dynasty.

The Han Dynasty, finally, declined and fell for reasons that had nothing to do with their western neighbors. The seeds of its collapse had been planted centuries before, when Emperor Guangwu, restoring dynastic control after the traumatic rebellion of Wang Mang, established a regime that encouraged local landlords to become increasingly powerful and allowed the imperial court to become progressively more remote. A series of disasters in the late second century (most famously, the Yellow Turban Rebellion), combined with a series of weak or underage emperors (often controlled by court factions) to create a power vacuum exploited by a generation of over-mighty generals.

There were interconnections between these four stories of decline, at least to the extent that the rise of the Sassanids destabilized both the Roman and Kushan regimes. And one could postulate meta-narratives of decline, based, for example, on the lingering effects of the Antonine Plague (which probably swept across Asia in the late second century), or on long-term climatic changes. But even if such a link could be proven, the declines of all four empires would have to be understood on their own terms.

Edit: Several insightful commentators have suggested that climatic changes may have played a substantial role in the general unrest of the early third century. This is certainly plausible, and some evidence exists for a period of cooler and wetter weather. I do not, however, believe that there is enough data to make any conclusion about the influence of climate, particularly since the decline of each empire can be adequately explained by social and political factors.

Finally, I have to say that I've been a bit overwhelmed by the response to this post; thank you all for your comments, for your awards, and for taking the time to read!",0
"Well, first off, you're about 60 years too late.

I'm going to begin with a classic summary quote from the single best book on narcotics regulation, the late, great David Musto's *The American Disease: Origins of Narcotic Control*.  This essentially encapsulates the difference between the two:

>""The only question publicly debated with reference to narcotics
was how to control, not (as in the case of liquor) whether to control.""

Let me start off with the alcohol side of things because before anything else I need to address a part of your question that implies something that's inaccurate.  Prohibition did not appear as a bolt out of the clear blue sky in 1919; in fact, by the time of the 18th Amendment's enactment 27 of the 48 states already had some form of Prohibition on the state level and countless more local restrictions were already in place - like one of my favorites, a 2 mile dry zone around what became UC Berkeley that had existed since the 1870s.  (For that matter, as I've written before, once national Prohibition was repealed [several states were not exactly prompt in their efforts to repeal it within their borders.](https://www.reddit.com/r/AskHistorians/comments/s9iuqi/the_us_lowered_the_federal_voting_age_to_18_in/htou9va/))  Some clever people made quite a bit of money importing liquor from wet states into dry states until the Webb-Kenyon Act took that loophole away in 1913; the Supreme Court held it to be constitutional at the beginning of 1917 in one of the earlier examples of a stronger Commerce Clause.

But what Prohibition did was to force the other 21 states to join them, generally unwillingly; Wayne Wheeler and the Anti-Saloon League were probably the single most feared political force in the 1910s, and the Women's Temperance Christian Union was not far behind.  I won't go into detail about the political reasons why national Prohibition ended up passing when it did; I might some day if if I'm ever caught up on questions and have a couple of days to kill, since I can't think of how to do it in less than a 4 part post.  To sum it up, though, a lot of things had to line up perfectly and simultaneously to finally enact Prohibition, from anti-immigrant and anti-Catholic sentiment to the income tax to the right of women to vote to the underlying goals of many Progressives, all of which had been building for at least 40 years, and in some cases far longer than that.  These were often of independent origin but ultimately became linked in ways that shrewd political operators like Wheeler understood and used; if you're interested, a good place to start for an introduction to this is in Okrent's *Prohibition*.  Overall, though, probably less than half the country supported Prohibition even when it was enacted.

The origin of narcotics regulation was different, partially because as Musto points out that the target of it wasn't the otherwise upstanding citizens who had the moral failing of consuming alcohol.

>'""But almost no one ever used the term temperance in discussing the use of opiates or cocaine after 1900; by the teens of this century both classes of drugs were deemed in public debate to have no value except as medicine. The closest a public spokesman would come to defending such drugs would be to say that they were not especially harmful as compared say, with alcohol, and with a vigorous effort in progress to outlaw alcohol, the description did not protect narcotics from criticism. By 1914 prominent newspapers, physicians, pharmacists, and congressmen believed opiates and cocaine predisposed habitues toward insanity and crime.  They were widely seen as substances associated with foreigners or alien subgroups. Cocaine raised the specter of the wild Negro, smoking opium dens the devious Chinese, morphine the tramps in the slums; it was feared that use of all these drugs was spreading into the ""higher classes.""'

A brief history of narcotic use in medicine prior to 1900 includes the significant use of the opioid mix laudanum as a universal cure-all as early as the late 1700s.  A much more concentrated version, morphine, comes into play starting in the 1830s with improved chemical extraction processes; crude opium contains roughly 9% morphine, and hypodermic needles allow for injection starting in the 1850s.  The Civil War does not create the market for morphine dependency - for some reason it's not widely used during the war for pain relief - but there's some evidence that after the war Civil War veterans may have used it and introduced others to it and morphine imports grow tremendously in the 1870s along with the isolation of cocaine from the coca leaf.  Crude opium imports peak even later, in the 1890s at over 400% per capita higher than it'd been in the 1840s, and heroin starts becoming an issue in the late 1890s.

But were there people shooting up on the streets back then?  Some, but by and large the middle class didn't need to, since instead their exposure generally came in the form of patent medicines.  Need hay fever relief?  You could buy any number of mixes that had cocaine as an active ingredient that provided it since when inhaled the drug shrinks sinuses (and yes, Coca Cola contained it as a stimulant until 1903, and it and purer cocaine were sometimes given to laborers to get them to work longer; it was Sigmund Freud's preferred drug.)  Your baby crying?  Plentiful tonics to soothe them were on the market with their primary ingredients being one opiate or another.  

Unsurprisingly, as the medical community starts to get more professional and organized (state licensing requirements for pharmacists and physicians didn't really begin until the 1890s), it slowly dawned on some of them that perhaps creating something like 250,000 Americans addicted to opiates - let alone infants dying from what a handful realized was the ingesting of them - might not have been a particularly wise course of treatment, and varying state laws restricting them began to take effect.  Most patent medicine manufacturers fought tooth and nail to not disclose their ingredients, right up until Congress under a significant push from Roosevelt finally does something about ingredient lists with the wider food safety movement that results in the Pure Food and Drug Act of 1906.

Afterwards, the concentration of opiates in patent medicines begins to decline (some tests measured 50% morphine!) with consumers horrified to learn what they've been ingesting for years; overall sales fall something like 25-50% once the ingredient lists are mandated.  One particularly vehement bureaucratic advocate for disclosure and regulation starts going after things like caffeine - the Coca-Cola Company gets indicted for that rather than cocaine since they don't list it on their ingredients - and all goes well when he meets with Roosevelt to lobby for additions to the list until he gets to the evils of saccharin.  Teddy Roosevelt stops him and goes into one of his legendary outbursts, ""Anybody who says saccharin is injurious is an idiot. Dr. Hixby [the White House physician] gives it to me every day!""; it turned out Roosevelt had largely given up sugar, and the bureaucrat was largely sidelined after that.

But the overall trend towards some regulation of narcotics picks up steam in the following years, especially given who was consuming pure product versus the ""medicinal"" version.  This gets caught up with foreign policy with the United States attempting to get China on board opium restrictions, and the result of this ends up being 3 years of major power conferences at the Hague.  Germany - whose chemical industry is now making a significant amount of money extracting things like morphine - points out that any attempts to legislate narcotics in the United States has gone nowhere, and gets the United States delegate to offer that the goodwill of his country towards enacting restrictive legislation is there if everyone else goes along.

This doesn't quite happen as the last conference ends 3 days before Archduke Franz Ferdinand is shot, but given the international pressure more restrictive legislation finally does get passed in 1914, with the help of Southern Democrats (who are paranoid about cocaine and blacks) now in control of Congress.  What results is the Harrison Act, which instead of regulating the sale of narcotics under the Commerce Clause does so via taxes, with quantity limits for physicians, record keeping requirements, and massive fines for violating both. This is the first effective drug control act, but unlike Prohibition it was barely mentioned in the papers of the day.  Even then, support for narcotic control for someone else - especially someone else who didn't vote - was universally strong; it was only when it affected your own medical tonics and relationship with your physician that it became an issue for voters. 

It gets upheld by the Supreme Court in 1919 - it's one reason the 1937 Marijuana Tax Act uses much the same framework - and remains that way until the Court revisits it in 1969 and overturns it.  That's why its replacement, the Controlled Substances Act in 1970, uses a Commerce Clause basis rather than the power to levy taxes as its enforcement mechanism; after 5 decades of expansion of it, it was not considered particularly groundbreaking, where in 1914 it would have been.

That is the overall answer to your question as to why narcotics were fairly easily regulated where national Prohibition took an amendment; there was widespread popular support for the first, but the second is now pretty much universally considered one of the most disastrous political acts in American history, partially because even on its passage it reflected remarkable political strength possessed by a loud minority rather than the views of the majority of voters at the time. 
 
Incidentally, one interesting sidelight is that in one state, morphine sales rose 150% in the ten years between it going dry and the Harrison Act.",0
"> ""Hey Mod, Nothing is better than coming to a post on this sub with an interesting question and seeing every answer, Including the highly upvoted ones, deleted.

> I understand you want context but maybe give the top few comments a chance to expand instead of leaving me blueballed. It turns me off of this sub as I'm sure it does others.""

Well, the one you're missing out on here reads: 

> ""People just did this at burning man. They do it all the time from stadiums. I guess I'm not sure of what you're having difficulty understanding.""

Another one I've just cleaned up is:

> ""A lot didn't go home. Some started the klan, others went west. Probably had groups going in the same direction just wander off for home.""

When we remove stuff, we're not doing it for laughs; we remove bad answers. When the occasion arises that we think a comment _is_ okay, but falling just a little short, then we usually ask that commenter to expand on their post so we can approve it. When you see us warning people, we're not handing out warnings for stuff that is generally on-track, but is just not quite deep enough. We're warning googlers, Wikipedia quoters, and shitposters. ",0
"While there’s a place for an answer related to the history of the 40-hour-work week, unions, and fair labor practices, it misses the point of Parton's song to take the title literally. That is, instead of “9 to 5” being a literal marker of the limits of a work day, the song, the movie, and the organization of the same name used it to a way to evoke a particular kind of employment, and the related benefits and perks, that had historically been denied American women. The history behind your question isn't about the work day: it's about second-wave feminism and signaling.

Though the boundaries and even the moniker are debated among women’s history scholars, “first-wave feminism” is typically seen as the efforts of women in the late-1800s and first half of the 1900s to get the right to vote and create social safety nets for women and children. These early feminists (though, again, many didn’t see themselves as such) organized alongside male-led unions to establish safe work practices, including changing and eliminating laws established during the depression that allowed school districts to fire women who got married. “Second wave feminism”, then, was about building on the work of earlier advances towards equity. Whereas the first wave was about big moves, the feminists of the 1960s and 70s tried built on those big wins to build the future they wanted for themselves and their daughters. A popular framing device related to second wave feminism was the phrase “the personal is political” (Milkman & Walkowitz, 1985) and we see that in how second-wave feminists organized, campaigned, and communicated their messages. 

Ms. magazine provides one example of what that looks like in practice. The name speaks to the “personal” (a gender modifier before a woman’s name) and the contents were clearly political (the first issue listed the names of women who were not embarrassed they'd gotten abortions.) The name of the magazine served as a signal to its readers: there is more to being a woman than being a Miss (unmarried) or a Mrs (married.) *Identity politics*, a term coined by the Black women of the Combahee River Collective in the 1970s, was another way to signal a particular political message related to Black women’s activism. In a [recent interview](https://www.newyorker.com/news/our-columnists/until-black-women-are-free-none-of-us-will-be-free), Barbara Smith, one of the founders of the collective explained:
>  “By ‘identity politics,’ we meant simply this: we have a right as Black women in the nineteen-seventies to formulate our own political agendas.” She went on, “We don’t have to leave out the fact that we are women, we do not have to leave out the fact that we are Black. We don’t have to do white feminism, we don’t have to do patriarchal Black nationalism—we don’t have to do those things. We can obviously create a politics that is absolutely aligned with our own experiences as Black women—in other words, with our identities. That’s what we meant by ‘identity politics,’ that we have a right. And, trust me, very few people agreed that we did have that right in the nineteen-seventies. So we asserted it anyway.”

Which leads us back to the phrase “9 to 5.” Regardless of what wave of feminism we’re talking about, advocates for women’s rights have had to reconcile the tension that in virtually all instances, men held the levers of power. In other words, a great deal of the work of feminists was about persuading men, mostly white, to change their minds. (Edit to add: this helps us understand why the hours of 9 AM to 5 PM were the norm for a particular kind of job, a job mostly held by middle to upper middle class men. Those are the hours that the men in power, which includes those who lead unions, wanted to work. Likewise, if your thinking is that 9 to 5 were the hours of the ""typical"" job, you're defining ""typical"" by the jobs held by a small segment of the American population.) This tension was what inspired Karen Nussbaum and Ellen Cassedy, two Boston-area office workers to create “9to5” (no spaces) in 1973, an organization committed to supporting women to speak up for what’s was rightfully theirs.

So, what did “9 to 5” signal? First, it was meant to signal a workplace free from harassment and subject to fair work rules, not the whims of a boss. One of the first protests that the organization 9to5 led involved an incident where a secretary was fired because she brought her boss a sandwich with rye bread, not white like he wanted. The women stood outside some of the largest office buildings in Boston (and later NYC and Chicago) and polled elderly office workers to collect names and data related to age discrimination and sexual harassment in the office. Second, it signaled a workplace that worked for all, not just men. In the 1980s, 9to5 partnered with medical centers to support studies of pregnancy hazards in the workplace and how the presence of computers (the machines, not [the women](https://old.reddit.com/r/AskHistorians/comments/9dnvra/im_a_female_computer_someone_who_performs/e5kbmsw/)) in the workplace impacted the secretarial workforce. They pushed for flex schedules, a practice that is mentioned in the movie. 

Many of these details come from articles in Boston papers that covered the work of the 9to5 organization and in one editorial in 1985 refuting claims that the organization was shutting down, Nussbaum and Cassedy wrote: “Employers and office workers alike should be assured that 9to5 is still going strong. **The battle will not be over until fair pay and decent working conditions for office workers are the rule, not the exception.”**

Third, ""9 to 5"" signaled respect. In theory, it meant clocking in at a set time (meaning your morning was your own), and being able to leave when your day was over, not when your boss was done with you. However, in practice, ""9 to 5"" was a grind. Just working the set hours didn't guarantee a safe workplace. *[American Songwriter](https://americansongwriter.com/9-to-5-by-dolly-parton-behind-the-song/)* did a nice deep dive on the history of the song and spoke to one of the founders of 9to5 who describes that gap between the promise of a 9 to 5 job and the reality inside a capitalistic society: 
> ""I think the song is brilliant. It starts with pride: ‘Pour myself a cup of ambition.’ It goes to grievances: ‘Barely getting by.’ It then goes to class conflict: ‘You’re just a step on the bossman’s ladder.’ And then it ends with collective power: ‘In the same boat with a lot of your friends.’ So in the space of this wildly popular song with a great beat, Dolly Parton just puts it all together by herself.”

It's difficult to tell if there was some retconning, but beginning shortly after the song and movie were released, Parton herself said she was inspired by the organization 9to5 when writing. Which is to say, the song isn't about the specific hours people worked - it's a way to signal the listener to a particular type of job - and the struggles of those workplaces - as she explains in the chorus:

* Working 9 to 5, what a way to make a living
* Barely gettin' by, it's all taking and no giving
* They just use your mind and you never get the credit
* It's enough to drive you crazy if you let it
* 9 to 5, yeah, they got you where they want you
* There's a better life and you think about it don't you
* It's a rich man's game no matter what they call it
* And you spend your life putting money in his wallet
* 9 to 5, what a way to make a living
* Barely gettin' by, it's all taking and no giving
* They just use your mind and they never give you credit
* It's enough to drive you crazy if you let it
* 9 to 5, yeah, they got you where they want you
* There's a better life and you think about it don't you
* It's a rich man's game no matter what they call it
* And you spend your life putting money in his wallet",0
"In simplest terms, NB Forrest didn't have a change of heart and become a champion of racial equality and rights for the freedman. In broader terms, his speech to the Independent Order of Pole-Bearers Association needs to be understood within the context of post-war politics and the southern Democrats, with whom Forrest was quite active.

Now, to start off, it is worth noting that Forrest's role within the Klan is somewhat exaggerated, based on the available evidence. He certainly wasn't present at its founding, and likely didn't become involved until 1868, at which point he exercised no real leadership, but existed as a figurehead role. The closest thing to confirmation even of this comes from his interview with the *Cincinnati Commercial* where he clearly implied it, but then of course denied it after publication. Later writings after disbandment played up his association, but in no small part because of his associated prestige, but on the flipside, their prestige was something he too had wished to benefit from. Being *entirely* secret was no use to him, and the main benefit to him was the prestige of being seen as the leader of the 'invisible empire', hence his coy ""I am, but I'm not"".

In any case though, I bring this up because it is worth noting that while a horrible racist in *many* ways, even by whatever ""oh, but it was different then"" standards you might try to use, there isn't too much to tie him to the vigilante violence of the Klan which they used to try and terrorize the freedmen, carpetbaggers, and scalawags. He certainly could be violent - he was tried, and acquitted on self-defense grounds, of killing a black man who worked for him in 1866 - but when it came to the maintenance of white supremacy, he was more interested in the conventional political path than that of mass campaigns of violence. His *vague* association with the Klan was one way that he could do this, clearly telegraphing that he was for white supremacy, but doing just enough to try and distance himself from the violence of its reality. Later when called to testify at the Joint Congressional Committee, he would try to claim that he had helped to end the organization in the interest of 'keeping the peace'. None of this is intended to absolve him of involvement in the Klan, but it is intended to illustrate that his relationship with the Klan was a bit less clear than popular memory relates, and for him, the association was very much about posturing and trying to telegraph who he was.

This translates to the broader image of white supremacist politics in the South during the time as well. The Klan attempted to enforce through clandestine means and direct, violent action what the southern Democrats sought to do through public, but less-violent (we can't say *peaceful*) means. Blunting the political power of the newly freed African-American vote at the ballot box was of vital importance to both, and accomplished through various means. The naked terrorism of the Klan was one way; measures to try and keep freedmen from exercising their franchise was another. But so too was attempting to bring black voters *within* the Democratic fold. 

Democratic Clubs of African-Americans existed in the South, usually in small numbers, and its members were left alone by the Klan, and they were accorded a *certain* kind of respect by white Democrats. But they also need to be understood for what they were and what they represented. To many of the members, it reflected a pragmatic reckoning with the circumstances in which they lived. I've written previously about voter suppression [here](https://www.reddit.com/r/AskHistorians/comments/9qjfjm/in_the_united_states_we_have_to_register_to/e8ar8pj/), and brought up the case of Silas Green who reflects what we are talking about here. Small numbers of black men were essentially given the privilege of voting, but only in a controlled way, and of course expected to vote for the Democrats, and for white supremacy. It was a means of ensuring further political power and control for white rule, and this was especially true in the waning days of Reconstruction, before Redeemer governments had taken power, or solidified power at least, and the full force of Jim Crow was not yet in effect.

So, this now gets us to the speeches given by Forrest and Pillow, which need to be understood against this background. They aren't speeches given in a spirit of proper equality, but rather, if anything, they are speeches trying to sell white supremacy to African-Americans as good for them. It is trying to turn them into Democratic voters who will, at the ballot box, ratify white rule. Forrest spoke first, according to the times, offering an olive branch, of a sort.

>I am here a representative of the Southern people, one more slandered and maligned than any many in the nation, I will say to you and to the colored race that the men who bore arms and followed the flag of the Confederacy are, with very few exceptions, your friends.

But he is also clear enough about what his intentions are when he states:

>I came to meet you as friends, and welcome you to the white people. I want you to come nearer to us. [...] Use your best judgement in selecting men for office and vote as you think right.

What he meant, to be sure, was clear enough. It wasn't an invitation to equality, but to obedience and subservency. It wasn't a call to use *true* judgement, but rather to recognize what was best for them *or else*. If there was any doubts as to his intentions, his fellow speaker, the former rebel General Gideon Pillow, was sure to push them aside. After extolling the things that the white government has done to provide for the black population of Tennessee, and how he is there 'as proof that the white race feel an interest in the welfare of the colored race', Pillow for his speech, isn't all that coy about what his is really asking when he declares:

>The highest duty you owe your country, as citizens in the exercise of the elective franchise is to vote for none but honest and capable men for any office. My advice would be to discard all partisan vows, to disband all colored political organizations.

What he is calling for is an end to black politics, and subservience to white politicians who are better, and of course will look after them. And in case the point wasn't clear enough then, after lambasting how the Yankees have ruined everything and turned the black population of the South against the white, he puts it as plainly as possible:

>Left to themselves, the white race having direct interest in your welfare and prosperity, would seek by just laws to advance your interests and to quality you for good citizens. [...] If you cease your hostility to the white race of the South, and fall into the general policy and intents of the South and identify yourselves in interest with them, and vote for none but honest and capable men for office, we would correct the abuses which have crept into every department of business.

So to return to the beginning, Forrest didn't have a change of heart and become a champion of racial equality and rights for the freedman. He was a politically minded man, and as with many other southern Democrats, they recognized that there was utility in bringing some black voters within the fold of their political coalition. But it was not to elevate them; rather it was to attempt and keep them in-line, and subservient to the new white supremacist order that was replacing the old one. Their olive branches were carrots, backed by clear, unambiguous sticks, and the direction of politics by the mid-1870s. Tennessee had already been at the forefront of returning to white control with the Redeemer government of 1869. Black political power was already quickly diminishing the audience would have understood underlying implications that white power was in control of the state either way, and would solidify that control no matter what. Obliging them and showing that they knew their place would mean the 'gracious' white government would give them more crumbs than if they fought tooth and nail against it.

So in sum that is what is going on here. Forrest isn't trying to save his skin. He is stumping for white supremacy, and trying to convince African-Americans that supporting it is in their self-interest. Not in a way that all white people would necessarily agree with - more than a few would, by that point, no doubt see no need to bother, and Forrest received some disagreement with his choice to appear at all (giving a kiss on the cheek to the black girl who gave him the flowers was not viewed kindly by some)  - but it is a matter of degrees, rather than the broad, general aim, and he was, in essence, a white supremacist to the end.

**Sources**

An offering of peace: GENERAL FORREST, OF CONFEDERATE FAME, ADDRESSES THE COLORED PEOPLE. 1875. The Sun (1837-1994), Jul 14, 1875.

Frantz Parsons, Elaine. *Ku-Klux: The Birth of the Klan During Reconstruction.* UNC Press Books, 2015.

GENS. FORREST AND PILLOW.: REMARKABLE SCENES AT THE CELEBRATION IN MEMPHIS THE COLORED PEOPLE HEARING AND APPLAUDING THE EXCONFEDERATE GENERALS. 1875. New York Times (1857-1922), Jul 09, 1875.

Hardin, David. *After the War: The Lives and Images of Major Civil War Figures After the Shooting Stopped.* Ivan R. Dee, 2010.

Mitcham, Samuel W.. *Bust Hell Wide Open: The Life of Nathan Bedford Forrest.* Regnery Publishing, 2016.",0
"Hey, I'm still alive!  Just buried in enough work that when I came up for air it'd been long enough so I was outright logged out from Reddit (along with just a few inbox messages waiting for me, heh.)

Here's the short answer: originally, it was supposed to be 7.2 million in gold bullion out of Washington, but after a nearly 18 month delay it ended up going as a Treasury warrant to a DC bank to Barings Bank in London as a fiduciary.  The saga of why this happened, though, is a much longer and more interesting story.

The United States had a less tempestuous relationship with Imperial Russia in the early to mid 19th century than with France or Great Britain, starting with John Quincy Adams' appointment and continuing to when relations became warm during the Civil War.  Both Britain and France showed significant Southern sympathies until late in the war, but Russia had thrown in with the North and a unified United States early on.  There were all sorts of reasons for this that weighted heavily towards European politics, most notably that the enemy of Great Britain was often Russia's friend.

In one of the more overt displays of mutual interest, Russia sent a good slug of her fleet to both Atlantic and Pacific American ports in early 1863 with the cover story of a goodwill tour - [and Russian sailors very definitely enjoyed themselves](https://www.history.navy.mil/research/library/online-reading-room/title-list-alphabetically/r/the-russian-navy-visits-theunited-states.html) - and the covert one being planned merchant raiding of British and perhaps French shipping if Russia went to war with either or both powers over Poland, with the possibility that if either power intervened in the American Civil War (Great Britain with the *Trent* in 1861 for instance) that the fleet might help out a bit there too.

So this was the relatively friendly diplomatic basis for where the United States and Russia were shortly after the war.  /u/The_Alaskan has [previously posted a terrific answer about why the sale took place from the Russian perspective](https://www.reddit.com/r/AskHistorians/comments/5m6fmy/the_alaska_purchase_in_1867_is_often_called/); another major factor was that Russia had watched from afar as American settlers had more or less ignored territorial claims first in the Southwest prior to the Mexican-American War and slightly later on the West Coast when Great Britain and America spat at each other for a number of years over the Oregon territory and its boundaries.  This was starting to happen to Alaska and was considered likely over the next decade or two; Russia in short had all sorts of reasons to get out and in December 1866 pretty much most the Russian powers that be had signed off on what had been gelling since the 1850s.

This timing worked out rather conveniently with the ascension of William Seward, who had been pushing for years for a variety of territory outside the continental United States.  One bill that he sponsored while a Senator - designating uninhabited islands mined for guano as American territory - slipped through completely unnoticed but served as the basis for successful American claims on a number of Pacific islands that proved highly consequential in World War II.

Seward had long targeted Alaska as an ideal territorial gain for a variety of reasons, and come February 1867, probably thanks to Thurlow Weed - Seward's long time primary political patron - providing company to Russia's ambassador Eduard de Stoeckl rehabbing an injury in New York for a couple weeks after a voyage back from St. Petersburg, the long simmering deal was executed in a hurry.  Over a two week period starting on March 11th, the two went from theoretical discussions to coming up with a price (5 million immediately raised to 5.5, with Stoeckl thinking he might get 6 or 6.5 million, which then got raised to 7 million more or less spontaneously by Seward to move negotiations along as they wrote the treaty) to cabling Russia for approval on March 25th, to Stoeckel visiting Seward at home on the evening of Sunday, March 29th to let Seward know that the Tsar had indeed done so - to Seward shocking Stoeckel by opening up the State department that night to get the treaty concluded at 4 am.

Seward had informed the Cabinet and gotten Andrew Johnson's tacit approval for $7 million - but only on the late evening of the 29th did he so much as bother sending his son (who was Assistant Secretary of State) to inform Charles Sumner, the chairman of the Senate Foreign Relations committee, *that not only had negotiations begun but a deal had been reached!*  While Sumner ultimately supported the purchase, in the rapidly deteriorating relationship between Congress and the President in early 1867, this slight did not go unnoticed by him and others and played a role in the nasty fight over the payment in 1868.  One of the minor sticking points in the wee hours of Monday morning was that Stoeckel had asked for payment to be made out of London - we don't know why, but bullion did need to be shipped - and in return for waiving most of them (including the relatively profitable franchise selling Russian Alaskan ice to thirsty San Francisco), Seward added another $200,000 to the offer for a grand total of $7.2 million.

I'm skipping most of the details over the ratification process - /u/The_Alaskan covers the most important point, which is that the purchase was widely supported and ""Seward's Folly"" came into the lexicon later when he tried to buy the Dutch Antilles - but worth noting was that 19th Century Congresses generally did not meet until a full year after their election, so it took a special session of the Senate in April 1867 (very unusually, they'd already convened a couple weeks earlier to squabble with Johnson over Reconstruction) to ratify the treaty, which wasn't particularly well received given who negotiated it and how he did so - Seward already was becoming anathema to most Republicans - but was generally agreed as a good deal by and ultimately passed 37-2 on April 9th.  Stoeckel got paid only 25,000 silver rubles by the Tsar for his work, which initially annoyed him given he'd gotten such a better deal than anyone expected, and then it got even worse when he discovered he was on the hook for a nearly $10,000 telegram that he thought Seward had paid for!

But the House - which under the Constitution must initiate all appropriations - wasn't back in session until December to do the financing of the deal, and in the meantime relations between the Johnson administration and Congress had moved from hostile to the most toxic in American history.  Meanwhile, American settlers (and soldiers) moved in over the course of that year...and on top of it, a nearly $400,000 claim against the payment was made by the widow of a merchant mariner, Benjamin Perkins, who'd theoretically acted as a Russian agent in 1855 by advancing the purchase of gunpowder and other supplies to them.  The Russian government had stiffed Perkins on any payment for years, but his widow had gotten powerful backers in the House and Senate, probably thanks to promises that they'd get a percentage of what she got back, much like attorneys litigating on contingency do today.

Meanwhile, the political context had reached its nadir as the House impeached Johnson in late February 1868 (after multiple attempts by Thaddeus Stevens prior to this) and pretty much everything else on the Hill ground to a halt for months.  This presented a dilemma for Stoeckel, who actually wrote back to Russia after an inquiry that they could either wait (his preference) or just outright offer Alaska for free!  In May, the House Foreign Relations committee - with a number of supporters of the Perkins claims who noisily dissented - recommended approval of a payment, but that debate got stalled until July and the final 113-43 vote didn't take place until July 28th, with the treasury warrant that you've posted being presented to Stoeckel on August 1st. 

Stoeckel in turn endorsed the warrant over to George Riggs (of the Riggs Bank, which lasted for almost two centuries as D.C.'s major financial institution), who then disbursed it to the London office of Barings Brothers, which handled Russia's foreign accounts.  That was how the payment was processed.

However, it wasn't over; there was all sorts of perceived scandal and investigation afterwards about the lobbying that went on behind the scenes, with opponents at the time claiming that Russia only received about $5 million after all the various bribes had been paid out.  It wasn't quite that bad, but George Riggs admitted during the investigation that he left $165,000 in the United States on Stoeckel's orders, of which about $135,000 can't really be accounted for but at least some of which is believed to have gone quietly to influential members of Congress.

Precisely who benefited from this graft is a mystery lost to time, but here's a fun bit of speculation on my part: David O. Stewart in *Impeached* presents a very good case that substantial bribes were paid to various Senators to encourage them to acquit Andrew Johnson.  As William Seward was at the heart of the dirty money fundraising efforts for the trial, one rather nutty possibility is that Stoeckel reimbursed himself after being shaken down for that too!  A couple decades back, someone even found a note from Tsar Alexander confirming that he'd OK'd a diversion of part of the payment to Stoeckel for undisclosed expenses, so the evidence is conclusive and it's probably an underlying reason why the payment method switched from shipped bullion to domestic Treasury warrant.

On the bright side for Stoeckel's ethics and pocketbook, the one disposition that we do know about is a $10,000 portion of it: a particularly litigious telegram company finally got their bill settled!

Sources: *Seward: Lincoln's Indispensable Man* (Stahr, 2012), *Seward's Folly: A New Look at the Alaska Purchase* (Farrow, 2016)",0
"No, the names of legendary figures in ancient Greece, such as Hercules, Icarus, Midas, Narcissus, and Odysseus, were not commonly used among the general population. These names were associated with mythological and heroic figures whose stories were passed down through oral traditions and written texts. In ancient Greece, personal names were typically derived from a variety of sources, including gods and goddesses, local heroes, ancestors, and nature.",1
"The reception of ex-Nazi scientists by the Jewish scientific community in the United States was complex and varied. It is important to note that not all Jewish scientists had the same perspective or experiences, so their reactions were not uniform. However, I can provide a general overview of the different attitudes and responses. Many Jewish scientists who had recently fled Nazi Germany were deeply skeptical and critical of the recruitment of ex-Nazi scientists by the United States.",1
"Very interesting!

> Speaking of medieval western Europe: for leisure, no, not really. The rise of beach travel is generally associated with the nineteenth and early twentieth centuries.

Prior to this, however, was there perhaps a decline followed by a resurgence that would be what you mention?

Marcus Aurelius, after all, wrote ""men seek out retreats for themselves in the country, *by the seaside*, on the mountains"" which sounds very much like our modern vacation destinations.  I don't know if this meant swimming but would love to learn.

-----

In searching for a reference, I found this [earlier thread](https://www.reddit.com/r/AskHistorians/comments/190l1i/did_ancient_peoples_go_to_the_beach_as_we_do_today/) which touches on Romans and others but didn't have that quote. It does mention sand castles!

-----

> The biblical story of Jonah being eaten by a very big fish was equated, in Christian preaching, to people going to hell.

Nowadays people just give a [big ol' thumbs up](https://www.msn.com/en-us/news/world/man-swallowed-by-a-whale-and-survives-it-might-not-be-as-fishy-as-it-sounds/ar-AALP3Ub)  :-P though maybe Jonah did when he got out too?",0
"Yes. The long and the short of it is what befell the population of Bukhara... soldier eradicated, artisans and people “of use” led back to Mongolia in chains, and everyone else “herded” toward the next city to be a mass arrow-sponge.

That’s if the Mongols weren’t pissed off. Later in the war, ca. 1220, when the defenders of Gurganj ambushed and killed 3k Mongols constructing a dam to divert a river to flood the city... in response, the whole of the captured population was divided up among the 50,000 Mongol warriors in groups of 24, and then systematically executed to the last. If those numbers can be believed (and that remains a big “if”) that works out to 1.2 million over the course of a day.",0
"Super complicated! There wasn't a single ""Catholic perspective"" because American Catholicism was so regionally varied. Antebellum American Catholicism included: 

-Jesuit missions in what's now California and New Mexico 

-French Catholicism and the Acadians in what's now Louisiana and the Great Lakes areas

-Black Catholicism across Louisiana

-German and Irish immigrants to the Northeast (although the big waves later in the 19C) 

-slaveholding Catholics of Anglo descent in Maryland & Louisiana & the coastal Carolinas

-syncretic Catholicism in the Caribbean and Florida

Not only are there multiple different doctrinal interpretations within Church teaching (Jesuit art tradition, Counter-Reformation emphasis on Four Last Things, etc) -- there's also many different cultural traditions that inform Catholic depictions of Heaven. And, the Catholic Church had been grappling with the spiritual implications of colonialism (including racialized slavery) since the 15th century. 

Focusing on the antebellum American South, white US Catholic parishes tended to follow Anglican parishes in describing Heaven. The focus on nobility, high ideals, and veneration of hierarchy was very common in Catholic sermons about Heaven and Paradise. (It also overlapped with a fetishization of European aristocracy, particularly English aristocracy, which I left out of the primary answer.) However, I have not yet found sermons about Heavenly segregation in Southern Catholic archives. Nor have I found more assertions that whites and blacks are spiritually equal, than I've found in comparable Anglican and Presbyterian sermons and publications. 

Catholic history during the Civil War is also very complicated. Famously, Catholicism was the only American Christian denomination that did not schism over the issue of slavery in the 1840s, but sharp regional differences emerged in the US Catholic church. Catholics fought for both the Union and Confederacy, serving as soldiers, medics, chaplains, and journalists. But unlike Protestant debates about slavery-- which tended to focus on the authority of the Bible-- Catholic debates focused on the Church's historical teaching. Historic teaching is ""yes, slavery,"" but European colonialism prompted the Vatican to re-examine the conditions under which slavery could be considered good. Pope Gregory XVI's 1838 bull ""In supremo apostolatus"" muddied the waters considerably. English and Italian commentaries on the bull interpreted it as a condemnation of slavery as a system, not just the slave trade; US Catholics mostly interpreted it as a condemnation of the slave trade and not slavery. (There were some notable exceptions, like James McMasters).",0
"Anyway, there are a few things that we haven't covered yet, so I'll get to them right fast. Mayonnaise is easy as hell to whip together (and I highly recommend you make your own variants at home, it tastes *excellent*, assuming you have access to eggs and vinegar. Eggs were an extremely popular staple in the Roman world (In the above *De Agricultura*, Cato actually recommends them as medicine for cows at one point), so finding some eggs to not only bind your burger patty, but also make the mayo? No problem, that's just a quick trip to the marketplace. You got any preference for the type of eggs? Cause, assuming you're a fancypants Roman, you probably had many different preferences for egg types - quail, dove, ostrich....and yeah, chicken eggs were also available. But hey, you have options! The other main ingredient in mayo is oil which, as I mentioned before, the Romans (and Greeks) adored. Mustard's another major topping, and that one might be a little more fancy, but no less difficult to come by - it was so well known, in fact, [that some random illiterate peasant in a backwater Roman province is recorded as having used it as an illustration in an extended metaphor about morality.](https://en.wikipedia.org/wiki/Parable_of_the_Mustard_Seed) Mustard isn't too hard to make from the seed, and, since Rome had the aforementioned trade routes allowing the Spice to Flow, getting your hands on enough mustard seed would be reasonably unproblematic. 

As a final sauce, the Romans, while not having ketchup, would probably default to *garum*, the fish sauce which they put on basically everything and had a ravenous appetite for. Now, while we're reasonably unsure as to what actually went *into* this sauce, we've got a couple of analogues. In the western world, one of the closest things you could probably use to approximate it would be the impossible-to-pronounce Worcestershire sauce - a substance which is often used with ubiquity, and which my own stepdad likes putting on his burgers. *Garum* would probably have been thicker, but again, we're not 100% on the details, other than the ""fermented fish sauce"" bit.

Let's finish off with that most basic ingredient that I definitely did not forget to cover! The buns! The Romans certainly knew how to make breads, both regular and sweet, and actually have a way to know the *exact* makeup of at least a cheap form of this bread. Archaeology. 

Yep, you heard me, we have a legitimate loaf of bread that's survived since antiquity. Since 79 CE, to be precise. Probably sometime in October-November. How in the world can I date it so perfectly, you might ask? Why, dear reader, this loaf of bread was found at Pompeii. It's no longer edible, sure, but it's not so hard to sample the carbonized bread (not with your mouth, with a lab) and to figure out the exact makeup. [The British Museum even has a recipe and how-to video posted up](https://www.britishmuseum.org/whats_on/exhibitions/pompeii_and_herculaneum/pompeii_live/live_event/bread_recipe.aspx). A sourdough bun doesn't sound half bad, and since the Romans definitely knew how to make smaller sizes of bread than large loafs, a bun wouldn't be extraordinary in the least.

The rest of your burger is just preparation, and considering that the Romans had dishes that haven't actually changed much in the past 2000 years ([here's a cast iron dutch oven](https://imgur.com/Qme5JVj)), the preparation wouldn't be a problem. I'm reasonably confident that, assuming you had the resources (i.e. wealth and a few slaves who knew where to shop), you could decide on a cheeseburger in the morning and have one for dinner. Hope that helps, and please let know if you have questions!

EDIT: forgot the pickles and the sesame seeds. The sesame seeds would be relatively easily imported from Egypt, while vinegar was a super common thing - it's just turned wine, and Roman soldiers basically drank wine that was all but vinegar. Add that to [cucumbers](https://en.wikisource.org/wiki/Natural_History_\(Rackham,_Jones,_%26_Eichholz\)/Book_20), and boom, pickles! 

1: Strabo, Geography, 2.15.12.

2:  Strabo, Geography, 17.45.; MacLaughlin, R. *Rome and the Distant East: Trade Routes to the Ancient Lands of Arabia, India and China.* 28. 2011.
 
3: Lytle, E. “Early Greek and Latin Sources on the Indian Ocean and Eastern Africa,” in: G. Campbell, ed., *Early Exchange between Africa and the Wider Indian Ocean World.* Palgrave Macmillan 2016. 116.
",0
"The shift from sexual liberation to social conservatism in the Soviet Union can be attributed to several factors. Firstly, it is important to note that the initial period of sexual liberation in the 1920s was part of a broader social and cultural revolution known as the New Economic Policy (NEP). This period saw a relaxation of state control and experimentation with various social policies.",1
"In 1835 Lord Chancellor Lyndhurst introduced the bill to correct an earlier ambiguity in the law. The existing law about marriages within ""prohibited degrees"" (so family and family in law, as well as the sister of your deceased wife) was based on a law from 1533, under Henry VIII. This law stated that a marriage within prohibited degrees could be annulled at any time by the Ecclesiastical church. Any children from the marriage would become illegitimate. Lyndhurst argued that this created insecurity and inconvenience for the married couple and their children. His special motive was to protect the seventh Duke of Beaufort, who was in such a marriage, and his son, who would inherit his estate. 

To solve this issue, parliament passed an edited form of Lyndhursts bill. All marriages with a deceased wife's sister (DWSM) up to 1835 were considered valid. All such marriages from that year on were prohibited. 

Starting in 1835, this bill was debated almost annually, before being finally revoked in 1907. This was reverenced in the Gilbert and Sullivan opera Iolanthe as 'pricking the annual blister'. Some years it passed the House of Commons but was stopped in the House of Lords. During these years, the issue was hotly debated in the public sphere by use of pamphlets, literature and papers. In fact, the bill was part of a much larger debate about society versus the individual, how much the law could intervene in an individual's life, and the purity of marriage. The bill was used to argue about the ability of government to legislate morality, control individual behavior, and regulate the family. In a way, the yearly debating of this bill helped Victorian society to shape and determine itself.

Obviously, being discussed over so many years, there is an incredible amount of arguments in favor and against, not all of which I'm going to name in this comment. Here are a few examples: 

**Arguments in favor of the bill (and against DWSM)**

The Victorian era looked upon incest differently then we do. It was thought that man and wife, upon marrying, became actual family. There was no difference between blood relations and in-laws in term of nearness. Since the wife is now family, so is the sister. To then marry the sister was considered incest. 

There was a big taboo on incest, and a focus on seeing the bond between brother and sister as pure and not ""tainted by passion or irregular desire"". The argument was that the bond between brother-in-law and sister-in-law should be similarly pure. (The article by Nancy F. Anderson goes quite extensively into the fact that incest between brothers and sisters was probably fairly common and the lines between sibling-love and romantic love were sometimes blurry. The article is quite old and arguably some theories are dated. I still want to mention it but invite you to explore this aspect on your own.)

There was a biblical argument, though through the years the passages were interpreted differently. In the early years arguments in favor of the bill cited Genesis 2: ""man and wife become one flesh"" and Leviticus 18:16, which prohibits to ""uncover the nakedness of thy brother's wife.""  

By 1870, science replaced religion in the debate. It was claimed for example that sexual intercourse changes the physiological makeup of the marriage partners and makes them blood relations, which again makes the sister in law a blood relation. 

Lastly, there was the fear of gliding scale effect (still very popular in current politics): if this law would be revoked, other marriages within prohibited degrees might follow. There was a fear of opening the floodgates to ""unnatural"" marriages. 

&#x200B;

**There were also arguments against it:**

Rich people could take their wife's sister abroad (usually Scotland) and get married there. We have examples of this happening. Poor people could not, therefore the bill was felt to discriminate. In the later 19th century, there was a strong focus on caring for the poor, by making bills, charity, banning child labor etc. So this argument fits into that sentiment. 

Oftentimes unmarried sisters would already be living with a family or with a widower. Especially poorer families would not be able to afford other childcare. Poorer people lived in small quarters and often shared bedrooms. DWSM would help these people to live decently instead of in sin.

Opponents also claimed that marriage did not cause a blood-relation but only a psychological connection, which would make the marriage allowable. 

The validated marriages prior to 1835 were also brought out as an argument. 

&#x200B;

The bill had been revoked in many of England's colonies around 1880, and its final downfall started in 1906 with the Colonial Marriage Act, which granted full inheritance rights to children from deceased wife's sister marriages from the colonies, in England. This bill paved the way for the final revoking of the Wife's Sister Bill. In 1908, a new Punishment of Incest Act took its place, but this bill only prohibited sexual relations with mothers, daughters, sisters, granddaughters. Sisters in law were not mentioned in this law. 

&#x200B;

**An interesting note:** 

Vanessa Stephen, sister of Virginia Woolf, fell in love with her half-sister Stella's widower Jack Hills. There's a line in Mrs. Dalloway about how ""no decent man should let his wife visit a deceased wife's sister"".

&#x200B;

**Sources consulted & further reading:**

The  ""Marriage with a Deceased Wife's Sister Bill"" Controversy: Incest, Anxiety and the Defense of Family Purity in Victorian England, Nancy F. Anderson, Journal of British Studies, Vol. 21, No. 2 (Spring, 1982), pp. 67-86 

Thomas Hardy and the deceased wife's sister marriage bill, Shanta Dutta, The Thomas Hardy Journal, Vol. 11, No. 2 (MAY 1995), pp. 61-64

Triangular Desire and the Sororal Bond: The ""Deceased Wife's Sister Bill"" Diane M. Chambers, Mosaic: An Interdisciplinary Critical Journal, Vol. 29, No. 1 (March 1996), pp. 19-36

Husband, Wife, and Sister: Making and Remaking the Early Victorian Family, Mary Jean Corbett, Victorian Literature and Culture, Vol. 35, No. 1 (2007), pp. 1-19

The Marriage to a Deceased Wife's Sister Narrative: A Comparison of Novels, Charlotte Frew, Law and Literature, Vol. 24, No. 2 (Summer 2012), pp. 265-291

The Annual Blister: A Sidelight on Victorian Social and Parliamentary History, Cynthia Fansler Behrman,  Victorian Studies, Vol. 11, No. 4 (Jun., 1968), pp. 483-502 

Public opinion on marriage with a deceased wife's sister: 1875 to 1888, From the collection: Selections - university of Manchester British Political Pamphlets Collection",0
"Great question. I had actually originally included this in my post, then deleted it for being tangential -- so thank you for asking because it means I can now give my answer!

You are right that 50% ABV now corresponds to 100-proof alcohol. *In the United States*, and most other countries, that has been true for most of history. But *in the United Kingdom*, that is actually a recent change.

Originally, the UK did use the system of proof explained above -- but it was one among many ways of determining proof. There was the above method, used the same way for alcohols other than rum (such as whiskey); the addition of oil of a specific density (in a strong alcohol, it would sink); and most popularly, shaking the spirit in a glass vial and examining the speed and count of bubble formation. The latter, known as the ""bead"" or ""crown"" test, remained common into the 19th century. You can see how the latter two tests would be inconvenient for sailors, but accessible to distillers or traders.

The system of measurement was not standardized, much to the complaint of merchants. Their goods would suffer more excise taxes if over-proof, and they complained of inconsistent results and thus over-taxation.

The solution was measuring spirits' density, as alcohol is lighter than water. This was not an easy feat at the time, however, and many competing devices sprung up to fulfill this need. By the 1740s, distillers were using Clark's hydrometer for this purpose; by 1818, the Sikes hydrometer (an improvement on the prior by Bartholomew Sikes) was established as the standard for measurement.

That standard would be used for most of the next two hundred years. Proof was roughly equal to 1.75 times the ABV. It was not until 1980 that Britain adopted the (admittedly simpler) American scale, due to the European Economic Community's decision to harmonize various European regulations -- including the measurement of spirits.

My sources are below. **I strongly recommend reading the first one if interested in this topic**, it is a comprehensive, thorough, and quite readable analysis of the various ways Britain measured alcoholic proof (and the political context surrounding them). 

----

1. Ashworth WJ. ""Between the Trader and the Public"": British Alcohol Standards and the Proof of Good Governance. Technology and culture. 2001;42(1):27-50.

2. Grindal R. Measure for Measure. Alcohol and Alcoholism. 1979 Dec 1;14(3):129-31.",0
"Ok- there's a lot to unpack in this question, so I'm going to start with ""not about witches"" because it both is and it isn't about witchcraft.  Witchcraft was a reality in the 17th century world.  A witch was someone who made a deal with the Devil, and they could then send their supernatural form- their specter- out into the world to cause harm.  New England in 1692 was very much a world of wonders- the devil was fighting to dominate the Puritan world and lure people into temptation, thus gaining their souls for Hell.  This battle between good and evil happened daily in large and small ways.  Individually, it was your responsibility to adhere to Puritan tenets, and collectively, everyone needed to ensure each individual did.  One person's spiritual failings could bring tragedy to a town by invoking a storm sent by God as punishment.  It was a scary world, and even scarier when your neighbors might betray you and your community for the devil.  In this way, it was about witchcraft.

&#x200B;

But finding a witch is where it gets to not be about witchcraft.  I'll focus on the Putnam family to explain this.  Thomas Putnam Jr.'s daughter Ann Putnam Jr. named more people than anyone else with 83 confirmed accusations.  I've counted 893 unique accusations against 177 people so she makes up a fair amount as a twelve year old.  There is a long history of the Putnam family's decline in local politics and a number of family and community feuds that pit the Putnams against a lot of other people.  Many of Ann Jr.'s accusations line up with people on the other sides of those feuds- allies of local opponents to Thomas' political agenda, people on the wrong side of a lawsuit with the Putnams, easy targets that have some benefit for Thomas if accused.  Thomas Putnam Jr. had the most to gain from a lot of the accusations and his daughter often pointed the finger, if not pointing the finger first.

&#x200B;

However, this isn't to say the accusations are a premeditated conspiracy.  Yes, Thomas and his allies all have daughters and family making accusations along political factions.  But this doesn't mean Thomas, Rev. Samuel Parris, and others had meetings to say 'lets accused her, her, and her, so then we can accuse him.'  Its not a pre-planned witch hunt because you can't plan this.  Witch hunts most often found 1 or 2 suspects at a time, but in 1692, its a conspiracy of witches in the minds of these overzealous and self-righteous murderers.  They build a narrative as they go by taking confessions naming others and running with it.  Other towns had political issues and family feuds where someone nearby realized 'hey, we can do that too'.  But this doesn't mean they didn't think Sarah Good or Rebecca Nurse weren't witches.  They might have, although we can't be certain and I'd guess not everyone on the accusing side fully believed it.  This is what the thought process became- witches are real, there are witches here, who are they?  We don't like this person, they must be a witch because why else would they oppose us in local church politics.  The people you hate are the people you believe are witches.

&#x200B;

This is the scenario I imagine for how the accusations take off- the afflicted person started to suffer, people in the room begin to ask ""who is afflicting you?  Is it Martha Corey?""  This offers the accuser a name that they can just agree to, or in the case of younger accusers like Ann Jr., they probably know someone the parents hate.  Houses were small, often one or maybe 2 rooms.  If mom and dad sat up late by the fire complaining about Sarah Osborne, the children will hear the name and at the right time, Ann Jr. knows what her parents want to hear.  In many of the cases, its reasonable to believe the names were indirectly fed to the accusers to start an accusation.  After an arrest, more of the afflicted would jump onto an accusation since a accused witch was implied to be inherently guilty- specters of witches could only appear if you were a witch- so naming them gave credibility to other accusations.  It was a self-reinforcing cycle of accusations.

&#x200B;

Now onto the question of when people realized it wasn't about witches, and simply put, there were always people who knew.  On the day of the first examinations (a pre-trial hearing after an arrest to decide if a trial is necessary) Martha Corey tried to prevent her husband Giles from attending because she disagreed with the idea (she later hanged for witchcraft and Giles was infamously pressed for standing mute at his trial).  The Porter family, the leaders of the faction opposed to the Putnam faction, mounted a defense for their friend and ally Rebecca Nurse immediately writing a letter to the magistrates.  Nurse's sister Sarah Cloyce walked out of church when the minster referred to Rebecca's infiltration of the church as a witch. Another sister, Mary Easty helped the Porters circulate petitions on Nurse's behalf.  Cloyce was accused and arrested.  Easty hanged alongside Martha Corey, a few months after Rebecca hanged.  From the outset, people knew but accusations followed outspoken advocates.

&#x200B;

It was only as the increasing number of trials and executions continued that public sentiment grew more and more against the trials.  By September, the Court of Oyer and Terminer started to rush through cases because of the number accusations and because it seemed like their popularity was draining fast.  11 executions over the summer followed by 8 more and a pressing that month helped turn the tide.  There were also figures in Boston beginning to pushback- ministers like Revs Willard and Moody in Boston's Third Church even helped Philip and Mary English escape.  Critics like Thomas Brattle started to write down their thoughts, some things were published during the fall but many of the critiques were published much later.  Brattle, Calef, Maule, and Willard all have publications from the era that were critical of the trials.

&#x200B;

These critiques also came as more members of the Boston elite faced accusations.  Rumor even spread through the city that Lady Mary Phips, wife of Gov. William Phips, was named.  We know the rumor spread but not if anyone actually accused her.  But with that rumor, with a feeling that King William III would disapprove, and with public sentiment changing as influences like Rev. Increase Mather started to call for more caution, Gov. Phips paused the trials to reevaluate spectral evidence.  When the trials resumed without it in January 1693, convictions were virtually impossible.  There were no witches.

&#x200B;

Some people did apologize years later.  In 1697, Judge Samuel Sewall apologized for his role on the court.  His repentance was lasting and generally true.  He even wrote against enslavement later on after realizing his mistakes.  (However, his brand of anti-slavery was based on keeping races separate so we shouldn't be too impressed with him.) Ann Putnam Jr. confessed in 1706 that the accusations were a deception by the Devil.  Her apology is much more of a non-apology since it seems to be more about protecting her local standing by fighting the stigma of her involvement.  Still, she is the only accuser to seek forgiveness, but she placed the blame on Satan and not herself, other accusers, or the family members that pushed her to accuse.

&#x200B;

In 1711, a Reversal of Attainder exonerated many of the convicted suspects- most of those executed and those convicted but in jail at the time of the pause and reprieved.  It also gave some reparations to families of the victims related to money spent for jail fees and other related expenses.  In 2001, Massachusetts exonerated the remaining victims who were not previously granted a reversal.

&#x200B;

Now, one last note to respond about the land- sorta.  There were financial motives for some people, but not many.  Accusers and their families had no direct access to land or property, but the sheriff certainly did.  George Corwin arrested a lot of the accused suspects, but was also responsible for collecting jail fees (you basically rented you spot in jail and the chains used to hold you).  When suspects were convicted, Corwin didn't always stop looting property- the Elizabeth Procter escaped death due to a pregnancy while her husband hanged.  Corwin took almost everything from the house.  Essentially, movable goods were up for grabs to him but the land itself was rarely at stake except if there would be no heirs.  But accusations in no way guaranteed anyone could get land if someone hanged for witchcraft.

&#x200B;

I can give a lot of recommended readings and citations here, but I'll give a selected list for what I think most AH readers would enjoy and be most relevant:

Baker, Emerson W. *A Storm of Witchcraft.*

Boyer, Paul and Stephen Nissenbaum. *Salem Possessed*.

Norton, Mary Beth. *In the Devil's Snare*.

Reis, Elizabeth. *Damned Women.*

Roach, Marilynne K. *Six Women of Salem*.",0
"Ah, yes! I meant to link an earlier answer of mine on the topic!

* [How were medieval women not constantly pregnant?](https://www.reddit.com/r/AskHistorians/comments/5lvsv1/how_were_people_not_constantly_impregnated_during/dbz6c8i/)

It talks about some physiological measures, such as marital celibacy, extended breastfeeding, and non-PIV sex. But also about the use of herbs, and potentially magic spells.",0
That was a great read! Do you know anything about how occultism has shaped hell in culture? Like did Aleister Crowley or other similar figures impact how Hell is perceived?,0
"The shortest answer is time. The Allied efforts were flawed, and tended more towards rubbing the faces of the Germans in it than actually helping them come to terms with it.  

At the end of the war, the German population had been through an enormous amount of suffering. Their cities had been pounded into dust, and a generation of men had marched off into the East and never come home. Food was scarce. After what they had found in a dozen camps across the Reich, the invading armies were not predisposed to kindness as they marched towards Berlin. After all this, the German population saw themselves as victims rather than perpetrators. 

While the Allied attempts at denazification were aimed at rooting out the Nazi party, there was no way that in such a short time they could actually make the Germans face what they had done, and also come to terms with it. This process wasn't helped by the need to rehabilitate the German armed forces so that it would be politically acceptable for West Germany to have an army. The idea that the Wehrmacht didn't take part in war crimes was perpetuated by Nazis and persisted until 1995 without a major challenge and is even popularly believed today. 

Simply put, the Germans didn't feel guilty and the Allies couldn't make them feel guilty. The actual process of coming to terms with the past was drawn out over several decades and made easier as the length of time since the war grew. This was formed of symbolic steps - such as Willy Brandt kneeling before the Warsaw memorial - but also an increasing willingness by the German population to accept what had happened.

It's difficult to discuss this without making reference to modern politics, but fanatical hatred of Jews has not completely disappeared from Germany. Neither has far-right beliefs and holocaust denial. However, these beliefs are far less commonplace than they were in 1945, and that is part of a process which the Allies couldn't have hoped to have achieved in the few years they were in control of Germany.",0
"(2/2)

Obviously that is pretty lengthy, but that more so than any specific leaders, is what united the different party chapters. The goals are clear. And this led to a lot of really great things. The most frequently cited actions by the BPP are the ones you bring up in your initial question, the defense of black men and women against police brutality. They did so primarily through education - if you're holding a fucking machine gun, the police are less likely to beat the shit out of you, allowing members to share information about legal rights with those being detained by police.  If you're not holding a machine gun while sharing legal advice, the police will just also beat the shit out of you too. They led rigorous study groups in LA, where they thoroughly educated members on all of their legal rights as American citizens. Before members could be sent out on armed patrol, they needed to pass a written exam proving they understood their rights. These armed patrols were entirely legal. They also led trainings that focused on being sure members could calmly stand up to police, and always patrolled in pairs. This is their first and most widely known social action program. Eventually (after a very public protest that included armed members in I believe the California senate building,) gun control laws were passed in California that prevented these highly visible protests. For *some* reason, a bunch of black, pissed off, highly trained, Vietnam veterans, walking around giving legal advice, scared the government enough that they passed some gun control laws. Crazy how that works.

 Further programs include, but aren't limited to:

 - Free Breakfast. This was common in almost every single chapter, and focused on feeding primarily the children of the community before school. This got a lot of attention and is commonly cited as essentially shaming the federal government into providing breakfast at schools for low income students.

 - Free education. BPP schools for children and adults focused not only on their rights, but on things like teaching illiterate adults to read, and spreading information about their own black history and culture that was not (still isn't) taught in schools. This also included the legal classes I mentioned before, as well as other political classes, classes about first aid, parenting, and self defense. 

 - Health clinics, including clinics during the AIDs crisis. They also had programs that provided testing for sickle-cell disease and ran emergency response ambulance programs.

 - Drug and alcohol rehabilitative programs

- Prison transportation so people without a vehicle could visit incarcerated friends and family members. 

 - Clothing distribution. Not just making sure kids had shoes and coats, but working together to provide appropriate clothing for interviews and the like.

I also want to emphasize that, while the BPP was the *black* panther party, their assistance was typically not restricted to only black community members. The impoverished of many races historically live in much closer proximity in many cities than one would think. If you're too poor to eat you're too poor to give a fuck about the color of your neighbors skin. There were white members of the BPP, as well as white non-members who benefitted from some of the community programs. 


 This got ***really*** long so I'm going to try to wrap it up. The party did have a positive impact on their communities, including impoverished and disenfranchised people of every race. We will never know what exactly the longterm impact of the party could have been without interference, because of what happened next. The tl;dr of the next few years included COINTELPRO, an FBI program that directly targeted the leaders and members of the BPP, and led to mass incarceration and multiple instances of straight up federally sanctioned murder. This includes the death of 21 year old Fred Hampton, who was perceived as incredibly dangerous due to his charisma and ability to unite impoverished people from all ethnicities. They drugged him so heavily with benzos that he never would have woken up anyways, then fired unprovoked on the home he was sharing with other party members, as he was comatose in bed with his pregnant girlfriend. They systematically eliminated almost every (male) capable leader and thinker within the party, which led to many women stepping into leadership positions, which ultimately led to the downfall of the party after the return of Newton, as he was a sexist, chauvinistic, abusive prick. And don't forget all of the police departments across the country that acted against BPP members independently of the FBI, due to their racially motivated hate and anger against the black community.

So. The final answer to your question is really, they stopped because the FBI systematically dismantled them from within, eliminating stable leaders, community programs, and the gun protests, while allowing the real crazies to drive everybody else off. They likely would have continued to do this stuff if they weren't literally hunted down for wanting rights. As a matter of fact, you can see much of this behavior present in the BLM movement today, including the wide spread sharing of legal rights information, recording police interactions, and a disembodied structure to prevent a top-down dismantling. And honestly, I just fucking love their motto. Power to the people!

edit: Just want to say that, although this is an in-depth answer to the specific question, it is necessarily an oversimplification of a really in depth and complex situation. It involved a vast amount of people, and is completely tied up with similarly complex situations involving the black power movement, individual police departments, the FBI, institutional racism, media portrayals, the Vietnam War, and so many other things that are all their own vast fields of study. There is overarching information but experiences could be vastly different depending on individual chapters, time period, or even ones gender. There are some things that are glossed over simply because this is not the appropriate time and space to explore them in depth. However, I would be happy to answer any follow up questions anybody might have to explore things more in depth. I am used to teaching high schoolers so my tone can get informal at times, but this is a topic that I am very passionate about and have studied extensively over the last few years. I genuinely enjoy working to spread any information that dispels the commonly held beliefs that the BPP was a black hate group on par with the KKK.",0
"In the [scene in question](https://www.youtube.com/watch?v=G4546UkGWSw), the Mammy character specifically mentions that ""well brought up young ladies take naps at parties,"" although the book *Gone With the Wind* [describes the scene this way](https://books.google.com/books?id=c5THDQAAQBAJ&lpg=PT105&pg=PT105#v=onepage&q&f=false):

>Afternoon naps were a custom of the country and never were they so necessary as on the all-day parties, beginning in the early morning and culminating in a ball.

So author Margaret Mitchell at least seems to imply that naps were not specific to parties, but a normal daily occurrence that were particularly important on such long days. But the question remains, was this actually a custom of the time period, and would it have happened during a party?

I can find one specific mention of napping at a barbecue in the pre-Civil War South. Susan Bradford Eppes, in her book [*The Negro of the old South : a bit of period history*](https://babel.hathitrust.org/cgi/pt?id=mdp.39015008386651;view=1up;seq=9) (1925) offers her memories of life growing up at [Pine Hill](https://en.wikipedia.org/wiki/Pine_Hill_Plantation), her family's Florida plantation. Describing a Fourth of July celebration, she writes (pp. 13-14):

>Only second in importance to Christmas was the Fourth of July; always a holiday, it was also a feast day, being celebrated by a ""barbecue.""...

>The form of amusement most popular after dinner was a nap. The day was hot, they had eaten to repletion, and men, women, and children rested in the shady nooks and corners.

This description is not identical to the scene portrayed in the film, but it suggests that the idea of a mid-party nap is at least plausible. She noticeably includes men and children in her description, however, so perhaps this custom was not limited to women. She also does not clarify whether she's writing only about the partygoers or if her description includes the slaves.

Elsewhere in the book she describes a typical summer day and reinforces the idea that afternoon naps were commonplace (""the post-prandial nap is an institution in this summer land"") but again does not specify gender or race. Although she does offer a hint in a subsequent paragraph that not *everyone* on the plantation is napping (p. 22):

>The cook has finished her work in the kitchen until supper time, and she and her helpers have all gone home. From the distant laundry comes the subdued murmur of voices, but the only real wide-awake on the scene is Aunt Ginnie, the housekeeper at Pine Hill.

But if trying to ascertain the difference between the napping habits of the wealthy and the slaves, Eppes' work is probably not the best source. Overall, her book actively pushes a romantically nostalgic memory of the Antebellum south, an agenda she makes explicit in her introduction (p. ix), and thus does not likely paint an accurate picture of a plantation slave's experience. (The book contains a chapter titled ""The Dark Side of Slavery."")

Further confirmation that afternoon naps were common among (at least some) women in Southern society in this era comes from Sociologist Wilma Dunaway. In her book [*Women, Work and Family in the Antebellum Mountain South*](https://books.google.com/books?id=NwGoZ_xqlMoC&lpg=PA241&pg=PA241#v=onepage&q&f=false), she touches on the topic of naps while outlining some stark differences in the way pregnant women in particular were treated (p. 241):

>Masters denied to enslaved women the prenatal care they afforded to their own wives in the way of nutrients and rest, for owners assigned black women to work during pregnancy that they considered to be too taxing for their own wives and daughters when they were not pregnant... Even when masters espoused a ""lighter"" workload, their schedule of moderate work and shortened work hours was still too arduous. Southern doctors recommended that affluent pregnant women limit their physical exertion to activities no more strenuous than those conducted ""in carriage,"" and **elite women took regular afternoon naps**. [emphasis mine]

So regular napping was probably reserved mostly for those of a certain class. The film itself of course does not show the slaves napping. Much the opposite, the black women and children are tasked with preparing the beds and [keeping the white women cool during their nap](https://books.google.com/books?id=ekvkZo0mAXcC&lpg=PT39&pg=PT40#v=onepage&q&f=false). The nap scene actually highlights some of the film's problematic qualities. Author Stephen Marche references this scene in a recent [review](http://www.esquire.com/entertainment/movies/a30109/gone-with-the-wind-racism/) of the film:

>During a nap before the war, white girls sleep while black girls fan them with peacock feather fans. Whether those black girls have any feelings whatsoever is a matter of the strictest irrelevance to this movie.

In his essay [""Gone With the Wind (1939) and the Lost Cause: A Critical View""](https://books.google.com/books?id=0aaGDAAAQBAJ&lpg=PA20&ots=cfp09yxF21&pg=PA20#v=onepage&q&f=false), historian Melvyn Stokes reads the scene as slightly more nuanced:

>[A reminder of the reality of child labor] is underlined even more emphatically by the young black girls who are fanning the sleeping white 'belles' on the afternoon of the barbecue at Twelve Oaks. Doubtless reflecting the assumption that whites could not work in the heat, while blacks were used to it, the stereotype is nonetheless challenged by a close-up of one little black girl tiredly stroking her hair.

Just how much the film is endorsing the racist stereotypes it portrays is open to interpretation and is a separate discussion. But possibly problematic details aside, a mid-party nap taken by the white women is within the realm of realistic possibilities in this era.",0
"On the other hand, I know quite a bit about the forming of the Northwest Mounted Police. I'll try to keep it quick:

American ""free-traders"" had been coming into Alberta and conducting shady business. One such operation built a small trading outpost, and then got everybody drunk (and I've read the recipe, it wasn't just whisky, it was more like ""whisky+red bull+sugar"") for a few days, and then escaped with all the furs before people woke up. The local population had a hard time reconciling the fact that they had just traded a year's worth of furs for a few drinks, didn't believe that anyone would think that was fair, got very mad, and burned the outpost to the ground. 

From the American's perspective, the operation was a great success. They came back the next year and built a huge fortification, complete with bars on the windows and drawbridge. There were more tears and more business, and the Blood tribe was nearly destroyed in the process. It was deemed that the situation was intolerable, and the mounted police were formed and dispatched from Ottawa to Alberta with a heavy payload: a cannon, large enough to destroy the American's fortification. 

No military unit had ever tried to cross the prairies in Canada before. There were some surprises, but this is by far my favourite: at the time, the prairies were like the African savannah, wild and teeming with life. There were so many buffalo that the herds would literally drink *all* the lakes dry. Lakes that were shown on their rudimentary maps turned out to be completely absent; yet the ground was still damp enough in many places to bog down wheels and cannons. For a few weeks, they resorted to scooping up urine out of the footprints of their horses. Half the horses died. Their trip might have been more successful if they had dropped the cannon and showed up with a lighter military force, but they might not have been able to take the fort if they put up resistance. Also: what would happen to the cannon? They did not have a policy of arming the natives. 

Eventually, they made it to the fort. It was abandoned, the traders had heard about the advance of the NWMP and headed back to the US. But they didn't come back, and Canada's actions are generally credited with saving the Blood people. It established the modern-day RCMP and legitimized Canada's claim to the territory. 

Source: March of the Mounties; Art Downs. 1983.",0
"So there's a lot in this question, which I'll do my best to unpack.

The connection between Lucifer and the devil arises from this passage in Isaiah, Isaiah 14:12 (quoted from the King James Version).

>How art thou fallen from heaven, O Lucifer, son of the morning! how art thou cut down to the ground, which didst weaken the nations!

and this passage, Luke 10:18 (KJV):

>And he said unto them, I beheld Satan as lightning fall from heaven

More modern translations of this passage frequently translate the Latin word ""lucifer"" as ""morning star"" (NIV) or as ""Day Star"" (NRSV), whereas the KJV translates it as the proper name ""Lucifer""--I've used the KJV here because the translation of ""lucifer"" as a proper name most clearly illustrates the connection that some claim between these two passages. 

That said, many, including noted theologians, such as Martin Luther and John Calvin, have vigorously disputed the connection between Lucifer and the devil. The proponents of the view that Lucifer and Satan are not the same draw their main argument from the context of this passage. From Isaiah 14:4 (KJV):

>That thou shalt take up this proverb against the king of Babylon, and say...

14:12 is still contained in what the speaker (who is probably the historic Isaiah ben Amoz) is instructing. As such, theologians have claimed that ""lucifer"" is here referring to the King of Babylon, and not to the devil.  The analogy with a word relating to Venus, the morning star, may have also had specific import to the audience of Isaiah's work. The rise and fall of Venus (both literally and observably, in the sky, and figuratively, in the doings of the gods) played a role in Near Eastern religious myths, and so Isaiah may have been using religious imagery his audience would have been familiar with, but in a different context, to further his own points.

Another point in support of this connection being incorrect is the use of the Latin word ""lucifer"" to mean simply ""light-bringer"". This construction is seen elsewhere in Latin (cf. Catullus 62.8, 'noctifer', meaning night-bringer); and it is clear from other uses of the Latin word ""lucifer"" in the Bible that the word does not, at the time of writing, mean only the Devil. It is used elsewhere in the Bible with the meaning light-bringer, and is used to describe Jesus, both in the Bible and in early Christian hymns.

Another difference between Lucifer and Satan is the following: generally, Lucifer is used to refer to the Archangel before his fall, while Satan is used after. In his depictions in literature, Lucifer is generally placed in a position of high esteem before his fall. A quotation from the Cursor Mundi describes his high status, saying ""\[God\] sett him \[Lucifer\] beste in his \[i.e., God's\] halle / As prynce & sire of othere alle."" Later, Lucifer is often associated with bright light, making the name's connection to Venus and light more sensible. However, these depictions are all much later than the Bible's writing, and so their foundations are not necessarily sound. 

In summary, the word ""lucifer"" derives from the Latin for 'light-bringer'. Early Christian thinkers drew that Lucifer was the Devil from the biblical passages Luke 10:18 and Isaiah 14:12; while this idea was not without pushback, including from prominent theologians, it was eventually generally accepted. Depictions of Lucifer in literature often feature motifs of bright, dazzling light, so his name makes sense in that context. Given the rise and fall of Venus in the sky, the Roman word for Venus being used to describe an angel who rose to God's side and then fell from heaven is not inconsistent with nearby non-Judeo-Christian religious traditions either. However, the word 'lucifer' is used elsewhere in the Bible to describe Jesus, and is used with its meaning of ""light-bringing"", so its association with the Devil is not without controversy.

&#x200B;

Sources:

Smith, Gary V. (2007) *Isaiah 1-30*.

Bloom, Harold. (2005) *Satan*.

MARTIN, DALE BASIL. “When Did Angels Become Demons?” Journal of Biblical Literature, vol. 129, no. 4, 2010, pp. 657–677. JSTOR, [www.jstor.org/stable/25765960](http://www.jstor.org/stable/25765960). Accessed 6 June 2021.

Lee, Judith. “Lucifer: a Fantastic Figure.” Journal of the Fantastic in the Arts, vol. 8, no. 2 (30), 1997, pp. 218–234. JSTOR, [www.jstor.org/stable/43308294](http://www.jstor.org/stable/43308294). Accessed 6 June 2021.",0
"So, for me, I analyse pop *books* differently than I analyze pop *articles.* Pretty often pop articles don't bother with citations and references at all, so it's an easy top-level way to separate the stuff worth reading from the crap that's not even worth my time. For example, if I'm, say, looking for information about when waterwheels were invented, I'm going to start by googling it, because I'm not writing a dissertation or anything I'm just looking to learn something about a history thing I find interesting after having visited the Netherlands on vacations (not recently — this is just an example).

I'm not a Medievalist or anything, and I'm definitely not an expert on whatever you need to be an expert on for waterwheel to fall under your purview, so let's say I get hits for 3 books and 4 websites. I skip the books for now because I'm looking for an immediate answer to a simple question.

One article is from what looks like an encyclopedia website with a bunch of articles, but, drat! It's paywalled because I already read 4 great articles from there. I skip it for now — although if I hit this paywall often enough, I'll buy a membership (this is a made up example, but, this is roughly how I wound up with a membership to [ancient.eu](https://ancient.edu))

The next article doesn't have academic references, but it has a couple of hyperlinks to what look like sources and seems to be made by a hobbyist. Paydirt!

The third article is from a website trying to sell landscaping rocks that used to be millstones, so it's definitely not academic and they're definitely trying to convince me to spend $$ so I know that whatever they're claiming, I'm going to need to double-check, but I still read it because maybe I'll get some useful search terms out of it.

The 4th article reads like a ranty screed trying to convince me that \[insert ethnic group\] is the best because they invented waterwheels and \[insert technology\]. I stop reading after the third paragraph and make sure to click back as fast as possible so Google knows it was a crappy result it should de-prioritize in SEO.

Then, because I'm interested now, I go back to the books. First, I look at the cover. Does it look accessible or does it look like somebody's dissertation? I'm smart, but I dissertations are long, filled with a bunch of stuff I don't usually care about, and not usually written to a layman audience even when they're about something interesting. I skip the one that looks like it's intended for a super dry academic audience.

The next one looks interesting and is written at my level and is even filled with citations, but the reviews are a mix of creepy and scathing. I leave it off my wishlist.

The third one looks more promising. The reviews seem enthusiastic about how this book addresses new archaeological finds, it's making a claim I haven't heard before, and it's been published by a reputable publisher. The sample pages appear heavily footnoted. It even has a neat cover! I put it on my wishlist and get it for Christmas and start reading it.

But whoa, some of the claims it's making are not things I've ever heard before. The author addresses the previous state of scholarship and even sounds convincing about why they're right and their predecessors are wrong. (I know of no new scholarship about waterwheels, but I *did* recently get two ""popular history"" books for Christmas that do exactly this —  the Amazons by Adrienne Mayor and The Golden Thread by Kassia St. Claire and I am actually in the midst of going through this process).

So what do I do?

First, I make sure to read critically. I annotate heavily, making liberal use of the ORLY owl and #followup notes-to-self because honestly, even the absolute best, most well-considered, most thoughtful and well-intentioned books is going to say stuff that's outside of the expertise of the author. My husband was listening to a podcast that goes *super* into depth about I think WWII (I don't recall, but it was definitely an American war) and the guy was clearly an expert on that war. But he made some throwaway comment about dragoons that my husband took on faith and like, I *knew* it was wrong, dragoons were NOT invented by Napoleon or whatever outlandish thing he claimed, and I wound up in a long argument with my husband about it because he then felt like he couldn't trust anything the guy said and was really upset that a source he *thought* was reputable had recommended this podcast and like now he felt like he couldn't trust anything from that source and...

... look, nobody's perfect. So before you go getting excited about some hot new thing you learned, even if it's from a source that really seems great, double and triple check it. Because that waterwheel  story I was telling?

*Every Single One* of the sources I found online claimed that they were invented in 4000 BCE. Which is insane. The **wheel** was invented in 3500 BCE in Mesopotamia and I know that off the top of my head because I happen to teach a unit on Mesopotamia (I'm a social studies teacher, but everyone will have different bases of knowledge). The waterwheel is attributed to one of Alexander the Great's generals in the same breath as the 4000 BCE date.

So I google Alexander the Great, who it turns out was active around 400 BCE, and get my answer: somebody typo'd.

Anyway, once I've found a claim that I think I might ever want to repeat, I go looking for verification. Half the time I don't bother with the actual source cited by the book, because that stuff is usually behind a paywall I can't access, unless my Google Fu fails me. Then I try to get access to the source, or email the author (authors are often thrilled to respond!) for more information, or ask here on this subreddit (where for example I found a historian able to help me explain all the weird differences between sources claiming to explain the [origins of coinage](https://www.reddit.com/r/AskHistorians/comments/kpyt5c/when_did_metal_currency_metal_commodity_money/) — which is actually a GREAT example of this exact problem, because apparently people like to erroneously claim that their ethnic group is responsible for that invention.)

Anyway this is really long and I hope it helped show what my ""process"" is.

As for best practices, I have no idea; I'm a critical consumer of historian-created media, not a historian myself (except insofar as I am a history teacher and sometimes able to answer questions like this one). My pop history books are liberally annotated with things like ""ok WHICH scholars say that? Don't just say scholars says!"" and ""ORLY"" and ""#followup to confirm"" lol. I don't think that makes them bad books, it just means that it *is* a book, not a research paper, and the wordcount to cite every claim in a popsci book would be in excess of what a publisher would be willing to pay to have printed (this is speculation, but I'm basing it off of how liberally cited my education research papers were, which was... extremely. No one wants to read something with that many citations; your eyes would bleed. At a certain point you just have to decide whether the author is acting in good faith and then take responsibility for confirming any claims that *you* are planning to make).",0
"The Hadith in [Sahih al-Bukhari 5707](https://sunnah.com/bukhari/76/27)  is difficult to understand in modern days, as the Arabic word ""عدوى"" didn't mean infection, but it assumed the disease was conscious and defied god's will by residing in people in the same way as the other negations in the Hadith as omens. This is evidenced by the later use of theologians and doctors of the word in the Islamic golden age by using the word ""contact"" for describing infectious disease rather than ""عدوى"" as it was believed to be superstition. However it is still in use in Arab speaking countries to describe infection.

Caliph Omar did solve the problem but by saying: ""we run away from god's will to god's will"".

also i believe ""in context"" that it has everything to do with preventing spread of the plague evidenced by a lot of other Hadith as the same on above instructing to run away from the leper, also other hadith emphasizing cleanliness, and at last other Hadith as this Ibn Majah Hadith 3541 instructing the the separation between the healthy and sick, and the prohibition of self harm or death in Albaqara 195.",0
"First I am not a historian, I apologize if that is not allowed I am just a Dietitian with a passion for the history of food. I would defer to an answer from a historian on this topic. 

Much of my information will come from the article ""Making a global sensation: Vanilla flavor, synthetic chemistry, and the meanings of purity"" by Nadia Berenstein. 

A large part of why we enjoy the flavors that we do is hard to quantify, but the vanilla beans were used in the cuisine of many cultures in the region notably in the drink chocolatl which was served to Cortez by Montezuma.  Many of our spices that now are common were a rarity across the world. While vanilla was incredibly saught across the world, it wasn't until it was artificially created that the ubiquity of vanilla became solidified.

The early days of its use in European cuisine were primarily as a component of chocolate which enjoyed popularity in the seventeenth century. Though botanists throughout Europe attained specimens they found their efforts fruitless without the ""Melipona"" bee that pollinates the flowers. The stubborn plant was eventually transplanted to the French island of Bourbon. 

 The transplanted vanilla orchid would remain fruitless until Charles Morren a professor of Botany first successfully pollinated the orchid artificially. A method of hand pollination was also developed by a slave named Edmond Albius on a plantation in Bourbon. As demand increased in France for delicacies like vanilla chocolate and vanilla ice French colonies began to expand cultivation in the Indian Ocean, West Indies, and Tahiti, as well as Dutch plantations in Java and German East Africa. 

Vanilla requires a labor-intensive process that requires beans to be dried, sweated, and cured which that sometimes may require months of daily work and close attention. Fortunately, the decades of *vanilla planifolias* colonization of the globe coincided with rapid advances in synthetic organic chemistry.

Vanilla was not the first synthetic flavor (this belongs to various fruit flavors), but it was the first luxury flavor to be produced synthetically. In 1874 German Chemists Ferdinand Tiemann and Wilhelm Haarmann synthesized vanillin from coniferin. This discovery was widely reported and confirmed by Karl Reimer who derived it from creosote tar. Vanillin did not enter the market cheap listed at $1,500 a kilogram considerably more than the cost of an equivalent amount of vanilla beans. New innovations in the production of synthetic vanillin lead to a drop in costs. New manufacturers across the United States and Europe challenged French and German companies' dominance of the market caused the price to plummet from 560DM in 1896 to 126DM in 1897. 

By 1900 the US saw a rapid increase in the sale of both vanilla beans and synthetic vanilla corresponding to the expanding role of sweet foods in American life. Technological innovations created the ability to manufacture ice creams, chocolates, and other confectionary items on a large scale at a low price. Both artificial vanilla and vanilla beans prices continued to decrease throughout the early 20th century. Sugary vanilla flavored treats became everyday indulgences due to the wide availability of cheap sugar and artificial vanilla. 

Today over 95% of the vanilla products on the market come from artificially produced vanillin.  

Source:

  Berenstein, N. (2016). Making a global sensation: Vanilla flavor, synthetic chemistry, and the meanings of purity. History of Science, 54(4), 399–424. [https://doi.org/10.1177/0073275316681802](https://doi.org/10.1177/0073275316681802)",0
"Baumslager talks a bit about vaccine development in her book. The first vaccines appeared during the interwar period, and development continued during the war. Some testing of vaccines was done with the population of the Vilna ghetto during the war, but of course they were do distrustful of the Germans that they first gave it to dogs to make sure it wasn't poisoned.  They were absolutely *not wrong* to be distrustful, as testing was also done in camps... and of course the way to test was simply infecting with typhus and seeing if it worked, with some tests seeing mortality rates as high as 98 percent.

As you note, it isn't commercially available these days, presumably due to the rarity of typhus in the West resulting in low demand, but while I'm not a medical historian so can't comment too deeply, the impression I get is that it is a somewhat costly and time intensive vaccine to produce. They were certainly producing it during the war period, and looking for ways to improve both production and effectiveness, but it was for *Germans* they wanted it. Stores were occasionally available via black market purchase, but it was illegal for Jewish doctors to have it, and many ended up being diluted or outright fakes. 

It would also be good to mention here Dr. Ludwik Fleck, a Jewish professor and pre-war authority on the disease was sent to the Lvov ghetto, where he managed to create a reasonably effective typhus vaccine using the urine of typhus patients, which he was able to administer to roughly 500 people in the Lvov ghetto! 

As an additional side note, she includes an absolutely *fascinating* bit in there:

>During World War II there was also an international exchange between German scientists and the Allies for the production and testing of typhus vaccines.

But unfortunately I don't have the book she cites there, `Weindling, P. 2000. Epidemics and Genocide in Eastern Europe 1890-1945. Oxford: Oxford University Press.`. Using Google preview, it looks like the data exchange was done through Switzerland and Sweden, but I can't say much more on the matter, tantalizing as it sounds (have put in a request with my library though. I really want to learn more about this!). 

It certainly speaks to an awareness on both sides of the dangers it presented in the period, but I also have to wonder how the Allies weighed the ethical considerations of the testing being done on prisoners (if they even knew), as well as the shoddy data that resulted. The German testing on prisoners, mostly done at Buchenwald and Natzweiler and involving I.G. Farben, aside from simply using persons already emaciated and unhealthy, was next to useless. As Baumslager sums it up - and which applies to just about all the medical ""experiments"" conducted by the Nazis - they ""yielded no real useful results owing to their flawed design and fabrications"".

Edit: Have the chapter in hand now, but while it has some good additional info on vacinne development, generally, it offers little more specifically on the international exchange.",0
"During the campaign and subsequent presidency of Nixon, much of his rhetoric revolved around two things in particular - crime and drugs/drug use. Nixon was the progenitor of the War on Drugs and even coined the term (although Reagan would supercharge it during his time in office).

Nixon's presidency is the time where academics begin to refer to a period defined by 'mass incarceration', where we started to heavily enforce and criminalize drug offences, particularly marijuana. Behind such efforts was forms of racist pandering, often seen as a backlash to the Civil Rights and Black Power movements. We can see this particularly when it comes to drug usage, as the Nixon administration sought to associate African-Americans with drug use in order to excuse the criminalization and policing of their leaders and communities. We see this in particular with the infamous quote from Nixon's top domestic advisor, John Ehrlichman:

*""The Nixon campaign in 1968, and the Nixon White House after that, had two enemies: the antiwar left and black people. You understand what I’m saying? We knew we couldn’t make it illegal to be either against the war or black, but by getting the public to associate the hippies with marijuana and blacks with heroin, and then criminalizing both heavily, we could disrupt those communities. We could arrest their leaders, raid their homes, break up their meetings, and vilify them night after night on the evening news. Did we know we were lying about the drugs? Of course we did.”*¹

Nixon himself wasn't above racial remarks either. In a taped recording, he stated in regards to marijuana legalization supporters: *""You know, it's a funny thing, every one of the bastards that are out for legalizing marijuana are Jewish. What the Christ is the matter with the Jews, Bob? What is the matter with them?""*²

During this time, there was even a commission appointed by Nixon to study the effects of Marijuana (as the Controlled Substances Act was being authored). However, when the commission found the cannabis use did not present a danger to society as the administration often presented, it was ignored. 

Fast forwarding to Reagan, and we see the kickstarting of the conservative legal philosophy commonly known today as 'tough-on-crime'. It advocated for harsher criminal penalties for those convicted of a crimes (particularly violent crimes and drug offenses). Mandatory minimum sentences were established, forcing those convicted of a crime to have to serve a specific period of time for that crime (in the Clinton years this would be followed by 'truth in sentencing' laws that made those convicted of a crime have t serve a signficant portion (85%) of their sentence before being eligible for parole). Criminal penalties for drug offenses were increased heavily, particularly with the Drug Abuse Act of '86. This law also introduced penalties for crack cocaine possession, which were far harsher than for powder cocaine (something often regarded as a way to unfairly discriminate against African-American). This particular way of thinking in regards to crime lasted through Bush Sr.'s presidency.

From the period of 1970 to 1994, the prison population increased exponentially - from around 200,000 in 1970 to 1.5 million in 1994. Without a doubt, this trend was guided by the actions that the federal government took during this time in regards to how it handled crime, drug use and dependency, as well as race relations. 

Sources or related readings:

1. 'Legalize it All' by Dan Baum, Published in Harper's Magazine April 2016

2. Nixon Oval Office Tape, May 26th, 1971

- The New Jim Crow by Michelle Alexander 

- I recommend looking into the Shafer Commission, as well as the earlier LaGuardia Report which was published in 1944 and came to many of the same conclusions as the later Shafer Commission. Both are important in regards to the history of drug prohibition in the U.S., and the LaGuardia Report is important especially in regards to the relationship between prohibition and racial discrimination.

Edit: corrected a typo describing the Shafer commission being appointed by Reagan instead of Nixon

Edit 2: if you have a response or question to my post, please DM me. I'm seeing notifications for responses but they don't appear when I check them.",0
"Disclaimer (edited): most of what I'm talking about here is from the perspective of higher society white culture in the North and South. There were some differences in the antebellum South, but little that impacts this specific question because as you'll see if you read the full text it tl;dr, I very strongly suspect the subject is apocryphal* (edit). White plantation ladies hosted parties, entertained their peers, lessers, and leaders, and maintained appearances at all cost.

I have several books and magazines from the era that go into excruciating detail on what to do and not do at parties and I can find no mention of this whatsoever. I've checked my etiquette books from that era and after ([here](https://i.imgur.com/lr6dRI8.jpg) are two [examples](https://imgur.com/KqBkv9S) of later era books I checked, because often etiquette books will recommend *stopping* a practice like that if it's been going on, but no dice), plus a plethora of magazines for anecdotes, stories, etc. I have several years' worth of [Harper's](https://imgur.com/mio3OET) and Chautauqua Society magazines, which have lots of slice-of-life articles and social commentary. Nothing.

I *suspect* this is apocryphal for a few reasons. One, etiquette of the period was more rigid, yes, but more importantly ladies were expected to be social and maintain a level of decorum. For a tiny taste of what I mean, [here's a sample from one of my etiquette books covering general etiquette](https://imgur.com/hS2WwCa). (Sorry about the terrible quality.) Everything was about making others feel at ease, and indeed, that's the theme in every single book and article of the time. Don't be selfish. There's a famous (and possibly apocryphal) story from the era of a society lady who was choking at a dinner party and, rather than embarrass her host, went into the toilet and quietly died. Very Victorian. Mass lady naps during a party just sounds selfish and anti-Victorian. What if a guest wasn't tired? Shall the hostess sleep while her guest stares at the wallpaper? That's a horrifying concept in 1860.

Point two, why? It wasn't the corsets, I can promise you that, especially pre-1880. White women of any means weren't binding themselves into oblivion that early. The corset was a supportive foundation garment and when properly fitted was not uncomfortable or restrictive at all. And keep in mind that they'd been wearing them their entire lives so they were quite used to it. Corsetry had been an established industry for at least a century and even a woman of modest means could afford a decent one; a well-off socialite could choose from the best silk and spiral steel numbers on offer. Even when they did get a bit silly and wasp-waisted in the height of the bustle-era, regular ladies weren't torturing themselves en masse. Well, no more than people do with plastic surgery and extreme body modification today. Those were the elite, the Kardashians. People like me could breathe fine. Speaking of me, here's how I know this: [this is me in a reproduction 1880s French seam corset](https://imgur.com/eDrMZ8z) that I made using authentic period techniques. I've been making and wearing them for 20+ years. It's very comfortable, I can breathe fine, touch my toes, and everything. It's boned with steel. [Here's a new one I'm working on.](https://imgur.com/u8m4e7b) I've been researching and speaking on this topic for decades, but there's also a well-researched [How Stuff Works article](http://people.howstuffworks.com/corset.htm) that's worth a read if you're interested.

That said, is it possible that, in the sweltering heat of the Deep South, some groups of friends or some families may have done this as a habit? Sure. Maybe Margaret Mitchell's own family had this tradition and she extrapolated it to everyone. I just personally doubt it was an institution throughout the southern states. If it were, I'm shocked I can't find a single mention of it outside one work of historical fiction. Edit: I also perused online articles and Google Books.

Is absence of evidence evidence of absence? Maybe not, but I've got about a thousand sentences telling women not to hum in public and far more on the various uses of vibrators to cure hysteria (and they weren't supposed to talk about that). I've also got actual Victorian pornography, which isn't supposed to exist. But I can't find this.

Tl;dr: I've been pretty immersed in this era for a long time and I've never seen a single mention of this outside one work of historical fiction, plus I can't think of a supporting reason for it to be true.

Edit1: at mod request.

Edit2: A very kind thank you to my anonymous benefactor who gifted me with Reddit gold. It's very nice to be appreciated and you've brightened my day!

Edit3: mod request, moved race/class disclaimer to opening paragraph.",0
"> Of course there are many stories of Native people who converted ""willingly"", and many Native people are Christian today. Major movements within Christian history such as liberation theology have been actively informed by Indigenous actors. It can become easy for settlers, whether comfortable in their atheism or in their ""orthodox"" Christianity, to sneer at the devotional lives of Christians among colonized populations. Whether it's because they believe that the Native peoples need to be ""rescued"" from their colonizing religion and ""saved"" with atheism, or because they believe that there is something theologically ""impure"" about religious rites that incorporate pre-Christian practices, it is really just the same old story of ideologies being forced onto Native peoples from outside in order to ""save"" them. That is why discussions of this topic are so fraught, both inside and outside of Native communities.

You know, this part is especially interesting to me. Your use of the word ""settler"" immediately struck me as reflecting an Anglo perspective, since the settler-colonized dynamic isn't as clear cut in Latin America, where lines are blurred when you take into account that the dominant Mestizo population is, well, *Mixed*. Meaning, we (for I'm Latin American) have Indigenous blood in our veins. Naturally, the Catholic Church has still committed monstrous atrocities in its quest to ""civilize"" the Indigenous populations, but the fact of mestizaje and its centrality to the identity of the nations that appeared following the end of Spanish rule has resulted in a quite unique picture. And so, Indigenous peoples here in my country are often some of the most devout Christians (whether Catholic or Evangelical), and can both celebrate Inti in traditional rituals and pray to God in Church. It has resulted in a world of contradictions that both Mestizos and Indigenous peoples seem unable to fully confront, such as the most prominent Indigenous leader justly denouncing the Church for its contribution to colonization, but having been an altar server (monaguillo) when he was young, or appeals to traditional Incan deities coming from the Organization of *Evangelical* Indigenous Peoples of Ecuador. The Church, to be sure, has appropriated and fostered this syncretism to serve its own ends. One example is that you can find paintings of Jesus and the Apostles eating guinea pig (cuy) at the Last Supper. 

I bring this up mostly because, despite mentions of Latin America such as how ""Jesuits burned Mayan books and Inca quipus"", your answer seems to reflect mostly the experience of the United States, and I wanted to ask if you could add anything regarding the Latin American perspective, which from both study and experience seems to me to have many key differences.

(Out-topic, but in the context of people sneering at Indigenous populations while not understanding their situation, I remember socialist and left-leaning sites citing the Church's mediation of the conflict between Ecuador's government and its Indigenous organizations as another example of colonialism. In a sense, it clearly is, what with the Church only being able to influence politics like that due to the colonial past. But they seemed to believe that the Church was once again imposing beliefs and organizations over the Indigenous peoples, when, as commented, most are believers and accepted the Church's mediation offer quite willingly, joining in prayer when the talks started).",0
">That said, there's no indication Sailors mixed the two for purposes other than this.

There's a prevalent and fairly old (false) belief that potassium nitrite reduces men's libido. It's a common story passed around in mess halls even in modern militaries that they add it to the food or water to keep everyone's sex drives manageable. I've even heard it said that this was done on naval ships hundreds of years ago for the same reason. Do you have any input? Any idea how old the myth of saltpeter causing soft peter is?",0
"cont.

The pressure during the time for the British was largely a question of if it would be worth carrying on the fight. One that at the time seemed to be impossible to decisively win. Or if it would be expedient to end the fighting and save themselves the trouble of having a long drawn out and costly war. Much of this determination to remained committed to a war that looked increasingly impossible to win and was costing the British dearly in both money and manpower stemmed from the position adopted by Churchill and his government. While many popular dramas like Darkest Hour tend to play up the divided between Churchill and less pro-war individuals like Halifax and Chamberlain, the difference was in reality much narrower. There was sufficient political will in Britain to continue the conflict and there were sufficient resources provided by the Empire to do so, even if it would take time to realize this advantage.

&#x200B;

Churchill’s political speeches from during the early war have become famous in his position of defiance against Nazi Germany and his emphasis on the moral nature of the conflict, beyond just being political or economic. What Professor Edgerton notes is how Churchill’s rhetoric framing the war changed from during to after the war. In 1940, most of his speeches emphasized that Britain would not surrender despite their precarious position but they also do not emphasize being alone. Churchill and very much the popular British consciousness was fully aware of the support coming from the Empire. It was not only visible but publicized and acknowledged being a source of Britain’s ability to continue the fighting. Churchill was also adamant that the United States would eventually join the war and tip the scale. This tone and image of an imperial and latter Allied war effort steadily changed after WWII had ended.

&#x200B;

Edgerton argues that this idea of Britain being alone only became embedded in the popular consciousness from the 1960s onwards before evolving further becoming the idea of a people’s war. Edgerton argues that this is due to the need for a new “founding myth” for the post-colonial, welfare state, Britain which WWII became an easy rallying point for. More recently, scholarship and to a lesser extent popular consciousness as returned to examine this idea of Britain being alone more closely and found it too inaccurate. Initially, the origin of Britain being alone was chalked up to it being propaganda from the war that seeped into popular consciousness. Though Edgerton points out that this can hardly be considered an accurate explanation gave the clear Imperial framing of the conflict during the war. He ultimately argues that the history of the war was steadily nationalized afterwards which distorted reality and that the myth of Britain being alone is more a part of its coping with a loss of empire than an actual reflection of the history or even propaganda of the war.

&#x200B;

Ultimately, if we want to consider the British Empire as a single unit (though this is not advisable) then we can see that Britain was in a way largely alone. Alone in the sense that their remaining Allies not in exile were in effect all imperially linked to Britain and were likely unable to politically exit the war independent of Britain. In terms of the military situation, while precarious, Britain was never really in danger of being defeated by sheer force and it was largely a question of their political will to continue the fight despite the mounting heavily losses. Though to qualify this, there was the real panic of a possible invasion of Britain and later on India and Australia. While physically impossible or at the very least highly likely to end in disaster of the Axis powers to attempt, the spectre of this possibility was pressing on them.

&#x200B;

My area of expertise is largely on the Malayan Campaign during WWII so I can only comfortably further elaborate on the specifics of the British military position and preparation there. If you are interested in that you can see my previous answers on that [here](https://www.reddit.com/r/AskHistorians/comments/77zpku/what_was_the_reasons_for_the_disastrous_from_the/dor4tl8/?context=3) and [here](https://www.reddit.com/r/AskHistorians/comments/9ajvo2/the_fall_of_singapore_during_ww2_was_considered/e4wdeka/?context=3).",0
"The time that you would have had to react to the news that France had sold Louisiana to the United States, may not have been months, but probably weeks or days. In the early 1800's news traveled slow by today's standards, and the news would have been slowed down even more due to secret treaties and colonial revolutions in Haiti. It helps to start the answer to your question in 1800, when Spain agreed to transfer control of Louisiana to France, as Spanish officials end up playing a role in the transfer of power and the answer to your question. 

The Louisiana Territory was controlled by Spain in 1800 and ceded to France by the secret Third Treaty of San Ildefonso on October 1, 1800. The terms of the treaty were later affirmed publicly by the Treaty of Aranjuez on 21 March 1801. However, the terms of these treaty did not go into effect until later. In fact, Juan Manuel de Salcedo, the last Spanish governor in Spain served until November 30, 1803. 

So, what was life like in New Orleans during this lengthy transfer? Well, Pierre-Clement de Laussat the French official who was supposed to prepare for the transfer of power from Spain to France would have arrived in early 1803 before the sale of Louisiana to the United States. A a french government official and autobiographer, he has left a lot of information that proves useful when trying to understand what New Orleans was like in 1803. 

Laussat was sent to prepare the colony to become the supply center for troops that would be stationed across France's Caribbean empire. While the sugar plantations and an economy fueled by people in slavery were the center of Napoleon's ambitions, Louisiana would be the hinterlands that supplied the island. There were also dreams of controlling the North American fur trade through Louisiana. However, Napoleon proved unable to quell the revolutions in Haiti, and gave up on his colonial ambitions. This is what led to him selling Louisiana. 

During this time, Laussat would be making connections in the government and trying to win over Spanish official and powerful residents. There were incessant rumors that Spain would refuse to turn over Louisiana because France had not upheld it's end of the bargain. In fact, the aforementioned Governor Salcedo was one of the many pushing for Spain to not give up Louisiana. At the same time, there were also rumors that the Americans may come down the Mississippi River and take New Orleans by force. The year prior in 1802, another Spanish official had shut the Port of New Orleans to Americans. While the latter issue was resolved, Laussat continued to struggle with the rumors of Spanish refusal even after the new of the Louisiana Purchase reached New Orleans. 

So what of the Louisiana Purchase? Well as Laussat worked to ensure that Spanish officials would hand over Louisiana once the powers in Europe finalized the details, he received shocking rumors that his work would be almost for naught. Records show that he wrote to France to inquire if the rumors were true in late July and that he did not formally receive word confirming the sale until August. The on-going legal struggles between France and Spain muddled any sure sense that France or the United States would actually come to possess Louisiana. In fact, it was in October that the French ambassador to the United States wrote Laussat to assure him that he would receive orders to take possession of Louisiana from Spain on behalf of France (up to this time, Laussat was only preparing the situation for the arrival of a general who has the orders to take charge on behalf of France). The ambassador also provided guidelines on what to do if the Spanish refused to transfer power. The uncertainty was furthered by the fact that some Americans argued that Thomas Jefferson could not buy land without congressional approval. With all this uncertainty and arguments, it's easy to see how Laussat could have first doubted the news as would have many others. 

So in a swirl of rumors and political intrigue, what did life look like for the average French (or more likely Spanish) citizen living in New Orleans? Life and government continued to be administered as usual. Like many frontier towns and port cities, there was a hustle and bustle of immigrants and traders. During all of this, Laussat kept a diary and on one day in May he wrote, 

“The products of Louisiana are already quite considerable. Wherever the Anglo-Americans settle, land is fertilized and progress is rapid. There is always a group of them who act as trailblazers, going some fifty leagues in the American wilderness ahead of the settlers. They are the first to migrate to a new area. They clear, populate it, and then push on again and again without any purpose other than to open the way for new settlers. Those who thus forge ahead into unknown places are called backwoodsmen. They set up their temporary shanties, fell and burn trees, kill the Indians or are killed by them, and disappear from this land either by death or by soon relinquishing to a more stable farmer the land which they had begun to clear. When a score or so of such new colonists have congregated into one location, two printers arrive—one a federalist, the other an anti federalist—then the doctors, then the lawyers and then the fortune seekers. They drink toasts, nominate a speaker, set up a town and raise many children. Finally, they advertise the sale of vast tracts of land, attracting and deceiving as many land buyers as possible. They exaggerate the population figures so that they quickly reach the sixty thousand souls entitled to form an independent state and be represented in Congress. And so another star appears on the flag of the United States! A district under the Spanish or French regime might begin, end, start again, and get lost again, and so successively until its fate is sealed—permanent existence or annihilation. Under the Anglo-Americans, a newly born state may thrive with more or less prosperity, but it will never decline; it keeps on growing and strengthening. One can hardly realize that forty years ago, on these vast expanses of land from the shores of the Mississippi to the Alleghenies, there was not a single farmhand to cultivate the soil. Today, these same regions flood the New Orleans market, by way of the Mississippi, with their abundant harvests.”

It would seem that life in New Orleans was marked by the arrival and departure of American colonist heading up river to the interior. The Spanish government continued to explore, maintain, and exploit the frontier. After the transfer of power Laussaut used his 21 days as governor to appoint the first municipal government for the city of New Orleans, establish a city fire department, formulate police regulations, and begin the preservation of public archives. The sources vary on how people actually felt during this transition time, but they point to the fact that feelings varied from surprise to anger to apathy. The majority French population did face skepticism from the Americans. Only a few decades earlier in 1766 there was a major Creole rebellion against the Spanish rulers. The records also show skirmishes between French and American residents before and after the transfer. 

Finally, an interesting tidbit, while there was a period of French governance, in St. Louis the administrative seat for Upper Louisiana, power transition from Spain to France to the United States in the same day.  March 9-10, 1804 is know as Three Flags Day. On March 9, a flag raising ceremony transferred power from Spain to France, on the next day  a flag raising ceremony transferred power from France to the United States.  

&#x200B;

Sources: 

LOUISIANA: EUROPEAN EXPLORATIONS AND THE LOUISIANA PURCHASE A SPECIAL [PRESENTATION FROM THE GEOGRAPHY AND MAP DIVISION OF THE LIBRARY OF CONGRESS](https://www.loc.gov/static/collections/louisiana-european-explorations-and-the-louisiana-purchase/images/lapurchase.pdf)

[Treaty of San Ildefenso](https://avalon.law.yale.edu/19th_century/ildefens.asp)

[Louisiana, Napoleon and the United States: An Autobiography of Pierre-Clément de Laussat, 1756-1835](https://www.google.com/books/edition/_/vaJ93a_QtqAC?hl=en&gbpv=0) (referenced here on google, but cited in several other sources)

[Louisiana Governors 1766-1812](https://www.sos.la.gov/HistoricalResources/AboutLouisiana/LouisianaGovernors1766-1812/Pages/default.aspx)

[Citizen Laussat: A Retrospective on the Louisiana Purchas](https://www.nps.gov/jeff/learn/historyculture/upload/bush.pdf)

 Jean-Marc Olivier. Bernadotte, Bonaparte, and Louisiana: the last dream of a French Empire inNorth America. The Impact of Napoleonic Empire in the Atlantic World, 2009, France. pp.141-150. hal-00974220 (l[ink](https://hal.archives-ouvertes.fr/hal-00974220/document))

Lastly my apologies that this response has run on so long and come so late. As a native New Orleanian I had inchoate inkling of the answer based on stories I've heard and have been told, but the rabbit hole of research quickly dispelled a lot of my notions.",0
"""Patchwork families.""

I love Ann-Cathrin Harders' term for it, and I should've thought to include it in my recent answer on [single mothers in medieval Europe.](https://www.reddit.com/r/AskHistorians/comments/gm5ciz/what_happened_to_medieval_unwed_mothers/) One of the most important things it shows is: **contraception and infanticide were not the only option.**

With such messy and depressing mortality rates in the ancient and medieval world--and we're not just talking about death in childbirth here, which was less common than you probably think--even wealthy children had a strong chance of losing their father by mid-adolescence. A family which the father *possessed* was certainly the ideal, but it was by far not a given.

If we're talking about ""well-regarded"" sex workers, which I interpret as ""with more resources,"" I think **single mothers** is a good model to start with, especially regarding children once born. Roman sexual relationships were already more fluid than we might think of today, and children born out of wedlock were common enough to have a single word designating them in law: *spurii*. (Which, as the root of our ""spurious,"" does not have the best of connotations today.)

Women in the ancient and medieval worlds often cultivated a strong network of female family and friends. It was to them that single mothers tended to turn. Essentially adoptive mothers, stepmothers, aunts and uncles raising children--this was not the *norm*, but it was *normal*. 

Hence Harders' ""patchwork families,"" with the emphasis on *families*.

Would sex workers be treated any differently after giving birth to a child? ...Why would they?

**A second option was, indeed, abortion.** Many, *many* recipes for contraceptives and abortifacients are presented in classical medical texts--all the herbal combinations you could want. John Riddle, one of the major scholars working on birth control and abortion in ancient and medieval Europe, even suggests that some may have had at least a slight impact on the probability of preventing pregnancy or producing an abortion. 

Three problems, though: literacy, access to texts, and access to ingredients.

...On the other hand, contraceptives and abortifacients tend to be recipes--whether or not the same ones recorded exclusively by men--passed down or provided by other *women* as oral tradition.

**And then there is That Topic in scholarship**, the one where scholars go round and round in circles: infanticide.

As /u/kooking_pot discusses [in this thread](https://www.reddit.com/r/AskHistorians/comments/4cpy2c/nsfw_how_did_prostitutes_in_brothels_non_get/d1kg6sj/), archaeological evidence from Ashkelon can easily be interpreted as demonstrating a common practice of infanticide. Also not Pompeii, but in the Roman Empire (England), some scholars have suggested that a burial site containing the bodies of 97 babies demontrates systematic infanticide as well. Significantly for our purposes, the general assumption by these scholars is that the burial site/cemetery marks a brothel. Other scholars, Dominic Wilkinson points out, simply see a burial site for infants whose bodies were buried, not cremated.

There is plenty of strong evidence, however, to show that some women certainly left their children ""exposed""--but not necessarily in our view of the little baby on the mountaintop torn apart by wolves. Rome, at least, even had specific locations for parents or their delegates to leave babies they could not or would not raise--think of our Safe Spaces today, even. W.V. Harris points out that the intention was typically rescue, not death, if you consider that infants were often even clothed.

And in literature (which albeit is, well, literature), these babies are indeed often rescued.

**So, as an ancient Roman sex worker with some financial resources who found herself pregnant, a woman had real choices for her body, and perhaps later for her baby.**

~~

Further Reading:

* Ann-Cathrin Harders, ""Roman Patchwork Families: Surrogate Parenting, Socialization, and the Shaping of Tradition,"" in *Children, Memory, and Family Identity in Roman Culture,* ed. Véronique Dasen and Thomas Späth (2010)

* Dominic Wilkinson, ""Exposure and Infanticide in Ancient Rome,"" in his own *Death or Disability?: The 'Carmentis Machine' and Decision-Making for Critically Ill Children* (2013)

* John Riddle, *Contraception and Abortion from the Ancient World to the Renaissance* (1992)",0
"*I'm a communication professional who minored in music in college and specifically did courses in music history and music psychology/affect (and heard this song plenty as a skater at actual skateparks in the early 2000s), but there's an element of this that's sociological and a bit beyond my academic expertise (EDIT: which /u/noelparisian adds some academia for in his excellent answer). I'll do the best I can on the history of music and skateboarding, though:*

First off, I think you may not quite be getting the lyrics: where you say that they paint the ""skaters as low on the pecking order"" all that it explicitly says is that the ballet clique thought the skaters were beneath them. This is an important distinction because it says as much about the dancers as it does about the skaters. That out of the way...

Skateboarding, like punk, has historically been seen as a counter-culture activity all the way back to its origins as ""sidewalk surfing."" Counter-culture in general has carried varying degrees of ""cool"" cachet over the years (beats, hippies, and so on) and skateboarding and punk were no different. The punk movement really started in the 1970s and skateboarding coalesced into the form we know it today (polyurethane wheels and purpose-built trucks) around the same time.

Lavigne's song came out right at the peak of skateboarding's popularity in the public consciousness. Skateboarding finally became shortlisted as a potential Olympic sport in 2015 and made its debut in ~~2001~~ 2021—after the X-Games launched in 1995 which put skateboarding on ESPN for the first time, and the Tony Hawk's Pro Skater video game series launched in 1999 and spread awareness of the sport/activity even further—and it's no coincidence that Lavigne released *Sk8r Boi* in 2002 (ETA:) during the period when X-Games attendance was at an all-time high in the 200,000+ range ([source](https://www.statista.com/statistics/205228/total-x-games-attendance/#:~:text=The%20first%20X%20Games%20in,attended%20by%20111.5%20thousand%20people.)).

One of the best primary sources for how connected punk culture and skater culture were at that time is soundtracks of the Tony Hawk video games: punk rock, ska, hip hop, and alternative rock. Obviously these genres are also associated with other activities, but the music in those games is basically what you would've heard at any skatepark in the early 2000s.

So why is the boy seen as somehow less? Because the girl represents highbrow culture (""she did ballet"") and the boy represents the counterculture. To make a musical analogy, this is like a classical music aficionado in the 1920s looking down on jazz or a middle-America mom in 1958 reacting negatively to Elvis (who took a lot of stagecraft and musical flair previously associated with black artists and set it in front of a white audience). Whether the girl and her friends are right to look down on the boy is open to debate, and the whole point of the song is to take the stance that that idea of a cultural pecking order is flawed because their perceived social class is inverted in adulthood.

In short:

* Punk and skateboarding were seen as related cultures at the time (and to some extent still are) which draw their ""cool"" image as a consequence of being counter-culture movements
* The point of the song is to challenge the mainstream culture's preconceptions about the counter-culture, one of which is that the counter-culture is ""lesser"" in some way

EDIT: Here's the soundtrack listing for games [1](https://tonyhawkgames.fandom.com/wiki/Tony_Hawk%27s_Pro_Skater_Soundtrack), [2](https://tonyhawkgames.fandom.com/wiki/Tony_Hawk%27s_Pro_Skater_2_Soundtrack), and [3](https://tonyhawkgames.fandom.com/wiki/Tony_Hawk%27s_Pro_Skater_3#Soundtrack) as a reference (these were the games which had been released by the time of *Sk8r Boi's* release)

EDIT 2: For a dramatized sense of what skateboarding looked like in the 1970s as it transitioned from ""street surfing"" to the park-focused tricks we think of today, I recommend watching [Lords of Dogtown](https://www.imdb.com/title/tt0355702/). This is based on the real Zephyr skateboard team, who are also covered by the documentary [Dogtown and Z-Boys](https://www.imdb.com/title/tt0275309/?ref_=nv_sr_srsg_3).

If you want to know more about the history of punk music and culture, I recommend reading [Please Kill Me](https://www.goodreads.com/book/show/14595.Please_Kill_Me), which is an oral history of the punk subculture. Getting into whether the music of 1990s and 2000s skateparks was truly ""punk"" is another discussion, but there is still generally held to be a sort of musical lineage even if the newer music isn't truly ""punk"" in the 1970s sense.

EDIT 3: Since this is a bit confusing and I don't clarify (I wrote the original answer just before falling asleep): as with any sport, skateboarding becoming widely popular must necessarily predate its inclusion in the Olympics (snowboarding is a good parallel as it was added in 1998 after the first worldwide competitions coalesced in the mid 1980s; this shows how relatively slow the IOC can be to adopt a sport). The ""peak popularity"" here refers to the period in the late 90s/early 2000s when skateboarding was prominent in popular culture; this is in contrast to earlier periods when it was relatively niche, and certainly in contrast to the 1980s when it experienced a relative lowpoint—there are numerous articles out there that discuss the sales dip in skateboard equipment in the 1980s relative to the 1970s and the resurgence in the mid/late 1990s. I certainly could've explained that better in the original post, but hopefully this clears up why I mention 2015 as an indicator of popularity in 2002.",0
"  [2/2]

As I've gotten more into the history of the black liberation and women's liberation movements of the late 60s/early 70s recently, it's made an enormous impression on me just how radical a lot of the ideas and even some of the changes were, even down to the philosophical level. (A universal childcare bill passed Congress, FFS. Nixon vetoed.) Naturally, the changes made a lot of people very anxious, including many of the traditional power brokers (rich white men). Much has been made of the Republican ""Southern Strategy,"" that is, how to make racism The Issue without making racism the issue. The point of that, though, was to win voters over to a broader agenda of social, economic, and political retrenchment against an evolving society, and politicians were prepared to look anywhere they could.

Abortion was newly on the minds of Catholics following the papal decree *Humanae vitae*, which sought to ban the ""consensus of conscience"" between Catholic women, God,and their confessors on matters of family planning. Nixon is typically credited as the first politician to attempt to lure voters with a ""sanctity of life"" argument against abortion. But even as more rigorous Catholics tended to oppose abortion on theological grounds, other issues continued to drive dominant Catholic voting patterns, not least of all related to the Catholic emphasis on scripture as well as tradition, and individual conscience as well as canon law.

If workaday Catholics weren't yet paying attention, certain politically conservative evangelical leaders *were*. They recognized that they had precisely the voting bloc so sought by their political allegience on a moral and intellectual lockdown. ""Biblical inerrancy,"" the idea that the Bible says one thing and it's what your pastor teaches you to proof-text, was an eager partner to a ""sanctity of life argument."" The modern academic reads ""Before you were in your mother's womb, I knew you"" as a hellenized Jewish text and thinks, THE PLATO, IT BURNS. The evangelical Christian takes it on its own as says, ""God said it; I believe it; that settles it."" At the end of the 1970s, abortion shifted diametrically from that thing those Catholics (hardly Christian! in the evangelical mindset of the time) didn't like, to the driving belief of a large portion of Americans. Abortion, according to the guided-by-proper-authority interpretation of the text they believed was the One True Textbook Of All Subjects, was murder.

This biblical hermeneutic and the necessary orientation of a sect that organizes its philosophy around it, has driven the dominant evangelical/conservative political agenda in its more recent permutations. Same-sex marriage as a political issue is largely a violation of the 20 year rule (protease inhibitors for AIDS had barely been introduced by 1997; Willow and Tara didn't kiss until 2001 although Carol and Susan got married in 1996). Global warming/climate change has is also not really for AH discussion, but you can see the same anti-science, anti-intellectual ideas in the creationism/ID/evolution mixture evolving.

 I stress, in the end, that you can't separate the biblical interpretation method from the authoritarian and reactionary mindset. They are mutually reinforcing. And in conservative Christianity in America, the warm and cozy certainty of authority and preservation of the lost past seen to have worked so well for so long had become cherished ideals. After all, who doesn't want to protect their kids and give them what they see as the best world?",0
"I cannot answer your first question, but I can provide insights on the second.

The idea that America was built on multiculturalism appeared very early. By the 1780s, the term ""melting together"" was a widespread metaphor in use that was meant to positively portray incoming immigrants. Of course, America had just ousted a foreign power from their backs, so nationalism was running high.  Hamilton, an immigrant from  Nevis, argued vehemently against immigration, arguing that immigrants brought pro-monarchy and ethnic views that would undermine their new, fragile country. He also argued that America's bountiful resources and exploding population meant that America didn't need to rely on newcomers. From the Hamilton Papers,  Examination Number VIII, Jan 12th, 1802:

*In the infancy of the country, with a boundless waste to people, it was politic to give a facility to naturalization; but our situation is now changed. It appears from the last census, that we have increased about one third in ten years; after allowing for what we have gained from abroad, it will be quite apparent that the natural progress of our own population is sufficiently rapid for strength, security and settlement.*

At first, the two dominant parties of the time (Federalists and Democratic-Republicans) mostly agreed on keeping out immigrants. Thomas Jefferson wrote in his *Notes on the State of Virginia* in 1781 that foreigners would be anti-Democracy. However, by the turn of the 19th century, the Democratic-Republicans viewed pro-immigration policies as a great way to undermine the Federalists.

See, Hamilton and the Federalists were afraid of the French, who were at the time being led by Napoleon after the bloody French Revolution. Jefferson and the Democratic-Republicans pursued pro-French policy—and more proactive foreign relations with other countries—which included immigration. Jefferson saw immigrants as future voters for the Democratic-Republican party. As with many of these social issues, divisions in public opinion came from the very top, so voters tended to side with their party's views. So pro-immigration stances tended to saturate more educated Democratic-Republicans. The election of 1800 was a particularly nasty affair, as the battle lines between both sides slung serious vitriol to get their candidates elected. John Adams, the Federalist incumbent, faced off a challenge against Jefferson. Adams was pro-class and cultural hierarchy, while Jefferson wanted to model the country's Democracy on the new post-revolution French model.

It should be pointed out that both Adams and Jefferson saw the 1800 election as a fight over America's soul—that the election would set in stone the standards for how America would treat the subject of immigration for the rest of time. Jefferson would later write: *The revolution of 1800... was as real a revolution in the principles of our government as that of '76 was in its form.*

Jefferson and his Democratic-Republican allies would use the melting together metaphor extensively in their campaigning, making 1800 probably the biggest pivot on pro-immigrant sentiment for the general public as self-identified D-R party members adopted Jefferson's messages.

Finally, the D-R party wasn't free from bias, and fought internally (and extensively) over what type of immigration was considered acceptable by them:

*The meaning of the recently popularized concept of the melting pot was subject to ongoing debate which centered on the issue of immigration. The debate surrounding the concept of the melting pot centered on how immigration impacted American society and on how immigrants should be approached. The melting pot was equated with either the acculturation of the total assimilation of European immigrants, and the debate centered on the differences between these two ways of approaching immigration: 'Was the idea to melt down the immigrants and then pour the resulting, formless liquid into the preexisting cultural and social molds modeled on Anglo-Protestants like Henry Ford and Woodrow Wilson, or was the idea instead that everyone, Mayflower descendants and Sicilians, Ashkenazi and Slovaks, would act chemically upon each other so that all would be changed, and a new compound would emerge?* (Baofu, 21-22)

The term ""melting together,"" and other various close iterations of the same idea, was solidified in the public's vernacular as ""melting pot"" in 1908, when the play *The Melting Pot* by Israel Zangwill was released and became popular.

Edit: Thank you for correctly pointing out that Hamilton was from  Nevis, not Puerto Rico.

Sources:

[https://founders.archives.gov/documents/Hamilton/01-25-02-0282](https://founders.archives.gov/documents/Hamilton/01-25-02-0282)

Alexander Hamilton (Lucius Crassus), *Examination of Jefferson’s Message to Congress of December 7, 1801*, viii, January 7, 1802, in Henry Cabot Lodge, ed., The Works of Alexander Hamilton, Vol. 8 (New York: Putnam’s, 1904)

“Alexander Hamilton on the Naturalization of Foreigners.” *Population and Development Review*, vol. 36, no. 1, 2010, pp. 177–182. *JSTOR*, JSTOR, [www.jstor.org/stable/25699042](http://www.jstor.org/stable/25699042).

Blumenthal, Sidney. ""How the Heated, Divisive Election of 1800 Was the First Real Test of American Democracy."" *Smithsonian.com*. Oct 2016. [https://www.smithsonianmag.com/smithsonian-institution/election-1800-first-real-test-american-democracy-180960457/](https://www.smithsonianmag.com/smithsonian-institution/election-1800-first-real-test-american-democracy-180960457/)

Baofu, Peter. *The Future of Post-Human Migration: A Preface to a New Theory of Sameness, Otherness, and Identity.* Aug 2012. Cambridge Scholars Publishing.",0
"While lack of access to spices and flavorings due to shortages and rationing in the late 19th and 20th century as described by u/GrunkleCoffee served as a kind of ""killing blow,"" British cuisine had been known to be bland and terrible long before then. Up until the 18th century we see in British cookbooks a great love of heavily spiced and sugared food. However this school of cooking dropped off throughout the 18th century and had been replaced in the Victorian era with a love of bland, overcooked foods, incorporating as few fresh foods as possible. How did this happen?

* Moral philosophies in the Victorian Era denigrated heavily spiced, flavorful foods as indulgent, irrational, and inflammatory to the senses, while holding up foods that were heavily processed and bland as healthier for the digestion and morally uplifting. Flavorless and mushy food became a moral virtue. You might already know about people like John Kellogg who explicitly linked the consumption of flavorful food to moral decay, specifically masturbation.

* As the British Empire expanded and spices became cheaper and more widely available they lost their power as status symbols. Instead people glorified the French style of high-class cooking which focused on elaborately designed meals with an emphasis on highly refined items requiring intensive manual labor such as decorative jellies and pates. These foods were more attainable now because of industrial technology as well as the expanding ability of the middle class to hire kitchen staff. However they retained their high status because they were now produced with science! Meanwhile fresh ingredients like vegetables and dairy were often adulterated in industrial cities, leading to the following point -

* Fresh food could be rotten or tainted with disease and additives. While in French cuisine fresh ingredients were key to making food taste good, in industrialized Britain they were getting harder and harder to obtain so they substituted them with processed preserved foods. Canned and frozen food had the stamp of approval of modern science and the sheen of new technology, and were embraced in new middle class homes without regard for lost flavor and texture.

* Many of these newly urbanized, middle class families were not able to rely on the peasant foodways which had sustained most of the British population up until then. Housewives had the role as the protectors of the home and morality and took on the load of all the factors I just listed, and were tasked with creating contrived and complex meals to assert their dedication to their family and the kitchen. It was seen as a way for women to be creative as well as scientific in a way that had never really been extended to women before. The emphasis was heavily on appearance and morality, not flavor.

These converging and related factors all led up to the situation described by u/GrunkleCoffee. By the time these rations and shortages happened there was already a culture that idealized bland, refined food and heavily relied on a dedicated kitchen staff. Then war came, further limiting the range of ingredients available and subtracting from the labor force. These rationing systems made the bourgeois palate into rule of law. The traditional bland British food we see today only goes as far back as these ideologies and economic/social changes. 

Also, to include a positive note, Victorian people absolutely loved ice cream and it became much more popular during that time. And ice cream is delicious :) 

Reading:

* *Consider the Fork* by Bee Wilson
* *British Food: an Extraordinary Thousand Years of History* by Colin Spencer",0
"*I'm going to go ahead and put a warning for racism and language here. We're talking about Lincoln, so if talking about racism, racist violence, slavery, or the word nigger upsets your sensibilities then you should probably skip this. I'm not going to be gratuitous but there's no real way to avoid those things.*

Yes, he  was.

Abraham Lincoln was from a working class to perhaps middle class family. His father was a tradesman and early on Lincoln worked at a corner store, which are still roughly of the same social status now as back then. This isn't to say he came from absolute poverty or destitution: his family would be closer to lower middle class, say. But they were certainly not aristocrats even in their own locality, let alone on the national scale. Nor was he raised in the manners, hobbies, or religion of the American elites. Lincoln and his friends got into fights in his youth. When he served in the militia, he didn't make officer. He became a lawyer by reading a law, a pathway that has since  been banned or regulated out of existence. Instead, more elite and professional routes are the only ways to get into law. He was, on the whole, a middle class type at best.

Of course, he had his own talents. Not the least of which was a keen political instinct. But in pure class terms he was not someone who could be expected to rise to the highest office. It was somewhat unusual that he even rose to prominent state office, something enabled by the rural and farmer based society he lived in. In the East or South he would have likely stood much less of a chance. And this was key to his entire political career. Lincoln generally played up his roots. This in turn defined him to many people, either for good or ill.

The second scene in *Lincoln* has some white country soldiers come up to Lincoln stammering about how great his oratory is and how inspiring he is. That's fairly accurate. (Less accurate is their referencing the Gettysburg Address, which was not famous at the time. It's possible just not likely.) Lincoln was a noted orator who spent a lot of time on rural circuits. He was ""one of them"" to rural, low church religious white farmers. Those were the people who put him into office early in his career and remained a key constituency long after.

They were the solid base of support that propelled him to the Republican nomination. At the 1860 Convention, Lincoln didn't win any voting rounds until the last one. A candidate had to win a majority of present delegates. Lincoln came in second in the first round, behind Seward. Lincoln's supporters were the representatives of rural voters in the Midwest, Appalachia, and northern New England (Maine and New Hampshire). There were three more votes after that and this core base of support not only stayed loyal to him but expanded to include places like Kentucky, Kansas, and Connecticut. It was only in the last vote that a few urban states turned to Lincoln. And even then, most of them didn't. DC's Republicans, for example, voted for Seward.

Seward was just as anti-slavery as Lincoln. But he was better educated from a much more aristocratic background. So were most of Lincoln's opponents for the nomination. In fact one of them, Chase, was a lawyer like Lincoln but from a much more respectable background. Lincoln would appoint him to the Supreme Court to both get an avid anti-slavery advocate on the court and to neutralize him politically. Out of all the serious candidates at the 1860 Convention, Lincoln was undoubtedly the least wealthy and educated and generally aristocratic. This was true in the general as well. This was an advantage electorally but not socially.

The cities, meanwhile, were almost all Democratic. You will note the cities are where most playhouses and high art flourish. Though certainly not *all* art. For example, Lincoln's first *stage* play was in DC but not his first play. As you'll note, living in a city is definitionally the opposite of a bumpkin. There's a point to be made about prejudice here but that's for another time.

This Democratic affiliation meant urban politicians and the urban elites had, for decades, been in an alliance with slaveholders. For some it was an alliance of convenience and an increasingly uncomfortable one. A few broke broke ranks in 1856 and 1860 and voted Republican over the issue of slavery, most notably Boston. But most Democrats, and by extension most cities' politicians, were at least moderately pro-slavery. The difference between the Northern Democrats and Southern Democrats were how pro-slavery they were and their acceptance of radical action to preserve slavery. But neither was Abolitionist. New York was particularly noted to be a hotbed of pro-Southern sentiment. And, then as now, was noted for its arts scene. (Remember, John Wilkes Booth was an actor.)

DC was another very pro-slavery city. While they didn't have the ability to vote in presidential elections, they largely supported Breckinridge, the most pro-slavery option. There was no significant Republican presence in the city on Lincoln's election and Lincoln had so little confidence in the city's loyalty that he hired private security to help him get into the city for his inauguration.

Often US textbooks mention the belief of Abolitionists that the government was controlled by a sinister ""slave power."" But they often do a very bad job of describing why Abolitionists thought that. Abolitionists, searching for a reason they hadn't been successful despite winning elections, knew that many American elites and the important cities like DC were pro-slavery. Slave power was the idea that pro-slavery advocates used bureaucratic, partisan, and cultural power to pressure or persuade Abolitionist politicians to not push the Abolitionist cause. Or, failing that, to sabotage their initiatives.

A classic (and charged) example was the tendency of American social climbers to convert from low church religions like Methodism to Episcopalianism, the traditional religion of the American elite. Episcopalianism was one of the more slavery tolerant denominations. In a time when a lot of politics was done from the pulpit, the transition from an anti-slavery to a moderate or pro-slavery church was significant.

Lincoln failed to make these transitions, failed to conform to standard elite expectations. And this was a political advantage for him, especially among Abolitionists. This was by no means a unique innovation: other American reformers like Jefferson and Jackson had similarly struck a populist pose. But it represented a challenge to the elite consensus of the time, including the cultural consensuses that made up the difference between the urbane and the gauche. And like previous reformers there was a reaction, including culturally against his supposed coarseness.",0
"Frank (Frances) Folsom's age at her marriage to Grover Cleveland was something noted by the press, absolutely. However, any criticism seems to have been rather mild.

One article, printed in multiple newspapers including (here) the *Indianapolis Sentinel*, [merely said](https://newspapers.library.in.gov/cgi-bin/indiana?a=d&d=IS18850322.1.2):

> She is the daughter of tbe former law partner of Mr. Cleveland, and is spoken of as a lady of great brilliancy, and one who would be an ornament to the White House and to society as well. 

> She is perhaps a trifle young for an old fellow of forty-eight, for as the President celebrated his birthday on Wednesday, it is hardly worth while to try and conceal his age longer. 

>Her age, if so delicate a subject may be touched upon, is probably about half that of the President. 

> She is spoken of as very handsome...

Even when a newspaper column mentions her age, there is nothing said about their earlier relationship. I'm especially a fan of [this clip from an Iowa paper I found online](https://www.newspapers.com/clip/2437041/newspaper_piece_about_frances/), in which the author discusses Frank's *childhood handwriting* and even her childhood nickname with nothing but approval for the woman...and comments (apocryphal or not) about how her handwriting was setting the trend for local women.

Grover's 1923 biographer, Robert McElroy, repeats an unsourced, possibly apocryphal story about the president's reaction to the rumors circulating in advance about his possible marriage...to Frank's *mother*:

> I don't see why the papers keep marrying me to old ladies all the while--I wonder why they don't say I am engaged to her daughter.

I am not sure I would take this particular story seriously, especially because McElroy doesn't cite it beyond ""an old friend and her daughter."" On the other hand, I've never seen any disagreement that the press *did* expect Grover to marry the widow Emma Folsom, not her daughter.

This makes some modern scholars' argument that Grover began a romantic relationship with Frank and (especially) married her as an election/reelection strategy...hm, interesting. On the other hand, that such an argument can be made in the first place likewise points to a general public acceptance.

So you can see the marriage did not quite seem the most *natural*, but wasn't a widespread problem. The media's utter infatuation with Frank herself might have helped matters, as probably did the fact that Grover had already beaten one sexual scandal (a) by owning up to it, and (b) it still being less bad than the competition's political sins (c) probably because the parts of the sex scandal that reflected truly poorly on him depended on the word of a *woman*.

And by the time Frank Cleveland was departing the White House along with her husband, ""a trifle young"" had become a *good* thing:

> A modest, robust, enthusiastic girl she entered the White House. The entire nation was interested in her. It delighted in her youth and beauty.

...It is, however, noteworthy that eleven years later, the *Chicago Chronicle* was remarking not just *on* her youth at the time of marriage but remembering that people had *talked* about it.",0
"I can’t comment on Nader specifically, but the 1980 “cutoff” date for decreased US regulatory activity is largely the result of Ronald Reagan winning the presidential election that November and the ascendancy (largely but not exclusively due to the latter) of the libertarian rooted economic conservative movement in Washington.

The story of the conservative movement is complex and features a great many characters, but it is important in understanding the rise of Reagan as first governor and later president. Essentially, the conservative movement slowly grew out of a fundamental opposition to the economic activist policies of the New Deal by a select few who began to advocate for restrained government action in general. Fueled by anti communist fervor and by what many perceived to be US foreign policy setbacks (later taking the shape of opposition to the Johnson Administration’s lackluster performance in Vietnam) and spearheaded by the efforts of dedicated intellectual conservatives such as Clarence Manion,the conservative cause transformed into a veritable movement in the late 50s and early 60s featuring a highly organized (but still largely decentralized - as seen with the rivalry between various groups who each considered themselves rightful conservatives (e.g., John Birch Society vs. YAF)) superstructure replete with think tanks, youth societies, political action committees, and media organs (e.g., the National Review). This movement’s first major initiative was the successful draft of Barry Goldwater for the Republican nomination for President, who subsequently lost in a landslide to LBJ in 1964 but did not take the conservative cause with him, a fact that would be made clear by Nixon’s win (with heavy appeal to conservatives and their new positions re: states rights and race) 4 years later and Republican control of the White House for the next quarter century with the brief exception of Carter who ill come to later.

Having campaigned hard for Goldwater in 64, Ronald Reagan, the charismatic ex-actor and New Deal democrat turned conservative Republican, became a popular face among conservatives. This association with the movement enabled Reagan to win the governorship of California in 1967 and hold the office until 1975 (at the time California and much of the West was a Republican bastion and a fairly conservative state outside of some liberal circles such as Berkeley and the Bay). While not putting into effect much of the conservative economic agenda in California (he actually raised some taxes and increased spending) conservatives saw success on the social front as Reagan distanced himself from homosexuals, refused to enact liberal abortion laws, and cracked down on anti war protests which were becoming increasingly concerning to conservatives. 

In between Reagan’s governorship and his final and successful bid for the presidency in 1980, a couple of important developments occurred that greatly diminished the economic and foreign policy situation facing the US, all under the first democratic president in nearly a decade, Jimmy Carter. First and perhaps most important was the bastard economy that existed throughout the 1970s called “stagflation” (a stagnant or a depressed economy with heavy and sometimes rampant inflation - an apparent violation of economic norms which traditionally see inflation occurring in times of growth and deflation in times of recession). While explanations for this stagflation are many and diverse (the inflation caused by the end of the gold standard by Nixon and the 1973 Arab Oil Embargo mixed with government price controls reducing growth and leading to layoffs as companies dealt with higher input costs without being able to offset these in higher prices, being a popular explanation) what is sure is that it represented an economy in recession which was unable to be repaired by the New Deal-esque government intervention called for by Keynesian philosophy and espoused by most old school democrats. As such, the public’s faith in the regulatory and activist state decreased in the 1970s laying the groundwork for Reagan’s economic conservative run in 1980. Adding to this situation was an apparent softness of the Carter administration abroad that enabled communist gains in the 3rd world, most notably the USSR’s (up until then largely successful) invasion of Afghanistan in 1979, and an erosion of American influence more generally, most notably the 1979 Iranian revolution which deposed a major ally and the subsequent hostage crisis which continued to dog Carter’s late presidency and was a major factor in American antipathy towards his administration.

Together, all of these enabled Reagan to win a relatively easy (few Republican challengers were able to more than superficially contend with him in the Republican primaries, with the most significant being his later pick for VP, HW Bush) and famous land slide victory in 1980 which importantly led to a substantial coattail effect and a heavy Republican win in the Congressional elections, giving the party, largely bolstered by the conservative agenda, their first majority in the Senate since the Eisenhower’s first term and their longest hold (6 years) since before Great Depression. 

Once Reagan entered office, he essentially changed the game in terms of regulatory policy and taxation. He substantially reduced the amount of federal regulations during his time, illustrated by the fact that the number of pages in the Federal Register - a rough barometer of the extent of federal regulations - was reduced from 87,012 in 1980 to 53,376 by the essential end of his second term in 1988. The improving economy under Reagan after the 1982 recession in turn led to a new public perception of government regulations as hampering growth, itself translating into a reticence for advocating for such policies in congress for fear of voter backlash. Similar situations occurred with taxes which became a taboo issue and are credited with costing HW Bush a second term after he broke his campaign promise and raised taxes in an 1990 omnibus appropriations bill to reduce deficits. Indeed, Reagan’s legacy has loomed large with the new breed of democrats (even down to Joe Biden today who only claimed he would raise taxes for the very rich making over $400k) to eschew large scale regulations and tax increases, as seen with Clinton’s famous declaration that “the era of big government is over” and even Obama having to mask his tax increase in the language of the ACA’s “individual mandate.”  

Sorry for any formatting errors, on my phone. Sources and Further Reading:

Andrew Busch. Reagan’s Victory (for the 1980 election and the immediate background and a brief overview of Reagan’s Presidency)

Rick Perlstein. Before the Storm (for Goldwater and the conservative movement).

Edit: corrected Reagan’s coattail effect, Republicans only gained control of the Senate, not the House, although they gained significantly in the latter as well.

Edit: thanks for the silver kind people of Reddit!",0
"Gangs of New York has *many* historic inaccuracies, and this scene in particular is one of them. First, it perpetuates a popular myth that immigrants, especially Irish Catholic immigrants, were the bulk of rank and file in the Union army. To quote James McPherson:

>""The substantial number of immigrants in the Union army gave rise to longstanding southern myth that ""the majority of Yankee soldiers were foreign hirelings."" But in fact quite the opposite was true. Immigrants were proportionately under-represented in the Union's armed services. Of some two million white soldiers and sailors, half a million had been born abroad. While immigrants therefore constituted 25 percent of the servicemen, 30 of the males of military age in the Union states were foreign-born. Despite the fighting reputation of the Irish Brigade, the Irish were the most underrepresented group in proportion to population, followed by German Catholics. Other immigrant groups enlisted in rough proportion to their share of the population.""

McPherson notes that those who enlisted or were drafted into the Union army tended to be disproportionately Protestant and skilled laborers or farmers, not working class Catholic immigrants. Which would make sense because especially in a place like New York City the latter groups tended to be extremely loyal to the Democratic Party and suspicious of a war effort largely run by Republicans such as Lincoln and Seward. Indeed, it was the Democratic Party that promoted the ""rich man's war, poor man's fight"" slogan in political debates over passing the 1863 Draft Bill (the passage of which led to the July 1863 Draft Riots in New York City, which are depicted in a historically distorted and whitewashed form in the film). For what it's worth, according to McPherson 98 percent of the men drafted in heavily Irish-immigrant districts ended up paying commutation fees or hired substitutes (mostly though Democratic political machine Tammany Hall funds).

That's some background. I slightly sidestepped the question as to whether the scene in the film actually happened. It seems like Scorsese is depicting the scene mostly as a metaphor for the myth mentioned above, ie, that immigrants ""fresh off the boat"" were signed up to serve in a meat grinder (hence the coffins being unloaded right next to the fresh volunteers), so I don't think the viewer is supposed to take the scene as the literal truth.

I've downplayed the role that Irish immigrants played relatively speaking in the Union army, but (as McPherson himself admits), there were notable exceptions, most notably the Irish Brigade, raised in New York by Capt. Thomas Francis Meagher and Corcoran's Legion formed by Colonel Michael Corcoran (both were Irish immigrants and connected to the Fenian movement). 

It looks like the recruiting scene is a reference to recruiters outside Castle Clinton: the National Parks Service said that recruiters were located here to recruit recent Irish immigrants, but they are vague as to if this was before the Civil War, or before. 

However, Irish historian and archaeologist Damian Shiels (who wrote *The Irish in the American Civil War*) looked at the recruiting in this area and found a letter where the New York Commissioners for Emigration explicitly banned the New York County Volunteers Recruiting Committee from recruiting in Castle Garden (specifically because it would spread the idea that new immigrants were being duped into joining the Union Army, and the bad international press this could cause). 

Shiels' takeaway is therefore: 

>""The more I investigate the Irish experience, the more apparent it is that the type of incident portrayed in Gangs of New York rarely, if ever, occurred. Far from being duped, it was much more likely that many of these men had travelled to the United States with the express intention of joining the military, in the hope of benefiting from the financial rewards available for doing so. This was the primary motivation for Irish enlistment in the Union Army from at least 1863 onwards. These men were not stupid- they came from a country where enlistment in the British Army for economic reasons was commonplace, and they came informed about the Civil War.""

Sources: 

James McPherson. *Battle Cry of Freedom: The Civil War Era*

National Park Service. [""Irish Soldiers in the Union Army""](https://www.nps.gov/articles/irish-soldiers-in-the-union-army.htm)

Damian Shiels. ""Gangs of New York: Recruiting the Irish 'Straight Off the Boat'"". From his [website](https://irishamericancivilwar.com/2014/07/28/gangs-of-new-york-recruiting-the-irish-straight-off-the-boat/). ",0
"There’s a lot that can be said about marriage in Tudor times, which looked different depending on what social class you belonged to. My initial response is thinking through you, as a wife dissatisfied with the lack of physical attention, could argue your way out of a marriage if it was never consummated; the most famous example from your timeframe is from King Henry VIII, who used it to annul his marriage to Anne of Cleves.

but wait! Tudor was a Christian time (albeit changing denomination loyalties)! Clearly a wife needs to be married to enjoy her wifely duties, no?

Technically yes, but in reality such stark rules regarding sex (ie it’s only ever done after marriage) were ignored to a high degree. This time in England saw countless shotgun weddings, with estimates placing one in three brides as pregnant by Elizabethan times. The ladies enjoyed sex quite a bit and this was no secret — when the 52 year old king of France married a teenager, and then passed away a mere three months into the marriage, sympathies laid with him, for she must have worn him out in the bedroom for his death to come a-knocking.

If a wife wants to know her options for fulfilling her sexual appetites (besides, of course, extramarital affairs) that part of the good book you bring up wouldn’t have helped her much. Women’s rights in Tudor England were inferior to a man’s so that part of the Bible verse was generally ignored, a pastime most Christians have indulged in throughout recorded history. Yet Tudor England did allow for the nuclear option of divorce in some narrow circumstances, even allowing the woman in the marriage to bring it forth! If the husband was denying his wife sex because he became impotent, that would be grounds enough for divorce (though the onus would be on the wife to actually prove it). 

Of the four other reasons for divorce - including the aforementioned never consummating it, cruelty, adultery, and constant quarrels - the wife, if she was a persuasive sort, might have tried to argue it was cruel to leave her sexually starved, or that they argued constantly over how frequently their bedsport took place. I say “argue” because divorce was uncommon and never guaranteed; as one historian puts it, “death was the only sure release from unhappy marriage”.

Divorce being a non-starter in most cases, it’s not like the law forces husbands to perform when they don’t choose to. Our theoretical wife in question doesn’t have access to modern-day couples therapy; the closest she could get is discussing matters over with a priest, who, if he too believed strongly in the biblical command to a wife’s sexual pleasure, might seek to help them get livelier in the bedroom. This could work as the clergy had certain beliefs about a woman’s needs: “The Church believed that women were constantly craving sexual intercourse and that if they did not have sex they could get very sick. Thus it was important that after marriage a man had sexual intercourse with his wife not only to produce children but to keep her under control.”

But generally two things would resolve it before that took place: one, spouses were very close with both holding great power so they could simply talk things through to find an optimal, sexy solution. Two: while sex was important, it wasn’t a cultural priority. Love was not seen as this enduring lifelong state. When love (which encompasses sexual passion) faded, you still had to stay together no matter how dissatisfied you were. Which is to say, your sexual pleasure did not come close to what you prioritize in a marriage. If this wife has an otherwise agreeable marriage and relationship, she would have been expected to simply soldier on. After all, we all have our crosses to bear; this one may as well be hers.

Sources:
[1](https://www.tudorsociety.com/marriage-in-tudor-times-by-sarah-bryson/)
[2](https://www.historyextra.com/period/tudor/love-and-marriage-in-tudor-england/)
[3](https://books.google.com/books/about/The_Hidden_Lives_of_Tudor_Women.html?id=mv0oDwAAQBAJ&source=kp_book_description)",0
"There is no direct evidence or recorded statements indicating that imprisoned Nazi leadership expressed a sense of stunned relief after the atomic bombs were used on Japan. It is important to note that by the time the atomic bombs were dropped on Hiroshima and Nagasaki in August 1945, many high-ranking Nazi officials were already captured or dead.",1
"Although it's certainly true that the Mongol invasion enormously disrupted the various Arab societies they clashed with, particularly in mesopotamia, I want to start by saying that there is very little you can do to an irrigation system in a single invasion that can't be fixed at some point in the next 800 years. The Mongol invasions have acquired a very imposing and specific  reputation in modern pop history and culture - the Mongols were beastial, barbaric opponents of civilisation who took thriving, complex civilisations and just kind of tossed them onto a pile of skulls. There is *some* truth to this, of course, but it's not even close to the whole story here.

So to understand what happened to Mesopotamia you have to understand a little about how it's irrigation systems worked historically. In spring as the snows start to melt, huge quantities of water thunder through the rocky, bumpy terrain of southwest anatolia, picking up speed before crashing into a vast, flat expanse of grassland called the *sawad*. Although this provides a huge amount of water and allows very complicated irrigation systems, it is also a system very prone to failure if not maintained. The flatness of the land and the force of the rivers makes it very easy for canals to burst their banks and spread out across the *sawad*, forming permanent marshes instead of nice, arable, irrigated land. What this essentially means is that Mesopotamia can support huge populations when there's a central government powerful enough to maintain these canals, whereas when the power of this central government wanes, the land becomes much less useful for agriculture. This is compounded by the fact that the *sawad* is bordered by the Arabian desert, full of nomadic pastoralists who'd be more than happy to bring their livestock to those marshy wetlands caused by canal failure, so not only does the failure of central government lead to weaker irrigation systems, these weaker irrigation systems cause the land to slip even further out of that governments control - a vicious cycle that can cause massive calamities in mesopotamian societies.

This had happened once long before the Mongols - as the Sassanid state began to decline in the late 6th and early 7th century, Arab pastoralists began to move into this newly available land. These pastoralists obviously had little interest in the whims of Ctesiphon and even less in maintaining an expensive system of canals, so we see a massive decline in mesopotamian irrigation until a central authority strong enough to reorganise the canals comes along in the form of the Abbasid caliphate in the 8th century. The Abbasids poured resources into Mesopotamia (their own centre of power) and by the so-called Arab Golden Age the canals were largely back to their former glory. 

So what we have here is a pretty simple story. The canals are in good shape under a strong central authority, that central authority starts to decline leaving the canals to decline with them. As the land turns to Marsh, nomadic pastoralists begin to settle the region, reducing the population density greatly. If we move on to look (finally) at the Mongols, I hope you'll see a pretty similar story. 

By the 1200s, the Abbasid caliphate had long been in a terminal decline. Contrary to the popular narrative of a civilisation in bloom laid low by a horde of barbarians, the Abbasids of the 13th century were a shadow of their former selves. Bagdhad's grip on territory was slowly outsourced to small landlords and petty chiefs, leaving the central government with little control of the countryside. In this period, as in the 6th century, we also see huge movements by Arab pastoralists from the peninsula. Though this movement happened all over the Arab world (its in this post golden age period that we begin to see demographics in the Arab world become more obviously 'Arab' in character.) it was very severe in Mesopotamia where dozens of Arab tribes settled in plentiful wetlands of the *Sawad*. Its likely that at this point Iraq's population was already decreasing, but hard numbers are very difficult to come by for this era. 

... and then the Mongols arrived! The Mongols removed that final tiny sliver of central authority in Iraq, forcing the Abbasids out and into Egypt where they'd live for the next few hundred years. This was the death knell for a system already essentially laid low by centuries of neglect. After this, mesopotamia becomes a prize for outside power brokers rather than a home to central authority in and of itself. Local powerbrokers and landlords ruled the area on behalf of more important power centres in Persia and Anatolia, which left the canals in a constant state of disrepair. 

So realistically it isn't anything that the Mongols did themselves to the canal system that caused the massive collapses in Iraqi population. Rather, a familiar story of a weakening central government left a very complicated and failure-prone system in such an abysmal state that the hit it took from the Mongol invasion was sufficient to cause a near permanent system collapse, which was compounded by the already ongoing shift of regional power centres away from Iraq and into Persia and Anatolia from the 11th century onwards.

Edit: Some sources and fixes

 A History of the Arab Peoples, *Albert Hourani*

Empire and Elites after the Muslim Conquest: The Transformation of Northern Mesopatamia, *Chase F. Robinson*

The Formation of the Islamic State, *Fred M. Donner*

Land reclamation and irrigation programs in early Islamic southern Mesopotamia: self-enrichment vs state control, *Peter Verkinderen*",0
"I wish I could answer all these questions, they are fascinating in themselves. Sellers, and other historians who study the industrialization period, touch on some of these aspects, but they don't (I believe) cover it in the detail you're asking for, unfortunately. In addition to Sellers, I would check out *What Hath God Wrought* by Daniel Howe. The two of those represent a great *general* start to industrialization in America, and they approach the subject with more breadth than depth; still, you can probably follow up some of their sources to more specifically psychological or sociological studies.

What I can also say is that this was likely not a wholesale change in an obvious way at the time. Historians pick and choose the starting point of these processes, and though ~1840 is around the time *usually* chosen for when the industrial economy was running at full gallop, the fact is that these were complimentary and overlapping processes that weren't occurring everywhere at the same time. There are examples of what we'd consider labor riots in decades prior, and there were examples of work stoppage, resistance, and modes of behavior that we might view as being ""medieval"" in decades well after. There's also a big difference in how an economist might approach this subject versus how a historian does, etc. 

But I hope someone else might come by to respond to you, because I'm curious too!",0
"To my knowledge, Dwight D. Eisenhower's German surname was not the object of widespread distrust or suspicion that he might be a secret Nazi. While it is true that his German heritage was acknowledged, it did not lead to the same level of xenophobic innuendo as Barack Obama's middle name did during his presidency.",1
"Many would summarise it in two words: Bruce Lee.

The full story is a little more complicated. Bruce Lee rode the crest of the craze, and helped drive it to its peak, but the craze began before him.

Asian martial arts had started appearing in cinema and television already in the 1960s. For example, judo in *The Avengers* (the British TV series), karate in *The Manchurian Candidate* and *Goldfinger* and ninjas in *You Only Live Twice*.

Meanwhile, in Hong Kong, the Shaw Brothers revitalised the kung fu film in the late '60s, starting with the hugely successful and sequel-spawning *One-Armed Swordsman*. Shaw Brothers saw they were onto a good thing, and continued in the genre.

Next, Warner Brothers, in some financial difficulty, gambled on distributing some of these Hong Kong movies in the US - a cheaper gamble than making new movies. They were the first major studio to distribute such movies in the US, beginning with *Five Fingers of Death* (AKA *King Boxer*) in 1973 (first releasing it in Europe, and then in the US). *Five Fingers of Death* stayed in the top ten box office hits for months - Warner Brothers' gamble had worked. The Shaw Brothers had competitors: Golden Harvest was providing stiff competition for them, and Warner Brothers distributed Golden Harvest product, too, also with success. Warner managed three simultaneous hits, with *Five Fingers of Death* still in the top 5 after 2 months, *Deep Thrust* (AKA *Deep Thrust: The Hand of Death* AKA *Lady Whirlwind*) from Golden Harvest at number 2, and their most recent release, *Fists of Fury* (AKA *The Big Boss*), also Golden Harvest and Bruce Lee's first Hong Kong movie, at number 1.

*Five Fingers of Death* wasn't widely advertised, and word-of-mouth played a big role in its success. The kung fu craze was not just a product of marketing - clearly, people liked the movies. Box office success and popular visibility grew with successive movies, with Bruce Lee's US movie debut, *Fists of Fury* followed by his even more successful *The Chinese Connection* (AKA *Fist of Fury*), which was a major hit in Hong Kong, Europe, and Japan as well as in the US. The kung fu craze was running hot. Warner Brothers co-produced *Enter the Dragon* with Golden Harvest, for an even bigger hit. For Bruce Lee, this was a posthumous hit, with the US release about 1 month after his sudden death.

Imitation followed success, and many more kung fu movies hit the cinemas, many of low quality, many Brucesploitation movies (starring Bruce Li, Bruce Le, etc.), and with the genre no longer fresh and new compared to what had come before, the craze faded in the box office. It persisted longer in popular culture, crossing over into comics, toys, etc., and drove the popularity of kickboxing. Many young people looked for martial arts training, and found it. Following WW2, there had been growth in the teaching of Asian martial arts in the US, with ex-servicemen who had learned karate and judo in Japan starting to teach and immigrants from Asia starting to teach. Martial arts schools grew, and some trained to sometimes dubious levels of skill and went off and started their own schools. Karate and other Asian martial arts moved from being an often rather rough hobby for adults into much more of a youth activity, eventually leading to modern strip-mall karate/TKD where 3/4 of the students are young children.

While the kung fu craze died down, it didn't go away completely. It came back in a new form in the 1980s as a ninja craze, again driven by cinema, with *Enter the Ninja* an early contender in 1981. A significant part of the ninja craze was the ""white ninja"" sub-genre, exemplified by the commercially-successful and influential *American Nina* (1985), and this new craze was much more driven by American product than the earlier kung fu craze which centred on Hong Kong movies. The Hong Kong makers didn't ignore the craze, and churned out quantities of product to meet the perceived demand (including movies of amazing low budget and quality). However, while ninjas became popular, ninja movies didn't have the same kind of box office dominance seen in the '70s when *Fists of Fury* sat at number 1, and *Enter the Dragon* was a huge hit. Similar, when martial arts cinema had its next surge in the West, driven by Chinese movies like *Crouching Tiger, Hidden Dragon*, the movies were successful, the best of them being very profitable, but they sat amongst other successful and profitable movies rather than sweeping all before them.",0
"So the stage is set, the body is in the street, bloodied dagger some yards away, and you, the guilty party, have fled across town (probably not all that far truth be told given the size of London at this time [though it is still by far the largest city in England]).

Now the time frame that you've specified places us in Norman England though early enough that the majority of the populace was still using Anglo-Saxon law codes and practices; we don't really get into major Norman overhauls of the legal code until the 12th century.  London is a little bit of a weird case, but we can be general enough to lay out some possibilities.  

For the sake of simplicity I'm going to assume that you and your victim are both free (slavery was still around legally for at least 5 more years in England) able bodied men engaged in some local craft.  So you've killed someone, left the body in the streets, and made it back to your dwelling seemingly undetected, what happens next?  Well obviously someone will find the body in the morning and report it to a local authority.  Who would that be?  

Now for much of the world until modern times there was no independent (ostensibly) and investigative group of institutions that sought to enact, enforce, and execute the law. Modern media translates modern municipal police departments into medieval-esque institutions such as the ""town guard"" (think the *Skyrim* Hold guards), but is that how it worked in the actual Middle Ages? In short, no.  Nor would soldiers of any description be permanently stationed within the city of London at this time for the express purpose of hunting down and investigating criminals.  So the people who are going to be looking for you are not the medieval equivalent of detectives and beat cops.  Instead word will likely spread until someone directly connected with the victim can identify the body.  Though people might simply know them based on sight depending on how well known the person was in the community.

In Anglo-Saxon England vigilantism, not a town guard or much less police force, was more or less the ""default"" method of conflict resolution between equal members of society. There was no real distinction between vigilante justice and ""official"" justice, nor did independent courts, lawyers, etc... exist at this point in England's history. Especially in the Early Anglo-Saxon period our ideas of objective justice and an independent judiciary/police force just simply were not a part of society. Justice and revenge were extremely interlinked, and the line between official prosecution and a lord taking justice ""into his own hands"" was significantly blurrier than we might imagine. 

So the kinds of people who would be investigating the murder would be the people who were responsible for/to the dead person in some way.  These could be their surviving relatives such as brothers, sons, fathers, in-laws, or it could be someone connected to them by social/political bonds, if you've killed a thane or some other dependent of a local big wig he might get involved and round up some others to look around.  Let's say that your hapless victim's family is made aware of their death and have rounded up some cousins, people who owe them favors, and others to look around and find out what happened, your victim's local patron has also been notified.  So these people who were tied to the victim in some way would then start essentially an investigation, this could be questioning potential witnesses, tracking down people known to have had problems with the deceased and so on. 

But how could these people find you specifically?  You've gotten away scott free, or so you think.  While you may have discarded the weapon and left the scene of the crime unseen, did you arrive home unseen?  You probably don't live alone in a medieval city so who saw you come into your dwelling at some ungodly hour?  Did they see the blood on your clothes that you surely have from the murder (if you are not extremely wealthy you probably don't have tons of spare clothes lying around to change into)?  Do you have nosy or chatty neighbors who might rat you out?  And let us not forget the murder weapon!  

You like any other self respecting Anglo-Saxon well to do man probably have a knife on you most of the time, and its probably what you used in the murder.  So know you're out a knife that people might recognize as yours that is alongside the dead body, and your neighbors might be a little suspicious if you were making a noise at some freakishly early hour.  So things aren't looking good, but this is all circumstantial right?  Well...that might not matter much

All of this would add up and make you a figure of suspicion, but in the absence of damning obvious evidence what would happen next?  Unfortunately for you, your personal weapon has been found at the scene of the crime and one of your neighbors recalls that you came home very late and in something of a rush with blood all over your clothes.  So things aren't looking good.

Your victim's brother formally brings a suit against you at the next meeting of the local hundred meeting (basically a monthly community court/assembly) and accuses you of murder, what happens next?  

Well this depends a great deal on a variety of factors.  Do you want to admit that you killed the man, but there were extenuating circumstances?  Did he owe you a great debt?  Had he recently insulted you?  Was he involved in a crime against you or your family?  All of these factors would be taken into account and could change both the way the ""trial"" is handled and potential consequences you might face.  In that case local notables such as important peers such as other craftsmen, the local churchmen, and a representative of the king would likely convene and decide on a sentence that befits the crime and any extenuating circumstances.  This could be exile, the death penalty, or if the extenuating circumstances were strong enough, you could instead be ordered to undertake penance instead.

But lets say that you deny everything, you've never shared more than a word with the man, your knife was stolen a week ago, and you were arriving late because you were out and about on business.  So its your word against the word of the people accusing you.  

Now this is where it can get complicated and it really matters who you know and how much they like you.  You have the option of essentially calling in a series of character witnesses to swear that you would never be involved in such a heinous crime.  These kinds of people would be your relatives, in-laws, your patron and his networks, etc...  If you are a man of good repute with no prior black marks against your honor, and able to call in some favors from your own influential patrons, this will probably be the end of the matter...unless it can be demonstrated that you're lying.

This would be a huge problem for you as you have now sullied not only your own honor by lying, but the honor of those who stood up for you.  This could entail its own punishment (likely some form of mutilation of your face or mouth alongside not being able to call witnesses or bear witness in court in the future)

However if you cannot call in these favors, or if you have previously run afoul of the law before, the number of witnesses you need will be higher and you might be in trouble.  You can appeal to an ordeal instead if you'd like to try and play for time or put the matter in the hands of God, but the Church and local authorities might not allow this if they think you are plainly guilty and trying to get out of punishment.  The ordeal is a process by which you must plunge your hand or (if you have an ill reputation) your arm into a cauldron of boiling water and retrieve a stone.  The would would be examined some time later to determine if God had said you were guilty or not.  Now this is actually probably not what would actually occur however.  If you invoked an ordeal, it was more like declaring an appeal to the Church according to Peter Brown, and in the time before the actual ordeal itself the Church would try and find some agreement between the two parties.  

This could be something like a monetary payment, dependent on the social status of the person you killed, and an agreement to undergo penance as well as swearing an oath to not pursue the matter further or try to take further violent action.

So if you decide to not go with an ordeal, the assembly will likely render a judgement based on the evidence presented, the witnesses (both in the sense of actual witnesses and character judges/patrons), and taking your own history into account.  The end result, especially in Norman times, is more likely to be physical punishment up to and including death, rather than the earlier practices of paying wergeld (literally man price, or the amount of money that was attached to people depending on their station in life).  Now depending on if you've actually been physically restrained or apprehended, you might try and escape, and this would have you declared an outlaw, able to be killed on sight by anyone, little more than a wolf as far as the law was concerned.

Now notice what isn't mentioned here, evidence, trials by lawyers, juries of your peers, judges evaluating evidence, precedent, and case law.  All of these hallmarks of modern law quite simply did not exist.  Indeed, the actual fact of your guilt isn't really as important so much as your ability, or the suing party's ability, to claim innocence and have it backed up by other members of good standing, the actual facts, evidence (rudimentary as methods the of collecting it were), and so on.  What matters is not your actual guilt, but your ability to be convicted based on your social reputation and that of your victim.",0
"The issue isn't with AskHistorians. People should instead be complaining to reddit and getting them to change the freaking comment count to whats actually in there. Its incredibly stupid to push these problems off on the subs to have to try and find a work around, when the big issue is on the sites end.",0
"This is an honest and good question. Thank you for asking it. As with almost anything, it will depend on the exact time period, your social class, your employment and your geographical location. Speaking extremely broadly, in western Europe and the United States, **male facial hair grew in popularity as the 19th century progressed, peaking in the 1870s, then declined toward the end of the century.**

There are two principal sources that I would point you to: Rebecca Herzig's fantastic 2016 book *Plucked: A History of Hair Removal*, and a 2005 paper by Christopher Oldstone-Moore, entitled ""The Beard Movement in Victorian Britain"". This was published in the Autumn 2005 issue of *Victorian Studies* and is almost exactly what you're looking for. Oldstone-Moore subsequently published a 2015 book entitled *Of Beards and Men: The Revealing History of Facial Hair* that is also worthwhile but less focused on your particular subject.

Herzig's book is necessary because it provides historical and social context that Oldstone-Moore doesn't have the space to give, even in a 27-page journal paper. 

The bottom line is that from the 1840s onward, the beard (and facial hair in general) is seen as a representation of inner qualities of white, Western manliness. This is socially important in a society that is rapidly industrializing, because someone who works in a factory or behind a desk might not have other physical signs of his inward qualities. A beard was seen as a definitive sign of manliness, contrasting to the feminine face, and it was a sign of whiteness, the European ideal, because as white American and European writers frequently pointed out, beards were uncommon in Asia and among American natives. (They conveniently ignored Africans.)

T.S. Gowing's 1854 *Philosophy of Beards* was [reprinted a few years ago as a humor/satirical work](http://www.slate.com/articles/arts/books/2014/12/beards_history_and_theory_of_male_facial_hair_and_comedy.html), but in its time, it used much of the same language as pro-beard authors and publications that were published seriously. (I can't speak to how Gowing intended his work.)

The same year that Gowing's work appeared in print, an article entitled ""The Beard"" appeared in *Westminster Review*. As its anonymous author stated:

>""His potential beauty is not less than hers, but of a different, more complex, and severer order. When man's physical system is perfectly developed, his capacious chest and stalwart frame, overlaid with muscles of high relief, seem to us to require the beard for the completion of features fitted to harmonize with their vigorous outline. ... (The beard) identified as it is with sternness, dignity, and strength ─ is only the becoming complement of manliness.""

Let's also talk about race with regard to facial hair (and body hair in general). This was a *huge* issue in the nascent United States, which was grappling with its policy toward American Indians. During the 19th century, there were serious attempts to classify race ""scientifically,"" to pin down exactly what makes a person ""white,"" ""black,"" ""yellow,"" or ""red.""

Anyone who says science is pure or somehow untainted doesn't know their 19th century history. As Herzig points out in a chapter entitled ""The Hairless Indian,"" there were literally hundreds ─ if not thousands ─ of pages written by American researchers on American Indian hair removal techniques, all in the quest to determine if a lack of beards among American Indians was because they were naturally beardless or because they practiced some cultural technique that restricted facial hair growth. 

This was seen as a critical question, because if Indians practiced hair removal, they *could* have beards, and were therefore closer racially to European whites. If they *couldn't*, then they were seen as more distant racially, and therefore inferior:

""As the Pennsylvania-born naturalist and traveler William Bartram posted the problem in 1791, at issue in Indian hairlessness was whether Indians might be persuaded to 'adopt the European modes of civil society,' or whether they were inherently 'incapable of civilization' on whites' terms."" (Herzig, p. 20)

This issue of facial hair as a racial divider continued throughout the 19th century, particularly after *On the Origin of Species* became popular, and the wider public became interested in racial and ethnographic studies. The great scientific question of the 19th century was whether ""white"" people were so superior that the other people of the world could never match them, or whether those other people were held back simply because of their cultures. 

The beard factored into the ""answer"" to this question. 

But let's pull back now.

At the start of the 19th century, facial hair was not a mainstream fashion (and **you *should* consider it as fashion**, for all the discussions we've been having, which means it wasn't universal) in western Europe. It was the domain of radicals and artists. The English Chartrists adopted beards, as did political thinkers on the fringe (including Friedrich Engels, who grew one in 1840).

Following the Napoleonic Wars, European militaries adopted facial hair patterns pioneered by the French cavalry's famed moustaches and the French sappers' burly beards. Some English regiments even required their soldiers to have moustaches ─ even to the point of acquiring fake ones, according to an 1828 article in *The Times*.

In the 1840s, the British military star Gen. Charles Napier wore a beard in his Indian campaigns and encouraged his officers to do the same. At the time, there was a belief (reinforced by occasional popular and medical articles) that beards promoted good health because of a filtering effect. A beard, it was thought, was a relief to bronchial stresses caused by bad air (a perennial concern of the Victorian era). 

In 1851, Col. Edward Elers Napier (no relation to the general) went so far as to write an article suggesting that British soldiers should be required to have a beard because of its healthful effects. The Crimean War led to new standards regarding beards in the French and British militaries and inspired newspaper editorials (from *The Times*, most interestingly) in favor of the beard.

When soldiers returned from the Crimea, they brought their beards with them.

The Victorian love for medieval history (at least, as they saw it) encouraged the growth of beards as well. Think of the 1835 painting *The Chivalric Vow* or the 1839 Eglinton tournament re-enactment, which encouraged beards as part of costume. This neo-medieval attitude encouraged beards as well.

Of course, with a rise, there is a fall. In the latter part of the 19th century, the industrial revolution is well established. There is no longer a crisis of masculinity about whether an industrial job is manly or not. The need for the beard decreases, and it is spurred out the door by bacteriological and health studies that rebuke the idea of beard as filter. 

You can track the swing in popular fiction. The authors Thomas Carlyle and Charles Kingsley repeatedly wrote popular novels in which beards were representative of virtue. These novels, starting in the 1840s, seem to have been a big push in the beard movement. As Oldstone-Moore writes:

>""Carlyle and Kingsley worried in particular about how men of mind and spirit might also be true men of the world. To both of these men, the great problems of the age depended on a resolution of this problem of manliness. In a time that seemed to them both soulless and sedentary, they imagined heroic men beyond the temporal and geographic boundaries of urban modernity who succeeded in being both assertive and self-disciplined. And they gave their heroes beards to enhance this image of timeless and balanced masculinity. In so doing, Carlyle and Kingsley articulated a new rationale for beards well before they gained general respectability.""

Kingsley's classic *Westward Ho!* is the most obvious example of this trope. To quote Oldstone-Moore, ""he deployed a veritable beard code by which the reader might discern the moral worth of each character according to the condition of his face.""

In other words, the better the beard, the better the man. The villains were clean-shaven or had tiny, dyed beards.

By the time *Tarzan* becomes a famous novel, its titular character is clean-shaven because he fears that his beard is a sign that he is turning into an ape: ""True, he had seen pictures in his books of men with great masses of hair upon lip and cheek and chin, but, nevertheless, Tarzan was afraid. Almost daily he whetted his keen knife and scraped and whittled at his young beard to eradicate this degrading emblem of apehood. And so he learned to shave ─ rudely and painfully, it is true ─ but, nevertheless, effectively.""

The implication is obvious: In popular fiction, the beard has swung from being a symbol of manliness to being a symbol of crudity and unrefinement. Practically speaking, World War I and the demands of gas warfare would all but eradicate the last vestiges of beard popularity. Under the hood of a gas mask, only a narrow strip of facial hair beneath the nose would work, and that style would subsequently become famous (and infamous) as [one of its wearers gained prominence ...](http://www.charliechaplin.com/) 
",0
"It might be instructive to compare essentially the same scene in two poems - in both Homer's *Iliad* and Virgil's *Aeneid*, the king of the gods (Zeus/Jupiter) looks down upon a young, noble men, about to die in battle,(Patroclus and Sarpedon in the *Iliad*, Pallas in the *Aeneid*).

**EDIT:** This is an illustrative example, but by no means the last word - [I go into more detail about how things look when you move past literature and mythology towards practice and belief in this post.](https://www.reddit.com/r/AskHistorians/comments/htyia8/the_roman_pantheon_was_presented_to_me_in_school/fyn574j/)

>\[Achilles\] poured the wine on the ground, gazed at the heavens, and prayed to Zeus the Thunderer who listened ...  
>  
>‘Pelasgian Lord Zeus, who lives far off, ruler of wintry Dodona, ... fulfil my prayer now. I will stay here by the beached ships, but I am sending my friend \[Patroclus\] with a host of Myrmidons to war. Grant him glory, far-echoing Zeus, and fill his heart with courage ... And when he has rid the ships of the foe and their battle-cries, let him return to the ships resplendent in my armour, he and his men unscathed by the close combat.’  
>  
>So he prayed, and Zeus the Counsellor listened. One wish the Father granted, but the other he denied. Patroclus would indeed drive the enemy from the ships, ending their attack, but would not return safe from that battle.  
>  
>...  
>  
>Zeus, gazing down on them, felt pity, and spoke to Hera his sister-wife: ‘Alas that Sarpedon, so dear to me, is fated to die at the hands of Patroclus! Even now I am undecided, whether to snatch him up and set him down alive in his rich land of Lycia, far from this sad war, or allow him to fall to this son of Menoetius.’  
>  
>‘Dread son of Cronos,’ ox-eyed Queen Hera replied, ‘what do you mean? Are you willing to save a mortal from the pains of death, one long since doomed by fate? Do so, but don’t expect the rest of us to approve. And think hard about this fact too. If you send Sarpedon home alive, why should some other god not do the same for their dear son, and save him from the thick of war? Many who fight before Priam’s great city are children of immortals, and those divinities will resent it deeply.

The key points I want to pick out here are:

* Zeus' absolute control over events - he chooses whether or not to grant Patroclus glory and/or a safe return.
* Zeus' emotions - pity for his son, temptation to break the 'rules' of fate and grief at what he sees as the unfairness of Sarpedon's lot.
* The fact that Zeus confides in Hera, his wife, and seems to be going to her for consolation and advice.
* The obvious tension between the gods - a plot point elsewhere in the poem, but evident here in 'those divinities will resent it deeply'.

Now look at how Virgil takes the same material and re-presents it in the *Aeneid* \- directly interpreting this exact scene:

>Here, Pallas pressed and urged,there Lausus opposed him, not many years between them,both of outstanding presence, but Fortune had denied thema return to their country. Yet the king of great Olymposdid not allow them to meet face to face: their fatewas waiting for them soon, at the hand of a greater opponent.  
>  
>But Pallas came forward first, when he thought Turnus mightbe within spear-throw, so that chance might help him, in venturinghis unequal strength, and so he spoke to the mighty heavens:‘I pray you, Hercules, by my father’s hospitality and the feastto which you came as a stranger, assist my great enterprise.Let me strip the blood-drenched armour from his dying limbs,and let Turnus’s failing sight meet its conqueror.’Hercules heard the youth, and stifled a heavy sighdeep in his heart, and wept tears in vain.  
>  
>Then Jupiter the father spoke to Hercules, his son,with kindly words: ‘Every man has his day, the courseof life is brief and cannot be recalled: but virtue’s taskis this, to increase fame by deeds. So many sons of godsfell beneath the high walls of Troy, yes, and my own sonSarpedon among them: fate calls even for Turnus,and he too has reached the end of the years granted to him.’So he spoke, and turned his eyes from the Rutulian fields.

The two are ostensibly similar, but there are some key differences in terms of how we are expected to think in a religious way, and how these gods are presented:

* There seems to be a much greater separation between 'fate'/'Fortune' and 'what the gods do' - and absolutely no suggestion that Jupiter would ever intervene to change someone's fate.
* While Hercules shows the emotions that Zeus displayed in the *Iliad*, Jupiter is much more stoical and resolute, coming across as an unflappably paternal figure with no doubts at all about what should be done.
* The fact that Hercules takes the place of Hera/Juno here - Heracles is a very minor god in Greek religion and the Homeric poems aren't totally convinced about placing him on Mt. Olympus, but here his father-son relationship with Jupiter is presented as centrally important.
* The apparent harmony between the gods - here, as elsewhere in the *Aeneid,* the will of Jupiter is presented as (essentially) the last word, while it's much more up for contestation in the Homeric poems.

What I hope this little sketch has demonstrated is that yes, you could say that the Greeks have a god called Zeus and the Romans have a god called Jupiter, and they fulfil more or less the same function, so you could see them as 'the same'. However, the way that Virgil thinks about Jupiter is clearly very different to how Homer thinks about Zeus - they're very much not the same or interchangeable, and presenting one as simply a version of the other is clearly a bit of an oversimplification. When you move beyond the superficial stuff about the family of the gods and the mythology, and start looking at how people think about and interact with the divine, you see a lot of major fault-lines between Greek and Roman practice. It's easy to forget those, particularly if you listen primarily to authors like Virgil and Ovid - our major sources for Roman religion and mythology - who have a vested interest in trying to bring the two together, and will selectively interpret, emphasise and elide in order to do so.

As such, I'd encourage you to look beyond the names - the Greek 'Apollo' (who, as we've seen, isn't really a single entity) isn't straightforwardly the same god as the Roman 'Apollo', and there's about as much similarity between the two 'Apollos' as there is between Zeus/Jupiter, Athena/Minerva, Mars/Ares, and so on - enough to make the case, but also fundamental differences in how they're viewed, conceptualised and interacted with. There are important similarities, and we can't ignore how far the Romans wanted to see the Greek gods (and the Celtic gods, and the Punic gods) as simply interpretations of their own - but, if you want to understand what Classical religion meant to those who believed in it, we need to complicate things a bit more than we usually do at school.

**Further Reading**

The best current history of Roman Religion is Jörg Rüpke's 2016 *Pantheon: A New History of Roman Religion.* Shorter and slightly older but still good is Mary Beard's 1998 *Religions of Rome.* Both are very good on the diversity of Roman religion as well as what makes it distinctively Roman.

An important book to understand the Lares and Penates - and therefore how Roman religion was both distinct from Greek and broader than the Olympian gods and goddesses - is Harriet Flower's 2017 *The Dancing Lares and the Serpent in the Garden: Religion at the Roman Street Corner.*

I'll find and recommend more when I'm back at home!",0
"There were two inescapable factors of the German military situation in the early sixteenth century: one, the Holy Roman Empire had no real standing army, as such, and princes were far more eager to pay off any commitments than to do the work of raising an army for the emperor. Two, the Ottomans were *right there.* Consequently, the Landsknechte ended up not just one of the most formidable late medieval/early modern ""mercenary"" flavors, but almost certainly subject to the biggest PR campaign. For every Johann Eberlin von Gunzburg, who denounced the Landsknecht as ""soulless"" and ""living entirely in the power of the devil,"" there were piles of woodcuts and songs from prominent artists--and from warriors themselves, in some cases!--selling what we now know is a rather romanticized view of the Landsknecht.

As songs like Jörg Graﬀ's 1518 ""Ein schön Lied von der Kriegsleut Orden"" (A nice song about the orders of men at war) would have it, the Landsknecht were the new, improved crusading military orders of chivalric yesteryear. Among themselves, they were ""free men of war,"" an equal brotherhood at arms. Within society more broadly, they were a cohesive elevated status, with respect and a chance for monetary gain beyond their station. Songs tended to stress the honour and adventure of knechting (yeah, I verbed the noun), not to mention the accoutrements of women and wine that went along with the itinerant lifestyle.

And for a good while in the fifteenth and sixteenth centuries, the publicity *worked*. When Landsknecht commander Georg von Holle recruited for a contracted (i.e. pre-funded) 3000 soldiers around Wildeshausen in 1555, he attracted almost twice as many hopefuls and had to turn the rest away!

But the picture scholars are increasingly painting of the Landsknechte *within* individual companies are notably less egalitarian than the ideal. (To say nothing of Eberlin's accusations of the Landsknecht actual lifestyle of ""whoring, adultery, rape,
gluttony, drunkenness...stealing, robbing, and murder"", or the company of soldiers that randomly burned down the estates they passed on the way to Münster in 1535--or that eager propagandist Graff, above, composed his lyrics *after being blinded*).

First, Landsknechts came from a full range of social classes, from farmhand day labourers up to the nobility. Apparently Emperor Maximilian I had some difficulties wrangling the imperial princes into serving as commanders, but the petty nobility and imperial knights (Reichsritter) were more eager. Niklas Stör's famous series of propagandistic woodcuts features a series of urban artisans who all join the Landsknecht for the money. Landsknecht lyricist Georg Niege, meanwhile, had actually attended and even graduated from the University of Marburg (graduation required significant extra money, to the extent that many university students just went for the coursework and not the degree) before joining up.

And these social stratifications in the ""outside"" world were reflected within the Landsknecht. The famously flamboyant Landsknecht dress code--colorful and stylish and adorned--resulted from imperial decrees excusing them from the sumptuary laws that applied to other Germans. Except--the privilege only applied to Landsknecht of higher ranks. Common soldiers were still banned from wearing slit and slashed (ergo more colorful and adorned) clothing. And ""rank"" meant outside social status, not just military promotion: news Landsknechte from the landed nobility and the urban patriciate typically leapfrogged their lesser-class counterparts in terms of military rank and *especially* pay.

The popularity of Landsknecht service, as suggested above, did raise the barriers for entry. Since there was no standing army and funding was bare minimum (...and company commanders strove to undercut that *even more*, to pocket the rest; Thomas Brady has described the Landsknecht as a multilayer crediting organization who ""made possible wars that no one could afford""), regment commanders and their subrecruiters frequently required that all comers be able to supply their own arms.

We know this was a barrier to entry for many by looking at recruiting patterns from just after the period OP asked about, the second and third quarters of the 16th century. A divide develops in the geographic origins of soldiers by type and rank: more mounted cavalry of higher status and better supplies (e.g. horses) come from central, north, east principalities; lower-paid (ergo lesser-class) infantry were recruited more heavily from the south/southwest. This, of course, was because of the larger number of troops--on both sides--who had armed up to fight in the 1524-25 Peasants' War. They evidently already possessed the tools, and perhaps the skills to fight (although non-propaganda sources are not always as keen on the discipline and prowess of certain Landsknecht companies).

The romance of the Landsknechte endured long after their time in the field. By 1600, ""Landsknecht/Soldat"" dichotomy was popular slang for upstanding versus dishonorable behavior. As the massive bibliography on the Landsknechte that came out of 19th century Germany demonstrates, the legends of brave free men-at-war that emphasized the honor and advanture and minimized the *arson, rape, and murder* resonated with German nationalist and imperial ambitions. And as Matthias Sprenger has demonstrated, the romanticized view of the Landsknecht was directly appropriated by NSDAP (that would be Nazi) publicists and *Freikorps* veterans to legitimize the Nazis' status as the true heirs of the German spirit. It's a sobering lesson to historians to delineate the myth from the ""reality"" of Landsknecht service--but also, to study the power of that myth as a recruiting and propaganda tool in the sixteenth century as well as its later legacy.

Further Reading (in English):

* David Parrott, *The Business of War: Military Enterprise and Military Revolution in Early Modern Europe* (2012)
* Stefanie Rüther, ""Dangerous Travellers: Identity, Profession, and Gender among the German Landsknechts (1450-1570),"" in *Travels and Mobilities in the Middle Ages* (2015)
* Keith Moxey, *Peasants, Wives, and Warriors: Popular Imagery in the Reformation* (2004)
",0
"This is a question of obvious contemporary political importance so I will endeavor to answer it cautiously and with respect to the emotions it no doubt raises.

The logic here is best found in some of the signatory nations’ legal interpretations and internal Law of Armed Conflict manuals,[neatly summarized by the Red Cross here. ](https://ihl-databases.icrc.org/customary-ihl/eng/docs/v2_rul_rule75). The Dutch manual of 2005, for instance, tells us the following:

>Riot control agents such as tear gas may not be used as a method of warfare (Chemical Weapons Convention Article 1). Use as a means of maintaining order, including the control of internal unrest, is not prohibited. Military use must be distinguished from this. **This conceals the danger that the use of a relatively harmless chemical may unleash the use of some other, more lethal one by the adversary**...\[M\]ilitary use of a non-lethal weapon may pose the danger that the adversary perceives it as a forbidden means, which may induce the adversary to use other, more lethal means. One example is the use of tear gas, mentioned above.

Chemical weapons pose particular problems on the battlefield as weapons of mass destruction. In the case of tear gas and other riot control agents, which do not pose major concerns in terms of environmental persistence, excessive painfulness, persistence of pain after the victim is removed from exposure to the gas, and potential for permanent injury, the problem posed is one of *escalation.* Consider two armies locked in combat, let’s call them Red and Blue. Each side is a signatory to the same chemical weapons treaties, each side has a robust no-first-use policy, but each side has a stockpile of lethal chemical weapons including nerve agents as a deterrent to the enemy’s use of chemical weapons. Neither side adheres to the 1993 rule on riot agents. A low-level Blue commander, Major Indigo, is having a hell of a time getting a Red battalion off an important hill. Major Indigo requests permission to fire tear gas onto the hill to dislodge the Red forces. It’s an important hill, taking it could turn the tide of battle, and so his boss Colonel Cyan authorizes it. Meanwhile, the Red forces under Major Crimson are taking no chances. They’ve been sweating in their gas masks and chemical suits all day, just in case. The call comes down the line - *gas, gas, gas!* \- and Red’s soldiers hunker down nervously, safe but uneasy in their protective gear. None of them are exposed, so it’s hard to tell immediately just *what* chemical they got hit with. Major Crimson calls *his* boss, General Ruby. General Ruby knows **one** thing: when weapons of mass destruction are in play, you *have to* maintain the credibility of your deterrence. Blue has to be shown immediately that use of chemical weapons will not go unpunished. With staff academy lectures on “escalation dominance” echoing in the back of his mind, General Ruby signs the paperwork authorizing a limited but punishing chemical weapon retaliation. Three short-ranged ballistic missiles loaded with nerve gas are fired at Blue’s position. Colonel Cyan, Major Indigo and their subalterns die a horrific, gasping death. An hour later, as Blue’s *own* bombers and missiles loaded with mustard and VX begin to launch, the battlefield lab analysis lands on General Ruby’s desk. Just tear gas.

The above scenario seems perhaps melodramatic or overwrought, but it highlights the stakes involved with weapons of mass destruction and the *extreme* consequences of incomplete information. The presence of nonlethal chemical agents on the battlefield creates a risk far out of proportion to the actual severity of the weapons themselves.

As for sourcing, in addition to the link given above, my perspective on deterrence, escalation risks, and the consequences for uncautious behavior with WMDs is heavily informed by Larsen and Karchtner’s *On Limited Nuclear War in the 21st Century* and the opinions on so-called “battlefield” nuclear weapons expressed by Michael Kofman in several of his CSIS presentations. These both do not directly connect to chemical weapons, but many of the concepts of deterrence are similar across categories of WMD; there is simply more literature on nuclear weapons than chemical.

EDIT FOR SOURCING: Savoy, Sagan, and Wirtz’s 2000 *Planning the Unthinkable: How New Powers Will Use Nuclear, Chemical, and Biological Weapons* was also at the back of my mind when I was chewing on this question.

OBLIGATORY MORNING-AFTER EDIT: Folks, please stop giving me gold. I appreciate the gesture, but giving money to reddit is probably the least useful thing you could be doing with that money. There are a massive number of nonprofits that need that money *far* more than reddit does. Reddit has a profitable ad revenue stream, and more importantly, reddit has spent the last decade platforming and giving shelter to white supremacist groups. Give your money to literally anyone else.",0
"Seneca the Younger's anecdote about Scipio Africanus is certainly an interesting one. First, though, I want to point out that just because people in Seneca's time, or Scipio's, considered a certain way of moving ""masculine"" or ""feminine"" doesn't mean *you* would see it the same way.

I also want to warn that we should be aware of the cultural associations in place in Seneca's writing. Romans had a strong narrative of decline as part of their national image; almost every single Roman prose writer expresses in some way the idea that Roman traditions (including gender roles) used to be great and are now corrupted. They use this trope in different ways, but needless to say, the perpetual romanticizing of various pasts is propagandistic and biased. When Seneca says that the custom in his day is for men to move femininely and the custom in the past was for men to move masculinely, he's making a moralizing judgment that connects bodily movement, gender stereotypes, and ethics, *not* reflecting an accurate knowledge of how people 250 years before him (least of all one specific general) danced.

So – the context of this anecdote is a text called *De Tranquilitate Animi*, in which Seneca is talking about how to calm one's mind. He gives examples of famous figures' pastimes, like Socrates playing with children and Cato drinking wine.

>...and Scipio would move his triumphal and military body to the music (literally *numeros*, 'numbers'/'rhythms'), not weakening himself effeminately (*molliter*, literally means 'softly'), as is now the custom of those who flow with more than womanly softness even in their very walking, but the way that those ancient men used to dance in triple-time (*tripudiare*) in a manly way amidst game and festival times, not taking a loss (*detrimentum*) even if they were seen by their enemies.

This is very reminiscent of accounts of archaic Roman religious dancing, such as the Arval Brothers, whose hymn included calls to dance and the repetition of the word 'triumph'. The comparison most often made is to the Salii, a Roman order of priests whose dancing is likewise described in military language and with the word *tripudiare*, meaning to do a three-step or three-beat dance. Their name comes from the word *salire*, 'leap, jump, dance' (showing that dance/movement was the key feature of their rituals), and they represented a continuation of very archaic Roman religious practices, which Romans of the classical period recognized as such (they were allegedly founded under the king Numa Pompilius). We have some fragments of their hymns (the *Carmen Saliare*), written in a form of Latin that the classical Romans who wrote them down no longer understood and that even linguists today don't fully understand. In our defense, the text is pretty garbled by centuries of oral ritual transmission by people who didn't understand it! My point is, the fact that Romans in Seneca's time saw the Salii as archaic contributes to the sense that their style is part of what Seneca has in mind when he alludes to manly, militaristic, old-fashioned dancing in triple-time.

We know a bit about the practice of the Salii from sources written during the Roman principate (though less about the original religious function of this practice). It took place in a procession at public festivals that lasted several days, and was in praise of the gods of war. We get the impression that their dance was ritualized and choreographed, done in a group of several armed men (there were 12 Salii at any one time). Sound was an important part of it, as they both sang hymns and clanged their shields while dancing, and the sound of feet on the ground was meant to be very audible. There was a lead dancer, and there may have been competitions between different groups of dancers. Plutarch describes the dance itself like this:

>Now the Salii were so named... from the leaping which characterized the dance itself. This dance they perform when they carry the sacred light-shields through the streets of the city in the month of March, clad in purple tunics, girt with broad belts of bronze, wearing bronze helmets on their heads, and carrying small daggers with which they strike the shields. But the dance is chiefly a matter of the feet; for they move gracefully, and execute with vigor and agility certain shifting convolutions, in quick and oft-recurring rhythm. (Life of Numa, XIII)

Then he describes the exact shape of the ceremonial shields in way more detail than I care about, and gives a bunch of wrong etymologies. From Dionysius of Halicarnassus we get some more details:

>They execute their movements in arms, keeping time to a flute, sometimes all together, sometimes by turns, and while dancing sing certain traditional hymns. (Roman Antiquities II.70)

**Sources**

Alonso Fernandez 2016, [""Choreography of Lupercalia: Corporeality in Roman Public Religion""](https://www.researchgate.net/profile/Zoa-Alonso-Fernandez/publication/309783868_Choreography_of_Lupercalia_Corporeality_in_Roman_Public_Religion/links/5e5ecfbda6fdccbeba182e8b/Choreography-of-Lupercalia-Corporeality-in-Roman-Public-Religion.pdf) about this specific Seneca passage

Lynch and Rocconi (eds.) 2020, *A Companion to Ancient Greek and Roman Music*

Habinek 2005, ""The World of Roman Song: From Ritualized Speech to Social Order"" talks a lot about the Salii

[Blansdorf 1995](https://books.google.com/books?hl=en&lr=&id=1oJ7gR78WoAC&oi=fnd&pg=PR1&dq=Blansdorf+1995+carmen+saliare&ots=6VOlLfgFzz&sig=zUwRIybQWbfoYlzCRXF8gFN7JWo#v=onepage&q&f=false) has the Carmen Saliare fragments",0
"In looking at answering your question, I think you can't escape from secondary issues, being the sourcing and the translation. Because those are underlying assumptions of your question that make it difficult to answer. 

those would be (1) that the story of Sodom and Gomorrah is, or is even intended as a literal historical account as opposed to a transcribed morality tale and (2) that the english version which references gang-rape is an accurate translation of the underlying hebrew stories. 

First, most academic experts agree that the Hebrew Bible (i.e. the old testament)  is a compilation of oral histories.  See [*Schinderman* How the Bible Became a Book: The Textualization of Ancient Israel (Cambridge University Press 2004)](http://assets.cambridge.org/97805218/29465/frontmatter/9780521829465_frontmatter.pdf) - [you can read a briefer summary in this PBS article](http://www.pbs.org/wgbh/nova/ancient/origins-written-bible.html)  

So when things like the story of Sodom and Gomorrah appear in the Hebrew Bible, one of the first things you have to understand is that those are transcribed versions of oral histories told by the Israelite people.   Oral histories can be extensive and detailed, but they are not in the same category as detailed written histories.  The point is themes and morals.  The morale of the story is about hospitality.  Lot has a religious duty of hospitality to strangers. he honors his duty of hospitality at great cost to himself. he was a righteous man.  The inhabitants of Sodom and Gomorrah were not hospitable people, in fact they were terrible and the opposite of hospitable, and they were destroyed. 

Second, there is a substantial dispute among translators whether biblical account described in Genesis 19 actually describes rape.   The hebrew word used is ידע ""yada"" (don't honestly know whether the pasted hebrew characters will transcribe).  Some translators argue this word carries the cannotation of an ""intimate"" relationship, but only a minority of uses in the old testament use that word as a euphemism for sex.  [others argue the focus of the story is on inhospitability rather than an act.](http://rictornorton.co.uk/homopho2.htm) 


To answer your question directly.  The only references in ""ancient"" texts that I'm aware of also address the same story described in the bible.   The events of Sodom and Gomorrah are back-referenced multiple occasions in the bible and are referenced in Rabbinic literature and also in the Quran.  I'm not aware of any other independent ancient texts that describe strangers coming to a home and wanting to gang-rape newcomers to town for some reason. 

However, if we look at the alternative interpretation that the primary focus of the story is about hospitality.  There are ancient arabic/persian/indian folktales that likewise tell of various ""bad things"" happening to people that are inhospitable to strangers.  Some of these can be traced to at least the 8-9th century baghdad, with earlier oral origins.  See Van Gelder, G. J. (2000). God's Banquet: Food in Classical Arabic Literature. New
York: Columbia University Press

",0
"I know this is really terrible for people to read, but this is just the beginning of a reckoning.

The existence of mass graves is no surprise to people in Indigenous Studies in Canada. The recent news is only the confirmation of the number of bodies in this one site through radar. [This inquiry in 2008](http://archives.algomau.ca/main/sites/default/files/2010-061_015_024.pdf) identified 28 suspected child mass grave sites across Canada that needed to be investigated (including the Kamloops site).

The recent news is only really ""new"" in the sense that they were able identify the number of children at the site for the first time and found a higher than expected number of children in the mass grave, and surprisingly young skeletons.

(Sorry, this gets indelicate.) In places with as high rates of sexual abuse such as these religious and government institutions subjected the children to, there are pregnancies. The report that children as young as 3 were found at the Kamloops site is surprising (to the wider public) because that is younger than expected. As noted above, legally children only had to go to school from 7-16. Unfortunately, there are many accounts of baby and infant graveyards at residential schools. Kamloops is no different in this regard, and there are many accounts of pregnancies and abortions. This one is from Kuper Island in B.C.:

*""We regularly hear stories from our people about all the children who were killed at Kuper Island. I mean killed, not just died. A graveyard of these kids is just south of the old school building. The priests dug up part of it when they closed the school down in 1973. There are not only children but fetuses in there, aborted by the nuns themselves whenever a girl got pregnant by staff or the priests. Often the young mother would die too and get buried right next to their child.""*

This account can be found [Hidden from History: The Canadian Holocaust (3rd edition)](https://caid.ca/NoLonHid2010.pdf) by Kevin Annett which is free online if anyone wants nightmares, or to reckon with the true past and ongoing struggles of Indian Residential Schools.

(edit: I added the Kevin Annett link because it is free online, but I understand he is a controversial figure due to his past connection to the church.

*Behind Closed Doors: Stories from the Kamloops Indian Residential Schoo*l by the Secwpemec Cultural Education Society is a great resource. When I was in school we watched [Kuper Island: Return to the Healing Circle](https://www.youtube.com/watch?v=5UW8gojr2HM&ab_channel=ResidentialSchoolMagazine) by Christine Welsh which is available on youtube. So is [Death at Residential School](https://www.youtube.com/watch?v=9FydzIzkndA&ab_channel=CBCNews%3ATheNational) by the CBC: *""The numbers \[of deaths\] are much higher, perhaps 5 - 10x. It's because the records are so poor. They just didn't bother keeping track of children who died.""*)

That's all I wanted to say. As someone who studied this stuff in school, it grinds my gears wrong how everyone in power in my province/country are all ""This news is shocking!""

There was always a mass child grave there. It's just no one wanted to deal with digging it up until now.

Edit: Resources in Canada/BC

A National Indian Residential School Crisis Line set up to provide support for former students and those affected. Access emotional and crisis referral services by calling the 24-hour national crisis line: 1-866 925-4419.Within B.C., the KUU-US Crisis Line Society provides a First Nations and Indigenous-specific crisis line available 24 hours a day, seven days a week. It's toll-free and can be reached at 1-800-588-8717 or online at [kuu-uscrisisline.com](https://kuu-uscrisisline.com).",0
"Earlier on in this essay, I pointed out challenges to the idea that British actions in Ireland lacked intent. But here let me go on to challenge the notion of British intent as applied to their actions in India. Yes, even though the British explicitly confirmed their intent. See, I would argue that the British arrived at this intent specifically because of their misinformation about the culture in question. Similarly, the British arrived at their catastrophic solution to the Irish famine based on misinformation about the plight of Irish tenant farmers. So here's the question. Is it a choice whether or not to inform oneself about a subject? Because if so, then the choice not to inform oneself, or to inform oneself improperly, would constitute an implicit case of intent. If we exonerate the British of intent in Ireland, then we must question whether British intent in India was really within their control. But if we don't exonerate the British of intent in Ireland, then that raises certain questions about whether the British intended the status quo which ultimately fueled the famine.

If this seems like a non sequitur, that's because it is. See, what's happening here is that we're operating off an overly narrow concept of what genocide is, and how it happens. Much of the modern world's perception of genocide is heavily contextualized by Nazi activities during World War II, and one of the most weighty vehicles by which this history became communicated to the world was through a legal process ... the Nuremberg trials. Hence, we tend to frame genocide within the context of culpability and intent. This is not an inherently bad thing. It's good to have an international framework by which to hold parties responsible for heinous acts. But it doesn't necessarily make for the best social science.

From within a research perspective, we're interested not just in culpability for genocide, but also understanding the processes, mechanics, and causes for prejudicial violence. There are structures and complex systems at play here which go beyond the scope of just hatred. Want an example? Take the Indiana Jones franchise. A lot of people now view the portrayal of India as questionable, but you probably had no idea how offensive it actually was. Is that because you're hateful? Or is it because complex systems and structures of society occlude information from you? Because your concept of ""the normal"" influences your choice of which questions to ask? Because normativity is not actually the same thing as neutrality?

And hey, now you know about the Thuggee culls and the Criminal Tribes Act. But do you know about the theology of Tantra, its unique approach to the concept of canon, and the ways in which this threatened traditional Christian conceits of the time? You've probably been more exposed to the bible, as a historical and cultural artifact, than you've ever been involved in Tantric textualism. Even if you're skeptical of the biblical tradition, that doesn't correlate to familiarity with other traditions, because your skepticisms are still based around the very idea of the bible. Well, this context is fairly significant to Britain's choice of who to target with the Criminal Tribes Act. Now you know that. But what else don't you know? It's a conundrum.

Same goes for matters like the famine. Let's say that we can agree that famines aren't entirely the fault of poor people being irresponsible and having too many babies (though that rhetoric remains depressingly common). Well, there are still plenty of famines in the world. I don't have the solution for them. Do you? I have ideas, as you likely do as well. But how certain are you that they'll work? And do you think that the British parliamentarians with their worship of Malthus were any less certain? I'm not saying that you and I don't know anything. Personally, I think there are certain policies which, if supported, could substantially improve matters. But just because we know something doesn't mean that we know everything.

The cases I've just put forward constitute something called an epistemological dilemma. This is a situation in which an agent struggles to rationalize information, because in order to do so the agent must first plan out the process of rationalizing information, and the situation makes it very difficult for the agent to reason about what rationality actually means. In simpler terms, an epistemological dilemma isn't just a problem with how you think, but also a problem with how you think about thinking. And the specific case we're looking at here is one of the oldest epistemological dilemmas in the book. We're intuitively aware that reasoning can be affected by both the presence of information and its absence. That's why people ascribe so many problems to ignorance. But it's extremely difficult to figure out how your own reasoning is affected by your lack of information. You don't know anything about how that missing information might affect your reasoning, because you don't know what that information is. You might not even know that you're lacking it.

So that's it then? Genocide and ethnic cleansing all just comes down to human foible? Well, not exactly. See, all human beings may be equally susceptible to cognitive bias, but not all cognitive biases have equal power within society. Why? Because of society itself. We can't just examine these issues on an individual level. Biases, norms, beliefs, and value judgments tend to compound through social interaction to form complex systems of culture. When these systems of culture interact, the exchange is negotiated through an overarching system of power relations. This might sound like a politically assertive statement, but it's really a very simple concept if you think about it. Social systems most often perpetuate violence based on failures of information. Violence involves both a party which is brutalized and a party which brutalizes. Who gets to be which? The chief determinant tends to be the information/worldview that the interaction operates on. The party which controls the information basis for interaction is the party most able to perpetuate brutality, while escaping brutality against itself. Control of information first requires the power to control information.

Hence, systematic violence is typically contextualized by two key factors:

1. an asymmetry in the structure of information or social organization between two parties, and
2. a power imbalance for one party to leverage in acting upon their particular basis of belief

This goes for genocide and ethnic cleansing, but it also informs us about broader patterns of societal violence which might not meet these narrow legalistic definitions. I would put forward that the Irish famine very clearly fits the criteria of societal violence. The very fact that we are discussing intent with regard to British decision-makers, and not Irish ones, is confirmation of power imbalance.

So to answer your question ... you're asking the wrong question. Social Scientists might not refer to the Irish famine as a genocide on the regular, but that's largely because this just isn't the way that Social Scientists tend to think. It's far more common for us to think in terms of systems and dynamism rather than trying to shuffle things into their proper categorical definitions. I would argue that, when calling something a genocide, the definition itself does not supply meaning on the level that Social Scientists tend to think at. We need historical and cultural context, and we need an understanding of the systems underlying not just the historical events themselves, but also how the events were recorded, how the records were preserved until the present day, and how the present day has interpreted these records. While the Irish famine might not meet on particular set of criteria on which ""genocide"" is defined (given the requirement of intent by a state-actor), the ways in which Social Scientists look at the overarching picture of the Irish famine is actually quite similar to the ways we look at genocides.

\[disclaimers: I lived and studied in the Republic of Ireland for two and a half years, I'm ethnically Bengali, and my particular family background is one which was included beneath the Criminal Tribes Act ... also I'm an Anthropologist, not a Historian\]",0
"Great questions!

In point of fact, one legend credits the invention of the distillation process entirely to Rasis/Muhammad ibn Zakariyya al-Razi, the 9th-10th century *physician*. His writing even made him one of the leading medical authorities in later medieval Europe!

More broadly, though--the Islamic ban on alcohol was far less strict in the Middle Ages and early modern era than most people today think. My current favorite comes from a journal article by food history scholar and cookbook author Nawal Nasrallah, called ""What on Earth is Dadhi?"", which focuses on ""a mystery date-wine additive in medieval Baghdad""...A mystery date-wine additive that was often used to *make wine stronger.*

In Islamic and Christian Iberia alike, grape-based wine was very popular, too, including among rulers.

Rudi Mathee, ""Alcohol in the Islamic Middle East: Ambivalence and Ambiguity,"" *Past and Present* Supplement 9 (2004), spends a lot of time on Muslim attitudes towards alcohol and consumption of alcohol during the medieval and especially early modern eras. 

If you're interested in reading more--like the 17th century German visitor to Turkey who reported that Muslims believed it was a sin to drink no matter how much they did, so they just *got really super drunk every time*--that's where I'd start.

DOI: 10.1093/pastj/gtt031",0
"The prioritization of certain issues like gay marriage and abortion within some sects of Christianity can be attributed to a variety of factors, including theological interpretations, cultural influences, and historical contexts. It is important to note that not all Christians or Christian denominations hold the same views on these topics. However, I can provide some insights into why these issues may be more salient for some religious conservatives. 1. Theological Interpretations: Different Christian denominations and individuals interpret biblical teachings differently.",1
"The first point to make is that ""big, long dresses that trailed on the ground"" are not exactly common all throughout the past - they were fashionable during specific periods. For instance, the mantuas of the 1680s through 1700s often were made with long trains which might drag on the ground; in the 1780s it again became fashionable to have a slight train (in both periods, the visible petticoat worn under the gown would be cut to a ""round"" length and not touch the ground). Skirts in the 1840s through 1860s could be worn at about floor-length, and in the 1870s and 1880s a long train was again fashionable. However, between these eras it was generally fashionable for the skirt/gown to be made on the shorter side, leaving the feet clear.

The second point is that *not every woman was dressing like this*. ""Fashionable"" is a key word: it relates to social class, purpose, and intention. In general, the kind of woman who was going to wear a train as a matter of course would not be walking through muddy paths filled with manure - she was going to stay indoors, pop into a carriage just outside the front door, or stroll along a patio or tidy garden. A farmer wouldn't wear a train, a servant wouldn't wear a train, a traveler going far on foot wouldn't wear a train, because you're right, it would be very problematic. 

We're drastically cutting down the incidences that you're asking about to a small proportion of women and only a bit of the time. So, what did people do when they did find themselves in this situation?

One thing was simply to hold up or pin up the train. A number of ca. 1800 fashion plates show women draping the trains of their white muslin gowns over one arm or reaching behind to pull it up, as they would not last very long on the dirty ground; in general we can assume this was a fairly common ""quick fix"" when necessary. They weren't stupid. Given the choice between trailing your dress in the mud and having to hold up the hem, women would generally choose the latter.

On trained gowns of the 1870s and 1880s, we often see an extra piece added to the bottom of the skirt, a pleated ruffle of cotton organdy called a *balayeuse* (""sweeper""). These could be purchased ready-made and stitched in when needed, then taken out when too dirty and replaced, although I think they were typically meant for indoors dirt and dust rather than actual filth. Such skirts also sometimes had a cotton velvet or wool ribbon facing on the hem to take the wear of brushing over the ground.

Now, as to carrying disease into the home - I think it's unlikely that this was a real problem given how diseases generally spread, but in the late nineteenth and early twentieth centuries there was indeed a concern that women were bringing typhoid and other infectious diseases that terrified city-dwellers into their homes and infecting their families. Now, on the one hand this reflects the general focus on sanitary conditions that went along with this time period - brass bedsteads, white tile floors and walls, germ theory, etc. On the other, it reflects social anxieties of the day. Do women put following fashion ahead of their health and the health of others? Should women focus their attentions on domestic matters and avoid going out into the world? Are cities being polluted with disease because of immigrants? And so on. It's not a simple question and I don't believe there's a solid answer.",0
"Hi! Thanks for your questions. My answers this time around are going to be a little sketchier than usual, because a lot of my books are in my office, and my building and my university library are both shut down, so I don't have access to many of my usual sources. So those of you who think that my responses tend to be longwinded can breathe a sigh of relief!

I don't think I have to go into too much detail about child poverty in Victorian England. Suffice it to say that it was endemic, widespread, and absolutely heartbreaking. As the ""outdoor relief"" of charity gave way to the ""indoor relief"" of workhouses and Ragged Schools over the course of the 19th century, the measures being taken to try to care for the staggering number of poor children in some cases only made things worse. And much of that has to do with how the poor were seen and treated.

The term ""street Arab"" has its origin in Thomas Guthrie's influential 1847 ""First Plea for Ragged Schools,"" describing the awful plight of children in Edinburgh by comparing them to, among others, Arabs. Because I really can't put it any better, here I will extensively quote Lindsay Smith's 2008 article about the way ""street Arabs"" were depicted in photography:

>The by no means inconsequential coinage of the term 'street Arab' in Thomas Guthrie's 'First Plea for Ragged Schools', first published in pamphlet form in Edinburgh in 1847, makes the correlation between the child and a bestial other, between city and desert:

>""These Arabs of the city are as wild as those of the desert, and must be broken into three habits, - those of discipline, learning, and industry, not to speak of cleanliness. To accomplish this ... hard words and harder blows are thrown away here. With these, alas! they are too familiar at home, and have learned to be as indifferent to them as the smith's dog to the shower of sparks."" (30)

The association is clear: the people, including the children, on the street are associated with the figure of the racial Other, someone who is almost sub-human, associated with dirtiness, poverty, and criminality, an object of pity but also revulsion. Guthrie also tells the story of an orphan child in Africa coming out of the desert to a missionary (the implication is that a kindhearted missionary would help the child), making an appeal to the reader that such a story""will prove as successful with kind hearts at home"" (31). But Smith demonstrates that this framing carries a condemnation of the ""street Arab"" as well, the implication being that they are failing to be authentic, that they are a poor likeness of a ""real"" Arab, out of place in the British world but not the real thing, either, someone with no proper place or status.

For a mocking portrayal of this figure in the public imagination, check out the poem [""The Arab,""](https://www.bartleby.com/360/9/143.html) in which Charles Stuart Calverley imagines a conversation with an ""Arab"" who the reader soon realizes is a street vendor, selling newspapers and offering to polish the speaker's shoes. It's worth noting where the speaker tells this ""Arab"" that his skin color is not from his ethnicity (""due to nature"") but from blacking shoes: his low-class work has made his skin match that of a ""real"" Arab, at least in the eyes of the speaker (as has, it seems, his rough life, as the child always has ""at least one black eye""). Pretty tellingly, despite his pity for the ""Arab,"" the speaker gives him nothing and sends him away, filled with disgust: ""Flee, child, flee!""

*Crime and Punishment in Victorian England* describes the infamous slum of St Giles: ""Here, as in Whitechapel, Shoreditch, the Ratcliffe Highway and other places where survival is a struggle, many of the children seen 'fluttering in rags' are taking their first steps in criminal careers that may one day get them imprisoned, transported, or worse. These 'street arabs' are to be seen 'in vast shoals' at the street markets, loitering with clear intent by shops whose wares are displayed in the street and doorway and prowling in districts in which it might be possible to steal fruit and comestibles from costermongers' barrows..."" (etc.) ""Some children have been sent out to steal--often at the tender age of only six or seven--and some are hungry, while others are egged on by their mischievous friends"" (14-15). As is too often the case, the association of poverty with crime was absolutely ingrained in the Victorians, and often grinding poverty only became visible to other classes when it entered into their lives in the form of crime.

To answer your other question, it was not at all uncommon to give money to people, adults and children alike, to run errands. ""Casual"" work was becoming increasingly common in Victorian cities, and often people would survive by doing such odd jobs. Look no further than the child Scrooge pays to fetch the turkey to Bob Cratchit's house. Scrooge is rather more generous than Holmes, it seems, as he gives the boy ""half-a-crown,"" five times the sixpence Holmes offers. While I don't have on hand the going rates for casual work, I have found sixpence to be a pretty common price asked for small tasks performed for the middle class, such as holding a horse, carrying luggage, or wrapping a purchase in paper.

Edit: I've since heard that faculty and PhD students can still use the side door to the library to access books, so at least I've got that going for me, which is nice.",0
"This is actually an amazing question! Although it relates to subjects that are perhaps less light-hearted than Sir Mix-a-lot's song. So I'll just put a little CW here, then carry on with my answer. (This is my first post here, but it relates to my area of studies , ie. the intersection of race and fashion, so I'll jump right in).

CW: racism, sexism, degradation

The spoken word monologue is important to answering your question, especially this part:

*""****She looks like a total prostitute****, okay?*

*I mean,* ***her butt, it's just so big***

*Uh, I can't believe it's just so round, it's like out there*

*I mean, uh,* ***gross***\*, look\*

***She's just so... Black***\*!""\*

Criticism of the features of black bodies is something that goes back all the way to the XIXth century colonial empires, where pseudoscientist ideologies tried to justify colonialism. The prominence of certain body parts (in the case of women, the buttocks), were used as arguments to frame black women as extremely sexual and profligate. Some women were even exhibited in human zoos or during scientific demonstrations, the most famous of which is probably Sarah Baartman, who was dubbed the ""Hottentot Venus"" during her lifetime. This framing of black women during the XIXth lead to a popular perception of black women as overly sexual, promiscuous, etc. In other words, white colonialist fantasies were projected onto them. (For more on this topic, see Robin Mitchell's Book *Vénus noire: Black Women and Colonialism in Nineteenth Century France*).

These conceptions remained deeply ingrained in western culture. Patricia Hill Collins dedicates the entirety of the sixth chapter of her classic book *Black Feminist Thought: Knowledge, Consciousness and the Politics of Empowerment* to politics of black sexual womanhood in the United States. In this chapter, she talks about all the ways in which this colonial objectification of black women's body manifests itself in contemporary american society. The so-called hypersexuality of black folks was again used as a justification for the segregation system in the US. While an ideal of purity and virginity was projected as the ideal for white womanhood, black women served as the counterpart of this model and were strongly subjected to objectification. This was further strengthened by difficult economic context that brought some black women to prostitution.

This dynamic is observable in the monologue, with the valley girl establishing clear links between blackness, prostitution and ""gross sexuality"". There are residues of the colonial period that still permeates today in the form of stereotypes.

As for the 1990s, it is important to remember that this was the decade of the notorious ""heroin chic"" look, which meant extremely skinny, waifish models. Definitely the polar opposite of what we see today. I would say that theere was  a divide among racial lines in the sense that high fashion was (and still is, although not nearly as much) a very white and upper-class environment. Today, fashion coopted a lot of black culture (streetwear, black hairstyles, plumpier figures), although not always acknowledging its roots, which explains the frequent controversies we saw in recent years.

I don't want to make Sir Mix-A-Lot seem like a militant here, he's probably just a guy who love a nice round butt (who doesn't?) but there is certainly something daring and countercultural about his song, that seeks to empower women whose body has been historically invalidated and presented as something shameful. This is kind of the same reason why some people applaud WAP today as feminist and liberating, where others might see it as objectifying. I digress here, as I am more specialized in the colonial period than I am in the 1990s, but I hope this brought the answer you were looking for.

References:

Robin Mitchell, *Vénus Noire: Black Women and Colonial Fantasies in Nineteenth-Century France*, 2020. Athens: University of Georgia Press, 208pp.

Patricia Hill Collins. *Black Feminist Thought*, 2nd edition, 2000. New York: Routledge, 283pp.

Nicolas Bancel & Pascal Blanchard. *Sexe, race & colonies*, 2018. Paris: La Découverte, 544pp.

For the fashion industry in the 1990s, I like Vogue's podcast *Fashion: The 1990s* which is very well produced, although not very critical. A good complement would therefore be *Dressed: The History of Fashion* by fashion historians April Calahan and Cassidy Zachary.

&#x200B;

Suggested further reading on this specific topic:

Sabrina Strings, *Fearing the Black Body: The Racial Origins of Fat Phobia*, 2019. New York: NYU Press, 304 pp.

&#x200B;

edit: spelling",0
"All this talk of tentacles reminds me of the stir Victor Hugo caused by having an octopus as the non-human villain of The Toilers of the Sea (1866). It caused a fashion for Octopuses in Paris at the time, and his description of it is very much in the manner of Lovecraft's writing -

""To believe in the octopus, one must have seen it. Compared with it, the hydras of old are laughable.

Orpheus, Homer, and Hesiod were only able to make the Chimaera; God made the octopus. When God wills it, he excels in the execrable. And all ideals being admitted, if terror be the object, the octopus is a masterpiece.

Its most terrible quality is its softness. A glutinous mass possessed of a will — what more frightful? Glue filled with hatred.

At night and in its breeding season, it is phosphorescent. This terror has its passions. It awaits the nuptial hour. It adorns itself, it lights up, it illuminates itself; and from the summit of a rock one can see it beneath, in the shadowy depths, spread out in a pallid irradiation, — a spectre sun.

It has no bones, it has no blood, it has no flesh. It is flabby. There is nothing in it. It is a skin. One can turn the eight tentacles wrong side out, like the fingers of a glove.

The creature superimposes itself upon you by a thousand mouths; the hydra incorporates itself with the man; the man amalgamates himself with the hydra. You form but one. This dream is upon you. The tiger can only devour you; the octopus, oh horror! breathes you in. It draws you to it, and into it, and bound, ensnared, powerless, you slowly feel yourself emptied into that frightful pond, which is the monster itself.

Beyond the terrible, being eaten alive, is the inexpressible, being drunk alive.""

(Excerpted from five pages of Toilers of the Sea, II iv 2, “The Monster”)

It certainly seems to me, to be another of the influences on Lovecraft (I don't think there need be only one); Hugo's immense popularity and fame in the 19th century would certainly suggest he had an influence on some who followed him 50 years or so hence.",0
"I'm going to at least partially answer your question and piggyback onto what /u/bakeseal and /u/Georgy_K_Zhukov have already written, and broadly agree.

The meaning of ""pickle"" seems to have evolved over time and the critical period appears to be between 1870 and the 1910s.

In the OED entry Georgy_K_Zhukov cited, there's a sub-entry for the ""pickle worm"" which is a type of caterpillar ""which bores into ripening cucumbers, cantaloupes, squashes, etc.""

While squash *can* be pickled, it appears that the name was coined particularly for the bug's effect on cucumbers. The first instance the OED cites comes from the *American Naturalist* magazine in 1870, published in  Salem, Massachusetts:

> ""The Pickle worm..is a caterpillar which bores into the cucumbers when large enough to pickle, and it is occasionally found in pickles.""

While this by itself doesn't mean a whole lot, since cucumbers have already been mentioned in the passage before mentioning pickles, with these next sources, it begins to add up.

In [the August 22, 1872, edition](https://books.google.com/books?id=rOgxAQAAMAAJ&pg=PA534&dq=Pickle+rye&hl=en&sa=X&ved=0ahUKEwju_ZzC5NjqAhUCPK0KHS_tDGoQ6AEIPjAD#v=onepage&q=Pickle%20rye&f=false) of *The Cultivator & Country Gentleman*, which was an agricultural magazine published in Albany, New York, there is an article entitled ""Cucumber Pickles"". Here, the article is written by a reporter reporting from rural Massachusetts. After the title, the first line of the article reads:

> ""In the neighborhood of large cities, which afford a convenient market within ten or fifteen miles, the farmers plant considerable breadth of pickles.""

Again, this comes after cucumbers have already been identified, and cucumbers would be mentioned three more times in the body of the article. Still, it's notable that the author is referring to planting ""pickles"", not cucumbers, which he does again later on in the article. He also refers to the ""pickle crop"" and refers to the crop more often as pickles than as cucumbers.

And here is where a partial answer to your question can be gleaned from. If you read the full article, you see why pickling cucumbers were a useful crop. They were easy to grow in the soil of crops planted and harvested earlier in the season. The author specifically mentions rye, strawberries, grass, and ""early peas"" as early harvested crops whose soil could then be replaced with pickling cucumber seeds. Another advantage to the farmer was they didn't need to care much about how mature the pickle crop got: ""Every thing as large as the little finger is pickled"". These cucumbers also grew quickly. They should be planted between June 29th and July 10th, and five weeks later, they could start being harvested and ready for pickling. This left time for a late season crop to be planted in the same soil, if wanted, or else continue to let the smaller cucumbers grow until the end of the season.

The most direct evidence for ""pickles"" meaning pickles cucumbers comes from [the August 1889 edition](https://books.google.com/books?id=jDA_AQAAMAAJ&pg=PA379&dq=Pickle+rye&hl=en&sa=X&ved=0ahUKEwju_ZzC5NjqAhUCPK0KHS_tDGoQ6AEIYjAI#v=onepage&q=Pickle) of the *American Agriculturalist* periodical, published in New York City. In an article about ""The Manufacture of Pickles"", several crops in addition to cucumbers are mentioned as being raised for pickling in Farmingdale, New York, and Central Park, Long Island, but the ""22,000,000 cucumbers"" appear to be the most abundant one. But it goes on to give these key sentences:

> ""The central picture of our illustration gives a partial view of our cucumber field during the harvest, which begins in the latter part of August, and continues until the second week of October, unless sooner terminated by an untimely frost. **The 'pickles' as they are called from the first**, are gathered in baskets by women and boys, and brought to an assorting table which stands in the field, sheltered in a canopy of green boughs.""

In this article, then, there is direct evidence that the farmers and farmhands raising the cucumber crop were calling them ""pickles"" even before they were pickled. From this, it can be inferred that the earlier hints in the *American Naturalist* in 1870 and *The Cultivator & Country Gentleman* in 1872, when referring as they did to ""pickles"" probably *were* taking it from the language being used by the communities of people who raised them in the Northeast. But also, as can be inferred by the three articles, the authors didn't believe this would be generally understood by their wider readership, who may not be acquainted with ""pickles"" narrowed to a definition meaning cucumbers specifically.

But by the 1890s, this had started to change as recorded in the press, which was also likely a lagging indicator. In that decade, ads started to show up for grocery stores and other businesses referring to ""pickles"" in a way that leads to the conclusion that pickled cucumbers and only pickled cucumbers were being referred to.  

A bit earlier, the *New York Sun* ran [an article](https://nyshistoricnewspapers.org/lccn/sn83030272/1886-04-20/ed-1/seq-3/#date1=01%2F01%2F1830&sort=date&date2=12%2F31%2F1894&searchType=advanced&SearchType=phrase&sequence=0&index=17&am+p=&am+p=&words=pickle+sour&proxdistance=&to_year=1894&rows=20&ortext=&from_year=1830&proxtext=&phrasetext=%22sour+pickles%22&andtext=&dateFilterType=range&page=1) in 1886 about a Civil War regiment traveling by train to Washington DC for a reunion. During the trip, they had a picnic on board, and in one of the baskets was found ""two blushing little radishes nestled up to a big fat sour pickle"".

In 1895, a grocery store on City Island in the Bronx [ran an advertisement](https://nyshistoricnewspapers.org/lccn/sn2001061897/1895-02-16/ed-1/seq-4/#date1=01/01/1830&index=6&date2=12/31/1920&searchType=advanced&SearchType=prox10&sequence=0&words=Pickles&proxdistance=10&to_year=1920&rows=20&ortext=&from_year=1830&proxtext=Pickles&phrasetext=&andtext=&dateFilterType=range&page=1) for some of their goods on sale including a jar (or can) of ""40 sour pickles"". It's conceivable these could be other kinds of vegetables, but the fact that they did name other canned vegetables by name in the ad, but not pickles, seems to lend weight to the idea that pickled cucumbers were the ""pickles"" for sale (plus the fact that 40 were able to fit in a single container).

A [recipe for ""dill pickles""](https://idnc.library.illinois.edu/cgi-bin/illinois?a=d&d=NRF18990713.2.66&e=-------en-20--1--img-txIN-%252522dill+pickles%252522+cucumbers--------) published in Illinois in 1899 didn't mention cucumbers in the title, but did in the body, since it was an ingredient. An [1897 ad](https://newspapers.library.in.gov/cgi-bin/indiana?a=d&d=INN18970811-01.1.3&e=-------en-20--1--txt-txIN-%22dill+pickles%22+lunch------) published in Indiana for ""Baum's Homemade Dill Pickles"" didn't mention any vegetable at all.

***cont'd...***",0
"There are a lot of dimensions to this... as usual it's hard to say why something didn't happen, but I will give an overview of at least some of the concerns involved.

So, to start, it's really arguable whether Zoroastrianism can be described as monotheistic (it's a little bit perspective-dependent), but I think the more salient point is that it didn't historically _construe_ itself as monotheistic.

Let's make a simple comparison - consider Christianity and its trinity, and especially Catholicism with its veneration of saints. Are these _obviously_ monotheistic from an outsider's point of view? Well, early muslims for one had their doubts about that, and Islam has a more limited conception of monotheism (centering on the ""one-ness"" of God) that it applies to itself and Judaism, but not necessarily to Christianity. Certain Christian sects, like Jehovah's Witnesses, even reject the trinity out of these concerns.

Now within various traditions of Christianity, there exists a whole set of frameworks and lines of theological reasoning to explain why Christianity is _still_ monotheistic despite the Trinity, veneration of saints, etc. Of course, those frameworks exists because it's _not_ obvious how Christianity, having these features, remains monotheistic.

So the point I'm making with this is that ""monotheism"" is, in a sense, more of an _ideological_ statement than one that objectively describe a religion (as opposed to the set of texts, traditions, rituals, etc that makes up the religion's contents). And, crucially for this discussion, it is an ideological statement that Zoroastrianism _did not historically concern itself with_. Zoroastrianism includes the veneration of a whole array of deities that represent manifestations of moral concepts (e.g., ""devotion""), elements (e.g., ""fire""), and social order (e.g., ""power"").

Now, Ahura Mazda, the Supreme Creator, is certainly made very distinct from all these other deities and conceived of as the creator of all of them, but there is simply no concern in Zoroastrian tradition for this, prior to the subjection of Zoroastrians to Islamic rulers (where asserting at least a weak form of monotheism became important for the legitimacy and survival of the religion). Sasanian rulers had no problem identifying Ahura Mazda with Zeus, as Greeks had done before them, and especially in the east of the Iranian empires, all kinds of Zoroastrian-Buddhist-Hindu-Hellenic-Mesopotamian syncretisms had flourished across the ages.

Thus, while there is a lot of evidence for interaction between Jews and Persians (such as a number of Persian loanwords in the Hebrew Bible, which are especially significant as Persian was not an administrative language in the Achaemenid Empire), there is no particular reason for why Zoroastrianism, especially at that time when it was still solidifying as a religious tradition, would have stood out from Mesopotamian or Greek religion as particularly ""monotheistic"". (Indeed, the development towards actual monotheism in Judaism was probably pretty recent or even ongoing at that time, in the post-exilic era).

As I indicated above, when we talk about the Achaemenid and Parthian eras, these also are times when our understanding of what Zoroastrian theology actually looked like is very poor. It _probably_ wasn't written down but rather orally transmitted, in liturgical rather than common language. The only things that survive from these times are oral hymns, though. It is only towards the middle-late Sasanian era that we would expect there to be actual written works in Zoroastrianism on theology, especially in common languages like Middle Persian, some of which are still extant (at least in later iterations).

So while I tend to believe there was interaction and transmission of cultural and religious notions between early Jews and Christians and Persian Zoroastrians, it probably would not have taken the form of engaging directly with theology produced a religious elite, or anything similar.",0
"This is of course the age-old question, for the trans-Atlantic slave trade and other forms of abusive/exploitative labor situations. Ultimately, we have to assume individuals made decisions based on what they believed was most profitable for them (whether that meant following tradition/conventional wisdom, longtime personal experience, changing one's mind after a big mistake, etc), in monetary terms. It's certainly difficult or impossible to see white people operating with a sense of moral costs.

In terms of the journey from the interior to the coast, we have to talk about an inherently bad situation that was exacerbated by slavers' particular methods (especially forced malnutrition--both low calorie and nutrient deficient; a common complaint among slave ship captains is that their human cargo developed scurvy *way* too soon after leaving shore; clearly they were deep in vitamin deficiency long before deportation). First, it's important to keep in mind that Africans' involvement as traders in the trans-Atlantic trade grew out of slaveholding and slave-taking in sub-Saharan African societies. Indeed, the kingdom of Benin noped out of the exterior trade after a century or so because it needed to keep its slaves for its own use. (The Kongolese also tried this for a period of time, but ultimately re-engaged). African slavers were trading first of all to other Africans. And Africans tended to favor certain types of slaves: healthy women, young children and babies, then healthy men for labor. The majority of the slaves sold into European hands were younger men, and those would have been the ones perceived as weaker and less suitable for labor by potential African buyers at marketplaces along the way to the coasts. So as caravans moved closer to the European trading zones, they were comprised more and more of injured, weakened, even sickly men. NOT exclusively, of course, but the demographics are pretty clearly tilted towards younger men.

And sickness was a major problem, maybe *the* problem. Although it's commonplace on AskHistorians to point out that Africans shared exposure to a lot of European diseases, making them less susceptible on immune grounds than e.g. Native Americans, the picture gets more complicated when broken down into individual African peoples. More isolated groups from farther inland, Miller notes, did not have the same exposure to a wider range of disease that their urban and coastal counterparts did. In the course of the journey to the coast, which could take up to six months in some cases, stopping at villages, encountering other traders, and potentially being combined with a large group from another population spread germs that they could not yet handle. So disease was a big problem, and it would have been exacerbated by forced march conditions, malnutrition, and physical abuse from traders.

There are indeed some voices pointing out that hey, maybe mistreating your labor force is not the best move. Francisco Coste, whom I quoted above, was actually pretty forceful on this point. He's certainly no abolitionist, but his book does call emphatically for better treatment of slaves. (Kanonoja's essay that I linked is really good on this point). But there are centuries and centuries of data proving what an oddball thought even ""have slaves, but be nice to them"" was in the trans-Atlantic trade.",0
"**Part 1 - Context**

The source material for late medieval France specifically is a little sparse, but there wasn't anything particularly special about France compared to its neighbours so I hope you don't mind me using evidence from places other than France. The reason this question has taken so long for anyone to answer is that the source material needed to answer these questions are sparse and difficult. People writing in the Middle Ages cared about kings and feats of valour; when a trebuchet is demolishing a castle wall, no source cares to record the long chain of events that brought said trebuchet to that moment. The *best* we’ve got in terms of sources are financial records, mostly English, which record these people being paid. Imagine trying to work out say… the life and background of a modern pilot based only on the quarterly reports of American Airlines, but where most of said reports were damaged in a fire. So it’s best to see this answer as “Here is how I think it worked based on extremely fragmentary sources that are insufficient but it’s all we’ve got so I might be wrong about some things” rather than “Here’s how it worked, bask in my genius.”

An important thing to note about this question is that there were generally two groups of people who would be part of a trebuchet crew. The first were just rank and file soldiers, or local labourers, who were told to build and operate the machine. They provided the basic manual labour. The answers to your questions for this group are very simple: they volunteered or were ordered to do it, no vetting process required beyond 'can you use a hammer?'. The main downside was that your own machine might malfunction and kill/maim you, the main upside was bonus pay and avoiding the front lines, though the enemy's archers and artillery would've been doing their best to kill you.

The second group of people, who I suspect are the people you're more interested in, were specialised carpenters and engineers who designed the trebuchet, constructed the most complicated aspects of its design, supervised and directed the labourers, and sometimes aimed it using maths. They usually also had the job of repairing the wall they just knocked down after the castle or town was taken.

I also think a bit of context the logistics of western medieval trebuchets might be helpful. From the 13th century, it was increasingly common for trebuchets to be flat-pack. That is to say, the components were assembled before the campaign and then brought along to an enemy castle or town in wagons. They were then assembled by a team of carpenters and labourers. This innovation made them quicker to build, easier to transport, reusable across multiple sieges, and allowed for standardised designs that could be produced in bulk. The flat-pack trebuchet seems to have been an innovation of Richard I's engineers for the Third Crusade. Richard anticipated sieges but also knew that the Holy Land did not have sufficient forests with sufficiently strong wood to support ad hoc trebuchet and catapult construction. Richard decided to bring two massive trebuchets with him, but they needed to fit on the ships Richard was using to ferry his army to the Holy Land, which forced the innovation of a compact transport solution. By the later Middle Ages it was commonplace for armies to travel with wagons full of trebuchet parts.

This means that a trebuchet crew actually had two sides to it. There were the people who operated the machine in the field, and the people who originally built it. For the purposes of answering the question, I'm going to call that first group the 'field team' and the second the 'design team'. They would often overlap, especially for non-standard machines where the carpenters who built it might be paid to come on campaign and supervise their machine themselves. Due to the nature of the evidence, we know far more about the design team than the field team.",0
God this subreddit really is the best,0
"It was a bit frustrating to see this on the front page without any answers, so I've decided to do a little research of my own ^with ^the ^help ^of ^Google. 

From what I gather, there has not been much scholarly work on the topic. The most-cited book I found was Mohammad Gholi Majd's *The Great Famine & Genocide in Iran* (available in [preview mode on Google Books](https://books.google.be/books?id=5WgSAAAAQBAJ&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false)).

You can also have a look at [*Iran and Its Place Among Nations,*](https://books.google.be/books?id=PO_s9H5QTp4C&pg=PA16&dq=iran+famine+1917&hl=en&sa=X&ved=0ahUKEwjUlrC_7oXOAhUGXRoKHVbqDXYQ6AEIJTAC#v=onepage&q=iran%20famine%201917&f=false) which provides a summary of Majd's argument.
>One of the little known calamities of World War I and its aftermath in Iran is the widespread famine that the war engendered across the country. The most significant treatment of this subject is the book by the agricultural economist Mohammed-Gholi Majd, *The Great Famine and Genocide in Persia, 1917-1919.* [...] Majid quotes the American charge d'affaires in Persia at the time, Wallace Smith Muray, as having claimed that in 1917-1918, up to one-third of Iran's population was wiped out by famine and disease. 

>To be sure, the exact causes of the devastating famine are difficult to grasp. Majd blames the British who, after the Russian revolution of 1917, had become the main foreign power in Iran. Having isolated farmers from their customers inside Iran the British, Majd claims, made it impossible for the country's fertile regions in its north and northwest to remain connected to the rest of the country. At the same time, according to Majd, a significant amount of the grains produced in Iran were forcibly acquired for British troops in the country and shipped abroad to other British troops in the region.

>Majd concludes that, unbeknownst to most, one of the main genocides of the twentieth century occured in Iran during and immediately after World War I.

However, the authors also point out:

>It is difficult for rigorous academic research to corroborate these figures. The word ""genocide"" implies the willful killing of large numbers of noncombatants. The historical record in this area is murky. A more extensive scholarly treatment of this subject would have to utilize ""triangulation"" and provide evidence from others, including British, Russian, and Ottoman sources, to show the extent of the famine and the ways in which it was affected by the war and its aftermath. 

And, if you have access to JSTOR, you can also find [another review of Majd's argument by Persia scholar Willem Floor](https://www.jstor.org/stable/4311715?seq=1#page_scan_tab_contents). He is very critical of Majd - going as far as sarcasm:

>So good were the British in concealing that there even had been a famine that the document tabled by the official Persian delegation to the Versailles peace treaty trivialized the famine and obscured its causes [...] The scheme worked so well that scholars dealing with Iran, even if they did know about, did not write about it, until now. 

The thrust of Floor's argument is that Majd's estimate of the Iranian population is far too high and that he has it out against the British. He suggests reading James Barton's *Story of Near East Relief 1915-1930* for a overview of the difficulties of famine-relief in the broader region.

Barton points to light snowfall in the winter, spring drought and the breakdown of the irrigation system as partially responsible for famine in Iran's Kermanshah and Hamadan provinces. 

So, as you can see, there's some debate around this famine. I'm surprised there is no Wikipedia article about it.",0
"> Now, the negotiations took a turn when Napoleon decided he needed more money and offered the whole Louisiana Purchase for $15 million.

Note also a change between 1801 and early 1803 was the French Reconquest of Haiti went sour. 

Haiti was before the revolution there, the crown jewel of the Caribbean, with Sugar, Coffee, and Indigo plantations a major profit center. 

Haiti was also a massive slave state, with something like 80+ % of the island being slaves, with the majority of the slaves being first generation slaves born free in Africa and sold into slavery. The French revolution inspired revolutionaries in Haiti, and a general slave rebellion. 

The course of the Haitian revolution was many sided, with everything from attempts at taking the colony by the British and the Spanish, to the local white population attempting to rule as a republic, to the government of André Rigaud and Louverture's victory. Louverture had control of Haiti by 1801, declaring himself Governor General for Life. Louverture and the French were suppose to be on the same side, but Napolean was gradually reinstating slavery in other French Colonies, making it clear that was the plan for Haiti. Louverture invaded the Spanish side of the island, and passed a Constitution which made Haiti effectively independent.

In 1802, Napolean's brother in law Leclerc lead an expedition of ~31k troops to retake Haiti and restore the plantations. The French had a lighting campaign that retook the cities and major agricultural areas.

But there's been more than a decade of slave revolts and fighting. The irrigation works were destroyed, the vast majority of plantation either left untended, or turned to producing food. The crown jewel of colonies was no more. There were some plantations that survived, here and there, but the river of sugar was but a trickle. 

On top of that, the same thing that suppressed English and Spanish attempts to seize Haiti hit the French troops. Yellow Fever resulted in something like 20K+ of the expedition's forces dying. Including Leclerc dying in November 1802, and driving Pauline Bonaparte back to France, landing New Years day 1803. 

So by 1803 Napoleon had no real purpose for New Orleans, as Haiti would no longer be a base to build a New World Empire, nor would the wealth of Haiti fill his coffers. War with England loomed, and the British would just take New Orleans. 

Selling it all to the Americans would wash his hands of the whole thing. ",0
"Part of the issue in answering this is that there isn't really *an answer*. At best we can say there are shades of acceptability that have shifted over time, and at least in some senses, we still have not reached a point where we can decisively say it is unacceptable, as there are examples which could be pointed to which nevertheless break the 20 year rule.

That doesn't mean there aren't major points we can look to though in the evolution of the unacceptability! To be clear, there are multiple ones to concentrate, and I am not looking at all of them, but the span of time I'm looking at is nevertheless an important one:

>ARTICLE I

>The High Contracting Parties solemnly declare in the names of their respective peoples that they condemn recourse to war for the solution of international controversies, and renounce it, as an instrument of national policy in their relations with one another.

>ARTICLE II

>The High Contracting Parties agree that the settlement or solution of all disputes or conflicts of whatever nature or of whatever origin they may be, which may arise among them, shall never be sought except by pacific means.

>ARTICLE III

>The present Treaty shall be ratified by the High Contracting Parties named in the Preamble in accordance with their respective constitutional requirements, and shall take effect as between them as soon as all their several instruments of ratification shall have been deposited at Washington.

While it now, if anything, is the butt of jokes, the 1928 Kellog-Briand Pact is actually more deserving of respect than it ends up getting in popular memory. Signed in 1928 as an attempt to 'Renounce War' as 'an Instrument of National Policy' by the great powers of the world, as its name states, it seems quite laughable in hindsight thanks to that minor conflict that erupted a decade later known as World War II, and which saw signatories such as Germany and Japan involved as aggressors. And to be sure, it isn't an unfair criticism! Outlawing war seems like a fools errand even today, where perhaps we have avoided great power conflicts in our lifetime, but certainly have had no end to smaller outbreaks of interstate violence.

But nevertheless, the Kellog-Briand Pact *does* represent an *aspirational* turning point, and an expression in international diplomacy of the unacceptability of armed conflict as a means to solving international disputes in a way that surpassed previous international agreements. More importantly though, it provides a binding expression of international law. To be sure, the treaty was entirely toothless. [The three scant articles were entirely aspirational and included no method for enforcement](https://en.wikisource.org/wiki/Kellogg%E2%80%93Briand_Treaty), and did nothing, in the end, to prevent the Second World War.

What they *did* do though is nevertheless quite important! After the Allied victory over the Axis, trials were conducted at Nuremberg and Tokyo, and charged, among other things, with crimes against peace, a charge which they were able to substantiate with the signatures of the Axis power to the Kellog-Briand Pact, wherein they had renounced war of aggression. It should be noted that there *was* considerable debate over whether the pact provided sufficient justification, due to its lack of an enforcement clause, but in the end it was decided that enforcement was justifiable, with a key argument being that the Hague Conventions, which laid down the basic laws of war, likewise lacked enforcement mechanisms for its various clauses, yet had nevertheless been treated as such. The specific treaty having an enforcement mechanism was less important that its validation of the principle as a part of positive international law.

Once the contentious issue of whether the charges could be brought in the first place was settled, it was an incredibly easy charge to prove and convict for. The guilt of the accused, was all but self-evident, the only serious issue to establish being whether the treaty itself meant anything. The result then at Nuremberg, and the less remembered Tokyo Trials, was the enforcement of the spirit of the Kellog-Briand Pact, and giving meaning to its lofty ideals! At no point prior in history can we point to a situation which is so simple as to accurately summarize that 'War was outlawed, and violators convicted'.

But... let us not get ahead of ourselves and spike the football. One of the prosecuting powers was quite guilty themselves as whatever their protestations, the Soviet Union had sliced up Poland with the Germans back in 1939, and plenty more examples of war as 'an Instrument of National Policy' dot the post-World War II historical record, not only without prosecution, but in some cases conducted by the powers who had engaged in the prosecution that established the precedent in international law. 

The Kellog-Briand Pact, nominally at least, remains in effect. Likewise so do the precedents that resulted from conviction at Nuremberg. Inarguably, attitudes *did* change, and creation of the UN, whose charter provides similar sentiments to the Pact, helped to further reinforce them. At least occasionally the UN's charter even provides legal backing to intercession against violations of these principles, such as in Korea, which was fought under a United Nations flag, or Resolution 678 which authorized the use of force against Iraq in 1991. But we return to the beginning as well and must reemphasize once again that we can only talk of shades of acceptability. The mid-century period surrounding the Second World War was a critical junction, and *perhaps* the single most important shift insofar as we look at relationships between the 'great powers' and their use of military force against each other, but its impact on smaller powers, let alone great powers in relation *to* smaller countries is very much a changing shade at best, with numerous examples easy enough to point to as at least worth discussion as to whether they violate those principles, and which violate the 20 year rule but are no doubt easy enough to deduce. 

It is important *not* to go so far here as to argue simply that 'might makes right', but it certainly is important to emphasize that the strictures of international law are not ones which can be enforced uniformly. Does this mean we can't say norms concerning what was and wasn't acceptable in terms of aggressive war and conquest shifted through the 20th century? Of course not! But it does mean we must be cautious in talking about absolutes, or of seeing changes in *too* emphatic a light. We can look both before and after the period being concentrated on here and see other critical junctures in the shifting attitudes towards acceptability, but that itself speaks to why it can be tough to look at this question in anything other than shifting shades, rather than finding one, single turning point.

**Sources**

Hathaway, Oona A. & Scott J. Shapiro. *The Internationalists: How a Radical Plan to Outlaw War Remade the World.* Simon and Schuster, Sep 2017.

Hirsch, Francine. *Soviet Judgment at Nuremberg: A New History of the International Military Tribunal After World War II.* Oxford University Press, 2020.

Plesch, Dan. *Human Rights After Hitler: The Lost History of Prosecuting Axis War Crimes.* Georgetown University Press, 2017.

Krammer, Arnold. *War Crimes, Genocide, and the Law: A Reference Handbook.* ABC-CLIO, 2012.",0
"I would say that it’s not a recent phenomenon, but neither is it a universal human phenomenon. Domesticated animals, especially dogs, but also cats, horses, cows or cattle, are a as far as I know universal feature of human civilization. They have been with us from the very beginning, taking a great role in defining what it means to be human. One can go back to the earliest literary texts to find references to this special connection, just think of Odysseus returning to his home of Ithaca after twenty years, to find that his old hunting dog Argos had been waiting for him all this time, only to die after seeing his master again. That being said, the nature of our relationship to animals and dogs in particular is variable, from being used as a source of food or objects of blood-sports, to working as draft-animals or guardians, as hunting-dogs, racing-animals, or close companions and pets. Most of the dogs in history will have been part of the first two categories, the latter categories being clear status markers of luxury.
I’m going to talk a bit about Roman antiquity, since that is what I know best. In antiquity, while dogs were also commonly used as sources of food (usually it seems the puppies) or working animals, or roaming the streets and fields as feral dogs who would be killed as a nuisance, for members of the aristocracy, especially hunting dogs were a prized object of conspicuous consumption, and a clear status marker. Funerary reliefs from Greece, f.e., will often depict the young aristocrat with an elegant hunting dog to show his status as a member of the elite – someone who could afford both a dog bred for hunting, and someone who could afford this luxury pastime. In Rome, the conquest of the Mediterranean during the first two centuries BC brought with it a great influx of wealth and the establishment of a larger class of people with the means to live a lifestyle disconnected from the need for subsistence farming – also a disconnect with the world of animals-as-a-workforce. Urban rich, senators and aristocrats, but also a growing middleclass of traders or artisans, whose pride in their new status found its expression in lavish funerary monuments. At the same time, contact with foreign areas brought an appreciation for exotic animals as status markers, who often were paraded through the streets in triumph, such as lions, crocodiles, elephants, hyenas or bears.
In the first century BC and increasingly from then on we have more and more evidence for pet-keeping – keeping animals just for pure enjoyment and not exploiting them for their work or talents. Roman poets referred to their or their friend’s favourite animals, historians mentioned the pets of emperors, and dead pets were honoured with their own epitaphs. The language used to refer to these pets often reflect that used for kids or lovers, for example, *deliciae* or *delicius*. This doesn’t refer to their taste, but means something like ‘lovely’, ‘dearest’, used of close companions or favourite slaves. Cicero uses it to refer to his daughter, Tullia, Seneca to refer to the pet dog of the emperor Claudius in his satire *Apocolocyntosis* (the Pumpkinisation), Catull to refer to one of his lovers. Conversely, animal names could also be used to refer to beloved humans, in a way that shouldn’t feel too out of place to modern observers (‘my dove’, ‘bunny’ and so on).

The most interesting evidence of how widespread this phenomenon was are probably the funerary inscriptions for dogs, some of which with elaborate poems that sometimes reference popular works such as Vergil. This is the case for one of the most famous examples, a marble epitaph for the gallic hunting dog Margarita, from 2/3rd century Rome, on a [marble slab now housed at the British Museum in London (CIL VI 29896 = CLE 1175)](http://db.edcs.eu/epigr/bilder/$CLE_01175.jpg):

>*Gallia me genuit nomen mihi divitis undae / conchae dedit formae nomini aptus honos / docta per incertas audax discurrere silvas / collibus hirsutas atque agitare feras / non gravibus vinc(u)lis unquam consueta teneri / verbera nex niveo corpore saeva pati / molli namque sinu domini dominaequae iacebam / et noram in strato lassa cubare toro / et plus quam licuit muto canis ore loquebar / nulli latratus pertimuere meos / sed iam fata subii partu iactata sinistro / quam nunc sub parvo marmore terra tegit / Margarita*


> Gaul gave me birth, the shell of the rich waves my name: the honour of the name is fitting for my beauty. Taught to roam the unexplored woodlands with courage, and chase hirsute game across the hills, unaccustomed to be held by heavy shackles or to endure savage beatings with my snow-white body. For I used to lie in the lap of my master and my mistress, and mastered the art of resting wearily on a spread-out blanket. And though I was able to express more than I was entitled to say with the mouth of a dog, no-one feared my barking. But I have already met my fate, stricken down giving ill-fated birth, I, whom now covers the earth beneath this small marble plaque. Margarita.

This has been taken as a parody of Vergil, and more tongue-in-cheek than honest sentiment, which Irene Frings has argued against convincingly I think. Also, this is by far from the only example of such epitaphs for dogs. On the one hand, dogs (or cats) are often seen accompanying tombstones of little kids, as an upbringing and education together with pets was seen as something desirable and an indicator of higher socio-economic status, since not everyone could afford pets. [Here's an example from the province of Germania superior](http://db.edcs.eu/epigr/bilder/$Grabstelen_00242.jpg), near modern Saverne, the tombstone of the girl Belatula, depicted together with a small dog (or maybe cat?), holding a ball in her hand, both symbolizing a carefree elite childhood, put up by her father (AE 2015, 995). They are thus also expressions of social ideals and need not reflect actual reality for small girls or boys all over the empire, but certainly something their parents aspired to afford for their children. [This stele](http://www.edr-edr.it/foto_epigrafi/immagini_uso/131/131027-1.jpg?dummy=1613480970) from 1st century Rome (CIL VI 19019) for the slave girl *Helena* eschews depicting the little girl and instead displays the dog (if Helena is not the dog, which is unlikely). (*For Helena, foster-child of incomparable spirit, well-deserved*).

Further, there are lots more epitaphs specifically for dogs that show that this was not an uncommon phenomenon. Sometimes, they are very simple, such as [this funerary stele for the dog *Heuresis* (the Finder/Tracker)](http://db.edcs.eu/epigr/bilder/$CIL_06_39093.jpg) from Rome (CIL VI 39093), late Republic or Augustean age.

Others are again more elaborate, referring to the terrible feeling of loss every pet-owner knows, like this [funerary monument for the dog Aeolis from Praeneste in Campania (AE 1994, 348)](http://db.edcs.eu/epigr/bilder/$AE_1994_00348_1.jpg): 

>*Aeolidis tumulum festivae / cerne catellae / quam dolui inmodice / raptam mihi praepete / fato*

>The tomb of Aeolis, the cheerful little dog, whose loss to terrible fate gave me unmeasurable pain

Another poem is used to commemorate the loss of the dog *Patricus*, on a [marble slab from 2nd century Salerno](http://www.edr-edr.it/foto_epigrafi/immagini_uso/120/120622.jpg?dummy=1613481433), again using a very expensive material (CIL X 859), and apparently buried in the same plot which his master had chosen for himself, referring to their 'spirits' (*manes*) being joined together:

>*Portavi lacrimis madidus te nostra catella / quod feci lustris laetior ante tribus / ergo mihi patrice iam non dabis osculla mille / nec poteris collo grata cubare meo / tristis marmorea posui te sede merentem / et iunxi semper manibus ipse meis / moribus argutis hominem simulare paratam / perdidimus quales hei mihi delicias / tu dulcis patrice nostras attingere mensas / consueras gremio poscere blanda cibos / lambere tu calicem lingua rapiente solebas / quem tibi saepe meae sustinuere manus / accipere et lassum cauda gaudente frequenter / [---*

>I have carried you covered in tears, our little dog, as in happier times I did fifteen years ago. Now, Patricus you will no longer give me a thousand kisses, nor will you be able to lie affectionately round my neck. Sorrowfully, I have placed you in this well-deserved marble tomb, and I have joined you for ever to my own spirits. Your manners showed you equal to a human, alas! What a pet (*delicias*) we have lost! You, sweet Patricus, were used to joining us at the table and panting asking for food in our lap, you were accustomed to lick with your stealing tongue the cup which my hands often held for you, and often to welcome your tired master with wagging tail . . . .

I could go on with quite a few examples (many of them are collected in Herrlinger 1930), but I think they will suffice to show that then, as now, dogs were more than just companions. Was this just an elite phenomenon, restricted to the upper classes? We don't know - our sources are mostly from members of precisely those classes. Archaeology and History have recently been putting a new focus on our entanglement with animals, so our picture may get clearer in the future. What is certain is, that many animals in that time led a deplorable life. But for many others, they were members of the household that were, in death, honoured in much the same way as one would a dead child or slave (for which we often find the same kind of epitaphs). Some of them seem to have spared no expense to secure an adequate commemoration for their beloved pets, something most common people in the Roman Empire weren't able to afford for themselves. Then, as now, this comes close to a form of luxury consumption that for some may seem to border on the perverse, but, as /u/cthulhushrugged said - some things never change.",0
"This is an enormously complex topic with a lot of competing and evolving scholarly views, so there's lots of room to say more! In the meantime, our FAQ has a great section on [the Arab world before Islam and the rise of Islam](https://www.reddit.com/r/AskHistorians/wiki/middle_east#wiki_arab_world_before_islam). I'd highlight a few answers in particular:

* [What was the culture and religion of the Arab peninsula before Islam?](https://www.reddit.com/r/AskHistorians/comments/1976mo/what_was_the_culture_and_religion_of_the_arabian/c8lfyje/) - by /u/Daeres
* [How did Muhammad know so much about Christianity and Judaism?](https://www.reddit.com/r/AskHistorians/comments/3jirc1/how_would_muhammad_have_originally_known_so_much/cupo1n5/) - by /u/shlin28

And one other thing I want to highlight about early Islam's interactions with other religions:

Islam is supercessionist with respect to Judaism and Christianity; it incorporate their ideas and says ""Well, they got these parts wrong and we've got past it."" Hence the basic religious-kinship principle. However, Islam has no genetic relationship with a contemporary major Central Asian religion: Zoroastrianism. As early Muslims armies conquered Persia, they often treated Zoroastrians as *dhimmis*, as People of the Book by proxy. The ""Magians"" were not subject to forced conversion campaigns or massacres; their houses of worship were not initially destroyed. 

And this idea of applying an existing, tolerated religion to a newly-encountered faith/spirituality gets repeated in later eras of Islamic expansion. African traditional believers in West Africa are sometimes described essentially as Zoroastrians; Hindus are sometimes framed as a variety of ancient Greek pagan (somewhat respectable in Islam thanks to the strong scholarly tradition). In the 13th century, Iranian/Ilkhanate vizier and historian Rashid al-Din actually claims *Buddha* as a prophet of God, in the Islamic tradition, to facilitate interreligious relations and further court Eastern conversion to Islam. 

The tolerance of other religions wasn't a guarantee (at some points, such as under the Almohads in North Africa and Iberia, neither was it for Christians or Jews, either...or Muslims perceived as not zealous enough, for that matter). The ruling class of pre-Islamic Persia, specificially, was given the choice to convert to Islam or lose their positions. That's a perfect example, actually--Zoroastrianism had been inseparable from imperial ideology, so converting to Islam also represented a transfer of loyalty to the new caliphate. Thus, when it was *useful*, Muslims found ways to justify the inclusion of other religions even on a theological level.

What this highlights is the importance of *practicality* in the idea of dhimmi status, of Peoples of the Book. It includes, but transcends, the original Abrahamic ties.",0
"There's no question that the prevalence of myopia (nearsightedness) was much lower in the Middle Ages than today. No, we have absolutely nothing even remotely resembling statistics, and just because in the past century or several there's been a noticeable increase in myopia doesn't itself say anything about medieval western Europe (what I'll be talking about here). However, medieval popular Christianity relied on *visuality* in some pretty basic ways I'm not sure it would have if 50% of some such of people couldn't see their own shoes. ""Art is the book of the illiterate"" was a basic philosophy of religious instruction. Church sculpture, mosaic, decorated rood screens, stained glass were meant to glorify God but also to instruct (hence the popularity of Last Judgment scenes over the tympanum). And watching the priest consume the Eucharist (bread and wine) could substitute for the person doing it themselves--even in the case of a saint like Nicholas von Flue.

But we also know that some people were indeed nearsighted, and obviously eyesight deteriorated in various ways (presbyopia/farsightedness, AMD, cataracts, etc) with ageing. I want to briefly introduce a distinction that scholars of disability and medicine make between ""impairment"" and ""disability."" Impairment *in this sense* is used as a deviation from the normative (not from the normal! lots of things are normal); disability refers to when an impairment affects someone's functioning in the world. Here, I'm going to talk more on managing visual impairment to minimize (not necessarily eliminate) disability.


This being the Middle Ages, obviously the first way people tried to cope was by praying for miracles. And indeed, miracle stories attached to saints and shrines offer some really interesting cases of the extreme end of visual impairment that's ""not quite blindness,"" although we should keep in mind that ""blind"" is a subjective term that means different things in different contexts.

Two important themes that arise from miracle stories are the effect on mobility and on ability to work. One of St. Elisabeth of Hungary's miracles concerns a girl who, it's clear, has some vision but only a little; the story emphasizes that she can't even see well enough to find her way on a path. (Medieval ""roads"" could just be a trampled line through fields). There's a similar miracle concerning a middle-aged man who developed possibly cataracts (*macula*, stains) and similarly couldn't find his way on a path; he reported being ridiculed for it. 

In these cases, the key was to lean on the support of other people. We read a lot about blind and otherwise visually impaired people being ""led"" to shrines. That's probably the single most common theme in anecdotes of people with disabilities in the Middle Ages--the ad hoc, case-by-case reliance on friends and family, or in a few cases monastic charity.

With respect to work, it seems that in a lot of cases the key was to find a task a person could perform. The miracles of St. Bertin tell of one evidently nearsighted man who couldn't see well enough to perform outdoor manual labor, but up very close could still see well enough to do needlework. So he sat with the women of the duke's household all day, working on embroidery and weaving!

The later Middle Ages did have knowledge of and practice rudimentary cataract surgery, sometimes even successfully. In 1351, aging abbot Gilles le Muisit of St. Martin's in Tournai had his cataracts removed only to find that behind them, he was farsighted:

> I saw not as in my young age but as my age demanded, because I was
already an octogenarian, and I saw the sky, the sun, the moon, the
stars, though not perfectly recognizing people, and I saw everything at
a distance from me very well, but I was not able to write or read.

The first ""glasses"", which were designed to be held rather than worn, were for reading (i.e. for farsightedness). [Here's a photo](https://imgur.com/a/7KRHt) I took in the Dominican church museum in Eisenach of a 1510 altarpiece relief from Thuringia--the scene is the death of the Virgin Mary, and the man with the book and glasses is probably supposed to be a physician. Glasses were one type of iconographic shorthand for physicians (another being a flask filled with urine. You do you, Middle Ages.)

Ronald Finucane and Irina Metzler have both pointed out that we are much less likely to read about nobles with disabling conditions. There was then as there is now a stigma attached to disability, even more so because of the stronger link between sin (moral failure) and impairment. So even in miracle stories that *do* involve knights and other nobles, the recorder will allow the healed to express a sense of shame--even to the point of feeling suicidal, in one case--a consciousness of emotion denied to lower classes. 

That was the case with the knight Gilbert, whose story is told in the miracle of St. Foy. *Twice in a row* he received a head injury in combat--once while, apparently, breaking up a barfight--that caused vision problems. And he was, given the source, miraculously healed in several stages. (Draw your own medical conclusions.)

But one of the most interesting cases hops way above the others. Holy Roman Emperor Frederick II in the 13th century--the one who looked at the Crusades, said ""I got u, fam"" and *negotiated* his way into taking over Jerusalem from Muslim control--actually dealt with developing myopia over the course of his life. This could have been a hardship for him, as his true love was hunting. (He embarked on very few building projects over the course of his reign, but these included several hunting lodges, which is also where he preferred to spend much of his time). What did he do? He still hunted with great enthusiasm, apparently. But then, hunting in the Middle Ages wasn't about food (unless you were desperate and poaching), it was about power and masculinity and status and probably blowing off some steam. So even in the parts where humans didn't rely on their dogs and birds to do the work for them, perhaps there was not as much pressure to perform.

But then, of course, there's the ultimate solution. Can't fight like a knight, can't hunt as well as you want to?

Write a book about it.

And Frederick did.",0
"When it comes to educating about the history of sexuality and gender, historians employ several strategies to address concerns of erasure and provide a nuanced understanding. Here are a few approaches:

1. Evidence-based research: Historians rely on primary sources, such as diaries, letters, legal documents, and artwork, to uncover evidence of same-sex relationships or non-normative gender identities. This evidence helps to build a more accurate picture of historical realities. 2.",1
"While General Dwight D. Eisenhower played a crucial role in documenting the Holocaust and ensuring that evidence was presented to the public, he was not the only Allied general involved in these efforts. Several other generals also made significant contributions to documenting the Holocaust and exposing Nazi atrocities. One notable figure is General George S. Patton, who commanded the U.S. Third Army. Patton personally visited and inspected a number of concentration camps, including Ohrdruf in Germany.",1
Thank you for your answers here. They are really insightful. I had a hard time trying to figure out what the specific moral of that story is beyond “don’t forget about God or bad stuff happens” still one thing though. It’s a historical account that has a moral lesson involved?,0
"> On the contrary, chain worn simply as a small strip on the face
 
What would a curb chain look like and what kind of protection did it offer? Would this cover the entire head  or just vulnerable areas like the neck and face?",0
"Expanding from an earlier answer of mine:

-----------

The image and idea of Columbus has gone through several phases, historically, but the focus here is going to be on the current epoch, and the rise of Columbus Day, as that plays the most important part in your question and the current perception and how we got there.

Columbus Day is very closely tied to Italian-American identity, originating almost exclusively as a holiday celebrated by Italian Immigrants, who wanted to celebrate their early ties to the 'New World', and stake their claim to being part of the idea of America (it is interesting to note, also, that Italian immigrant communities formed a unified idea of their *Italianness* in a way that wasn't quite as present in Italy itself although it quickly expanded to be more broadly embraced by immigrant Catholic communities generally in the period. This was a time when immigrants, especially Catholics and those from Southern Europe, were looked down upon and excluded by the many within the dominant White, Anglo-Saxon, Protestant culture of the United States - something I've written about previously specifically with an eye on the KKK in the 1910s-1920s, [so may be of interest](https://www.reddit.com/r/AskHistorians/comments/9l4uus/italians_were_considered_nonwhites_until_1945_did/e75k1dv/).

It expanded from Italian / Catholics communities into the wider American public over time and by the turn of the century was a somewhat popular celebration, having been designated a time for national celebration in 1892, and celebrating ""American unity"" in the words of one historian, as Columbus made for a great time to celebrate the 'melting pot' concept of which was becoming important to the mythos of the American identity

Angelo Noce, an Italian immigrant, was the big proponent of making it a Federal Holiday, with the stated goal of celebrating Italian Heritage through it, as they had been celebrating it for some decades before everyone else, and considered it ""their"" holiday. The Knights of Columbus were also a big supporter. Being predominantly Irish-Catholic, they likewise saw promise the elevation of a Catholic figure into the highest pantheon of American history, since, as noted, Catholic communities were likewise looked down upon, and seen as at best half American, with dual loyalties not only to their country, but the Pontiff in Rome.

Mass parades on Columbus Day quickly became a way for Catholic groups to demonstrate their civic pride and patriotism, and make the public spectacle of their *Americanness*. Kubal quotes one journalist who, writing about the parade and speeches given by Catholic organizations in in the 1890s, noted how ""*if any doubt existed in the minds of any that Catholics are Americans in every fiber of their being, it ought to vanish in the light of the addresses made everywhere yesterday*"". 

Similarly, drives to put up Columbus statues were spearheaded by Italian-American fraternal groups such as the Columbian Federation or the Order of Sons of Italy in America (although as with the parades, some statues were pushed for by non-Italian Catholic groups). The statues were seen as important, visual symbols of their acceptance into American society, and also the growing ability for Italian-Americans to have political power. The very act of placing the statue in a public place by the government was an important reflection on what the Italian-American community was able to lobby for.

All of this lobbying and parading about saw real progress. In 1905, Colorado (Where Noce lived and had been strenuously pushing for this) became the first to recognize the day as an official holiday, dedicated as:

>created for Catholics, particularly immigrant Catholics, and their children, the special Catholic holiday of the year [...] Christmas and Thanksgiving are religious or family holidays for all the people; Columbus Day belongs to our Catholic people.

Other states such as New York and California soon following suit. It would become a Federal Holiday in 1937, although by that point Noce had passed away so did not see his idea reach culmination.

And for the most part, there wasn't any of the controversy around him we now have. Regressive ideas about the indigenous peoples of the Americas, seen by far too many people as savage heathens for whom the introduction of Christianity and ""Civilization"" was a clear and important good for them (or at least the ones who survived the waves of genocide over the next few centuries...), Columbus and what he brought about was a clear and unambiguous positive for many. To quote one example given from the period:

>Columbus was fired by the noblest motive that can guide the action of man. Every page of his life is teaming with evidence that he went forth on his perilous voyage to carry the Gospel to debased and erring savages, and to pass it to them with the torch of true Christian Civilization... Where shall we find a character worthy to be compared to with him? [...] Columbus was in a measure divine [....] Write his name beside no human hero.

Put plainly, that wouldn't have been to controversial in the 1890s when it was written. Columbus was a hero. He brought civilization to the virgin land of savages barely eking out an existence in the stone age, and made it a place where white people could put that land, which was being wasted by the backwards natives, to good use, and allow a great nation to flourish [ /s]. Italians, and Catholics, were pleased as punch to have this hero that they could point to as *theirs*, and stake their claim as being foundational to the American pageant.

Now, I need hardly point out that as time passed through the past century, attitudes changed *significantly* towards Columbus, and by the 1992 anniversary, Columbus was a very controversial figure, and has only gotten more so since then, as anyone looking at the news this week is clearly aware! Due to the 20 year rule, I'm not going to discuss the *current* stuff in-depth, and whether we should be destro**y**ing th**e**se **s**tatues but the indigenous peoples of the Americas are, unsurprisingly, the leader in opposition to the continuing celebration of a man who more and more are coming to recognize as the kickstarter of a mass genocide (and don't miss /u/Snapshot52 who [covers the this angle here](https://www.reddit.com/r/AskHistorians/comments/h7l1nv/christopher_columbus_was_arrested_and_ostracised/fumak0w/)); and as perspectives change, the Italian-American lobby has been at the forefront of holding onto what they consider to be their national holiday.

-----

McKevitt, Gerald. ""Christopher Columbus as a Civic Saint: Angelo Noce and Italian American Assimilation."" *California History* 71, no. 4 (1992): 516-33.

Timothy Kubal. *Christopher Columbus and the Rewriting of the National Origin Myth.* Palgrave Macmillan, 2008.",0
"One good source on this would be [Citizen Coke: THE MAKING OF COCA-COLA CAPITALISM](http://books.wwnorton.com/books/Citizen-Coke/) by Bartow Elmore (Ohio State). I don't have my copy handy here at home, but the [dissertation on which it is based](https://libra2.lib.virginia.edu/downloads/7w62f853v?filename=X030865291.pdf) is available online. Both have a full chapter on Coke's use and sourcing of coca extracts (ch. 3 in the dissertation) that note "" Originally the Coca-Cola formula called for coca leaf extract containing roughly 3/2001h of a grain of cocaine per serving, but company president Asa Candler decided to eliminate the miniscule narcotic content by 1903 **in the face of growing consumer fears about the adverse health effects of cocaine addiction**."" (emphasis mine, p.131). Concerned about taste, however, the company replaced this with ""decocainized coca leaves"" and kola nut extract, which was internally called ""Merchandise #5."" The sourcing of this material-- really a waste product of the cocaine processing industry --is the focus of the book chapter, following the author's pattern of one chapter for each of the major ingredients (water, sugar, caffiene, etc.)

Later in the book Bartow notes that there was no real reason to keep the coca byproduct (deconainized leaves) in the formula except to maintain the ""secret formula"" recipe, stating that ""there seemed to be no real material use for the coca extract other than to protect the company's brand image."" They just wanted to keep the word ""coca"" in the product name. He draws heavily on Paul Gootenberg's  [Andean Cocaine: The Making of a Global Drug](https://academic.oup.com/ahr/article-abstract/115/4/1200/34416) for the context of this chapter. One interesting factor is that Coke did not own plantations in Peru or processing plants, so was buying on the wholesale market. By the 1880s there was so much global demand for coca that the security and price of that supply came into question, so by removing coca from the product (except for the near-valueless ""decocainized"" leaves) the soft drink company was lowering its costs and severing its ties to the licit coca market.

In an article that summarizes the arguments of his book, Bartow writes about FBN (Federal Bureau of Narcotics) files which ""...reveal how Coke worked with the FBN to secure exclusive access to legal coca imports into the United States after 1914 when the federal government criminalized coca trafficking. The FBN approved special exemptions in federal counternarcotics legislation allowing Coca-Cola to contract with the Stepan Chemical Company in Maywood, New Jersey, to purchase decocainized coca leaf extract, or coca leaf fluid leftover after cocaine processing. The FBN, however, aggressively denied other applicants seeking legal coca imports into the United States, including many rival soft drink companies. By restricting buyer access to coca leaves, the federal government helped to create a monopsony for Coca-Cola, thereby keeping prices for this exotic ingredient down. "" See  Bartow J. Elmore. ""Citizen Coke: An Environmental and Political History of the Coca-Cola Company."" *Enterprise & Society* 14, no. 4 (2013): 717-731. [https://muse.jhu.edu/](https://muse.jhu.edu/).

As for the response among consumers, I'd suggest looking at *For God, Country, and Coca-Cola* by Mark Pendergrast. I don't have a copy at hand but recall it covers the formula, consumer demand, and the rise of Coca-Cola in this period in great detail.

# 

# ",0
"**SHORT-ISH ANSWER**:

Before the mid-1800s, Christmas celebrations (among those who actually celebrated it) was closer to how the modern Thanksgiving is celebrated. That is, it was celebrated at home with friends and family feasting and drinking. But there was a component to it where people, especially in cities, would hold ""open houses"" so that their extended friends and family—or employees, in the case of employers—could come by for a snack, and perhaps even receive their Christmas bonus. 

With little emphasis on the gift-giving aspect yet, many young urbanites spent the day and evening much like New Year's Eve is still celebrated. When drunken celebrations spilled out onto the street, there could be low level criminal mischief, such as vandalism and fighting. This could often be directed at stingy employers who had failed to have an open house, or came up short with the Christmas bonuses. The troublesome celebrants were in the minority, but, nonetheless, many people did complain about the celebrations, and there were several influential New Yorkers who made an effort to promote the domestic, sentimental, family-oriented celebrations. 

With the popularization of the Santa Claus myth in the aftermath of the publication of ""Twas the Night Before Christmas"" in 1823, an already-emerging gift-giving tradition was given a mascot. The emphasis on Christmas being a ""childrens day"" soon took hold.

**LONG ANSWER**:

If you follow the citation in the Wikipedia entry, it takes you to a Bloomberg News article, which, in turn, cites the book *The Battle For Christmas* by Stephen Nissenbaum. Nissenbaum's book is an excellent work, and well-supported. However, I think because it is commonly cited, it gives quite a warped view of what Christmas was before the mid-19th Century. Nissenbaum's book is an argument that there were multiple traditions being observed on how to celebrate Christmas before the mid-1800s:

One tradition was to observe Christmas in a strictly religious sense. This was particularly true of the Puritans/Congregationalists in New England, as well as of the Quakers in Pennsylvania, New Jersey, and New York. To them, the observation merely meant an extra church service in the morning, before spending the day as usual. Unless Christmas happened to fall on a Sunday, that meant an ordinary full day's work.

A second tradition was to observe Christmas as a family holiday, just as we do today. This was particularly popular in New York and New Jersey among families who were either part of the Dutch Reformed Church, or who had some Dutch ancestry. Anglicans/Episcopalians and Presbyterians in the ""Middle Colonies"" and the South commonly celebrated in this way, too. Observers of this tradition would also often start the day off by going to church, while the rest of the day was spent much like Thanksgiving - a big feast with the extended family. If not playing host, then you might be the guest at the home of a close friend or relative who lived nearby. There was also modest gift-giving, especially to the children in the family, though this was more variable the further back you go. Dutch New Yorkers usually celebrated St. Nicholas Day earlier in December as the ""children's day"", and many others used New Year's as the gift-giving day. And, often, there was no gift-giving at all - it wasn't quite yet a tradition set in stone. The main way to celebrate was with food and drink and indoor fun and games.

There was a third tradition, too, which was much more like how New Year's Eve or Halloween are still celebrated in New York City and elsewhere. That is, it was a social, public holiday, particularly in urban areas (at least, outside of New England) with a lot of drinking, dancing, singing, and the like. This was usually how young adults, without nearby family, would spend the day. The celebrations could spill out onto the street, and all the expected disorder and misrule that is associated with New Year's Eve, Halloween, or Mardis Gras went with it.

An added aspect to this third tradition was also the tradition of employers, and local authority figures, to hold ""open houses"" where they would accept visitors. The visitors would usually be the friends and family of the host, but also an employers' employees, or someone who had some kind of personal connection to the host. This wouldn't have been too different from how, say, a modern-day high school graduation party in the United States is celebrated. There would be a spread of food, the host would chat with all the visitors, and the visitors would pay their respects to the host. In some cases, the host may even essentially hand out door prizes, or Christmas bonuses to their employees who turned up. But at the very least, there would be some free food. The guests would stay for a little while before moving on to the next open house, or before going home to spend the rest of the day with their family. 

Sometimes, though, these open houses could end up being personal disasters. Employees often expected their Christmas bonuses, or at least a free meal, on this day, and if they turned up and got neither (or, worse, the employer was out of town or otherwise kept a ""closed house""), they might come back later in the day, drunk, and vandalize the property. 

Nissenbaum's book, then, argues that the ""battle for Christmas"" was to do away with the first and third traditions in favor of the second. Unfortunately, it seems that a lot of readers of this book, or lazy internet journalists, take from Nissenbaum's book that most people were observing the third tradition, and a small minority observing the first tradition manipulated the public into settling on the second one. In actual fact, the third tradition--at least, the public, disorderly drunken part of it--was probably the minority observation if you judge by the surviving accounts in diaries and newspapers. There are a lot more people complaining about the revelry than there are people complaining about uptight neighbors who refuse to partake. 

And that's not to say that Nissenbaum is wrong--far from it. He's right. It's just that the situation was more like this, which internet journalists don't quite grasp from Nissenbaum's book: Most people outside of New England were celebrating the second tradition, of a domestic feasting holiday, with perhaps a small, well-behaved helping of the third tradition. But there was a minority every year who were disturbing the peace by going overboard in their public revelry. Over time, especially with the emphasis on gift-giving, Santa Claus, and children, the public revelry was confined to the days after Christmas (especially New Year's Eve), while Christmas Day itself became the children's day. The celebrants who preferred the domestic tradition won out. 

In New England, the story was pretty much the same, except that it was the first tradition (religious observance and little else) that was more prevalent until after the first couple decades of the 1800s, with minorities who celebrated with the other two traditions. Although that's not entirely true, either: as Nissenbaum points out, New Englanders often made special foods and large meals for the day, too, but they just did it on a regular workday rather than on a day of leisure. But by the 1820s or so, the domestic celebrations, with shops closed for the holiday, began to become prevalent there, too. 

Nissenbaum quotes from an article in the January 4, 1787, edition of the *Hudson [NY] Weekly Gazette* that sums up the situation in New York pretty succinctly during the era:

> ...[I]n 1786, a newspaper in a nearby community pointed out the same contrast between the different fashions in which New Yorkers celebrated the season: “Some good people religiously observe it as a time set apart for a most sacred purpose,” some by “decently feasting with their friends and relatives.” But others observe the holiday by “revelling in profusion, and paying their sincere devotion to merry Bacchus.” The newspaper went on to rephrase the contrast in metaphoric terms: “in several churches divine service [was] performed,” while “the temples dedicated to the service of merriment, dissipation and folly, were much crouded [sic]; where the sons of gluttony and drunkenness satiate their respective appetites.”
>
>> ""The scene with these gentry generally concludes about midnight, when they sally forth into the streets, and by their unmeaning, wild, extravagant noise, disturb those citizens who would rather sleep than get drunk.""

Another such account can be found in the journal of Rev. Samuel Kirkland, who was a missionary to the Iroquois in upstate New York. He spent Christmas 1769 celebrating with some Anglo-New York locals who lived along the Mohawk River, and described the day this way:

> The manner in wch. ye ppl. in yse parts keep Xmas day in commemor'g of the Birth of ye Saviour, as ya pretend is very affect'g and strik'g. They generally assemble for read'g prayers, or Divine service—but after, they [e]at drink and make merry. They allow of no work or servile labour on ys day and ye follow'g—their servants are free but drink'g swear'g fight'g and frolic'g are not only allowed, but seem to be essential to ye joy of ye day.

***cont'd...***",0
"I initially wrote this in response to a comment which was then deleted. But I think I it is sufficiently detailed to be a top-level post. 

as /u/ckley said. 

>I think it's highly unlikely.

[Sylvester magee](https://books.google.com/books?id=bLEDAAAAMBAJ&lpg=PA10&dq=America%E2%80%99s%20Oldest%20Citizen%20Dies%20in%20Mississippi%20at%20130%22&pg=PA10#v=onepage&q=jefferson&f=false) was *allegedly* the last living person to have been a slave when he died in 1971.  I say allegedly because this is solely based on his accounts. He had no documentation proving his birth as a slave or his participation in the civil war (or of his age as the case may be).   Although not strictly historical, standard practices relating to super centenarians require some documentation of age, lest someone merely claim they were born in 1880 and say they're the oldest living person. 

Eliza Moore was the last known person to have been *documented* to have been born a slave when she died in 1948 at the age of 105.  As of the 1860's she had been documented as being the slave of a Dr. Taylor in Meigs Alabama. [A text of her obituary](http://files.usgwarchives.net/al/montgomery/obits/emoore.txt)   She would have been 20 when she was emancipated. 

Alfred Blackburn of North Carolina died in 1951 at the age of 108, he was allegedly the last living person in North Carolina to have been a slave, and the last person to have received a confederate pension in north Carolina. (he served as a manservant to an officer in the confederate army). 

[Cudjoe Lewis](https://www.independent.co.uk/news/world/americas/barracoon-cudjo-lewis-zora-neale-hurston-last-slave-ship-survivor-book-life-story-published-a8335776.html) died in 1935 at the age of 94-95 and is believed to have been the last survivor of the transatlantic slave trade, having come to the US in 1860 on the last slave boat.  Another person who was on that boat, [Sally Redoshi](https://www.history.com/news/last-slave-ship-survivor-redoshi-clotilda), lived till 1937 and had been a 12 year old in 1860. 


What can we draw from these examples?  


In 1954-55 when Martin Luther King Junior rose to prominence in the civil rights movement and led the montgomery bus boycott. The last documented slave had died four years previously.     

We can reasonably assume that there are some individuals who were born  as slaves in the late 1850's up through the early 1860's and emancipated as children, and were ""undocumented"" as it were.   But these people would have had virtually no memory of being slaves, and would have been 95-100+ at the time MLK first became a public figure.   

How many are possible?  

[According to US Census data](https://www.census.gov/prod/cen2010/briefs/c2010br-09.pdf)   .08 percent of the US population is between 95-99, and .1 is 100 or older.  So we can very roughly assume that even in the modern era, 1/1000 people live to be centenarians.  This is the deep end of the gene pool. 

  In the 1860 census there were 3.9 million slaves. If we make a ridiculously generous assumption and say that 1/1000 would live to 100, that means as of 1960, the top end for surviving slaves would have been something on the order of 3900 people spread across the entire US south. really, that's way off because the 3.9 mil is ALL slaves from 0-the oldest living slave at the time, and we're looking at only a tiny percent of slaves to begin with. Realistically, it was probably hugely less than that.  Dozens if not single digits. 

In any case, most of these people would have had zero memory of being slaves themselves, having only been 4-5 when the civil war ended.",0
"So, it's a bit more complicated than that, so far as I'm aware.

The first European/white\* Jew to visit the Beta Israel was Joseph Halévy, a French scholar of Semitic languages who also spoke Amharic (by then the regional Ethiopian language) and Ge'ez (the local liturgical language among the Beta Israel). In addition to teaching, he also worked for the Alliance Israelite Universelle, a French Jewish organization which saw it as its mission to provide education, culture, and material improvements to various MENA Jewish communities, many of which suffered from endemic poverty and persecution. They did a lot of good in many ways throughout the Jewish MENA (they did great work in having antisemitic laws and accusations struck down, for example), most notably through their French-language Jewish schools, though much of this involved secularizing and Westernizing long-standing Jewish communities to mixed response.

It should be noted that while I said that Halévy was the first white Jew to visit the Beta Israel, that's not quite true, and that becomes important here. The first, in the early 19th century, were actually technically Christians- Jewish converts to Christianity who became missionaries and who, alongside born-Christian missionaries, were sent to Ethiopia to minister to existing Christian communities and convert non-Christian ones. When they discovered the existence of tribes there who considered themselves to be Jewish\*\*, they were fascinated and attempted to launch a missionizing project. There was tremendous backlash from the group, especially the priest Abba Mahari, and in the end only a small proportion of the Beta Israel ended up converting to Christianity.

It was the reports of these missionaries that first made the Alliance Israelite Universelle aware of this tribe that apparently considered itself Jewish\*\*\*, and that prompted the group to send Halévy to investigate in the late 1860s, as he would be able to communicate with them in their language. He at first reported them as being very suspicious of him and he had initially hidden his Jewish faith from them, but when he asked them if they were Israelites (they didn't recognize the word ""Jew""), they affirmed this to be true. When he told them that he too was a ""falasha"" (the local derogatory term for Beta Israel), they were confused and doubtful- how could there be a white falasha? It really doesn't seem to me from Halévy's description of the conversation that they were convinced they were the last Jews, per se- in their conversation they asked Halévy if he had ever been to the Temple in Jerusalem, which they wouldn't have asked if they thought they were the only Jews- but it does seem clear that if they believed there were other Jews, they were not expecting them to be white.

The Beta Israel and Halévy got along on his visit, but his connection with them ceased after his trip. He returned to the Alliance Israelite Universelle headquarters in France, brimming with reports of these Jews, but was unable to convince the organization that these were Jews and therefore worth helping and funding. It wasn't until 40 years later that Halévy's student, Jacques Faitlovich, returned to Ethiopia, and by that point the Beta Israel were gunshy when he claimed that he was a Jew- the community had deteriorated into increased poverty at that point (it had already been a poor, oppressed, and nearly isolated community within Ethiopia for centuries) and were tired of encountering missionaries who claimed to be Jews in order to convince them to convert. Faitlovich did end up warming them up to him, though, and became the community's biggest advocate worldwide for the rest of his life, raising money, working to educate young men from the community at Jewish schools in Europe and Palestine, and establishing a Jewish school in Addis Ababa staffed in part by some of the boys who he had helped educate.

\[Edited to add later:\]

Especially once the Alliance and Faitlovitch have been mentioned, it's worth mentioning that Faitlovitch was seen as a controversial figure and fought an uphill battle on behalf of the Beta Israel. The first student he sent to a European Jewish school, a boy named Daniel who he attempted to register in an Alliance school in Paris, was rejected as non-Jewish- they believed he'd been bought in a slave market. The later boys he sent were more commonly accepted, generally at Orthodox Jewish schools in Germany, England, Italy, and Palestine, but faced a great deal of hardship including social isolation and illness (many of them died young). In my reading, I'm finding it hard to get a good read on what the Beta Israel community thought of this education and Faitlovitch's determination to educate them and their children in Orthodox Judaism- most of what I'm seeing reflects that many were motivated by the prospect of the social betterment that the secular education alongside this Jewish education would give them, as well as by a desire to become part of the greater Jewish community.

&#x200B;

\*Please no ""are Jews white"" discourse here- we're talking purely about skin tone here, as you'll see in a minute

\*\*What exactly it means for the Beta Israel to be ""Jewish"" is way beyond the scope of my knowledge and skill here. So far as I can tell, the main conceptions are either that they are descended from First Temple Era Jews (these are the origin stories held by the Beta Israel themselves, who maintain that they are descended from King Solomon and the Queen of Sheba, and by the State of Israel, who hold that they are descended from the Tribe of Dan) or that they are descended from dissidents from the Christian majority in Axum.

\*\*\*This was not the first awareness by European Jews of the existence of African Jews, or even just Jews in Ethiopia. The traveler Eldad haDani in the 9th century wrote a book detailing his claims to be from an eastern African Jewish country which was made up of the descendants of multiple of the Ten Lost Tribes, including the tribe of Dan, and travelogues and other works in the interim are dotted with references. In the 17th century, Rabbi David ben Zimra, a halachic authority, declared that Ethiopian Jews he had met were descended from the tribe of Dan, which led to the rabbinic authority in the State of Israel making the same decision. However, as far as I've been made aware, it's not until Halévy that a European Jew encountered the Beta Israel in Ethiopia. It is of course possible that North African Jews or Yemenite Jews did, but if so I haven't seen any mention.",0
"The Superman radio program's storyline featuring the Man of Steel taking on the Ku Klux Klan (KKK) in the 1940s was indeed a significant and interesting moment in American pop culture history. The storyline was titled ""Clan of the Fiery Cross"" and aired in the summer of 1946. The reaction to this storyline varied across the country. In general, it was received positively and had a significant impact in raising public awareness about the KKK's activities and ideology.",1
"Does this mean that every single medieval Christian was an imperialistic conqueror? No, of course not. Many people did convert out of genuine faith and worked to improve the lives of those around them in a way that was informed by Christian teachings about love and caring for neighbours. But by the time the ""New World"" was ""discovered"", Christian rulers and their sycophantic theologians had already built up over a thousand years' worth of theology that justified what they were about to do.

And make no mistake: What they did in the Americas was *abhorrent*. Christian missionaries were the agents of colonialism wherever they went. I could spend 10 posts giving you examples and it would still barely scratch the surface. In the 500 years or so since Christianity first came to the Americas, it has been the tool of colonialism. California missions were death camps where rape and disease were rampant. [Residential schools](https://www.reddit.com/r/AskHistorians/comments/np9lez/who_is_this_child_an_indigenous_history_of_the/) run by every type of Christian from Catholic to [Quaker](https://www.reddit.com/r/AskHistorians/comments/hemof4/have_the_quaker_always_been_on_the_right_side_of/fwokwkc?utm_source=share&utm_medium=web2x) terrorized, abused and murdered young children until the late 20th century, many of whom are still alive and feeling the brutal consequences. The [Doctrine of Discovery](https://www.reddit.com/r/AskHistorians/comments/kf01cw/is_it_true_that_the_native_americans_had_no/gg5xpwu?utm_source=share&utm_medium=web2x&context=3) nullified the land ownership rights of non-Christians. White missionaries led the coup of Hawaii. Jesuits burned the books of the Mayans and the khipus of the Incas. Mormons impersonated Native Americans when committing massacres and let everyone blame it on local Natives. Immigrant churches were set up on lands that had been violently cleared of Native people months before. The list simply goes on and on. You lift up any rock of Christian history in the Americas, and you will find Christianity's teachings being used to justify the genocide of Native peoples for the good of their souls. My descriptions here merely skim the surface: [The devil is in the details](https://www.theguardian.com/world/2015/sep/23/pope-francis-junipero-serra-sainthood-washington-california).

Of course there are many stories of [Native people who converted ""willingly""](https://en.wikipedia.org/wiki/Kateri_Tekakwitha), and many Native people are Christian today. Major movements within Christian history such as liberation theology have been actively informed by Indigenous actors. It can become easy for settlers, whether comfortable in their atheism or in their ""orthodox"" Christianity, to sneer at the devotional lives of Christians among colonized populations. Whether it's because they believe that the Native peoples need to be ""rescued"" from their colonizing religion and ""saved"" with atheism, or because they believe that there is something theologically ""impure"" about  religious rites that incorporate pre-Christian practices, it is really just the same old story of ideologies being forced onto Native peoples from outside in order to ""save"" them. That is why discussions of this topic are so fraught, both inside and outside of Native communities.

Coming back to the original question, the answer is, as I said, ""both."" Sometimes Christianity did spread because people saw value in its belief systems, whether that was the calling to a personal relationship with God or the appeal of joining one of the world's largest and richest intellectual networks. But we cannot let salvation history persuade us into believing that it was the ""quality of its teachings"" that led to the spread of Christianity among Indigenous peoples in the Americas. Indigenous peoples have *every right* to loudly and vocally call out the heinous, heinous histories of how Christianity was forced on them and the damage that Christians did to their peoples.

The quotation you opened your query with is from Eleanor Ferguson, an Oglala Lakota representative of the International Indigenous Youth Council. She uttered it just a few days ago at an [emergency meeting](https://nativenewsonline.net/sovereignty/oglala-sioux-tribe-temporarily-bans-all-christian-religious-operations) of the Oglala Sioux Tribal Council on 26 July. This meeting was called as a response to the [discovery](https://nativenewsonline.net/currents/christian-mission-ousted-from-pine-ridge-indian-reservation-after-distribution-of-hate-materials) that a white missionary called Matthew Monfore was distributing pamphlets that said that Tunkasila, the Oglala Lakota word for ""Creator"", was a ""demon idol"" and that ""Jesus is the one true god of the Native Americans."" The tribe ordered Monfore to leave. The Oglala Sioux Tribal Council just passed an ordinance mandating that missionary groups must now apply for permission with the Tribal Council in order to propagate their teachings in light of the exploitation of impoverished children on the Pine Ridge Reservation by missionary groups.

In the context of what is going on in her community now and what has gone on there for centuries prior, Eleanor Ferguson is absolutely correct. Her words may not apply to every single case across the past 2000 years when populations converted to Christianity, as this sometimes happened peacefully and enthusiastically. But her words don't *have* to incorporate all those other histories: She is speaking the truth about the history of Christianity's active complicity with the violent genocide against Native Americans in her own community, which, as Monfore's actions in the [poorest place in America](https://www.re-member.org/pine-ridge-reservation) vividly illustrate, is still going on today.

(2/2)",0
"The short answer to your question is that, yes, there would have been real people with the names of heroes and gods, but no, they were not common.

The long answer is that Ancient Greek naming conventions are a complex and fascinating topic. On the one hand, there are no surviving sources that explain to us how or why the Greeks came up with names for their children. Ancient authors do not seem to have found this topic interesting enough to write about. On the other hand, Greek names form a tremendously large body of evidence - the [Lexicon of Greek Personal Names](http://www.lgpn.ox.ac.uk/) claims to have published as many as 215,000 so far - and the fact that most names are composed of words with a direct meaning in Greek means that they present a unique window into the social world of Ancient Greece. Because of their components and their meaning, names can often tell us about people's place of origin, family ties, social status, cult practices, looks, and so on. 

With the exception of some restrictions on the naming of slaves, there do not seem to have been any particular rules about what people could and could not be named. We might expect, for instance, that it was frowned upon to give your child the literal name of a god or goddess, but there's no evidence that the Greeks actually found it taboo. Admittedly the names of divinities, while becoming more popular after the 1st century AD, are rare enough in the Classical period (5th-4th centuries BC) that individual known cases have been extensively discussed by scholars. However, they are not altogether absent. There certainly *were* people named Artemis and Leto. This shows that there were no hard limitations to what a child could be named, but only conventions by which most names were chosen. The only names that genuinely don't seem to have been used at all were those of underworld gods (Persephone, Hades).

So what were the conventions they stuck to? One of the most powerful factors, especially among elite families, was the names of ancestors; some sons were named directly after their fathers (such as Perikles, son of the famous Perikles), while others were named after their grandfathers (like Kleisthenes, father of democracy, grandson of Kleisthenes the tyrant of Sikyon). Modern scholars are on pretty firm ground when they assume that people with similar names are related; often a name would ""run"" in just one family, sometimes for centuries. A related strong influence was the desire to express social status (again especially among the rich), which meant that many wealthy people would have names that contained words like *aristo-* (the best), *-archos* (leader), and especially mutations of the word *hippos* (horse). The latter is often regarded as a firm indicator of high status, since only the richest men in Greece would be able to afford to own horses, and horse-riding was the favourite pursuit of the leisure class. Famous examples include Perikles' father Xanthippos (""yellow horse""), the physician Hippokrates (""horse power""), and Philippos II of Macedon (""horse lover"").

If there were no particular traditions binding new parents, they would be able to choose a name they liked. Endless possibilities are known. Particularly striking to us, though not necessarily the most common, are male and female names that contain a direct reference to warfare - Kallimachos (""beautiful battle""), Archestrate (""army-leader""), Nikomachos (""victory in battle""), Andromache (""battle of men""), Deinomache (""terrible in battle""). Other names are more straightforward, like Leon (""lion""), Kephalon (""head"") or Melissa (""honey""). People were named after cities, mountains, and, very commonly, rivers; Aristotle complains about rare and ridiculous names like Hermokaikoxanthos, a pile-up of the names of 3 rivers. Generally, words like *kalos* (beauty), *stratos* (army), *agora* (marketplace), *demos/damos* (people), *-anax* (king) and old values like *bia* (strength) and *kratos* (power) occur often in what we might call ""posh"" Greek names. Names ending in *-kles* (Perikles, Themistokles, Damokles) refer to *kleos* (glory). The combinations of these words don't always make sense; what sort of a name is Isagoras (""equal marketplace"") or Iphikrates (""strong strength"")?   

Interestingly, while the actual outright names of gods are extremely rare, by far the most common type of name in Ancient Greece was actually the *indirect* reference to a god or goddess. There were 2 ways to do this. First, the name of a god could be easily adapted into an adjective form, so as to become a name *derived* from a god rather than the god's name itself. Some of the most common were adaptations of Apollo (for example into Apollonios), Dionysos (Dionysios), Artemis (Artemisia) or Demeter (Demetrios); the latter is still in common use now, in its vowel-shifted form of Dimitri. One of the most famous examples of such a godlike name is Alexander the Great's companion Hephaistion (from Hephaistos). The second way to adapt divine names was to add words to them - most commonly *kleos* (glory),*-doros* or *-dotos* (gift, given). So we have authors like Diodoros (""god-given"" - the reference is to Zeus), Apollodoros, Asklepiodotos; in the Roman era, when Eastern gods made their way into Greek lands, new names like Isidoros (gift of Isis) crop up. While it has proved impossible to say with categorical certainty that names are an indication of which gods were being worshipped in a given place, regional patterns are very clear in the evidence, and names derived from gods are at least a clear indication of which divinities were considered important. Perhaps the most touching are the few cases when parents who consulted the oracle on matters related to childbirth would name their child after the god who had advised them or after the place of the oracle.

Given all these factors and trends, it's perhaps easy to understand why the literal names of heroes were not commonly used as names, even if there were no strict rules or moral taboos against them. There wouldn't have been any family tradition to do so; there was no social credit in making pretentious references; since mythological figures were not always the recipients of cults, they wouldn't often have been credited for advice or protection in childbirth. It's possible that their names would have been regarded as old-fashioned or an ill omen, given the fates of most of them. According to the searchable part of the Lexicon, Ikaros is attested just 20 times; Narkissos a more respectable 74 times. I can't find a single person named Odysseus. Only the hero Iason has a famous ""real-life"" counterpart in the 4th-century Thessalian tyrant Iason of Pherai.

Source for most of this: R. Parker, 'Theophoric names and the history of Greek religion', in S. Hornblower/E. Matthews (eds.) *Greek Personal Names: Their Value as Evidence* (2000), 53-79",0
"I talk a little about the weaponization of water sources in the Middle Ages in [this earlier answer](https://www.reddit.com/r/AskHistorians/comments/6kt7nh/poisoning_wells_is_a_common_espionage_activity_in/djp2a5u/), from which I'll borrow for the beginning of this post. There are a few times when we hear about actual *well* poisoning as a war tactic, but scholars are frequently skeptical:

> The perilous geographic placement of Svetigrad at the tip-top of a rocky hill [was] good for defense against a siege but very bad for access to plentiful water sources. Drawing out the themes of betrayal, [the author of the primary source record] condemns a local man for knowing exactly how to crush the citizens and soliders of the town psychologically as well as physically. This traitor, the story goes, cast the corpse of a dog into the local well. The town defenders had lost their source of water, and knew they had to give in.

> Whether it really took a local traitor to poison a well or whether the Ottoman army was actually just very strategic at cutting off the defenders' access to nearby water sources is an open question...The second is more plausible, but makes the Ottomans look much more effective so would not work well for the author's polemical purposes.

The most infamous and tragic cases of medieval well-poisoning, of course, were not actually cases of well-poisoning at all. The Black Death in particular, but also other periods of violent economic/political upheaval, led to mass accusations of poison/murder against already oppressed groups--lepers in some cases, but more frequently Jews. Which, in turn, led to pogroms, massacres, and massacres-by-arrest-and-execution of the accused. But while these cases have a lot to tell us about medieval *people*, they don't tell us what people thought about wells and how they interacted with them in the aftermath of accusations.

But! As the posts in the other thread curve around to, we can get some insight into how medieval people thought about potentially poisoned wells by looking at their attitudes towards *other* poisoned/polluted/supposedly poisoned water sources. Namely, fountains and sewer conduits (including pipes). 

In these situations, we see a deep and frequent concern for their pollution with animals, dead animals, feces, sludge, and disease--sometimes by accident, especially in the frustrations of suave urban sophisticates dealing with *rustici* (hicks); but sometimes deliberately, as malicious poisoning to target one person or a whole town. What does *not* happen is wholesale destruction of the fountain or system in the aftermath of a pollution event or poisoning case.

For example, by 1262, Siena already had a surprisingly advanced water distribution system of fountains and pools, for use in drinking, bathing, and daily life as well as firefighting. That year, a woman was accused and convicted of poisoning the all-important Campo fountain. We know about this case because its financial ramifications were meticulously recorded in the records of the Biccherna, the city chancellry. These recorded include point-by-point accounts of the costs of flaying the convicted poisoner alive and then burning her. They do not include ripping out a fountain or the entire system of fountains and rebuilding them.

So if we take Barleti's story of the Ottomans' poisoning the Albanian town's well as a dramatic story (true or not): the ""narrative effect"" of the poisoning is absolutely to shut down the use of the well in the short term. On the other hand, with a longer-term outlook and (surely not coincidentally) the economics and practical logistics of rebuilding an intricate sewer/fountain system, pollution/poisoning through excrement or dead animal bodies could lead to caution and delay rather than immediate destruction.

In closing, it's worth pointing out that medieval people absolutely understood the problems of ""dirty water"" from pollution, and that dirty water could be cleansed by boiling. A 14th century letter supposedly from a Spanish doctor to his sons at university in Toulouse rattles off the local water sources that are known to be bad (wells, a river, etc), and then reminds the reader(s) not to drink water from those sources unless it is boiled (""cooked""). The danger in a spontaneous pollution/poison case, considering the commonly cited pollutants/poisons of feces and corpses, was the *lack of knowledge.*

Further Reading:

* If this answer made you think, ""Wait a minute, *pipes and sewers* in the *Middle Ages*?"", then Roberta Magnusson, *Water Technology in the Middle Ages: Cities, Monasteries, and Waterworks After the Roman Empire* (2001) is the book for you.",0
">one chronicler even says they killed a baby, although that could easily be apocryphal or actually the work of a pig.

Wait, were baby-killing pigs really more common and believable than baby-killing wolves?

Edit: I actually lived on a small farm  when I was younger and we had a pig. It was an asshole. I just can't believe people would leave babies where pigs could get them. Because it's pretty well known that pigs can be assholes (and also sweethearts which are very smart and can make okay pets for the right household but I still wouldn't leave a pig with a baby)",0
"On her wedding night, Olympias, mother of Alexander the Great, dreamt that lightning struck her womb. Years later, when her son was about to launch his great expedition against Persia, she explained the meaning of the dream: Alexander was not the issue of her husband Philip. He was a son of Zeus!^(1)

Olympias probably never had such a dream - the whole episode is almost certainly the invention of a later historian, inspired by Alexander's divine aspirations - but claims of divine descent were still fairly common in the classical era. Typically, the god resided in a distant branch of the family tree; one thinks of Hecataeus of Miletus, who informed bemused Egyptian priests that he was descended from a god in the sixteenth generation.^(2) But some Greeks at least hinted that their own parentage was divine. The great pankratiast Theagenes of Thasos, for example, was rumored to be a son of Hercules. Hellenistic monarchs, following Alexander's lead, circulated rumors of liaisons between their mothers and various divinities: Seleucus I liked to be thought a son of Apollo, and pointed to a fortuitous anchor-shaped birthmark as evidence.

Claims of divine paternity, in short, are well-attested, at least among the Greek elite. Such claims, however, seem to have typically been made by prominent men trying to sanction and burnish their reputations. In this case, as so often in classical history, we know much less about the actions and agency of women.

In most parts of the Greek world, paternity was extremely important, since paternity determined citizenship. Under the stringent laws proposed by Pericles, for example, Athenian citizenship could only be conferred on children born to a citizen father and citizen mother. With a few exceptions - notably Sparta - similar restrictions were in effect throughout the Greek world. In addition to the social handicaps set on her children, a woman who gave birth out of wedlock was liable, especially in cases of adultery, to a host of grim of legal and social penalties.

Almost everywhere in the Greek world, in short, women who gave birth outside of marriage suffered very serious social penalties.  They certainly had incentive to claim divine parentage for an illegitimate child. But did they make such claims? And if they did, were they believed?

The first source that came to mind was not Greek, but Roman. Near the beginning of his History, Livy - talking about Rhea Silvia, the mother of Romulus and Remus - observes that Rhea ""named Mars as their father, either because she really believed it, or because the fault might appear less heinous if a deity were the cause of it.""^(3) Here, at least, we see that ancient authors understood why women might make such a claim.  It is less clear how often historical women actually did.

It certainly happened occasionally. The paternity of Demaratus, a king of Sparta, was disputed, since his mother had given birth to him less than nine months after her marriage. When Demaratus approached his mother and asked her to tell him the truth, she said:

""On the third night after \[my husband\] Ariston brought me into his house, I was visited by a phantom exactly resembling him. The phantom came to my bed, and afterwards took the wreath it was wearing and put it on me...the wreath proved to have come from the shrine of the hero Astrabacus, by the courtyard gate, and when we questioned the diviners, their answer was that the phantom who visited me was Astrabacus himself.""^(4)

Astrabacus was a local hero - not a major god, but supernatural, and a perfectly legitimate father for a king.

&#x200B;

(1) e.g. Plutarch, *Alexander* 2.3 (2) Herodotus, *Histories* 2.43 (3) 1.4.2 (4) Herodotus, *Histories* 6.69",0
"I would preface of course first to say that trying to find logic in racial pseudoscience is often an exercise in futility. Thinkers almost always contradict in some way as they each are, in the end, creating their own proofs for an untruth, and even single theories often have obvious points of contradiction and inconsistency. But in any case, that dispensed with, the term ""Aryan"" as we think of it in the strains of racial pseudoscience where it is most commonly associated with Nazism is most significantly a result of the writings of Arthur de Gobineau, a Frenchman with pretensions of aristocracy (self-styled as Comte de Gobineau) writing in the mid-1800s, and best known for our purposes for this work *Essai sur l’Inégalité des Races Humaines*. 

Gobineau wasn't doing anything particularly new in his approach to writing on ideas of race and his work reflected common ideas of a noble Germanic race which was superior to all, terms the ""Nordic"" myth, and which traces back at least another century to the writings of another Frenchman, Henri de Boulainvilliers, and which subdivided the people of France into the Nordic, Alpine, and Mediterranean types, placing the Nordics at the top, and exemplifying them as, put succinctly, the ""descendants of ancient Germanic tribes, the originators of all civilization, and the only peoples capable of leadership"".

Gobineau took this and expanded on it, but writing in the 1850s, added onto it new ideas which had entered into the intellectual milieu of the time, and one of these was the use of the term ""Aryan"", which although first applied in India, was a linguistic term created by a British official to describe the ur-language shared by a vast swathe of the globe, what we would now term Indo-European. The idea of Aryanness language was promoted by several figures before Gobineau, but he made it essentially his own with the way he melded it to racial thinking. The logic of course was quite simple, if the Nordics were the originators of all civilization, and Aryan language the roots of all language, presumably the Aryans and the Nordics were the same thing. But of course he doesn't stop there either, adding in ideas about purity of the blood and corruption caused by mixing of the races. 

Whites were the superior race, Aryans the best of the whites, and mixing with the inferior races - ""White"", ""Yellow"", ""Black"", which he based on the three sons of Noah -  risked the very existence of civilization itself. The purer the Aryan blood, the better the civilization, the weaker, the worse. Aryans had propagated far and wide - settling Europe, Iran, and India -  hence why traces were found even in the latter, but even in Europe, where the Germanics represented the purest specimen remaining, they faced great dangers of decline which Gobineau already believed to be well advanced and needed to be arrested quickly. Gobineau was hardly ignorant of the place of India in all of this, and in fact based much of his work on readings of the *Rig Veda* which he believed supported the story of Genesis. Figueria summarizes other Sanskrit literature he drew on thusly, using them as evidence for the eventual degradation of the Aryans in india due to dilution from long intermingling with the ""aborigines"":

>The *Mahabharata* bore witness to the manner in which Indian society had been invaded by foreign elements.9 Savage vices, absent from the *Ramayana*, appear full-blown in the history of the Pandavas, who had been raised to divine status in order to veil the blood sins of their mothers. In other words, Gobineau read the epics as chronicles of non-Aryan promiscuity and Aryan battles to avoid the dilution of their bloodlines.

This he held up as a warning for what Germany was going through, and the threat to Aryan purity of the Germanics, which they needed to work to stop.

Later thinkers built off this further. Ernst Haeckel's *The Riddle of the Universe: At the Close of the Nineteenth Century* would prove to be an especially influential one in his specific focus on tying in the Aryan Myth with Germanic nationalism and ideas of *Volk*, and the preeminence of the blonde, blue-eyed ideal, which of course are so closely entwined with the popular image of Nazi ideas which would come later. 

But perhaps none is a more important bridge between Gobineau and the Nazis than Houston Stewart Chamberlain, who wrote *Foundations of the Nineteenth Century*. An Englishman by birth, he was fascinated by *Germanness*, marrying Wagner's daughter, and writing German propaganda through the First World War. *Foundations* pushed the Aryan Myth heavily, and was a work admired by Hitler himself, and of many things, is known for being a key part of the attempt to demonstrate that Jesus was, in fact, not Jewish but of Aryan persuasion, which of course ties into the virulent anti-Semitism pushed by all of the writers mentioned here, Chamberlain being a key part in developing Hitler's belief that the Jews needed to be entirely removed from German society.

It also of course it worth mentioning the American Madison Grant who wrote *The Passing of the Great Race: or The Racial Basis of European History* which likewise played in important part in the propagation of the Aryan Myth, and played a key role in the creation of the eugenics movement in early 20th c. America.

But in any case, that is the rough summary of it all. ""*Aryan*"" was a term latched onto by 19th century thinkers for their writings on racial pseudoscience as it fit well with their ideas of a master race, a ruling class, which had once been spread far and wide. Gobineau, as did others, used esoteric and wildly ahistorical readings of texts and evidence to support this view, explain away incongruities, and place their Germanic ideal in the place of this master race in their way of thinking. Although India was a key part of the origin of the term ""Aryan"", these racial thinkers were careful to explain why India itself, of course, was not the exemper of the Aryan race, and instead held it up as an example of how far the white race could fall.

**Sources**

Evans, Richard J.. *The Coming of the Third Reich.* Penguin, 2005.

Figueira, Dorothy Matilda. *Aryans, Jews, Brahmins: Theorizing Authority through Myths of Identity.* State University of New York Press, 2003.

Saini, Angela. *Superior: The Return of Race Science.* Beacon Press, 2019.

Sussman, Robert Wald. *The Myth of Race: The Troubling Persistence of an Unscientific Idea.* Harvard University Press, 2014.

Tattersall, Ian & Rob DeSalle. *Race?: Debunking a Scientific Myth.* Texas A&M University Press, Sep 2011.

Footnote: We try not to mod where we plan to post. But I had a meeting cancelled *after* removing something, so time to kill. In the interest of disclosure, [this was the incredible and insightful comment which you were prevented from seeing](https://imgur.com/a/NHMRYUq).",0
"The visit of Hitler to Paris in 1940 was indeed a remarkable event, and the seemingly relaxed security measures you mentioned have puzzled many historians. However, it is important to consider a few factors that help explain the situation. Firstly, by 1940, Germany had achieved significant military successes in Europe, including the rapid defeat of France. This military dominance could have led Hitler and his entourage to believe that there was little immediate threat to his security in Paris.",1
"Sappho is really a very unique case, and while I think you'll be hard pressed to find a scholar call her a lesbian in a scholarly work, there's certain broader theoretical acceptance that gay people existed in the past, and that people have complex inner lives and expressed their agency/subjectivity in specific ways, that we only somewhat know about. Part of the point I was trying to make is that is difficult to translate the acceptance of that idea onto individuals, which is why you see the split between classicists willing to call Sappho a lesbian in the New Yorker, but not at a meeting of the Society for Classical studies. We accept that gay people existed-- we certainly have evidence (literary, artistic) for sexual encounters between men (and less but still some for women), but we hesitate at calling any one person gay because of the very fraught calculations that entails. I am not a biographer of any kind, and I study how groups interact with ideas, so I rarely have to think about those nitty-gritty questions about individual personal lives. 

It also depends though on whether you write about Sappho the historical figure or Sappho the character/poetic abstraction? There's certainly lots of academic-adjacent queer coded contemporary poetry on Sappho, and arguably the best scholarly works on Sappho (Eros the Bittersweet, and the translation If Not, Winter) lean into this creative side. But again, there's a lot to say about Sappho, and every classicist seems to have their own pet theory and almost none of them really matter because we know so little, but I'm going to keep making comments about her being a Lesbian, and I think that doesn't undermine my strengths as a historian it just means I come down a certain way on the relationship between past and present.  Reasonable people can disagree. Which is the other takeaway (hopefully) from this. Lots of scholars (and lots of gay scholars) think different things on this topic. Which is fine. And we don't need to tamp down scholarly disagreements on the topic because people have trouble understanding nuance-- we should fight for more diverse and nuanced histories that allow us to understand the limits and potential of our new contemporary terminology about issues like sexuality.",0
"The *Odyssey* absolutely and explicitly sets up a double standard, but it's not the one OP's question claims.

In book 5, we're introduced to Our Mighty Hero...sitting on a rock on an island beach, staring at the sea and sobbing. He is somewhere been stranded and an actual prisoner on the island belonging to the ""lovely goddess"" Calypso. The poem implies that there was some mutual attraction between them initially. But in the *years* he's been on the island, that has evaporated. They sleep together every night, sure, but Odysseus isn't happy about it. It is possible, although not definitive, to read the situation as coercion/rape.

But this isn't the concern of the poem itself, which doesn't frame this as a ""normal"" relationship between men and women. Why not? Because Calypso isn't a woman, she's a *goddess*. When Hermes comes along to pass on the order for her to help Odysseus leave the island, *that* is the framework with which she protests. From the Penguin Classics translation:

> You are hard-hearted, you gods, and unmatched for jealousy. You are outraged if a goddess sleeps openly with a man even if she has chosen him as her husband. You were the same when rosy-fingered Dawn fell in love with Orion. Free and easy yourselves, you were outraged at her conduct, and in the end chaste Artemis of the golden throne rose, attacked him in Ortygie with her gentle arrows and left him dead. And so again, when the lovely Demeter gave way to her desire and made love with her beloved lasion in the field of the three ploughed furrows, Zeus heard of it quickly enough and struck him dead with his blinding thunderbolt.

The contrast with, of course, [Zeus](https://imgur.com/a/ODdV1i9) is clear. Calypso gets to essentially denounce all of Greek mythology as sexist, even if it is not a sparklingly feminist message today.

""Adultery"" in the traditional sense, thus, is not the issue in this case.

That said, K.J. Dover and other scholars are exquisitely clear that under Athenian law (to be fair, not necessarily the situation reflected by tradition of singing the *Odyssey*), adultery with citizen women was very not okay for men. Naturally, it was a crime committed not against morality or the polity, but against a specific man who had legal control of the woman involved:

> But Greek laws were not lenient towards adultery, and *moikbeia*, for
whch we have no suitable translation except ""adultery,"" denoted not
only the seduction of another man’s wife, but also the seduction of h s
widowed mother, unmarried daughter, sister, niece, or any other woman
whose legal guardan he was.

Dover adds that adultery was considered *worse* than rape, because rape was considered a crime of passion and the moment whereas to carry on an affair involved *seduction*--that is, a long period of time and effort. Xenophon's *Hiero*, which is more or less a work of political philosophy, compares the two:

> At any rate, it is not uncommon for the laws of
communities to allow people to kill seducers, and only seducers, with impunity, and
the thinking behind this law is obviously that seduction impairs the affection a wife
feels for her husband. After all, if sex takes place without the woman’s consent, this
does not make the slightest difference to the regard her husband feels for her, as long
as the affection she feels for him remains inviolate.

Now, it's important to keep in mind that we're dealing primarily with normative or prescriptive sources here, not descriptive. (And Xenophon, for his part, is exaggerating a bit for rhetorical effect, as Edward Harris points out--seduction/adultery was by no means the only crime for which one could be executed.) The cuckolded man was afforded, in theory, a significant amount of say over how/whether the philandering man was punished. So even under the law, there would be a lot of room for flexibility or ambiguity in what actually happened.

However, more literary/normative sources are particularly useful in reflecting one aspect of the situation: being seen as an adulterer was *not* good. The *fama* for such men would swirl with scorn and satire. Dover suggests that because women were seen as easily seduced and swayed by temptation meant that adulterers rather than the cheated-upon received the brunt of social derision.

But just as we have to consider the context of Odysseus sleeping with (under ambiguous terms of consent) a *goddess*, the status of women in relationships outside marriage also mattered. For example, enslaved women were subject to rape by their owners and by whomever their owners gave them to as a ""gift.""",0
"This is a very common topic on AskHistorians!

Some previous answers:

* [Why did Hitler specifically chose to call his desired race ""Ayrans"" rather than ""Nordic"" or ""Germanic""?](https://www.reddit.com/r/AskHistorians/comments/5w5yki/why_did_hitler_specifically_chose_to_call_his/) - by /u/Jan_van_Bergen
* [Why were nazi historians wanted to connect links between German people and ancient indo Iranian tribal communities of Aryan origin (from central Asia)?](https://www.reddit.com/r/AskHistorians/comments/g3k3cc/why_were_nazi_historians_wanted_to_connect_links/) - and be sure you read the [follow-up answer](https://www.reddit.com/r/AskHistorians/comments/g3k3cc/why_were_nazi_historians_wanted_to_connect_links/fns4uax/) by /u/Cyberapostle
* [Why was the Nazi aryan race (blonde hair blue eyes) reinforced even though Hitler had black hair and eyes. Is the aryan concept one that was established before Hitler and the Nazi regime?](https://www.reddit.com/r/AskHistorians/comments/fnwtxg/why_was_the_nazi_aryan_race_blonde_hair_blue_eyes/) by /u/Semijohn - especially addresses the ""blonde and blue-eyed"" matter",0
"The Warren Commission formed after the assassination of President Kennedy produced [this appendix](https://www.archives.gov/research/jfk/warren-commission-report/appendix7.html) on the history of presidential protection. They note that before Lincoln, ""there was remarkably little concern about the safety of Presidents and few measures were taken to protect them."" This persisted even after President Jackson survived an assassination attempt due to a misfiring gun. In the 1840s Congress did assign a guard to the White House — but apparently with more of an aim to protecting the building and grounds than its primary occupant. 

During the Civil War Lincoln was guarded irregularly — sometimes by Washington, D.C. policemen, and sometimes by Army units. Lincoln ""was reluctant to surround himself with guards and often rejected protection or sought to slip away from it."" This, the report notes, ""has been characteristic of almost all American Presidents.""

The congressional report after Lincoln's assassination, ""with traditional reluctance, called for no action to provide better protection for the President in the future."" The Warren Commission notes that some people at the time saw Lincoln's assassination as a unique reflection of the Civil War, which was now over, and not something indicative of a need for broader protections. Indeed, for more than a decade after Booth, presidents continued to walk about in public alone and unguarded. (I remember, but cannot find a source, an anecdote about either Grant or Hayes, that upon receiving a threat on his life, the president didn't change his routine other than to purchase a heavier walking stick. Sadly, you should disregard this anecdote unless someone else can find a source for it, but it's delightful enough I thought I'd mention it as an aside.)

After President Garfield was shot, the New York Tribune contrasted ""the simple Republican manner of life which the custom of nearly a century has prescribed for the Chief Magistrate of the United States"" with ""the forms and safeguards of courts"" — implying that bodyguards were seen as a courtly, monarchical affectation, where as a republican president should be open and accessible to the people (July 3, 1881, quoted in Warren Commission report). Surprisingly or not, despite the second assassination of a president in two decades, Congress ""took no steps to provide the President with personal protection.""

The Secret Service, initially a group for combatting counterfeiters and financial crimes, first began providing part-time presidential protection in 1894 — apparently a coincidence because a group of Colorado gamblers were suspected of plotting to assassinate President Cleveland. President McKinley actually had Secret Service protection when he was assassinated, but it was his death that finally spurred Congress to give the Secret Service ""full-time responsibility for the safety of the President.""

Returning to Lincoln, he was certainly aware that many people wanted to kill him. President Theodore Roosevelt later quoted Lincoln saying, ""Though it would be safer for a president to live in a cage, it would interfere with his business."" He had sporadic guards but no formal protection.

Fascinatingly, General Ulysses Grant was originally invited to attend Ford's Theater with Lincoln on that fatal night. He begged off with an excuse about needing to be with his children, but [possibly because Julia Grant and Mary Todd Lincoln hated each other](https://featherfoster.wordpress.com/2015/04/20/mary-lincolns-rivals/). 

From a [recent biographical article on Grant](https://www.damninteresting.com/the-reconstruction-of-ulysses-s-grant/):

> Grant would never forgive himself for begging off, certain that had he accepted the invitation, his bodyguards stationed outside the door would have stopped Booth. **As lieutenant general of the U.S. Army he was entitled to armed protection around the clock. The president in those days was not.**

Hopefully this was helpful!",0
"Speer actually directly addresses Göring's claim and dismisses this outright.  From Gilbert:

>I told him that Goering had told me they were only a couple of months behind. ""Oh Goering— he doesn't know beans about science. He just liked to talk a lot. I had no sooner discussed the first experimental model of the jet-propelled plane, when he ran to Hitler and told him he would have 500 jet planes ready in 3 months—utter nonsense."" He agreed that such wishful braggadocio may have helped support Hitler's belief in a miracle that would win the war.

Also, Göring's overall conduct at Nuremberg has proven a fascinating subject for historians. The colonel in charge of the prison decided he was going to make the ""fat man"" of legend competent for trial and forced him cold turkey off of his morphine addiction for the first time in two decades, along with a brutal diet that had him lose something like a third of his body weight.

This had the unintentional effect of transforming the drug addled incompetent back into a rather terrifying figure who correctly assumed he wasn't going to leave Nuremberg alive and who spent most of the time there plotting his epitaph and browbeating his fellow prisoners.  It got so bad that he was eventually placed in isolation.

Thus, the veracity of many of his claims there has been debunked as he was speaking for the cameras in an attempt to polish his reputation and that of the party.  Unsurprisingly for someone who had propagated bald faced lies from the beginning of the movement, he kept doing so up until the very end.",0
"I feel like that allegation was just from Suge Knight boasting and joking after Eazy E's death, no real evidence or proof for it.

Eazy E was a known sex fiend and took pride in bedding multiple women per day and not wearing condoms. 

I don't know how to time stamp on my phone, but if you go to the 31:30 point of [this documentary](https://youtu.be/Q7HoVypBqug) they discuss his daily point system for having sex with girls and how he used to tease his group members for being against him because he was a ""thrill seeker"" who didn't wear condoms. Good way to contract HIV without Suge's help.",0
"To answer this question and to warn everyone else reading this thread: **AskHistorians is an actively-moderated subreddit, and we remove all posts that break [our rules](https://old.reddit.com/r/AskHistorians/wiki/rules).**

To anyone who wants to write an answer:

* Your father's cousin's roommate's classmate's little brother's anecdata does not qualify as an answer.
* Low-effort answers that are the result of 5 minutes of Google do not qualify as answers.
* YouTube links do not qualify as answers.
* Offensive stereotypes about Islam or Muslims do not qualify as answers and will get you an invitation to please keep your bigoted arse off this subreddit.

Please mind that it takes time for a good answer to be written and that answerers have lives to live, so patience is appreciated while prospective answerers get their books and thoughts together.",0
"Maybe I wasn't clear. The post is quite obviously discussing opium and cocaine, I didn't think I had to mention them specifically again. The comment I replied to mentioned alcohol, tobacco, caffeine, marijuana, and then asked

>Where would cocaine and morphine rank on such a scale compared to other drugs of the time in 19th-century Britain?

My question was, *were* there other drugs of the time? 

I'm sorry if that was confusing.",0
"Mithra is an old Indo-Iranian god, the personification of the Contract. He's one of the more important deities in Zoroastrianism as well as in the old Vedic religion (and Hindu gods like Vishnu have taken on aspects of him over the ages), but it's also conceivable that Persian worship of him as a high god existed, though as far as I know there is no documentation on this. I believe the Parthians in particular built a lot of temples to Mithra, but I'm honestly a bit hazy on this as it's the least well understood period of Iranian empire.",0
"When he emerged at last from the Great Mosque of Bukhara, Genghis Khan strode to the center of the square and called out, demanding that the wealthy tradesmen, merchants, and head families of the city – some 280 in all – be found and brought forth. When they had been amassed and assembled, Genghis strode up the steps of the Mosque before turning to the waiting crowd, elite and commoner alike. Through his interpreters, he lectured the denizens of Bukhara on the nature of sin, and the specific sins committed by their Amir and the elites of the city. Juvayni would later put pen to parchment and write down, according to him, at least, the Great Khan’s words: 

> “O people, know that you have committed great sins, and that it is the great ones among you who have committed these sins. If you ask me what proof I have for these words, I say it is because I am the Punishment of God. If you had not committed these sins, God would not have sent a punishment like me upon you.” 

Man notes that such words, though fearsome in their own right, were understood by those notable present to be neither personal nor vindictive, but fitting of the circumstances surrounding them all.  Khwarazm’s leadership had been appalling, and the Muslim populace had long been tearing their own society apart – perhaps God had sent such a fate unto them for such blindness and arrogance. (Juvayni's own potential rationale for giving such fawning praise - that he was writing essentially with a Mongol spearpoint in his back - is a story for another thread).

When he had concluded his harangue, he instructed that the wealthy headsmen of the city be divided up among his soldiers, and they were then to hand over their treasure – especially those treasures they had gotten from the pillaged Mongol caravan. Before allowing them to depart for their task, he admonished the wealthy to not waste anyone’s time trying to show the soldiers where the wealthy they stored “above ground” was kept – his men would find and gather all of that up easily enough on their own. Instead, they needs must lead their captors to their secret treasure stores, that buried or hidden away. Leaving them to their task, the Great Khan turn at last to the last redoubt of Bukhara: the citadel at the city’s heart.

Within the impenetrable stone walls of the central citadel, some 400 cavalrymen who had apparently not received the memo about trying to break out sat in wait. With nothing else for them to do – their lives were surely forfeit in any event – they resisted the Mongol storm for a further 12 days. No doubt these warriors knew enough or – and had likely faced – enough about nomad warriors to know that, no matter how fearsome they were in open battle, the stepperiders absolutely had absolutely no means to penetrate heavy fortifications. To this, however, Genghis had little problem – because that’s when the Chinese designed-and-manned siege machine began rolling in and being assembled. Catapults, trebuchets, and mangonels hurling not just stones, but incendiaries, explosives and long-burning liquids at and atop the structure. Sappers dug under the walls to undermine them, while portable siege towers with extendable ladders pressed against them. Moreover, the Mongol commanders were only too happy to demanded that the surrendered populace – man and woman, old and young alike – serve their new Khan in the further “liberation” of their city, “forcing prisoners, in some cases the captured comrades of the men still in the citadel, to rush forward until their bodies filled the moat and made live ramparts over which other prisoners pushed the engines of war.”

When the surviving soldiers in the citadel had finally had enough and surrendered, they were led out of the fortress, divided into units of ten, and then exterminated to the last man with the sort of ruthless, cold, almost industrial efficiency that the world had almost never seen before… it was at last time for the remaining populace of Bukhara, they who had dared to profit off of the blood and stolen property of the Great Khan and his Mongol brethren, to learn of their own collective fates. For the wealthy merchants and traders, would be that of absolute exile. Genghis decreed that their city, and everything in it save the clothes currently on their backs were forfeit. They were to depart immediately, and never return, and anyone caught trying to remain, or to hide, or to return seeking their possessions would be ruthlessly slaughtered. To the populace at large, though the Khan had himself admitted that they were not personally to blame for this fate, their sentences would prove little better. They were now servants of the Great Khan, and they would be utilized exactly as such. From De Hartog, “The people were divided into various groups. The artisans went as slaves to Mongolia; the strong young men had to follow the army to provide an expendable vanguard at the next storming of a town. Families were separated for ever. Women were also shown no mercy and there were heart-breaking scenes. The Mongols raped the women under eyes of their fellow victims, who could only weep at their helplessness. Many chose death in the face of such terrible things.” The imam of the Great Mosque, even knowing that he’d be slaughtered for such defiance, felt compelled to speak out against such evils – and as predicted was cut down in short order. Finally, either by accident or simple malice, the city was set ablaze and – made almost entirely from wood and straw, burned almost entirely to the ground.

___________________
Sources Cited:

De Hartog, Leo. *Genghis Khan: Conqueror of the World.*  
Franke, Herbert. ""The Rise of the Mongolian Empire"" in *The Cambridge History of China, Vol. 6: Alien Regimes and Border States, 907-1368.*  
Juvayni, Ata-Malik (trans. J.A. Boyle). *Genghis Khan: The History of the World-Conqueror.*  
Man, John. *Genghis Khan: Life, Death, and Resurrection.*  
Onon, Urgunge. *The Secret History of the Mongols: The Life and Times of Chinggis Khan*

__________________
I run my own [podcast](https://podcasts.apple.com/us/podcast/the-history-of-china/id741606139) called [The History of China](http://thehistoryofchina.wordpress.com) and we're right in the middle of this whole world-changing Mongol explosion! Check it out!",0
"#Part 3

##Religious Freedom

For many Indigenous Peoples, this concept is also present at the core of our fundamental values. Indigenous perspectives on religious freedom can be inferred by explanations provided by Vine Deloria, Jr. (2003). He notes that historical narratives of people, as understood from their religious perspective, informed their cultural identities and this fortified their integrity in their beliefs, which then could accommodate for differing beliefs. This notion created space for individuals to harbor certain beliefs that were within the boundaries of the established stories even if they did not strictly adhere to the details espoused in their legends.

>The tribal religions had one great benefit other religions did not have and could not have. They had no religious controversy within their communities because everyone shared a common historical experience and cultural identity was not separated into religious, economic, sociological, political, and military spheres. It was never a case, therefore, of having to believe in certain things to sustain a tribal religion. One simply believed the stories of the elders, and these stories had significance as defining the peoples’ identity. Today we can say they have specific themes, but that is our interpretation and not the way people originally understood them. No tribe, however, asserted its history as having primacy over the accounts of any other tribe (p. 99).

While Indigenous Cultures are typically collectivistic in nature, they do not deny the existence of the individual. In other words, one may choose to harbor particular beliefs, in the case of religion, but these particular beliefs are centered on the same values and principles held by the community, which means they do not drift far from the beliefs developed by the community as a whole (Cordova, 2003, pp. 176-178).

As such, spiritual/religious perspectives did not conflict with each other either outside the community or between other Indigenous communities, in the sense that their existence side by side could be accounted for by the beliefs of neighboring groups. This ability to tolerate the existence of other beliefs while keeping our own respective groups’ beliefs intact is important because it explains how Indigenous Cultures are able to hold multiple positions regarding their beliefs without violating the tenants of their cultures. Melissa Nelson (2008) stresses this when it comes to understanding the diversity of Indigenous ways of knowing:

>Within diverse Indigenous ways of knowing, there is ultimately no conflict . . . In fact, it points to two very important insights generally practiced by Indigenous Peoples: for humans to get along with each other and to respect our relations on the earth, we must embrace and practice cognitive and cultural pluralism (value diverse ways of thinking and being). We need to not only tolerate difference but respect and celebrate cultural diversity as an essential part of engendering peace . . . As the late great Lakota scholar Vine Deloria Jr. has written, ""Every human society maintains its sense of identity with a set of stories that explain, at least to its satisfaction, how things came to be."" (pp. 4-5)

>Many Native Peoples believe that the center of the universe or the heart of the world is in their backyard, literally. And there is no conflict over this as the Wintu of California can perceive Mount Shasta in northern California as the center of their universe while the Kogi of Colombia can understand that they are from the ""heart of the world"" in the Sierra Nevada de Santa Marta of Colombia. Place-based spiritual responsibility and cognitive pluralism are imbedded in most Original Teachings. It is good that each nation, each tribe, each community perceives their ancestral lands as the center of the universe, as their holy land... (pp. 10-11)

This tolerance toward the religious beliefs of other peoples allows for the mitigation of conflicts based on differences of the sacred and divine. It creates a pluralistic environment, or the ability for them to coexist, that does not violate the integrity of their specific Culture because it upholds their very values. The explanation for what some may see as irreconcilable contradictions between beliefs is a point that can be used to elaborate on Indigenous religious freedom. Contradictions that result from differing details related through beliefs are often reconciled simply by letting them be. For Indigenous peoples, trying to choose a narrative as being ""true"" or ""correct"" over another is not necessarily an issue—nor is it considered the ""right"" thing to do. They are seen as mutually existing and overlapping where they do, but parting where they may.

This tolerance goes even further when applied to individuals who claim beliefs that appear irreconcilable. Because it is up to the individual to decide if the beliefs they harbor are compatible, their decision creates a state of being for them to retain multiple views from any given portion they accept. The multiplicity of this state of being means that an individual can even entertain diametrically opposing beliefs as part of their system of faith as long as they are conforming to a standard devised from a focal point of thought.

##Criminal Justice

David. E. Wilkins and Shelly Hulse Wilkins go into depth on the topics of banishment and exile in their work *Dismembered: Native Disenrollment and the Battle for Human Rights* (2017). They describe a brief history of these acts as methods of dealing with crimes and infractions in both a Western context and among Native communities.

Physical exclusion, which is what banishment and exile are, were used as forms of punishment performed by the community as a whole, considering the individual(s) in question to no longer be considered part of the community. Occurrences of this are common throughout many cultures and time periods such as with the Chinese, Germanic Peoples of the Medieval Ages, the ancient Babylonians. Ultimately, ""banishment was viewed as a mean to reorder social and power relations by removing individuals who were viewed as undermining the community's security and prosperity"" (p. 14). In the United States, this form of punishment and justice was also prevalent on local scales, though the practice has been largely rejected by the federal government since its inception, even being legally disavowed in inference since this method has never been fully explored as part of federal punitive measures (p. 16). This, however, has not stopped colonial and state entities from engaging in the practice.

For Indigenous Peoples, Wilkins and Wilkins work to analyze both a contemporary understanding of the method of banishment^2 and how this contrasts to historical instances. Historically, this means of exclusion was practiced rarely.

>In fact, Native peoples historically resorted to banishment only as a last resort or for particularly horrific acts like premeditated murder or incest with children. Institutionalized penal system were virtually absent across Indian Country because, as the scant available documentary evidence suggests, given the familial, egalitarian, and adjudicatory nature of tribal societies--which were more focused on mediation, restitution, and compensation aimed at solving ""the problem in such a manner that all could forgive and forget and continue to live within the tribal society in harmony with one another""--permanent expulsion of tribal members was rarely practiced since the clan and kinship systems were highly effective mechanisms that help regulate member conduct and any transgressions that arose.

>Given the kin-based nature of tribal nations and the fact that many Native societies refused to employ centralized and coercive methods of dispute resolution or formal institutions of social control, tribal citizens generally acted with great care in how they behaved toward one another. The fear of being socially ostracized or treated as an outcast was generally sufficient to maintain relatively peaceful interpersonal behavior (p. 20).",0
"[Here it is](https://politika-v-rashke.ru/wp-content/uploads/2018/10/andityi-v-malinovyih.jpg). It's part [stereotype](https://s3.cdn.eg.ru/wp-content/uploads/2017/06/76555667308103831.jpg) (here's a [still](https://myrussia.life/upload/post/2019/06/13/9414/gallery/94288i7dykynra0.jpg) from a 2000s parody movie), part reality. There really, actually were newly rich bandits and semi-bandit businessmen who went all out on looking ostentatious, and their version of Liberace suit was something closer to Versace on overdrive. The quintessential, stereotypical ""new Russian"" nouveau riche is a wide, tall, wrestler-sized thug, covered with prison tattoos, wearing [enormous and numerous gold chains](https://r.mt.ru/r15/photo94AB/20989621679-0/jpg/bp.webp), a gigantic double-breasted red suit with gold furniture, sporting a ginormous early cellphone, and driving THE 600 Benz. Note that normal uniform of a thug was a Turkish leather jacket and, yes, track pants, so the suit was the status upgrade that meant one's made it.

I think the crimson color for the suit coat was just a local trend (and far from universal even then), after all, it is ostentatious, and also it hides blood. Not that they chose the suit for that literal purpose, but maybe for poetic reasons. Also, ""beautiful"" in Russian is derived from ""red"", and the quintessential handsome male in Russian folklore cannot wear anything but a red shirt, so this probably goes way deeper in cultural sense. The specific getup, though, is now iconic and even used as a [carnival costume](https://next2u.ru/upload/auto-880/ca/c4/de/2e/a1/61/e7/aa/d2/c7/1d/98/40/37/29/fd.jpg) for 90s themed parties.

I also found [one source](https://politika-v-rashke.ru/otkuda-u-banditov-i-novyih-russkih-poshla-moda-na-malinovyie-pidzhaki/) that says that in 1992, Gianni Versace presented a pret-a-porte collection whose centerpiece was a double-breasted crimson suit with black trousers. You can find out if it's true, but if so, it sounds very probable as the initial cause. Versace was the shining beacon of style of tasteless people in Russia in the 90s.",0
"This hits upon a genuinely interesting issue with cultural history, in that while we often remember the more colorful parts of eras, their prominence tends to obscure the wide variety of experiences possible living at the time. One could imagine people 100 years from now (hopefully not historians) wondering if every person from our era lived like an ""influencer"". 

The ""Roaring 1920s"" are a different matter, insofar as in the most general sense they're not referring to a particular movement but a period of great optimism and
economic growth. (Before the 1920s the title [has been used before](https://archive.org/details/otherdaysrecolle00braduoft)
to refer to England's ""roaring seventies"" -- that's the 1870s -- meaning the same thing.) They weren't even bequeathed the title until about halfway through and it wasn't in common use until *after* the Great Depression, and it's possible some who lived in the cultural fog didn't know it surrounded them. 

For those it benefited, the economy was definitely noticed. In 1928, Hoover, when accepting the Republic nomination for president, said

>We in America today are nearer to the final triumph over poverty than ever before in the history of any land.

Yes, the economy improved. Who did it improve for?

The general consensus is that income inequality had a large drop by the end of World War I, and then through the 1920s there was a surge. You can see [a chart here of the share of income held by the top 10%](https://imgur.com/a/VVtYew1); notice they have 40% of the share of incoming all the way up to WWII. (To be fair, there were some adjustment made to income tax in that time period which counts some income which wasn't included before, so the jump isn't as bad as it looks, but still, we're talking about a ""peak moment"" in US inequality.)

That should hint that if you're more on the bottom end of the scale, you did not do as well. The average worker wage was flat.

If you were a farmer, you had done well in the War, but not after -- farm income went from $17.7 billion (1919) to $10.5 billion (1921). Prohibition meant a significant drop in barley demand (9.2 million acres harvested in 1918 down to 6.6 million in 1919, the year Prohibition was passed) and you were affected by a tariff war kicked off by the Emergency Tariff in 1921. (The tariffs were well-meaning at first intended to benefit farmers who getting hurt by dropping exports to Europe, but Europe responded with their own tariffs and farmers ended up hurting as a whole.)

For context, farmers made about 30% of the US population in 1920. (It dropped to 21.5% by the end of the decade.)

If you were growing cotton, you were already having a tough time -- the dread boll weevil insect was spreading in a ""wave of evil"" and was particularly prominent in the 1920s. This also affected black sharecroppers; about three-quarters of a million lost their jobs. A great many ""traditional"" industries had overproduction remaining as a legacy of the war; coal in particular also faced the rise of oil.

Poor immigrants? Not only did they suffer economically, there was an upsurge in anti-immigrant bigotry, and at a more practical level, the Emergency Quota Act of 1921 (targeting what was considered undesirable Europeans) and National Origins Act of 1924 (targeting both Asians and Europeans). The 1920s also had the second-wave revival of the KKK, which expanded from just targeting black people to immigrants. ([Here's a 25,000 person march at the National Mall.](http://mallhistory.org/files/original/ca7f02a0f2b5506dc538a90437484c01.jpg))

The effect on farms in particular was so pronounced it essentially was its own depression within the boom. If you focus on the city workers *and* on non-immigrant whites you can get something approaching an economic ""roar"", but of course that all went away once the Great Depression started in 1929, and it was only in contrast to what happened after did the term ""roaring 20s"" become common.

...

The Oxford English Dictionary claims the earliest ""roaring '20s"" reference is from The Evening State Journal (Lincoln, Nebraska) although it wasn't common; The Reader's Digest has a ""roaring '20s"" reference from 1930 in referring to automobile production.

Dimitri, C., Effland, A., & Conklin, N. C. (2005). The 20th century transformation of US agriculture and farm policy (No. 1476-2016-120949).

Payne, P. G. (2015). *Crash! How the Economic Boom and Bust of the 1920s Worked*. United States: Johns Hopkins University Press. (**This is non-technical and recommended for general reading.**)

Piketty, T., & Saez, E. (2003). Income inequality in the United States, 1913–1998. *The Quarterly journal of economics*, 118(1), 1-41.

Payne, B. (2013). Poverty in the Prosperous Years: The Working Poor of the 1920s and Today. *Bridgewater Review* 32.2.

Smiley, G. (2000). A Note on New Estimates of the Distribution of Income in the 1920s. The *Journal of Economic History*, 60(4), 1120-1128.",0
"Hi! My first answer here, I'm quite excited. I can help with the mythological part, but only just a bit and you'll see why. So let's get down to it.

Short answer is: we're not aware of any mythological reason, myth, prophecy or something related, as to ""why"" they were taken. There are no holy scriptures in West African religions. They certainly did know where they were being taken. There is one myth pertaining to one particular Orisha that goes in search of the slaves in the New World, however. There are no recorded myths about what they did when they found them here or anything of the sort, just about the search. 

In the introduction to the rare, rare book Dieux D'Afrique - Orixás, deuses yoruba na África e no Novo Mundo - by the franco-brazilian Pierre Verger, he states that from historical records he could find and source, the idea was basically playing the natives against one another, trading slaves and other goods for manufactureds and firearms - weapons which in turn would help the local kings, tribes or chieftains win their conflicts, raids and thus produce more slaves. He described the loose collection of city-states and regions as micro-states if my memory isn't failing me. Ostensibly, if one asked where exactly where they being taken, the answer was ""the New World."" 

Here we go back to mythology, as the general culture was deeply tied to religion. Most African leaders and Kings were said to be descended directly from the Orishas, as while they were indeed divine and inferior only to Olodumaré, the monotheistic-like creator of everything, they had lived among humans at one time or the other, and their myths were basically that of their people. Nana, Verger exemplifies, which is an older female orisha, is said to have migrated from east-central Africa to West Africa once upon a time, and indeed her tribe did do that - or so the story goes because I don't remember the particular proof he offers for this. Because, most disheartening to historians, it's all very much oral tradition and what's left of it even today is closely guarded by the high priests and priestesses. It's why Verger became one himself.  

So at the Portuguese ports and inland forts, for example the São João Batista de Ajudá at Benin, the colonial power would have an arrangement with the local king or chieftains. Good, and the first records pertaining the religion also start to pop up. Jesuits and missionaries of this time were not that much worried about understanding much of the Oyo/Benize/Dahomey/Assorted people's cultures and religions, and mostly passed judgement of what they saw, but they did record parts of it. Because while it was mostly oral tradition, the iconography was very much plentiful with that book and some other works like [what this site has about the main Osun temple in Nigeria](http://fabricedubesset.free.fr/lagos4.htm) having amazing examples, some of the spirits worshiped were represented by sculptures with long phalluses. Among mild representations of the others, the Portuguese missionaries focused particularly on those and classified them as devils, as such the religion as a whole worshiped demons, and that was that. They did not record anything about a native prophecy about slaves being taken or anything of the kind, so no point speculating if there was. 

Different from what happened in North America, the Portuguese were a bit more lax in their enforcement of things - not that they were any less cruel or willing to, they just didn't seem to be able to stomp it all out. The Yoruba religion and culture survived, firstly hidden in the senzalas and then even already somewhat syncretic by the time of the second and last Brazilian emperor. Ogum, for example, which is one of the war Orisha in Africa - think the greek Ares and this is him - was syncretic to St George the Dragonslayer here in Southeastern Brazil, and the oral history of Umbanda does state that the slaves that fought in the War of Paraguay - specifically in Humaitá - called upon Ogum to protect them. Badass, right? 

Here's the problem. Details vary by a lot sometimes. While the Portuguese didn't stomp it all out, they did disrupt it. Ogum, for example, was syncretized to a different saint in Bahia by some accounts (like Santo Antonio). Because there's also no one single religion back in Africa, although the Yoruba is the most well-preserved one, so there can't be a single ""truth"" in all of this. Here in Brazil we got, so you have an idea, 4 main lines:

1 - Candomblé Ketu (yoruba language root) 

2 - Candomble Jeje (Fon people language root)

3 - Candomble Angola (no idea language root, honestly)

4 - Umbanda - the syncretic faith with Catholicism

Cuba even has Santeria. There are people pursuing ""pure"" Yoruba rites. Each of these has a slightly different mythos, slightly different practices - or wholly different in some cases - and most important traditions they follow and want to pass forward. Worse, the four are still mixing even today. ""Umbandomblé"" is a term we use when a former Umbanda house starts doing things as one of the Candomble lines, and Candomble does ""mesa branca"" when they're mostly concerning themselves about spiritual guides and not orixas. Each high priest or priestess decides what's in and what's out, as such much knowledge is lost, altered, or otherwise not recorded as ""one"" singular source. Works well enough.

I'll be dealing with Ketu from now on to answer that mythos, which is what I follow so that the oral tradition reached me. First and foremost: Pierre Verger in the book makes a huge exception when talking about Oxalá - the most important and powerful of the orishas. It's the only he explains the three main subtypes of - Oxala proper, Oxalufã the old, and Oxaguiã the young (really don't know the English spelling for them, sorry). 

Remember when I said that each had an earthly life among humans? [The city of Ejigbo](https://en.wikipedia.org/wiki/Ejigbo) is one such example, being the city that Oxaguiã conquered and where he became Elejigbô - king of Ejigbo. They still do the Atori festival every year, in honor of one particular itan about a seven-year drought brought about by a curse. 

One day, however, he heard that his people where being taken imprisoned to far away lands. He was none too pleased and went around inquiring where they were being taken. And so, people told him they were being taken across the ocean to new lands. 

Oxaguiã then hopped on the trunk of a tree, since he could not touch salt water, being a funfun orisha (white) and started his own crossing. It took way long, and while he was there riding the waves he met one of the Yemonjá sisters, the orisha of all oceans. Oxaguiã was young, strong and brave, and that Yemonja liked those traits. After a time, they fell in love and had a baby.  They called the baby Ogunjá - the white Ogun (pretty vicious if you ever believe any priest) - and the baby was also strong, as had been his father and mother, and soon wanted to go to war too. And so he went, and his mother went with him, and so she was called Yemonja Ogunté. They now had a beautiful family, but Oxaguiã could not abandon his kingdom back in Africa, and so he went back after some time. Ogunja would spend some time with his mother, then some time with his father, warring and learning, learning and warring, a beautiful family indeed.

One version, common in many websites, has the first passage as ""One day, all Orishas heard that people were being enslaved, and so did Oxaguiã, who came atop a tree trunk across the ocean..."" but the rest then stays the same. But as I said, none deal with that they did once in the New World, just the search quest. ",0
"This question has received a lot of upvotes, but no answers, probably, I suspect, because it's a bit difficult to provide a one-size-fits-all response, and the concerns you express are also distinctly modern ones, for reasons that I hope will become clear in what follows. Indeed, what follows can be summarized by saying: (a) the ancient world was overal a lot poorer in material terms than our modern one and; (b) it is extremely unlikely that in the ancient world homes would be left entirely unoccupied for any stretch of time.

You refer to your wallet and depending on the time period, someone leaving the house would indeed bring money – i.e. coins, first introduced in the seventh century BC in the form of large-scale currency – with them, stored in a purse of some kind, if they intended to spend anything. An example would be a Roman – or more likely, a number of Romans (e.g. the mistress of the house and a slave or two) – going to the market or a shop to buy something). In a barter economy, this requires more planning because of how cumbersome the exchange of goods can become (even if the principles of trade are essentially the same). For more, you might want to check out, for example, the papers in W.V. Harris (ed.), *The Monetary Systems of the Greeks and Romans* (2008).

When we leave our homes today, we usually lock the door, usually when there's no one at home. A very brief overview of the development of more or less secure mechanisms to lock doors is useful here (there's a fun overview in Brian Fagan's edited volume *The Seventy Great Inventions of the Ancient World*, published in 2004). Doors with hinges are a development of the Iron Age -- before then, doors turned on pivots set into lintel and threshold, and could be barred from the inside only. Latch-lifters were developed in Greece and China in the earlier part of the first millennium BC. Tumbler locks, using wooden pegs, emerged later in the Greco-Roman world as well as China. Locks were mostly used to secure areas that were left unattended or where it was important to slow thieves down (e.g. the store room of a temple); seals were used for this purpose, too, from the Bronze Age onwards, but they did little to stop someone from accessing whatever was sealed (e.g. Tutankhamun's tomb).

As you might suspect, the relatively late development of means to effectively lock doors means that this wasn't considered much of a problem before the Iron Age, and even then, there are different factors at play then than there are today. In ancient times, you would generally try to avoid situations where no one would be left behind, especially since most houses were fairly easy to break into (both wattle-and-daub and mudbrick are susceptible to being holed using metal implements). There would usually be someone at home at all times or, depending on the type of building in question, guards would be posted at the doors. In the Greek and Roman world, women were mostly left at home while men went out; in the Bronze Age, women had more freedom, but even then, situations where a house was left completely unattended would probably have been fairly rare. In the Classical world, most families owned at least one slave, including smallholders.

It's a feature of the modern era that most residences are occupied by single people, couples, or nuclear families (couple + their young children). However, in ancient times, households tended to be bigger, with at least some of the children staying with their parents into adulthood: e.g. in Classical Athens, sons of at least affluent families, of which we know more, usually didn't marry until they were around 30 years old. Even e.g. craftsmen could rely on their (older) children, slaves, apprentices, or possibly even landless labourers who worked in their employ (i.e. the poorest of the poor), to hold the fort in their absence if necessary.

As regards to what people would take with them when they left the home, the answer is ""usually very little"", because the ancient world was, compared to our own, a lot poorer in material terms. But it depends on the context. Archaic Greek vase-painting coupled with e.g. the evidence from the Homeric epics (which mostly refer to Homer's own time, i.e. around 700 BC at the earliest: for more on this, see my article [here](https://www.badancient.com/claims/homeric-epics-source-bronze-age-aegean/)), suggests that no able-bodied high-ranking man would leave his house without at least a spear at his side; ""bearing arms"" was later considered not done and replaced, before the end of the Archaic period (ca. 500 BC), with some men carrying wooden staves instead (see Hans van Wees's paper, ""Greeks bearing arms"", in *Archaic Greece: New Approaches and New Evidence*, the volume he edited with Nick Fisher, published in 1998). If you had to travel far, you'd put on a hat and bring supplies (or have a slave and/or pack animal carry them for you), and so on.

I hope this goes some way to answer your question; feel free, of course, to post follow-up questions.",0
"Thus, when we talk about what it meant to be a companion (hetaera, ἑταίρα) in Classical Greece, it is important to acknowledge that the creative ways that we can see women navigating the world between these two categories and making them fluid are inextricably bounded by a context of extraordinary violence. Indeed, Demeas the main character in [Menander’s play Samia](http://en.wikipedia.org/wiki/Samia_%28play%29) luridly describes, what will happen to his companion Chrysis when he kicks her out of the house for the supposed unfaithfulness that engenders the main plot:

>*“You think you’re so fine. Go to the city and you will see what kind of woman you really are. They live in a different world those other women, paid a paltry ten drachmas for running to dinner parties and drinking undiluted wine until they die, and if they hesitate or demure, they starve. You will learn the hard way like everyone else, and recognize the mistake you have made.”* ^((As quoted in) [^(Courtesans & Fishcakes)](https://www.amazon.com/Courtesans-Fishcakes-Consuming-Passions-Classical/dp/0226137430)^())

Indeed a comic character later expounds on this idea:

>*“Apart from that its easier, isn’t it, to get along with a ‘married’ hetaera than with a wedded wife. Of course it is. A wife stays indoors, her haughtiness licensed by law, a heteaera, on the other hand, knows that if she wants to keep her man she must pay for him with good behavior, or go and find another one.”* ^((As quoted in) [^(Courtesans & Fishcakes)](https://www.amazon.com/Courtesans-Fishcakes-Consuming-Passions-Classical/dp/0226137430)^())

Even extraordinarily vulnerable women had reason to fear the life of a pornēs in a kinētērion (literally place of business for fucking), according to [Eupolis](http://en.wikipedia.org/wiki/Eupolis) 99.27 K-A,

>*“They stand virtually naked, lest you be deceived; take a look at everything. Perhaps you are not feeling up to the mark; maybe you have something on your mind. The door’s wide open; one obol’s the fee. Pop in! No coyness here, no nonsense, no running away, but without delay the one you want, whichever way you want her. You come out; you tell her where to go; to you she is nothing.”*

He isn’t being entirely serious in his salesmanship as he makes clear soon afterwards when he describes the girls as being “*the ones* [*Eridanos*](http://en.wikipedia.org/wiki/Eridanos_(Athens)) (then an open sewer that the waste of Athens flowed into) *refreshes with its pure waters.*” 

The life that the character Chrysis was navigating as a hetaera, being a free woman who wasn't enslaved but also was not a wife, was in a paradoxical sense very explicitly ambiguous. Requiring the economic support of men with at least some means to maintain herself, she committed to one man and also received gifts from that man that were necessary to maintain herself and her status. Important aspects of this arrangement that distinguished her as a hetaera to contemporaries from the pornēs are that she chose Demeas freely, neither money nor things of value were exchanged directly for sex and certainly not with fixed prices, there was no pimp involved, and their relationship before it fell apart was conducted with a level of discretion and in private spaces. ...But was she exchanging either sex or something less easily defined for gifts? In Memorabilia III, [Xenophon has Socrates attempt to perversely dissect this question and the life of a specific wealthy hetaera Theodote](https://halshs.archives-ouvertes.fr/halshs-01825159/document) in dialogue. While Xenophon absolutely has an unambiguous answer that he doesn't quite put into the mouth of Socrates, he writes Theodote as very stubbornly insisting on the ambiguity in the nature of her lucrative friendships that she has constructed a successful and powerful life around. In cities across the Hellenic world, women described as *megalomisthoi,* or 'big fee' hetaera, like Theodote were often able to leverage their status to wield extraordinary amounts of wealth, influence, and power. Indeed, visitors would sometimes remark that the nicest house in a city belonged to a megalomisthoi, and Phryne was renowned not just for her beauty but for donating the mind-boggling amount of wealth necessary to rebuild the walls of Thebes after Alexander knocked them down. 

So would it have been possible for your Classical Greek everyman to seek out and find a girlfriend? If we were to have an opportunity to ask him, he would perhaps be likely to say yes of course, but would he really mean the same thing you do? His explicit conception of the relationship he would be able to have with a hetaera would be relatively familiar. She would be understood by him as a friend rather than someone with whom he exchanges commodities, he may or may not understand the relationship to be sexually exclusive depending on what they would have negotiated together, and he would at least not explicitly understand the relationship as being coercive in any way (unless perhaps it was contextually convenient like it suddenly was for Demeas for most of Samia). However, the relationship would also inescapably exist in the context of a ubiquity of economic and sexual violence that is perhaps not gone but is still now very different. Maybe this is a question that would not productively benefit from putting our conception of what it means to be a girlfriend under the same kind of creepy [observer effect](https://en.wikipedia.org/wiki/Observer_effect_(physics)) inducing microscope that Xenophon had Socrates subject Theodote to. However, what exactly does it mean today to be a girlfriend or a woman who has sex in a consenting negotiated relationship outside of the context of marriage? 

Our great grandparents would have had a mess of answers that would be perhaps remarkably similar to those of your everyman two thousand years before. However, perhaps the way that we are increasingly able to more genuinely mean an array of mutually supportive and freely negotiated styles of relationship can easily get in the way of really seeing the lives of even our recent ancestors much less historical texts. For example, when English translations of the New Testament talk about ‘sexual immorality’ they are really translating the Greek word porneia. it’s used almost every time the topic of sex comes up and often when talking about the worst sins in general. Now porneia has traditionally been translated into Latin as fornication, while being understood by many to just be a 1:1 stand in for ‘any sexual expression not between husband and wife’. However, whether Porneia in post-classical Koine Greek could have been rhetorically used to mean generic sexual sin, or broadly sex outside of marriage exactly, is perhaps a complicated question that would require us to interrogate the context of sex outside of marriage. [What fornication meant in actual Latin certainly was perhaps less complicated.](http://en.wikipedia.org/wiki/Fornication#Etymology_and_usage) If we really want to understand what Paul meant when he instructed followers [not to associate with men who assaulted pornēs](https://www.biblegateway.com/passage/?search=1+Corinthians+5%3A9-13&version=NRSV;SBLGNT), and to shun the not-specifically-defined broad concept of porneia, what did that really mean to him then? If he meant to include men with relationships to hetaerae collapsing the distinction, as he almost certainly did, would that necessarily mean the same thing as a man with a relationship to a girlfriend today?

I think I would broadly rather leave you with questions to interrogate instead of answers that we don't really have:

* What does consent really mean in the context of a relationship like the one that Chrysis was portrayed as having with Demeas?
* As we struggle to understand the very foreign world that women like Chrysis lived in, what does it mean to center the context of the exploitation of her deeply shitty boyfriend, like I broadly did, or the strategies that she used to navigate her world? Does this question her agency, do we really get to question her agency?
* How does dissecting a relationship, like Xenophon had Socrates dissect Theodote's relationships to her friends, change their nature? 
* What assumptions about the nature of relationships have I brought to this comment? what assumptions did you bring to it?
* Can we really translate hetaera as girlfriend?",0
"Under Nazi rule in occupied France, the teaching curriculum would have undergone significant changes. The Germans aimed to control education and shape it in accordance with their ideological agenda. Here are some key aspects of how your teaching curriculum may have been altered:

1. Indoctrination: The Nazis sought to indoctrinate young minds with their ideology, so your curriculum would have included elements of Nazi propaganda.",1
"It is so important to remember the history and the movement that brought us to this point. My first relationship out of the military was with an older man who had partied through the 80s. While the relationship ran it's course in a few years, this man had *catalogues* of gay films and bookcases full of gay literature, and I went down the rabbit hole. Many of his friends had passed away from AIDS and drugs, and it just opened my eyes to the incredible struggle that these people went through to get us to where we are today.",0
"It can sometimes be a slightly murky concept.

But the gospel of Luke is thought to attest to a particular Christian theological development in which not only were some of the end-times events thought to be imminent — particularly those that pertained to the rewards for the righteous — but had in fact *already* been inaugurated.

One of the main focuses of this is the ""kingdom"" (or perhaps more accurately, king*ship* or reign) of God. It seems that in its likely earliest conception, the coming reign of God meant the tangible earthly rule of God and/or God's people on earth, over its various enemies or the unrighteous in general. Big emphasis here on *tangible*: this was going to be a pretty undeniable event that would likely involve a big shake-up in the international political order, etc. 

But it appears that fairly early on in first century Christian theology, the notion emerged of a smaller-scale or more ""spiritual"" kingdom, present in and among the lives of the earliest Christian communities. Traditions like those found in Luke 11:20 (though paralleled in Matthew 12:28 too) are sometimes mentioned here, where a successful exorcism is conceived as the ""coming of the kingdom"" to an individual. Similarly Luke 10:9, where this is extended to *all* those healed. (An even earlier potential reflection of the tradition of a spiritualized kingdom can be found in Paul's epistles, for example in Romans 14:17.)

Luke 20:34-36 is also particularly interesting, which seems to suggest that the righteous — the ""sons"" of the age-to-come — have already achieved a sort of proleptic immortality and angelic existence, even while still alive on earth. (The background of this concept is explored at length in various publications by the scholar Crispin Fletcher-Louis.)

But perhaps the most important text for the notion of realized eschatology is the unique tradition found in Luke 17:20-21 where, having been ""asked by the Pharisees when the kingdom of God would come,"" Jesus responds that

> ""The kingdom of God is not coming with things that can be observed; nor will they say, 'Look, here it is!' or 'There it is!' For, in fact, the kingdom of God is within you."" 

Although there's some debate as to whether the original Greek of the last line is to be translated ""the kingdom of God is within you"" or ""the kingdom of God is among you"" (I think it's much more likely the former, following Ramelli 2009 and others), in either case, here there seems to be a radical reorientation of traditional expectations relating to God's reign, which instead arrives ""under the radar,"" as it were. One other interesting aspect of this, even beyond the fact that, as /u/Emperor_Pupienus wrote in reference to 1 Thessalonians, this may be ""surprising . . . as speculation about the end of the world [including that elsewhere in the New Testament itself] typically includes all kinds of signs that indicate when the end is nigh"" — in other words, that elsewhere the coming of the kingdom precisely *was* coming with ""things that can be observed"" — this sort of ""it's actually *within* you"" revisionism also seems to be a [well-known modern trope](http://tvtropes.org/pmwiki/pmwiki.php/Main/ItWasWithYouAllAlong), too.

In any case, moving on. The book of Acts is almost universally thought to be written by the same author as Luke, or in any case bears a very close relationship to it, theologically speaking; and similar to Luke 17:21, Acts 1:6-7 seems to directly address some of the uncertainty about traditional expectations about the coming kingdom -- and may appear to reorient this expectation in terms of the coming of the ""holy spirit"" to the Christian community, and their ongoing proselytizing mission to the world.

Several other texts in Luke might be relevant in conjunction with this. While not necessarily pertaining to a ""realized"" divine reign in particular, things like Luke 21:24, with its mention of the ""times of the Gentiles,"" could suggest the notion that the eschatological timetable has been extended. (It's not exactly clear what these ""times of the Gentiles"" refers to, though.) Further, Luke 22:69 seems to modify its source text (Mark 14:62) to remove the idea that Jesus' own contemporaries would be around to witness his return on/with the ""clouds of heaven,"" instead only emphasizing his current residence in heaven. (See also Acts 7:55-56, where the martyr Stephen quite literally sees this after the heavens are ""opened."")

The gospel of John also seems to have a peculiar eschatology, with affinities with the Lukan notion of the Christian community having already proleptically attained eternal life in some way.

For another classic example of realized eschatology outside of the gospels, the author of 2 Timothy 2:18 refers to some who seem to believe that the eschatological resurrection of the dead had somehow already taken place (!) — which is often described as an ""over-realized"" eschatology.

To sum up, there are certainly some traditions unique to the gospel of Luke in particular that suggest a radical kind of ""realized"" eschatology; though note that some of this can also be found elsewhere, as in the parallel to Luke 11:20 in Matthew 12:28. Also, as I hinted toward at the beginning of this comment, there are several complicating factors to this notion — including perhaps whether some of these early Christians conceived of more than one ""phase"" to the ""kingdom of God,"" even if these are otherwise both discussed without terminological differentiation.

(For example, there's an early tradition preserved by the second century Christian author Hegesippus, in which ""the grandsons of Jude, Jesus' brother according to the flesh"" are brought before the emperor Domitian; and when ""asked concerning the Christ/messiah and his kingdom — its nature, origin, and time of appearance — [they] explained that it was neither of the world nor earthly, but heavenly and angelic, and it would come to be at the end of the world, when he would come in glory to judge the living and the dead and to reward every man according to his deeds."" Here, at least in the way they frame their answer, there's decidedly *no* notion of a ""realized"" kingdom, and instead the kingdom only comes at the end proper.)

____

Works mentioned:

Ilaria Ramelli, “Luke 17:21: ‘The Kingdom of God is inside you’: The Ancient Syriac Versions in Support of the Correct Translation,” *Hugoye* 12 (2009), 259-286",0
">Cudjoe Lewis died in 1935 at the age of 94-95 and is believed to have been the last survivor of the transatlantic slave trade, having come to the US in 1860 on the last slave boat. Another person who was on that boat, Sally Redoshi, lived till 1937 and had been a 12 year old in 1860.

Why do you say Lewis, who lived to 1935, was the last survivor of the transatlantic trade, and then in the next sentence list a survivor of the same ship who lived to 1937? Was Redoshi's experience different in some way?",0
"I can answer part of this question, mainly the legislative background and history of the ECOA and partly how RBG is related to it. Please note that US social history is only tangentially related to my field, so i could be missing something on the ACLU or RBGs relation to the ECOA.

***

***Tl;dr***: Women *could* both get mortgage and have a bank account. Rather, their practical opportunity to do so was limited by discriminatory cultural views and the practices of banks and creditors, this especially hit married women, as the husband was seen (also by some laws) as the head of the household finances and responsible also for the rights of the wife.

RBG didn't solve this herself, it's not solved by a court case (though, it did follow in the footsteps of court cases), but by legislative action that RBG, along with many others, were advocates for, this being the ECOA.

***

***The Equal Credit Opportunity Act and it's background***

Credit discrimination was made illegal with the *Equal Credit Opportunity Act* (ECOA) *of 1974* *15. U.S.C. 1691*, amending title VII of the *1968 Consumer Credit Protection Act*. ECOA made it illegal to:

>discriminate against any applicant, with respect to any aspect of a credit transaction, on the basis of (…) sex or marital status (…)”.

The Federal reserve board, on the basis of this, gave further implementing regulations (*Regulation B*) stating that:

>“\[A\] creditor shall not require the signature of an applicant's spouse or other person, other than a joint applicant, on any credit instrument if the applicant qualifies under the creditor's standards of creditworthiness for the amount and terms of the credit requested

On the purpose of the ECOA, a disctrict court case (*CMF Virginia land, L.P v. Brinson*) states that the purpose is to eradicate credit discrimination against women, especially married women, who creditors typically refused to consider individually. Furthermore that:

>It is well-documented that, prior to the ECOA, it was customary for lenders to require the guarantee signatures of husbands whose wives sought credit, even when a credit check would have revealed that the wife was creditworthy on her own

So there is a reason why *ECOA* mentions both sex and marital status, there were special cases of sex discrimination that hit married women *extra hard*, because the intertwined attitudes towards both women in general, *and* the cultural views on marriage, both became a limiting factor. Both also having a long historical expression in law^(1).

In the deliberations up to the ECOA, we have a report (1972) from the National Commission on Consumer Finance, which had studied the availability of credit to women, and they describe five key issues, which Margaret Gates (1974) cites as following:

* Single women have more trouble obtaining credit than single men. (This appeared to be more characteristic of mortgage credit than of consumer credit.)
* Creditors generally require a woman upon marriage to reapply for credit, usually in her husband's name. Similar reapplication is not asked of men when they marry.
* Creditors are often unwilling to extend credit to a married woman in her own name.
* Creditors are often unwilling to count the wife's income when a married couple applies for credit.
* Women who are divorced or widowed have trouble re-establishing credit. Women who are separated have a particularly difficult time, since the accounts may still be in the husband's name.

Gates also describes further problems not mentioned, including:

* Refusing to issue her an account for which she would be eligible were she not married,
* requesting information concerning her husband's creditworthiness before doing so,
* considering her a ""dependent"" of her husband when calculating his eligibility for credit,
* applying stricter standards when the wife rather than the husband is the primary wage-earner, and
* altering her credit rating on the basis of her husband's credit performance.

So credit could be a problem both for single and married women. Single women in a sense had more freedom, as the restrictions on married women were often tied specifically to the concepts of marriage, but single women faced other issues that made it harder for them to get credit than men.

Part of what would have been a problem for single women was the fact that they were assumed to soon be married and then leaving the workforce, which would factor into their independent credit ratings. There was also just blatantly sexist reasons for denying loan applications that hurt all women, like the idea that they were worse with money or could not do property maintenance like men, thus the property would fall more in value. Furthermore women were often hurt by a lack of credit history.

All this was done *to the contrary* of evidence at the time, which indicated women were equal or better creditors. There were some laws that the credit companies claimed made it difficult to treat women the same, this was not really the case, but the laws do illustrate that the law often treated men and women differently as well, such laws were especially the case for married women^(2).

***So overall*** women had the same formal right to take up credit and buy a house, but there were practical, cultural and legal barriers in the way of doing so, as discrimination was allowed. Solving some of the practical and cultural barriers for women to get credit was the goal of ECOA.

^(1. An example is the old case) *^(Brandwell v. The State 1872)* ^(stating that the paramount destiny and mission of women is to become a wife and mother.)

^(2.   Examples being,) ^(support laws) ^(where husbands had to support women. Women could thus buy on the Husbands credit, at least to a degree, a concrete example are Family expense laws, which makes it possible for creditors to seek expenses from both the husband and wife for family expenses regardless of who signed it.)

^(Further laws were some state’s) ^(property laws, which automatically made the husband the manager of the property, though by this time those states had mostly changed them to allow women some independence in managing her earnings, with Louisiana being the holdout. There were also laws limiting the ability to have separate accounts,) ^(multiple agreement laws) ^(meant to limit creditors abuse of charging in practice higher interests by having them in several separate loan agreements.)

^(Divorce and separation laws) ^(also caused issues)*^(,)* ^(the issue not being the law itself, but rather how marriage and divorce typically meant the man has done all the borrowing, and the divorced woman would be a “new face” as a creditor, and culturally seen as risky or unstable.)

*Continued below*",0
"In late 19th century Western Europe, having facial hair was generally seen as a sign of masculinity and maturity. However, it is important to note that the ability to grow facial hair varied among individuals, just as it does today. If you were a man who couldn't grow facial hair during this time period, you would likely be seen as an exception rather than the norm.",1
"\>The idea of returning America to a mythical past is also integral to the Klan starting in the second movement. Many far-right movements have it as their designated goal.

In case people are interested in this concept, I'd like to point readers to the term ""palingenetic ultranationalism,"" which Roger Griffin argued in *The Nature of Fascism* (1991) is the key defining characteristic of fascism. Griffin defined ultranationalisms as ""forms of nationalism which 'go beyond,' and hence reject, anything comparable with liberal institutions.""

The palingenetic aspect of this ultranationalism refers to a core fascist myth of national rebirth or returning to the immutable core of the nation, which had been soiled/obscured/lost. This is the idea of an America (or Rome, or Germany, etc.) that was once great, but that has been contaminated through racial/religious/ideological ""impurities.""",0
"> Lovecraft at the Automat

Fun paper. Etymology on most of Lovecraft's entities is pretty much up to conjecture; Lovecraft doesn't give the origins in his fiction, letters, or notes. ""Chthonic"" is a tricky one because Cthulhu is never underground; it may have been an influence, but there doesn't appear to be a strong symbolic connection. ""Thule"" is even trickier; I know Lovecraft uses it in a letter as early as 1930, but not his first use of it - and again, not really a strong correlation between the meaning of the word and Cthulhu in his fiction. Lovecraft discusses his naming in one letter at some length:

> By the way—as to those artificial names of unearthly places & gods & persons & entities—there are different ways of coining them. To a large extent they are designed to suggest either closely or remotely—certain names in actual history or folklore which have weird or sinister associations connected with them. Thus “Yuggoth” has a sort of Arabic or Hebraic cast, to suggest certain words passed down from antiquity in the magical formuale contained in Moorish & Jewish manuscripts. Other synthetic names like “Nug” & Yeb” suggest the dark & mysterious tone of Tartar or Thibetan Folklore. Dunsany is the greatest of all name-coiners, & he seems to have three distinct models—the Oriental (either Assyrian or Babylonian, or Hebrew from the Bible), the classical (from Homer mostly), & the Celtic (from the Arthurian cycle, &c). Thus he invents Eastern-sounding words like ""Gyshow"", ""Sardathrion"", ""Bethmoora"", &c., Hellenistic names like ""Argimenes"", ""Poltarnees"", &c., & pseudo-Celtic names like ""Arleon"" & ""Camorak"". I myself sometimes follow Dunsany's plan, but I also have a way strictly my own—which I use for devising non-human names, as of the localities & inhabitants of other planets. It is clear (though most writers fail to realise it) that the language supposed to be used by non-earthly beings—without human vocal organs & with no knowledge of terrestrial traditions—out not to resemble human speech in any way. The sounds ought not to follow any human language-pattern, & ought not to be derived from—or adapted to—the human speech-equipment at all. In other words, the whole design ought to be alien to both the ideas & tongue of mankind—a series of sounds of altogether different origins & associations, & capable only in part of reproduction by the human throat & palate & mouth. Just how far, & in what direction, such a sound-system ought to differ from human speech, must of course depend on how far & in what direction the imaginary users are represented as differing. In representing such sounds on paper, it is of course understood that our Roman alphabet can do it only imperfectly—since the alphabet was designed for a human language. Usually my stories assume that the non-human sounds were known to certain human scholars in elder days, & recorded in secret manuscripts like the ""Necronomicon"", the ""Pnakotic Manuscripts"", &c. In that case I likewise assume that the prehistoric or ancient authors of these MSS. gave the non-human names an unconscious twist in the direction of their own respective languages—as always occurs when scholars & writers encounter an utterly alien nomenclature & try to represent it to their own people. Thus when I cite the name of some wholly non-human thing supposed to be mentioned in the Necronomicon, I try to have the foundation of the word absolutely unearthly & alien, yet give it an outwardly Arabic aspect to account for the transmitting influence of the mad Arab Abdul Alhazred. Typical Necronomicon names are Azathoth, Yog-Sothoth, Shub-Nigguarath, &c. Often Clark Ashton Smith (who is almost as fertile a name-creator as Dunsany!) & I try to give different variants of the same unearthly or prehistoric name to represent the different variants under which different races refer to the same thing as remembered from primitive times ... ... as, for instance, some Nordic races spoke of Odin & Freya while others spoke of Woden & Frigga, or as the Hindoo Dyaus-Pitar became the Roman Iuppiter. Thus I have had Yog-Sothoth occur (in a story I wrote for a client) as Yocsototl among the Aztecs, while Smith (borrowing it from me) has coined the mediaeval form Iog-Sotôt for his mythical ""Averoigne."" And he has used his own Tsathoggua in various forms such as the pseudo-mediaeval Sodaqui, &c. Many Realists violently object to the practice of using these coined names, averring that it gives a childish effect to the stories concerned. I can see their point, but do not think their objection can be applied indiscriminately. Carelessly, injudiciously coined, or excessively used artificial names do rather cheapen a tale; but it is certainly advantageous now then to introduce a coined word which has been shaped with great care from just the right associational sources. At times I have dreamt certainly utterly alien names such as Kuranes, Nasht, & Kaman-Thah—all of course having vague linguistic sources in things I have read. As for writing out the hellish & forbidden Necronomicon—that would be quite an order, though I might manage to produce an isolated chapter now & then. Incidentally, as a sort of a joke anent the synthetic names we use so much, Clark Ashton Smith & I can each other Klarkash-Ton & E'ch-Pi-El. I once built up a while cycle of legend around Smith's Tsathoggua & used it in a story I wrote for a revision-client, but unfortunately the story was rejected.

H. P. Lovecraft to Duane W. Rimel, 14 Feb 1934, LFLB 140-141
",0
"The study of the history of decorative arts does present challenges like this, when artifacts survive without sufficient contextual information like written descriptions, illustrations, and the like. In my field (14th-19th century British silver), one entertaining example comes to mind.

The introduction of drinking chocolate to Britain in the mid-late 17th century entailed the need for a variety of new vessels and implements for preparing and serving. Chocolate was expensive and its consumption was initially restricted to the upper class. Silver was the mark of sophistication in home goods for the elite, and so any of these vessels or implements that were to be used in public (as opposed to in the kitchen) were wont to be made in silver. It's fascinating to trace how craftsmen serving the British upper class (i.e. silversmiths) responded to this demand, with a proliferation of diverse designs for chocolate pots and the like.

One key difference from modern hot chocolate is that in the 17th and early 18th century, hot chocolate lacked the emulsifiers that allowed the beverage to be smoothly, homogeneously mixed (and to stay that way). As a result, to prevent the drink from becoming an oily and chunky mess, it needed to be stirred, in the pot, immediately before pouring. This couldn't be done in the distant kitchen, as the liquid would congeal before it could be brought out and served. This stirring implement, then, also needed to be made in silver. Such items came to be known as ""molinets"" from the French *molin*, meaning ""mill"". Again, there were various designs, but typically with a little imagination they resemble whisks (at the risk of self-promotion, [here's](https://shrubsole.com/products/a-george-ii-antique-english-silver-molinet) an image of a fairly typical example, from my firm's website).

Now, with the introduction of emulsifiers in the mid-18th century, molinets were instantly obsolete. And because silver is a currency metal, the vast majority of molinets were melted down to recover their value. Only a few managed to survive until today, and because we see them so rarely, even specialists in the field sometimes scratch their heads when they come across one.

If you look at the above image without context, I think you can see how it might be misunderstood. In fact that very example was recently sold at a major English auction house whose antique silver specialist had described it as a ""mace"".

I suppose in a pinch you *could* club someone with it...but better to make chocolate, not war.",0
"Really interesting question! The White Album actually caused quite a bit of a stir when it came out as Back In The USSR was not the only politically charged song - in fact, all 3 of the songwriters in the group added something to the double LP that the media latched on to: Paul with USSR, John with Revolution and George with Piggies. It just seemed to be on their mind, especially considering they had just spent a considerable amount of time away from politics and international relations in an ashram in India.

As you brought up, Back In The USSR was a kind of mashup parody - McCartney was lampooning Chuck Berry's overtly nationalistic ""Back In The USA"" as well as Beach Boys-esque harmonies. I'll start with the Beach Boys, as this was actually not the first time the group had poked fun at their American counterparts; ""Girl"" from Rubber Soul featured Paul and George singing ""tit tit tit"" against John's bridge in a style emulating The Beach Boys but drawing attention to their 'good boy' image. However this wasn't an aggressive dig, just lighthearted fun, and Back In The USSR was intended in the same way. In fact, Mike Love was with The Beatles in India and (supposedly, according to Mike Love) was the one who suggested the bridge should be like California Girls! His autobiography looks back at the song fondly (he also has claimed to have written other chunks of White Album songs and ""jokingly"" asked for royalities), so we can infer that if the most notoriously grumpy member of The Beach Boys was okay with it, the rest of the band must not have minded! Chuck Berry and other early rock n rollers were going out of fashion by the late 1960s, so poking fun at their style and vocal delivery was inline with The Beatles' humour - especially one so American as Chuck Berry

The media reaction is more difficult to generalise - McCartney was banned from performing in the USSR for a period of time, but so were all of The Beatles and a heap of other artists so its difficult to tell  how much of it was down to one specific song. We do know that Elton John was asked not to sing it amongst others (he did it anyway), but again the USSR was a big and complicated media machine. On home soil, a great deal of music reviewers and journalists recognised it for what it was - a parody. Other songs on the album like Piggies were much stronger in their critique of capitalism and the upper classes. Back In The USSR was the album opener, however, and some Beatles historians have looked back on this as a distinct choice to take the listener by surprise - recognisably American music backed with the most un-American lyrics imaginable.

What we have to remember with the Soviet reaction is that the government and media *didn't really like The Beatles in the first place*. They were a bastion of Western imagery and a pro-USSR song wasn't likely to make a difference! Its very easy to hype up USSR reactions or American outrage after-the-fact, but what i've gathered from reading too many Beatles books and musciology articles is that it was a joke, and most people laughed.

Edit: Grammar",0
"The concept of ""ethnic"" restaurants, specializing in the food of a particular foreign culture, has a long history, although it may vary in terms of how it is defined and manifested across different time periods and regions. In ancient Rome, the city was a melting pot of cultures due to its vast empire. As a result, there were numerous opportunities for cross-cultural interactions, including in the realm of cuisine.",1
"The two things to consider here are the Soviet dominance of human intelligence and a far superior Soviet counterintelligence units. So I will answer in two parts.

Soviet human intelligence vastly outpaced the CIA during the early to mid Cold War. The Soviet Union recruited young, ideologically motivated, foreign nationals (those in target states) with a high potential to reach important government positions. When many of these agents reached those positions it became clear that the CIA, MI5, and MI6 were not even under siege but instead had already become thoroughly penetrated. Author Wilderness of Mirrors, David C Martin says that ""[the CIA] was utterly ignorant of Soviet espionage operations."" Agents in the employ of the Soviet Union often installed themselves in the organization's core structure before rigorous internal counterintelligence programs were instituted. This means that the infiltrators were trusted members of the team and with this status that effectively exempted them from scrutiny, the Soviet Union knew of the vast majority of high profile intelligence operations against them before they could even leave the planning board. A good example of this is Operation Gold, a plan to dig a tunnel underneath East Berlin in order to tap regional Soviet military cables. But before the tunnel was even made, a man named George Blake compromised the operation, effectively opening up the plan and the agencies behind it to manipulation through misinformation. It is my view that the CIA was only kept in the game by strong counterintelligence agents like James Jesus Angleton and William King Harvey, but mostly Soviet walk-ins or defectors. Many of the CIA's counterintelligence victories were achieved through defectors like Walter Krivitsky and Michael Goleniewski.

In terms of counterintelligence programs, the early Soviet model was also far superior. Naturally, secret police, surveillance, and counterintelligence all come easily with a totalitarian, autocratic government. This enabled Red Army units like SMERSH (a name that amounts to ""Death to spies"" in Russian) to have free reign in convicting and killing spies as they were created as an external organ to existing Soviet military/intelligence structure. They were so effective because they targeted any threat to information security, both real and imaginary which resulted in the executions of many innocents.

So, as a brief answer to your question: during the early Cold War, the balance of power in intelligence (and counterintelligence) heavily favored the Soviets.

Sources:
Vadim Birstein's SMERSH,
David C. Martin's Wilderness of Mirrors,
Michael Warner's The Rise and Fall of Intelligence
",0
"The Boston Pops Orchestra led by Arthur Fiedler were a popular ensemble by the 1970s. They were broadcast on the radio nationally, and starting in 1970 had a nationally-running TV show, *Evening At Pops.*

They also performed a series of free concerts on the Esplanade in Boston every summer, which were initially well-attended. However, both Fiedler and patron David Mugar noticed that attendance was dwindling during the 1973, and wanted to do something big to stop the trend.

Fiedler suggested a July 4th spectacular for 1974, complete with fireworks during the 1812 Overture and the original cannon fire. He was a good conductor, but an even better showman, and supposedly waved away concerns about the complicated nature of the undertaking by saying
""just let all hell break loose at the end of the piece."" And just as he suspected, the audience loved it. 

Two years later the US celebrated its bicentennial, and Boston was naturally a big part of the festivities. Fiedler decided to do the same thing he did in 1974, ending the program with cannon fire and fireworks in the 1812 Overture. Thanks to the hype surrounding the bicentennial, over 400,000 people showed up to the Esplanade for the concert.

This started a tradition in Boston that spread across the US of playing the 1812 Overture during 4th of July festivities. Bigger cities that can afford it use actual cannons, smaller towns and orchestras use the same percussion or rifle shots that have been used as substitutes. And so, a piece with both the Tsarist anthem and the Marsellaise, written to commemorate a Russian victory over Napoleon, somehow has become one of the most patriotic tunes you can hear in America.",0
"This is an interesting question and something that people have been wondering about for a long time.  Before I can answer it, I feel like it is worth disclaiming that I am not British, in terms of national origin. I am American, was educated from elementary school through my graduate program entirely in America, so when I speak here, I am talking about what academic historians say about the American Revolution on the university level stage. I have no knowledge what schools, books, or teachers say for students beneath college level.  Fortunately, since the American Revolution is something studied globally, I’ve studied historiographical debates of modern and previous historians in Britain, and I will be happy to discuss that here.

Overall, the majority of historians who studied and reported on the American Revolution currently have no major conflicts with American historians who study the same time period, but this is not true for all of the last two centuries.  The first person to tackle this question was British historian Richard Middleton, who conducted research and wrote about it in his article, [“British Historians and the American Revolution”](http://www.jstor.org/stable/2767062) . His article mainly seeks to study what early British historians said and reported on the American Revolution.  I feel obligated to point out that in these early days of the discipline of academic history scholarship, being objective was not something that was desired.  

Early historians, generally speaking had very little problem with letting their biases show.  This drastically changed during the middle of the 20th century, as historians sought to become more objective in their study and reporting of history.  It’s also worth noting that the vast majority of British scholarship in the first century and a half after the American Revolution took place was mainly focused on British perspectives; such as British politics and economics during that period.  Americans, especially during the 19th century conducted a lot of history related to the biographies of the ""Founding Fathers"" and also many sub-topics within the war itself (like military history of the Continental Army). Far fewer British historians studied American perspectives on the war, such as the Continental Congress, the Founding Fathers, or the Contiental Army. 

Middleton noted that many British historians were British apologists, who did side more favorably with the British Crown's decisions during the war and were very critical of the rebelling colonists.  This article does a few interesting things, first it accounts for a history of early British Historians who studied the American Revolution and notes that some of the early historians from the late 18th century were in fact more favorable in their views of the rebelling colonists, but they were the minority.  Historians tended to be torn between two radical positions in the U.K. One historian, Sir George Otto Trevelyan created a three-volumne account of the American Revolutionary War between 1874 and 1880.  Trevelyan, a liberal, reported that the first twenty years of King George III’s reign was a period of regression for English laws and and rights.  His writing reflects that the Americans were justified in overthrowing the British government, calling the Americans “law respecting people, who did not care to encroach on the privileges of others and liked still less to have their own rights invaded.’ [Middleton, 50].  This created one of the first historiographical debates in this particular field, with other men, like William Massey and William Lecky arguing for the traditional “Tory” view of the war.  Where Trevelyan came down hard on royal governors and British officials in America, historians like Lecky was much more gentle on them, saying that they were just loyal men trying to fulfil their duties to the King. [Middleton 51]. Ultimtely, Middleton noted a few key differences between these earlier types of scholarship between American and British scholarship on the war:

> From this study of British historians and the American Revolution, one or ore general conclusions can be drawn. In the first place, there has bee na curious dichotomy between scholars on both sides of the Atlantic as to what actually constituted the Revolution. Most British writers do not appear to consider it to be properly underway until the fateful year of 1775, while for most American writers, it was then merely a matter of dotting the ‘i’s’ and crossing the ‘t’s’ on the declaration of independence [sic].  There has also been little attempt to understand the revolt of the thirteen colonies by analysis of their social, economic, or politiocal development, lines of approach for so long popular with the American historians. [Middleton,  58]

Historians during the 20th century, especially in the post World War II era appeared to settle more soundly into the more objective approach to history.  Middleton spoke also of, at the time of the publication of that article in 1971, how there was a revival of the British study of the American Revolution during his present because of the infusion of American culture, history, and politics that exploded in those preceding decades. [Middleton, 56]
In recent history, the vast majority of historical scholarship that has come out (at least as what I’ve seen and studied] has not conflicted with the current consensus of scholarship  coming out of the United States.  Some British historians, like Stanley Weintraub’s *Iron Tears* has [pointed out some differences that he sees in views of the American Revolution](https://www.npr.org/templates/story/story.php?storyId=4727956), but nothing really seems too far out that it would conflict with what other historians would say.  For instance, he shows that from the British perspective, the “taxation without representation” rallying cry of the colonists was a bit weak of an argument from the British perspective because many of the Englishmen living in England were not represented in Parliament either.  (His book though more-so focuses on how the Average person in Britain felt about the war, rather than focusing on Historians studying that period).

Overall, modern scholarship between historians of the United States and Britain tends to add to the historical conversation in general on this topic, rather than causing conflict between them.  Historians will disagree in every field, it's why consensus does not have the same power as a fact, but there isn't anything fundamentally different between scholarship coming out of the U.K. versus what is coming out of the United States or other countries. 

Edit: fixed typos",0
"So /u/toldinstone has already written an excellent answer to the question in relation specifically to Socrates, but I study Ancient Mesopotamia and in particular science and religion especially in the late First Millennium BCE (around the time that people like Socrates were living) and so I thought it might be nice to talk some about how people that worked in Mesopotamian scholarly endeavors made money themselves as it can perhaps give us a better understanding of how people who pursued these types of scholarly pursuits in the ancient world would have been employed. The picture in Mesopotamia will absolutely be quite different than it was in ancient Greece and to our knowledge there wasn't really a scholarly discipline that remotely looks like Greek philosophy in Mesopotamia, but the Mesopotamians were nonetheless very interested in especially the movement of the stars and the planets and so if we want to extend this question more generally to how people who spent their life doing things that seem to have no immediate monetary gain like watching the stars supported themselves then I think the picture at Mesopotamia is certainly worth discussing.

One very major point to discuss before talking about Mesopotamian ""scientific"" endeavors is that the entire idea of ""science"" certainly did not exist in Mesopotamia in any capacity. In the same vein, ""religion"" would never have existed as a distinct idea and so the vast majority of people engaged in the things that modern scholars identify as precursors to modern ""science"" were doing so for religious reasons. To the Mesopotamians the stars and the planets were literally their gods and their movements were interpreted as being messages from the gods, ""the heavenly writing,"" and so by far the biggest employer of Mesopotamian astronomers was the temple. The most detailed astronomical record we have in cuneiform are the tablets referred to as the Astronomical Diaries, which include night by night recording of the movement of all the planets during the year as well as different pieces of information about the motion of the moon, coupled with information on the weather, the level of the Euphrates river, some prices for a set of commodities in the market, and even historical information. These regular recordings likely began by around 400 BCE and ended sometime before 100 AD, and the vast majority of recovered tablets are from between 400-200 BCE. The bulk of recovered tablets containing astronomical diaries were compiled by individuals who were employed by the Temple of Marduk, known as *Esagil*, in Babylon and their job was specifically to go out at night and record the position of the planets and then write those down on tablets which they would use to collect all the recordings onto yearly tablets later. Another temple that was certainly very closely involved in the production of astronomical diaries and where similar tablets have been discovered was the *Bit Resh*, the temple of the sky-god Anu located in the city of Uruk. Uruk and Babylon were both ancient centers of cuneiform scholarly endeavors and became some of the last bastions of cuneiform writing well into the Hellenistic period when the people of the city and most certainly the scholars themselves would have spoken Aramaic to each other and when administrative writing would have been done perhaps in Aramaic at the local level and most definitely in Greek at the royal level.

As such, the astronomers working at temples would have been paid out of temple stocks and this shows increasingly the strength of the temple institution as a whole in Mesopotamia into the later periods. The temple would have obtained their stock from a number of different places. Part of it would be from donations by people in the city, but likely more of their wealth would come from land that would have been donated to the temple by the king or other senior government officials and those lands would be maintained by slaves also donated by the state. Temples in Mesopotamia would have certainly been exceptionally wealthy but it is probably also the case that the point of this wealth was almost as a ""bank"" in that if there were issues in the city as a whole such as famine or war the temple could use their stores to help people. At a minimum, it was expected that the wealth of the temple was meant to enrich the ""life"" so to speak of the god who called that temple ""home"" (there is no distinct word in Akkadian for temple, the word *bitu* which is used to refer to temples is also used as the general word for ""house"" and so temples were simply the ""house of ... god""). Their gods were literally statues that they performed specific rituals on in order to allow the spirit of the deity to ""inhabit"" the statue, and so caring for these physical manifestations of their gods was a large part of the temple's job. They provided daily meals to the gods, and it is likely the case that the physical remains of the ""meal"" which obviously wouldn't actually be eaten by the statue were a means of payment for temple personnel including the scholars working at the temple. The temple also facilitated the various religious festivals which often involved taking the god on a trip to other cities or parading him about the city, and these could be accompanied by elaborate feasts. The belief was that by providing for their gods in the form of these statues they would keep the god happy and this would in turn bring prosperity and success to the city as a whole. While I do expect much of this wealth was used to improve the lives of the priests themselves and make them personally wealthy, we should not be so cynical as to believe the goal of the temples was to be a personal business for the priests.

Some scholars in Mesopotamia would also have been employed by kings and we can see that this practice most certainly remained in place in ancient Greece. Aristotle, for example, is famously known for having been employed as the personal tutor for Alexander the Great. Kings in Mesopotamia, most clearly evidenced by a massive body of letters discovered from the Library of Ashurbanipal dating to the years around the reign of Esarhaddon and the ascension of Ashurbanipal, personally employed an army of scholars who would help watch the stars and interpret events. The Mesopotamians believed that there were many ominous events that could be inferred from the movement of the stars and the series of celestial omens *Enuma Anu Enlil* was likely over 70 tablets in total. The scholars commanded a great deal of respect by the king, and they were capable of in effect influencing the actions of the king through these omens. We should not see them as ""puppet masters"" though who tried to actively control events; the king would check the predictions of his scholars against other scholars at court and if he believed a scholar was lying to him he would at the least imprison that scholar and punish his entire family if he did not execute the scholar outright.

Being a court scholar in Mesopotamia was certainly very profitable though and these positions were highly coveted and moved down through families. The senior court officials often had their children employed if not in their exact position at court at least in other related senior court positions. This was likely in part because of the physical access they had to tablets themselves which contained the omens that made up the entirety of their jobs. They would have jealously guarded their tablets and in later periods they even describe tablets as ""secret information"" and instruct people not to show them to ""one who does not know"" on the colophon. Because of this restriction of physical access to tablets, not everyone could obtain the information that would allow them to be a senior court scholar and positions remained in families generally. This would also have been beneficial to the king as it created a reason for senior court officials to be more loyal to him as it was not simply their own livelihood that they risked by being disloyal but also the potential for their children and grandchildren to obtain similar positions in the court. Court officials likely were paid in part by what is referred to as ""the leftovers"" of the god, which potentially refers to that same food that was given to the statue of the god for their meal that I discussed previously. Eventually the scholar would inherit and be gifted land in the same way the temples were. The king would provide his senior officials with land in the country that they would have slaves maintain for them and that would make them generally relatively wealthy. We have documents with examples of senior military officials buying land or selling land to court scholars, indicating that they did participate in economic transactions. 

While this situation may not necessarily translate perfectly to the situation in Greece and certainly does not mirror the way Socrates made a living at all, I think it is interesting to consider how individuals who pursued scholarly professions were able to live. As we see, being a scholar was certainly possible likely as early as writing was a thing. As soon as written texts which contained useful knowledge appeared and became increasingly standardized, there were people making a living by using the knowledge they obtained from reading those texts.",0
"> What reason is there to lie about this?

Holocaust Deniers primarily seek to remove this taint from the ideology of Nazism by distorting, ignoring, and misrepresenting historical fact and thereby make Nazism and Fascism socially acceptable again. In other words, Holocaust Denial is a form of political agitation in the service of bigotry, racism, and anti-Semitism. The reason to lie is to recruit people to your agenda of Fascism and Nazism by removing its biggest historical crime from the equation.
",0
"Damn, I love this sub! Went into this thread thinking ""way too specific question"" or ""way too close in history"", but here you are giving the perfect answer to this question!",0
"For background: there is a long history in English and American law of women losing rights upon marriage. Coverture, as I've explained in past answers like [this one](https://www.reddit.com/r/AskHistorians/comments/abhaht/jane_austen_and_the_brontes_are_among_the_most/ed94h3e/), meant that a woman who married was sucked into her husband's legal identity. Therefore a number of things that required a legal agreement required her husband's signature/consent alongside hers, and a married woman had no control of her earnings or property if her husband chose to dispose of them. Women who had inherited property or money before they married would lose it to their husbands as well.

Coverture as a legal principle began to be dismantled over the nineteenth century in both the United Stated and the United Kingdom. In the United States, the repeal of coverture and the advance of women's rights had only happened on a state-by-state basis, which meant that there were different standards everywhere, but by the end of the century, all(?) states had laws on the books ensuring that married women were at least entitled to their own earnings and property.

But we aren't just talking about coverture, we're talking about women's equality to men, period. So, the fourteenth amendment of the US constitution states:

>No state shall make or enforce any law which shall abridge the privileges or immunities of citizens of the United States; nor shall any state deprive any person of life, liberty, or property, without due process of law; nor deny to any person within its jurisdiction the equal protection of the laws.

If this principle were always followed, we would live in a utopia. In the mid-twentieth century, states had routinely made and enforced laws that abridged the rights of women and people of color, and failed to give them the equal protection of the law. The 1961 Supreme Court case of *Hoyt v. Florida* even explicitly upheld the lack of protection to women: Gwendolyn Hoyt had killed her physically and emotionally abusive husband in self-defense, and was convicted almost immediately by an all-male jury, as the state required all men to serve on juries and only allowed women to do so on request; SCOTUS ruled that women should be protected from the ""filth"" of the courtroom and that all-male juries were normal in the United States. As a young lawyer at the time, Ruth Bader Ginsburg began taking cases to challenge that lack of protection. And although she would eventually do great work against discrimination as a judge, typically people who are talking about what she accomplished for women's rights in her career are referring in large part to her work as a lawyer.

The landmark case that brought the Equal Protection Clause to bear on women's rights is *Reed v. Reed* (1971). Sally and Cecil Reed were divorced, and Sally had been unable to keep full custody of their son as she had wanted. While at his father's house, their son apparently committed suicide with one of Cecil's guns. Both parents filed to be the administrator of his estate (Sally actually submitting her petition first), but it was in Idaho state law that men *must be* preferred to women when it came to estate probate, so Cecil was given their son's belongings. This traveled up the chain to the United States Supreme Court, and Ginsburg wrote the brief defending Sally Reed's rights along with the director of the ACLU. SCOTUS found in her favor, and it was deemed unconstitutional to enshrine a preference for one gender over another in law; Congress would go on to rewrite a number of laws that had done so.

While it wasn't the full closed-door to sexism that Ginsburg and others had hoped, it provided a great precedent for later cases - often involving Ginsburg! - to say, ""no, you can't legally discriminate based on gender"" in other specific ways, which is something that had not previously been done before. The following year, Ginsburg would set up and lead the Women's Rights Project in the ACLU in order to put and keep equal rights for women on the organization's radar, which does mean that she deserves credit for the good work the WRP would go on to do in fighting for equal rights for women. She was involved with other SCOTUS cases dealing with gender bias enshrined in law throughout the 1970s, including *Moritz v. Commissioner* (1972), which struck down a law that allowed a state allowance for hiring a home health aide only if you were a woman or widowed man, and *Frontiero v. Richardson* (1973), which required the military to give benefits to male dependents of female officers just as it did female dependents of male ones.",0
"In brief, one can approach this question down in a few ways:

**Were they likely to be considered literal war crimes and open to prosecution at the time?** No, because the Allies came up with the definitions and applications of the ideas of war crimes, and exempted themselves very deliberately from prosecution. Were they hypocritical? It was argued by some of those under prosecution for war crimes, and even by some who were more neutral, that there were obvious hypocrisies (aside from the US and UK participating in mass bombing of civilians, you also have things like the Soviet lootings and rapes, as well as mass executions at Katyn, and so on), and that this devalued the usefulness of ""war crimes"" as a category if it basically only applied to losers in war. 

**Were they radically morally different from other Allied activities?** It depends on how you want to parse out the morality, but they are alike in many ways (though not all) to the use of napalm (firebombing) against Japanese and, to a lesser but still significant extent, German cities. In these attacks, mass areas were targeted, with deliberate goals of destroying civilian housing and infrastructure, and with the knowledge that many civilians would die. This was especially true of the infamous raids against Tokyo and its environs in March 1945, which killed as many people as the atomic bombings did. One could argue, if one wanted, that the atomic bombs were slightly worse from this perspective: they were considerably more deadly for the area of target destroyed, especially compared to later firebombings, because of their surprise and speed of attack (with firebombings, there are ways to detect the attack ahead of time and flee, and also some measure of defense possible in terms of firefighting and fire breaks; these were not the case with the atomic bombings). The Allies also did warn, in way both vague and specific, about firebombing attacks; they did not warn (contrary to Internet myths) about the atomic bombing attacks. Is this splitting hairs? It doesn't really matter for this analysis: if you are saying that the atomic bombs were ""just as bad or maybe worse"" than the firebombings, you probably already are concluding that the indiscriminate targeting of civilians was some measure of ""business as usual,"" which does _not_ in any way get you off the hook for questions about ""war crimes"" (in fact, it is even worse — saying you _regularly_ committed similar offenses does not make them less heinous).

**Did people at the time worry about the morality of these kinds of attacks?** Yes, both inside and outside the US government. There were many people in and out of the US who condemned the attacks, or at least questioned the city-targeting aspects of them. Within the US, even those who plotted to use the atomic bombs saw them as being imbued with special moral hazards, and thought that indiscriminate targeting of cities was potentially not aligned with stated US values. Scientists on the project (at the University of Chicago) warned that targeting cities with the first bombs would lead the world down a very dark path, and could not be justified (see the Franck Report). At higher levels, even the US Secretary of War, Henry Stimson, warned Truman that indiscriminate firebombing might allow the US to exceed the reputation of the Nazis for atrocities, and worked (in a way that one might or might not judge meaningful) to keep the city of Kyoto off the target list partially as a means of mitigating the moral issues. In a forthcoming paper, I have argued that I think Truman himself saw the bomb in these terms, and that in agreeing with Stimson that Hiroshima, not Kyoto, should be the first target of the atomic bombings, he was (incorrectly) under the impression that the bomb would be first used on a ""purely military target"" (as he put it) and not a city. He did not, I argue, learn Hiroshima was in fact a city (and that 90% of the casualties were civilian) until August 8, 1945, as an aside. All of which is to say, if someone says to you, ""nobody had moral issues with this at the time,"" they are wrong. Plenty of people, _including the people who ordered the atomic bombs be dropped_, recognized that this kind of bombing _did_ present moral hazards, though of course they did not think they were considering them ""war crimes."" But it opens the door to us considering them as moral hazards without being accused of being ahistorical.

**Did the atomic bombings violate any treaties the US had signed _at the time_?** No. The US had not signed many treaties on the laws of war at the time, and the ones it did sign did not really come into play. One can make a very stretching argument that the atomic bombings might fall under the prohibition of the use of poisonous gases, but it is a stretch (they did not create significant contamination; the deaths were primarily from fire and blast effects). 

**Would the atomic bombings of Japan count as war crimes if they were done _today_?** The US has very lengthy guidelines for how it interprets the Geneva Conventions and the Law of War today, and how nuclear weapons play into that. In principle such an attack plan — target the center of a city for the purpose of destroying the city and terrorizing a country into surrender — would probably _not_ be considered a justifiable reason to use nuclear force, as it would violate the principles of discrimination (it would unduly target non-combatants) and likely proportionality (it is overkill for the goals it is trying to accomplish). This does not mean that you could not come up with a rationale for doing the same thing (e.g., you could re-frame the justification around military necessity, the limitation of conventional forces to do the same job, a focus on the military and industrial facilities in the target zone, etc.), but the rationale used _at the time_, which is to say, the destruction of a civilian population for the purpose of convincing Japan to surrender, would probably not pass the legal scrutiny of the JAGs. But these kinds of questions are notoriously difficult to parse in the abstract, so who really knows. Current US plans for the employment of nuclear weapons are structured around the ideas of discrimination, necessity, and proportionality, and so instead of saying, ""put a huge explosion in the city center"" they are about how you would destroy some specific military capability in the city. Is that a moral difference? This is a question for another day. But again, I suspect the US would _not_ find it easy to justify the attacks under the _present_ Geneva Convention it has signed to (some years after World War II) which is much more explicit about the illegality of targeting cities in this way. But we should also note, while we are on the subject, that the US has found the means to justify a lot of other kinds of city bombing after WWII, which makes me a wary about concluding that the lawyers could not find a way to justify it. They are clever, after all. 

In short: by modern standards they would probably not be permissible actions. By the standards of the time, they ride the line of what the Allies considered permissible _when they were doing them_, though this was seen by many as hypocritical. In any event, the fact that they were credited with ending the war (whether they did or not is a hot topic of scholarly debate), and the fact that the Allies created the war crimes tribunal, meant that not only were there no negative consequences for those who were involved in the bombings, but in fact almost all of those who were involved saw their careers flourish as a result of them. ",0
"We, in fact, have a text about a well-to-do Roman aristocratic woman from Carthage converting to Christianity in the late 2nd century: *The Passions of Perpetua and Felicity*, a first-person narrative apparently written by Perpetua about her time in prison and martyrdom for refusing to rescind her Christian faith. We unfortunately do not have any text about Vibia Perpetua's life before her martyrdom, and her husband is curiously absent from the text, despite her being described as newly-married and having a young son she is still nursing. This could be caused by a number of things: he could have truly abandoned her and not visited her in prison, leaving her to the mob; he could have been away or recently died (although that is unlikely or she would've been identified as a widow); he could have been edited out by later scribes who wanted to emphasize her as a woman of God (the earliest manuscript we have is from the 10th century, so there was plenty of time for the text to be edited in this manner); or he could have been a prisoner with her, as some have suggested Saturus is her husband, but again his presence edited out.

We can perhaps then look to her family for an example of how 2nd century pagan Romans would have felt about someone close to them converting to Christianity. Her family, and particularly her father, visit her while she is imprisoned begging her to rescind her beliefs and trying to grant her a pardon. The Roman officials too at her trial beg her to rescind and have pity on her grieving family, however when she refuses, she is sentenced to death.

From this account, converting to Christianity does not necessarily seem like something they are embarrassed about, as the father comes to the public forum for her trial and does not seem ashamed, but this could be the desperate acts of a grieving man. The official also does not seem to think that this is a shameful thing- Romans had lots of gods and they always welcomed more, after all- his biggest concern is that she offer a sacrifice for the emperors and also respect their state religion, in addition to her own. It could be a source of embarrassment to be consorting with slaves, as she was, but that isn't acknowledged in this text, as there is a great focus on breaking ""worldly"" familial ties (including ceasing nursing her own infant son and giving him to her family to care for) and focussing on the kinship of the Christian community and the world to come.

There is evidence that this affected the family's social standing, however, as the father is beaten during the trial, something that his status as a Roman man should protect him from, and the family definitely wants her to reconsider her decision. Though whether that's because it is an embarrassment to the family or because they know it may be a danger to her, it's difficult to say. Using this text is clearly problematic for a number of reasons, particularly because, as this is a religious text, it is not necessarily intending to give a clear and accurate narrative of events, but instead is more intent on Perpetua's spiritual journey and dreams.

I think it's also important to note that by the 2nd century in the Roman Empire, *sine manu* marriage was more common than *cum manu* marriage, which meant that a bride stayed under the household and *patria potestas* (power of the father or head of household) of her own father, rather than the husband's. Looking for a husband's view then may be less important than looking at her father's.

Further reading:

*Perpetua's Passions: Multidisciplinary Approaches to the Passio Perpetuae et Felicitatis.* Edited by Jan N. Bremmer and Marco Formisano. Oxford: Oxford University Press, 2012. 

Heffernan, Thomas J. *The Passion of Perpetua and Felicity.* Oxford: Oxford University Press, 2012.",0
"> This time in England saw countless shotgun weddings, with estimates placing one in three brides as pregnant by Elizabethan times. 

Were these shot gun weddings a result of widespread pre-marital sex amongst consenting romantic partners, or widespread rape? Weren't women at the time forced to marry their rapists since they were seen as having become ""damaged goods""?",0
"Hello. We have banned several users in this thread for denial of the American Indian Genocide(s) that occurred in the Americas. This topic is often controversial and can lead to inaccurate information. This message is not intended to provide you or other readers here with all of the answers, but simply to address some of the facts, as well as genocide denialism in this regard, and provide a list of introductory reading. Because this topic covers a large area of study, actions of the United States will be highlighted. There is always more that can be said, but we hope this is a good starting point for you.

##What is Genocide?

Since the conceptualization of the act of genocide, scholars have developed a variety of frameworks to evaluate instances that may be considered genocide. One of the more common frameworks is the definition and criteria implemented by the United Nations. The term ""genocide,"" as coined by Raphael Lemkin in 1943, [was defined by the U.N. in 1948](https://redd.it/6mg3j3). The use of this term was further elaborated by [the genocide convention.](https://treaties.un.org/doc/Publication/UNTS/Volume%2078/volume-78-I-1021-English.pdf)

Article II describes two elements of the crime of genocide:

1. The mental element, meaning the ""intent to destroy, in whole or in part, a national, ethnical, racial or religious group, as such"", and
2. The physical element which includes five acts described in sections a, b, c, d and e. A crime must include both elements to be called ""genocide.""

Article II: In the present convention, genocide means any of the following acts committed with intent to destroy, in whole or in part, a national, ethnical, racial or religious group, as such:

* (a) Killing members of the group;
* (b) Causing serious bodily or mental harm to members of the group;
* (c) Deliberately inflicting on the group conditions of life calculated to bring about its physical destruction in whole or in part;
* (d) Imposing measures intended to prevent births within the group;
* (e) Forcibly transferring children of the group to another group.

##American Indian Genocides – Did they happen?

Since the arrival of Europeans to the Americas, typically signaled with the appearance of Columbus in 1492, Indigenous Peoples have experienced systematic oppression and extermination at the hands of colonial powers. These colonizing governments either organized or sponsored acts of genocide perpetrated by settlers, targeting Indigenous settlements for complete destruction; eliminating sources of food and access to life-sustaining resources; instituting child separation policies; and forcefully relocating Indigenous populations to often times inhospitable tracts of land, now known as “reservations.” All of these acts constitute what scholars now recognize as genocide. The horrendous acts that occurred in the Americas was even an example proposed by Lemkin himself, where it is noted from his writings:

>[Lemkin applied the term to a wide range of cases including many involving European colonial projects in Africa, New Zealand, Australia, and the Americas. A recent investigation of an unfinished manuscript for a global history of genocide Lemkin was writing in the late 1940s and early 1950s reveals an expansive view of what Lemkin termed a “Spanish colonial genocide.” He never began work on a projected chapter on “The Indians of North America,” though his notes indicate that he was researching Indian removal, treaties, the California gold rush, and the Plains wars.](https://oxfordre.com/americanhistory/view/10.1093/acrefore/9780199329175.001.0001/acrefore-9780199329175-e-3)

These actions took place over the entirety of the Americas, exacerbating the rapid depopulation of Indigenous Nations and communities. Exact figures of the population decline are inconclusive, giving us only estimates at best, with Pre-Columbian population numbers ranging anywhere from as low as 8 million to as high as ~100 million inhabitants across North, Central, and South America. What we do know is that in the United States, records indicate the American Indian population had dropped to approximately 250,000 by 1900. Despite any debate about population statistics, the historical records and narratives conclude that, at least according to the U.N. definition, genocide was committed.

##Mental Element: Establishing Intent

In order for genocide to be committed, there must be reasonable evidence to establish an intent to commit what constitutes genocide. Through both word and action, we can see that colonial powers, such as the United States, did intend at times to exterminate American Indian populations, often with public support. Government officials, journalists, scholars, and public figures echoed societal sentiments regarding their desire to destroy Indians, either in reference to specific groups or the whole race.

>”This unfortunate race, whom we had been taking so much pains to save and to civilize, have by their unexpected desertion and ferocious barbarities justified extermination and now await our decision on their fate.”

[--Thomas Jefferson, 1813]( https://www.loc.gov/resource/mtj1.047_0147_0150/?sp=3)

>""That a war of extermination will continue to be waged between the races until the Indian race becomes extinct must be expected.""

[--California Governor Peter Burnett, 1851](http://governors.library.ca.gov/addresses/s_01-Burnett2.html)

>"". . .these Indians will in the end be exterminated. They must soon be crushed - they will be exterminated before the onward march of the white man.""

[--U.S. Senator John Weller, 1852, page 17, citation 92](http://digitalcommons.law.utulsa.edu/cgi/viewcontent.cgi?article=2654&context=tlr)

##Physical Element: Acting with Purpose

**U.S. Army Policy of Killing Buffalo (Criterion C)**

[In this post,](https://redd.it/4j42ag) it is explained how it was the intention and policy of the U.S. Army to kill the buffalo of America off in an attempt to subdue, and even exterminate, the Plains Indians.

**Sterilization (Criterion D)**

The Indian Health Service (IHS) is a federally run service for American Indians and Alaska Natives. It is responsible for providing proper health care for American Indians as established via the treaties and trust relationship between tribes and the U.S. Government. However, on November 6, 1976, the Government Accountability Office (GAO) released the results of an investigation that concluded that [between 1973 and 1976, IHS performed 3,406 sterilizations on Native American women.](https://cbhd.org/content/forced-sterilization-native-americans-late-twentieth-century-physician-cooperation-national-) Per capita, this figure would be equivalent to sterilizing 452,000 non-Native American women. Many of these sterilizations were conducted without the consent of the women being sterilized or under coercion.

**Boarding Schools (Criterion E)**

The systematic removal of Indian children from their parents and placement into boarding schools was a policy implemented by the United States meant to [force American Indian children to assimilate into American culture,]( https://redd.it/8zgozt) thus “[killing] the Indian, [and saving] the man.” These schools were operated by various entities, including the federal government and church/missionary organizations. While constituting cultural genocide as well, American Indian children were beaten, neglected, and barred from practicing their cultures. Some children even died at these schools.

##But What About the Diseases?

In the United States, a subtle state of denial exists regarding portions of this country's history. One of the biggest issues concerning the colonization of the Americas is whether or not this genocide was committed by the incoming colonists. And while the finer points of this subject are still being discussed, few academics would deny that acts of genocide were committed. However, there are those who vehemently attempt to refute conclusions made by experts and assert that no genocide occurred. These [“methods of denialism”](https://redd.it/6kywre) are important to recognize to avoid being manipulated by those who would see the historical narratives change for the worse.

One of the primary methods of denial is the over severity of diseases introduced into the Americas after the arrival of the colonizers, effectively turning these diseases into ethopoeic scapegoats responsible for the deaths of Indigenous Peoples. While it is true that disease was a huge component of the depopulation of the Americas, often resulting in up to a 95% mortality rate for many communities and meaning *some* communities endured more deaths from disease, these effects were greatly exacerbated by actions of colonization.

##Further Reading

Though there is much information about this topic, this introductory list of books and resources provide ample evidence to attest the information presented here:

* [*Beyond Germs: Native Depopulation in North America* edited by Catherine Cameron, Paul Kelton, and Alan Swedlund](https://books.google.com/books/about/Beyond_Germs.html?id=yUw-rgEACAAJ)
* [*American Indian Holocaust and Survival: A Population History Since 1492* by Russell Thornton](https://books.google.com/books/about/American_Indian_Holocaust_and_Survival.html?id=9iQYSQ9y60MC)
* [*Murder State: California's Native American Genocide, 1846-1873* by Brendan Lindsay](https://books.google.com/books/about/Murder_State.html?id=TfiD-E7VBKYC)
* [*Blood and Soil: A World History of Genocide and Extermination from Sparta to Darfur* by Ben Kiernan](https://books.google.com/books/about/Blood_and_Soil.html?id=XR91bs70jukC)
* [*American Holocaust: The Conquest of the New World* by David Stannard](https://books.google.com/books/about/American_Holocaust.html?id=RzFsODcGjfcC)
* [*Myths of Conquest*](https://redd.it/2vf565) by /u/anthropology_nerd
* [AskHistorians FAQ](https://www.reddit.com/r/AskHistorians/wiki/nativeamerican#wiki_european_contact_and_conquest)",0
"They were VERY different! In fact, in some ways it's almost easier to go through similarities (observance of some major Jewish holidays, circumcision, praying for Jerusalem, a seven day mourning period, just off the top of my head) rather than differences, as the entirety of the Oral Law (such as the Talmud) through which modern Judaism has developed was absent among the Beta Israel. Some Beta Israel practices, like animal sacrifice and purifying after contact with a dead body using the ashes of a red heifer, represent Torah practices that are no longer in use in normative Judaism. In many respects, the observances of the Beta Israel were quite similar to those of their Christian neighbors, but of course in form rather than in intent, as the  Beta Israel (at least those who didn't convert to Christianity at any point) rejected Jesus and the New Testament. 

As I mention in my post above (particularly the edit), European Jews reacted in a variety of ways. Some immediately cast doubt on their Jewishness, but others undertook to educate them in normative Judaism (in this case that meant Orthodox Judaism, as that was the leaning of Jacques Faitlovitch, the main figure behind this effort). It became a bit more of a contentious issue when Ethiopian Jews first arrived in airlifts to Israel, as their place in Israeli society was (both literally and figuratively) adjudicated, and this is still something of a discussion today, with most Ethiopian Jews in Israel generally integrating into normative Israeli society (and Judaism) but still retaining aspects of their identities, with Sigd, a major Beta Israel holiday, now a recognized national holiday in Israel. (All of this, of course, is a MUCH bigger discussion that I just don't have the energy for right now lol)",0
"In brief - for educational terms, it's a good-enough simplification for school purposes. But yes,  there were important cultural distinctions between the two.

I'm away from my books at the moment, so I'll do this one 'off the cuff' and lay down some suggestions for further reading: if you want to ask follow-up questions, I'll be able to get to those in a couple of days with the bibliography in front of me.

One thing that's often glossed over in school courses - and I say that as someone who teaches them - is that 'Greek Religion' and 'Roman Religion' (or even, perish the thought, 'Greco-Roman Religion' are not simple, single, fixed ideas. Both varied considerably over time, where you were, and who you were. I'm going to talk a bit about Greek religion first to lay some groundwork, and then try to describe what happens when it collides with Roman religion.

At this stage, we need to introduce the idea that the 'same' god could look quite different in different contexts. It was normal (particularly in the Greek world) to address a god not only by their name, but also by an 'epithet' - a short descriptor that gave a bit of detail about the god, but also reflected the particular aspect or characteristics of them that you were calling on. Apollo (we'll get to him properly later) is a great example - at Delphi, you would probably address him as *Apollo Pythios* ('Apollo of the Pythia - his oracular priestess at Delphi) and primarily conceptualise him as a god of prophecy who might advise and help you. On the other hand, if your city was struck by a plague, you might sacrifice to him as *Apollo Ekbolos* ('Apollo who Strikes from Afar') and conceptualise him as a terrifyingly inscrutable force for vengeance.  On one level, they're the same god, but they also feel rather different.

You might also have learned in school that the 'big gods' - the Olympians and so on - are only the tip of the religious iceberg: both Greek and Roman religion are full of 'small gods' of rivers and mountains, semi-divine heroes and other extremely local figures that mean that religious observation can change considerably from region to region, or even village to village. Even when talking about the greater gods, different people worship them to different degrees - most obviously, the people of Athens treated Athena as one of the most important of the gods, while she would be a relatively minor religious force in most other Greek cities.

This means that the 'international', 'official' Greek pantheon (which was never an idea that anyone in the Greek world, ever, would have accepted) was only ever a kind of *lingua franca* \- people around the Greek world could understand it and use it as a common point of reference, but it didn't really reflect anyone's actual, day-to-day experience of religion. Straight away, we're dealing with a religious landscape that is incredibly fragmented, and where any attempt to generalise about it smooths down the reality.

**EDIT:** [So is the Roman pantheon - on which more detail here.](https://www.reddit.com/r/AskHistorians/comments/htyia8/the_roman_pantheon_was_presented_to_me_in_school/fyn574j/)

Roman religion and Greek religion have a common source - they both descend from Indo-European belief and folklore, which included key concepts like the association between the gods and the sky, the ultimate descent of the gods from the union of the sky-god and the earth-goddess, and a set of 'divine twins' who were the sons of the sky-god and act as heroes and rescuers. However, Italic (and so Roman) religion developed distinctive aspects with no parallels in the Greek world. These include the belief in the *lares* and *penates*, small, unnamed gods of hearth and home which protected both people's homes and the state as a whole. They also included the practice of augury (reading the future from the flight of birds), and various beliefs about the spirits of the dead, such as the benign *manes* and the restless, malignant *lemures*. Though they've been somewhat neglected in the scholarship until recently, these played a huge part in the day-to-day religious life of the Roman world.

The Romans engaged in a practice called *interpretatio Romana*, where they 'interpreted' others' gods as reflections or misunderstandings of their own, and so worshipped them either as alternate names or as epithets of their own gods. Hence we see dedications made at ancient Celtic shrines of Sulis to 'Sulis Minerva', as the Romans 'interpreted' Sulis as their goddess Minerva. It's an interesting question to think about what was going through the heads of people worshipping at that site - are they just worshipping their old gods, and paying lip service to the Roman names? Or is there something fundamentally significant about doing even that in a Roman temple, in Latin, surrounded by Roman statues? *Interpretatio* is certainly an act of imperialism - by 'claiming' gods and situating them within the Roman pantheon, it turns their worship into an act of integration into Roman culture, and reduces any opportunity to use those gods as a locus of non-Roman identity, or as a focus of resistance to Rome.

It's really in the Late Republic and the Augustan period that we see a huge boom of Roman texts talking about the gods, and most of these have a vested ideological and/or literary interest in playing up the similarities between the two. In most cases, they come from writers trying to establish a place for Rome within the literary and cultural world created by the Greek classics, particularly the works of Homer, and to situate Rome's foundation and identity within the heritage of Greek mythology. Virgil's *Aeneid* is the great example - [I've written another post here about how that poem engaged with a complicated oral tradition](https://www.reddit.com/r/AskHistorians/comments/hlzwdv/did_the_aeneid_exist_in_some_form_prior_to/fx38ayh/), and how Virgil invariably made choices that emphasised the glory of Rome's foundation but also which constantly worked to create links between the Greek legends of the Trojan War and the Italic myths of Rome's foundation, even when that didn't make the most logical or narrative sense.",0
"  The short version is no, they didn’t really, at least not during the Middle Ages (I can’t vouch for earlier, not my area). The medieval battlefield wasn’t particularly well suited to long ranged precision archery – once everyone was in a melee there wasn’t much opportunity to engage in specific targeted shooting without a huge risk of friendly fire. Archers in fact would often be armed with melee weapons of their own and might join in the fight once things got particularly intimate, leaving their shooting to either covering the advance of the army, forcing a repositioning or disadvantageous attack by enemy forces, or covering a retreating force (although retreating in good order wasn’t particularly common in the Middle Ages so this was probably the rarest of all). In essence they were more of a support weapon than one suited to specific deadly killing – despite what some in the longbow-fandom might have you believe. 

As for assassinations, that raises a more interesting question. I’m going to limit my answer a bit by focusing on the crossbow because it’s arguably the weapon better suited to this task. The crossbow was generally more accurate, much easier to aim, and had the advantage of being able to be loaded held ready while the archer lined up the perfect shot (in theory anyway). The crossbow was also used in a few famous assassinations and assassination attempts, a few of which I’m going to discuss below. 

Probably the most famous person ever to be killed by a crossbow is King Richard I of England. On the twenty-fifth of March 1199, King Richard I of England decided to patrol the walls of the Château de Châlus-Chabrol. He had been besieging the castle for some time and he may have decided that evening to inspect the progress of his sappers in their attempts to breach the castle’s defences, we can’t know for certain. Whatever his reason he very quickly would come to regret his decision as that evening he was shot by a crossbow wielded by one of the castle’s defenders. Richard was transported back to his private tent, where the crossbow bolt was removed a process which, thanks to the dubious qualities of medieval medicine, badly mangled his arm. The wound soon became gangrenous. While the king lay dying in his camp his forces successfully completed the siege and put the defenders to the sword. Richard died on the sixth of April, just over a week after he had been initially wounded. His heart and entrails were buried in the castle’s chapel and his body was transported Fontevraud Abbey in Anjou, where his father had been buried almost a decade earlier. 

While King Richard has left us nothing in terms of a description of how he came to be mortally wounded the same cannot be said for contemporary historians. There are numerous accounts of the death of King Richard I – after all a king dying in a battle or siege was a fairly rare event even in the Middle Ages– from which we should be able to reconstruct the events of his death in greater detail. Unfortunately for historians, these accounts often disagree on major details and are often filled with errors – making constructing a single coherent narrative of the death of the king a difficult task. There are many interesting things to be learned from these accounts, though, and it is worth spending a time here exploring a few of the more interesting and informative ones. 

Remarking on his death, the French chronicler William le Breton found a certain poetic irony to his fatal wounding by the crossbow, as he accused the vile Richard of having introduced this most sinful weapon to European warfare in the first place, making it only fitting that it be the tool to relieve Europe of his presence. This theory for the origins of the crossbow is patently false. The crossbow, along with the bow, had been banned in inter-Christian warfare at the Second Lateran Council in 1139, nearly twenty years before Richard was born. The ban obviously was not very effective, Richard’s death is a pretty clear testimony to that, but the Lateran ban adds further confusion to the French chronicler’s statement. It seems unlikely that he would be entirely ignorant of the decrees Council of a major papal council. 

It is also interesting to note that this anecdote about Richard introducing the crossbow appears in only one of William le Breton’s two accounts of Richard I’s death. In William’s *Gesta Philippi,* a prose chronicle about the life of King Philip II, he nearly exactly copies the version of events presented in the work of Rigord, another French chronicler who was writing a few decades before William. Rigord’s account is not particularly in-depth and describes how Richard besieged the castle because he desired a recently discovered treasure – a treasure which Rigord describes as a golden figure of a Roman emperor – before he was shot and killed by an unknown crossbowmen.  

It is in his *Philippidos* where William really lets his imagination run wild and the death of Richard I is a 200 line literary set piece that closes out Book V of this panegyric written to praise King Philip II of France in celebration of his victory at Bouvines in 1214. The relevant passage mentioned above comes as part of a 31-line speech delivered by one of the three Fates who has decided that while her sisters are still weaving Richard’s life she feels it must end. She guides Lord Archard of Chalus – the lord of the castle and person who we are told found the treasure in the first place – to discover a hidden crossbow bolt because: ""This is how I want Richard to die, for it was he who first introduced the crossbow into France. Now let him suffer the fate he dealt out to others."" This speech must be seen within the broader context of the work – the *Philippidos* is a work meant to praise Richard’s long-time rival Philip II and as part of that work it frequently and vehemently condemns the English king in no uncertain terms. We are told that Richard I was killed because of his greed in demanding the treasure for himself despite no claim to it, and that he had no respect for God, broke treaties, and violated holy days. No crime is beneath Richard in this work, and so the suggestion that he was responsible for introducing the crossbow is just another exaggerated crime of the English king. 

The English chronicler Roger of Howden wrote what is probably the most famous account of Richard’s death and his confrontation with his killer. Roger of Howden was an English chronicler probably best known for accompanying Richard on the Third Crusade and providing a detailed account of the expedition. Roger tells us that Richard was outside Chalus Castle preparing for the imminent assault when he was shot by the crossbow, and upon being shot Richard rode back to camp and told the captain of his mercenaries to begin the assault without him. Roger tells us that Richard was shot by a man named Bertrannus de Gurdon and that when Richard learned that he would not survive he had Gurdon called before him – the castle having fallen by this stage and its defenders captured. We are told that Richard asked him: ""What wrong have I done to you that you should kill me?"" To which Gurdon responded: ""You killed my father and my two brothers and you wished to kill me. Take what vengeance you like. So long as you die I shall willingly suffer any torments you may devise."" Roger says that Richard forgave Gurdon and ordered him be released, but upon the king’s death the captain of his mercenaries, a man named Mercadier, had Gurdon captured and flayed alive. This narrative is probably the closest we come to having a clear assassin who was determined to specifically kill King Richard.

The historian John Gillingham has suggested there are reasons to doubt Roger’s account of events, however. While Roger has generally been regarded as an impartial and reliable source, Gillingham draws a distinction between what Roger was writing in the 1170s and 1180s from his work in the 1190s. While in his younger years Roger had been intimately involved in Anglo-French politics, by the late 1190s he had retired to Howden in Yorkshire and seems to have primarily concerned himself with regional matters in and around northern England. As such, he probably is not a particularly informed source about events in central France during this period. So while we know Roger was writing very close to the event – he only outlived Richard by a few years – Gillingham suggests there is reason to interpret the Bertrannus de Gurdon story as myth.",0
"You offer a satisfying middle ground between the other two answers given in this thread. Taking each into consideration, it seems to me that the North would win at least 8 times out of 10 in any given timeline.",0
"So, this is quite an interesting question actually.  I'm not sure I can tell you a precise answer about the Aztecs, but I can tell you about Spanish attitudes regarding Native Americans in general.  It is generally accepted that modern ideas about race had not really existed for the first century or so of colonization.  What did exist was a much more fluid understanding of races and peoples, usually based on religion and culture.  

To give you some idea of what I mean, we first have to grasp some understanding of the ways that early European settlers understood the New World in general.  The two most popular ideas were that the New World was an Edenic realm where the people had not been touched by original sin, while the other, not so coincidentally, was a land where Satan had total sway over the people.  Both of these views were religious at their core, but were polar opposites ideologically.  The important thing, however, is that in neither views were the natives *fundamentally* unchangeable.  A sinner can be saved and a Saint can fall, if you see what I mean.

Second, the Spaniards in particular and the Europeans in general did not see their own races as necessarily fixed.  The ideas of the humors was current at the time with the idea that you had to balance the four humors to be healthy.  This idea was complicated by the fact that outside factors, including environment, effected the necessary balance.  So, the upshot is that many people thought that if you lived in a tropical place, such as most of Latin America and the Caribbean, you could literally turn into a person who looked like a native.  This did not even have to happen over generations.  The first Spanish settlers into the Caribbean imported, at great expense, foods that they would have eaten back in Spain, most notably grain which did not grow well in the new colonies.  At the same time, they tried to limit the amount of corn (maize) that they ate since it would turn them into Natives.  The flip side is that if Native Americans ate European food they could become more European.
 The question of Native wet nurses for Spanish children was also critical since the nurse could transfer her humors to the child.  An interesting later corollary occurred in the British colonies as late as the eighteenth century where many people believed that having sex with an African person (Usually a white woman having sex with a black man.  Let's play spot the sexism) would result in the white person literally darkening.

A related issue, which the Colonists explicitly and frequently invoked to justify colonization was the classic ""good use of the land"" argument.  Most of Europe was heavily agricultural.  They saw the lack of cultivation of crops that they liked, i.e. wheat, grapes, etc. as proof that the Natives were inferior to the Europeans.  There were some attempts to ""Europeanize"" the Natives by giving them European clothes and teaching them how to cultivate the land like Europeans, but these efforts were often undercut by Imperial legislation, most particularly sumptuary laws.  The efforts to make the Natives European also raised a tricky practical question.  If these guys become like us, what is the justification for ruling them?

A final factor, also religious, is that slavery in the 1500s was primarily based on religion and war.  Christians were not supposed to enslave other Christians, but Muslims, and frequently prisoners of war, were ok.  Of course, it did not take long for the Spaniards to effectively enslave many Native Americans on the well known *encomienda* and in the Spanish mines that dotted Latin America.  However, the Natives died off really fast and many of them had converted to Christianity as well, leading to both practical and theological problems with the arrangement.  Bartolemeo de las Casas famously defended the Native Americans and attacked their treatment before the King, though he also famously suggested African slaves to do the labor instead of Natives.

Racial attitudes changed drastically during the seventeenth century, when more permanent notions concerning race began to develop.  By that time so many Natives had died that they posed little to no threat and the importation of African slaves began to take off in a big way.  

The upshot of all this is that the inferiority of the Natives was heavily weighted toward culture and religion, rather than a modern understanding of race.  However, the exact degree of inferiority is a question in and of itself.  Not surprisingly, opinions differed greatly.  But, as the empire continued to expand and generate more profit, the Spanish (and later the other European empires) came up with other ideas to justify their rule, increasingly relying on race rather than religion.

Hope this helps answer your question!

Further reading: Earle, *The Body of the Conquistador* (probably the most applicable to your question), Block, *Ordinary Lives in the Early Caribbean*, De la Fuente, *Havana and the Atlantic in the Sixteenth Century*, Carmagnani, *The Other West* (This one is more a general history of Latin America, but it's good.)",0
"Why? Nobody knows. In fact we’re not even certain that is what happened. What was the impact? Huge, but also debated. Let’s get into it.

First, if you [search here on AskHistorians](https://www.reddit.com/r/AskHistorians/search/?q=dipendra&restrict_sr=1&sr_nsfw=), you’ll see the question often comes up but doesn’t get any responses. My understanding is that it’s been quite  difficult to do real scholarship on this event. The royal family at the time and for a lot of Nepal’s history has been extremely guarded and insular. Popular culture among the people of Nepal tended to regard the King and the royals as untouchable, god-like figures. Access to the family was limited;  palace communications were tightly controlled. The media didn’t even know about the massacre as quickly as other governments did, which says something about the people working at the palace. We don’t really know what happened, other than reporting by a commission of two (!) people, the Chief Justice of Supreme Court and the Speaker of the House. They did interview some eyewitnesses and provide the story from the OPs post. But there was no forensic work done, it was only a few weeks long, and they rejected an offer from Scotland Yard to help.

In a word, the suicide story is sus.

If this is what happened, the question is _why_. Prince Dipendra was known to be a frequent user of alcohol and various other drugs, which some blamed. There is also a theory that he wanted to marry another Nepali woman (from the Rana dynasty) who he had met in the UK, but was rebuffed by his own family. Perhaps this was because of the woman’s family ties to India, or political disagreements between the families, or something as mundane as a wealth disparity. None of these seem particularly compelling though.

Also, the wounds from the alleged suicide don’t match up with the handedness of the Prince. Sure, the royals were known for carrying around guns all the time but there were also armed guards everywhereZ And the royal family member who eventually became king was not present. This was Prince Gyanendra.

And then there’s the politics. Dipendra went to Eton. His (murdered) father was educated in Japan. Both believed in a constitutional monarchy, with strong democratic ideals and with people in power. Gyanendra thought the country was falling apart and it was time to return to a traditional autocracy. Three days after the shooting, Gyanendra was king.

So what happened next? To answer that, we have to talk about Nepal in 2001.",0
"The thing is that the definite 'Seven Wonders of the World' - the Giza Pyramids, Statue of Zeus at Olympia, Temple of Artemis at Ephesus, Masouleum of Halicarnassus, Colossus of Rhodes and Lighthouse of Alexandria - is a relatively late tradition. I can't remember just now without checking when exactly does the tradition get fixed, but there were multiple different competing strands of ""seven wonders"" or more in Antiquity that included, for example, the Great Walls of Babylon or Palace of Cyrus in Persia. I guess, in general, the *thaumata* or wonders that the ancients so loved listing and describing were amazing architectural feats that pushed the limits of the mortal human experience and capacity, and challenged Nature. Thus a wonder was creating something eternal and so big that it challenged natural mountains, such as the Pyramids, or something so impossibly beautiful and skilled that it compared to the creations of Gods, such as the statue of Zeus. The Hanging Gardens, at least in the form it took in the Greco-Roman literary imagination, obviously was a great example of how human genius could beat Nature and achieve the impossible; manipulating water in order to create a wonderful, lush garden in the middle of dry plains where nothing should grow, and then challenging gravity by terracing the garden on multiple rising levels. The Greeks also loved everything to do with mechanical gimmicky so the complex watering system in itself was a marvel.

Trying to describe (or understand) ancient water engineering goes well out of my scope, but Dalley's book has a whole chapter on how the watering system might have worked. [Philo's description](http://www.plinia.net/wonders/gardens/hg4philo.html) is the most elaborate; ""Streams of water emerging from elevated sources flow partly in a straight line down sloping channels, and are partly forced upwards through bends and spirals to gush out higher up, being impelled through the twists of these devices by mechanical forces"". So, Dalley thinks that the gardens were partly watered with aqueducts coming from the mountains, partly by [screwpumps](https://en.wikipedia.org/wiki/Archimedes%27_screw) lifting water from the Tigris river. ",0
"The scope of this question runs from 1866 with the defeat of the Austrians in the Austro-Prussian war of the same year through to the split of Germany in 1945 after Hitler's defeat.

As a side note before I begin, the 'little' German nations, such as Hanover, Saxony, and Bavaria were not inconsiderable nations in their own rights. That Prussia, through the will of Bismark, was able to unify them under one flag was due in part to a nationalist zeal ignited in the post-Napoleonic period, and also in part to the declining influence of Austria/Austria-Hungary in the corresponding period. There was a desire by the rulers of the smaller constituent parts of the Germanies to avoid outright conquest by Prussia, as well as a desire to enact social reforms without a revolutionary bloodbath that had swept up France in 1792. The 1830 and 1848 revolutions stoked German liberalism and nationalism, and it took all of Bismark's political acumen to convince the disparate German nation states that the Prussian constitution and military might were worth binding under, vis-a-vie Austrian hegemony (which is why the Prussians fought the Austrian-Prussian war).  

Prussian desire for hegemony arose after the 1815 Vienna settlement in part because Prussia feared Austrian, French, and Russian encirclement. The treaty had two outcomes that would shape the rest of 19th century German history: 1) The establishment of the German Confederation, made up of 39 independent states, and 2) the treaty had ceded territory on the Rhine to Prussia. This had put Prussia under considerable internal strain, as geographically, culturally, and politically the west of the new nation was far removed from the Berlin Prussian monarchy. Napoleon's sparking of nascent German nationalism during his conquest of the Rhineland was fanned by the liberalisation of Europe during the period 1820-1848, which the Austrians under Metternich attempted to suppress. Prussia was kept in check by Austrian military and political supremacy  during this period, but as Austria lost political prestige Prussia began to reassert herself, especially when Otto von Bismark became Prussian Chancellor in 1862. Prussia, with the suport and aid of Austria, fought the 1864 Second Schleswig war against Denmark, which gave them control Schleswig-Holstein. Prussia then went to war with the German Confederation, led by Austria, in 1866, with led to the defeat of the Confederation and then to it dissolving as a political entity. in 1867 the Prussians unified the northern german states into the North German Confederation, which had a federal constitution with Prussian monarch as the head of state.

The German empire was formally declared at the Palace of Versailles on 18th January 1871, at which point the independent principalities and kingdoms of the North German Confederation, led by Prussia, merged with the south German kingdoms and principalities. At this point the various independent entities that had formally been part of the Germanies became states within the new German Reich, with their own laws, customs, and state legislatures. Central authority in the empire rested with the Emperor and the Reichtag, though the democracy that Bismark, the Prussian prime minister, envisaged was of a limited form, with ultimate power resting in the hands of the German Kaiser, who was also the King of Prussia.

Initially the 27 constituent states that formed the newly unified Germany were left to manage their own internal affairs without too much interference from Berlin, as Bismark did not wish to empower the Reichtag with too much power. Over the course of the period between 1871 and 1900 the German state centralised federal control in the hands of the civil service through the State Secretaries, with the effect that even though there was near-universal suffrage in Germany for most adult males, the power of their elected representatives was muted when compared to the bureaucrats who controlled the affairs of state at the behest of the Kaiser.

The effect for the 27 state rulers was that their pre-1871 powers were constrained by the 1871 German constitution. All states sent deputies to the the upper house called the Bundesrat, and although all states were nominally equal in the eyes of the law, as Prussia was the largest state in terms of both population and size they had 17 of the 58 seats, which meant they had little difficulty in obtaining the majority they needed to pass favourable legislation.

One key area the 27 states maintained their own control was the armed forces, so that the larger states, such as Prussia and Saxony, had their own armies, which in turn was modelled on the Prussian army. This had the consequence that during WW1 many regiments were led, or had a titular head, by members of the royal families from the various states.

In terms of the pre-1871 monarchs, Saxony's monarch Fredrick Augustus III abdicated in 1918, turning Saxony into a free state in the Wiemar republic; the same happened with Bavaria, and by the declaration of the armistice on 11th November 1918 all the monarchs who remained in power after 1871 had been removed from power in favour of republican ideals. The reasons for the loss of power are varied, ranging from a general disdain for support of Prussian policies during WWI (in the case of Bavaria and Saxony), to a ground-swell of desire to reform the country after the catastrophic defeat caused by the allies. The monarchical principals of power were discredited in the eyes of the German public by the actions of the ruling elite during the war, even to the point where the Bavarian's sought a separate peace with the allies.

Dues to this, many royal titles were abrogated in 1918, with all temporal power being transferred into the hands the people. The Wiemar government did not removed smaller titles, though, and those landowners, including the Prussian Junker class, were still classed as nobility through the Wiemar and Nazi governments. The fall of Germany in 1945 resulted in a complete overhauling of the governance of Germany in the East and West, with the nobility loosing all their privileges in the process. It is also worth remembering that many of the Prussian noble houses were absorbed into present day Poland and the Baltic states, were their estates were seized by the Communists and turned over to state control.

So, in summation to your question:
Most of the German monarchs, Dukes, and princelings of the pre-1871 era were allowed to remain as heads of state of their respective German state (much like a US governor). Due to the actions of the various monarchies during WW1 when the German Republic was declared in 1918 the states transitioned from monarchies and hereditary rulers to directly elected state legislatures, and the former monarchs either went into exile or were settled into civilian life.

If you would like to read further, please check out the books below.

Source material: 

'The Pursuit of Power: Europe 1815-1914' by Richard Evans

'The Army of the German Empire 1870-1888' by Albert Seaton

'Contesting the German Empire 1871 – 1918' by Matthew Jefferies

*Edits tidying up grammar, punctuation, and readability",0
"While I certainly hope you were being glib with your Oliver Stone comment, the short answer to your question is: ""Not much."" There certainly was no information that changed the official narrative of the assassination; though there were a lot of intriguing new bits of information about the case, and semi-unrelated cases.

The Washington Post did an absolutely terrific breakdown of the most notable revelations here:

https://www.washingtonpost.com/graphics/2017/national/jfk-assassination-files/?utm_term=.2937d722401c

Probably the most notable, at least from my perspective, were documents detailing some of the various CIA plans and plots to kill Castro. This, of course, has long been known, but this particular document (the first one listed in the Washington Post piece) is an official 1975 internal examination of all the efforts and fleshes out the various plots in much greater detail than was ever known before.

The 82 page memo refers to another, unpublished 1967 memo by J. Edgar Hoover with the absolutely wonderfully title of ""Central Intelligence Agency's Intentions to Send Hoodlums to Cuba to Assassinate Castro."" This, of course, speaks to the intra-agency conflicts between the FBI and the CIA on matters related to Cuba.

But your question is ""is there anything in these files that provides any type of indication of a conspiracy in the JFK assasination?"" and the answer is, ""No."" The whole release dominated the news cycle for what, about a day? The level of revelations--dominated by interesting but not particularly revelatory minutiae--seems to indicate that this was about the appropriate level of general interest.

Keep in mind that at the very last moment, Donald Trump elected to withhold numerous documents for further vetting. So until those specific documents are released, we are looking at a subset of all of the information still contained in the archives.",0
"Well, the answer is unfortunately not that satisfying, but I expect that you came into this question realizing it was a somewhat vain hope given the use of ""disappeared from history"". To provide a little background, Sally Hemmings had a number of children, some of whom died in infancy, which are generally accepted as being fathered by Thomas Jefferson (I would direct to [here](https://www.reddit.com/r/AskHistorians/comments/6aqa2e/panel_ama_slaves_and_slavers/dhj0rqm/?context=2) for more discussion, broadly, of the sexual relationships between masters and slaves in the antebellum South). Whether or not this was fact has been debated since before Jefferson himself died, and scholarly opinion has swayed about, but it is pretty much now the consensus.

Anyways though. Harriet, described as ""nearly as white as anybody and very beautiful"" had no trouble passing for white, and as you already are aware, made use of this. She was freed in 1822 at the age of 21, apparently on a promise Jefferson had made to Sally Hemings to do so when the children reached that age. The documentation of Harriet's liberation is almost next to nothing. Edmund Bacon, who worked as an overseer at Monticello, described her departure, being the one who gave her the $50 that you mention, and stating she had been headed for Philadelphia. And as for her life after she headed north, we only have one account which we can give any real credence to, that from her brother Madison, who wrote in 1873:

>Harriet married a white man in good standing in Washington City, whose name I could give, but will not, for prudential reason. She raised a family of children, and so far as I know they were never suspected of being tainted with African blood in the community where she lived or lives. I have not heard from her for ten years, and do not know whether she is dead or alive. She thought it to her interest, on going to Washington, to assume the role of a white woman, and but her dress and conduct as such I am not aware that her identity as Harriet Hemings of Monticello has ever been discovered. 

Washington City in this case refers to the District of Columbia as it was then known, and it would appear that while she kept a low profile, she did not choose to cut all ties with her family, at least immediately, as Madison claims to have remained in contact with her through the 1860s (he was writing in 1873). He also gives us reason to believe it quite possible that she has living descendants, but of course was much to guarded to allow any information to get out which could give much of a thread to follow. What the end of communications even meant is up in the air - perhaps she decided to cut her final tie to non-white society, or perhaps she simply died. We can only speculate. But in any case, Madison's account is the lone source we can rely on to reconstruct any sense of her post-emancipation life.

She was not the only child of Jefferson and Hemmings to follow such a route. While Madison and Eston both left records, writings, and known descendants, their brother Beverley was similarly allowed to leave Monticello for the North, and even less seems sure about his fate then his sister, doing a similar disappearing act but without, it seems, even the record of correspondence that Harriet left He was briefly in contact, long enough to communicate back that he had married a white woman and that they had a daughter, living in Washington City, but that seems to be the end of it. In both the case of Beverley and Harriet, it should be noted, Jefferson *officially* recorded them as being escaped slaves, as their departure was recorded in the 'farm book', but it was quite clearly allowed with his approval as a means of sending them North in technical compliance with the aforementioned agreement he had made with Sally. The backhanded means of liberation is thought to have been a means of following through without providing ammunition for those looking to prove the parentage.

Now, as to the second part of your question, you might perhaps want to x-post to /r/AskScience as it is less a history question that one for a geneticist. What I do understand of these things would imply that it is possible to find genetic matches that show relations by various degrees, but it isn't like those genes are signed ""T.J."" Establishing who that common ancestor is takes a lot more leg work. To compare to the famous claim about Genghis Khan's widespread DNA, this is based on finding the the same Y-chromosome (which records patrilineal descent) in millions of people that shows they share a common male ancestor a certain period back in time. Genghis Khan is then assumed to be the one based on historical circumstances. But I would hesitate to say more, as again, this is getting into the territory for a scientist to discuss.

Cogliano, Francis D., ed. *Companion to Thomas Jefferson*. Wiley. 2011

Gordon-Reed, Annette. *The Hemingses of Monticello: An American Family* W. W. Norton & Company, 2008

Ishida, Yoriko. *Modern and Postmodern Narratives of Race, Gender, and Identity: The Descendants of Thomas Jefferson and Sally Hemings*. Peter Lang, 2010
",0
"The main point of contradiction comes from the fact that there are many examples of male pubic hair being sculpted, not just painted. Whether or not the Aphrodite of Knidos ever had painted-on pubic hair we will sadly never know.",0
"Quick preface:  I see uncovered-history wrote a response while I was typing up mine.  I think they complement each other nicely.  

While there are certainly still major disagreements about the American Revolution, I cannot think of any that break down primarily along those geographic/national lines.  The context in which we live and our backgrounds do shape the kinds of questions we ask about history, though especially in our globalized world today, I don't see any areas in which we are talking past one another.  

I do think the differences in our approaches used to be more distinctive.  In the mid-twentieth century, the major schools explaining the origins of the American Revolution were generally aligned with one's political sentiments.  A conservative approach emerged after WWII aligning with Cold War-era thinking.  Historians (for example, Richard Hofstader) argued that the Revolution was fundamentally not as a radical overthrow of the social order but instead an attempt by Americans to preserve their rights and liberties.  They emphasize the democratic, literate, informed nature of Revolutionary-era Americans. 

Historians in the neo-whig school disagreed and argued that the American Revolution was fundamentally a political and social upheaval generated by political ideas. The paradigmatic work in this vein would be Bernard Bailyn's *The Ideological Origins of the American Revolution.*  Other historians in this vein include Pauline Maier (a Bailyn student) and Gordon Wood (though I personally think Wood is a bit more liberal and writes a bit more toward consensus than the other historians in this school).  

The neo-Whig approach, which focused on the political and intellectual worlds of middling and upper-class people, was challenged as the academy itself expanded and diversified in the 1970s.  Historians on the left have emphasized the role of ordinary people and devoted more attention to aspects of race, class, and gender.  These historians focus on economic and social explanations for the Revolution and often regard ideological or political rhetoric of the period as hollow.  Instead of seeing the Revolution as a contest for independence (home rule), they see it as a question of power on the ground in America (who would rule at home).  Historians in this vein include people like Jesse Lemisch, Gary Nash, Woody Holton, and Tim Breen. 

While Britain had some parallel experiences in the twentieth century, the above examples are of American scholars writing in the context of American debates and issues.  (By contrast, during the mid-twentieth century, British attention on the period surrounding the American Revolution was centered around the work of Lewis Namier, a Polish-Jewish emigrant to Britain.  His approach was heavily influenced by the context of the world wars, and in particular, he saw the British state as one that would never fall to absolutism.  He focused on the personal biographies of individual politicians, showing how alliances shifted and how the party system was based on personal animosities, loyalties, and local issues, rather than any larger ideological positioning.)

Constitutional explanations for the Revolution seem to have maintained more popularity among historians in the U.K. I don't think it's unfair for us to say that British scholars probably inherently care more about the British constitution and probably understood it better, especially in the days when early Americanists were trained solely in colonial American history. The constitutional approach was in fact an old way of explaining the conflict. (The work of Charles McIlwain in the 1920s is probably the best-known example).  However, Harry Dickinson, an English historian working in Edinburgh, has been explaining the constitutional origins of the crisis since the 1970s.  More recently though, prominent U.S. historians (including Jack Greene, Patrick Griffin, and Brendan McConville) have picked up on this line of thought. 

One of the current trends in scholarship about the American Revolution is placing the conflict within the imperial context.  Scholars on/from both sides of the Atlantic are doing this work (e.g. Stephen Conway, Andrew O'Shaughnessy,  Eric Nelson, Justin Du Rivage).  Advances in technology, I would argue, both in ease of travel to archives and digitization of records, have made this approach more doable in recent years. 

I would argue, perhaps, that perhaps one way the American and British views of the war had differed was in valuing the competence of British military strategy during the conflict. The book *The Men who Lost America* by Andrew O'Shaughnessy (a British-born scholar who's now based in the U.S.) demonstrated that we have drastically underestimated the skill of British politicians and officers in addressing the complexities of the war.  I mention it as an example of how our backgrounds do influence what we see and how we approach this very well-trodden historical ground.  For instance, scholarship on the place of Native people in the Revolution has been led almost entirely by scholars from the U.S. (e.g. Peter Silver, Greg Dowd), though historians working on the American Revolution in the U.K. would not deny the importance of that work.  Ultimately, we are all reading one another's work, as well working, traveling, and conversing across the Atlantic. 

(Edit for a few typos.)",0
"/u/UrAccountabilibuddy gave an excellent overview of general pedagogical trends in high school English education, but I thought I could add some information about why those books in particular are so popular.

First, I want to say that you're right to think that books written in the 50s are particularly common on high school reading lists in the US. It's an informal source, but the Goodreads poll of [Required reading in High School](https://www.goodreads.com/list/show/478.Required_Reading_in_High_School) reveals that most commonly read books were written in the 20th century, with peak publishing years in the late 40s to early 60s. Here's a breakdown of how many books in the top 50 canon were published each decade:

 Decade | Top 50 Books in Canon Published
:--|:--
1900s|0
1910s|2
1920s|2
1930s|3
1940s|5
1950s|7
1960s|4
1970s|0
1980s|0
1990s|0

The canon was largely formed in the 1960s-1980s, and has remained remarkably consistent in the last 30 years, aside from a stronger focus on works of literature by female and minority authors. A [report from 1988](https://eric.ed.gov/?id=ED309453) that found the most common taught works of literature includes many that would be recognizable to most high school students today, including post-war books like *To Kill A Mockingbird, 1984, Lord of The Flies, Diary of a Young Girl, The Crucible*, and others. In contrast, a [similar study in 1963](https://scholar.google.com/scholar?cites=5921842201840202229&as_sdt=5,24&sciodt=0,24&hl=en) found that only one of the top ten most read books was published in the 20th century. Furthermore, the canon stabilized in that same time period- the number of books read by at least 30% of classrooms tripled between 1963 and 1988.

So why were books written in the 1950s so much more likely to have made it into the canon? Partly this has to do with the canon stabilizing in the years after they were written- books that were popular in that time period would have been on the minds of educators, and many of the books in the canon were bestsellers. Partly this has to do with, as /u/UrAccountabilibuddy mentioned, a transition to books that were relevant to the struggles and experiences of adolescents, and a subsequent increase in publishing of said books. According to a [1974 study](https://files.eric.ed.gov/fulltext/ED097703.pdf):

>""The forties saw the development of a new literary, genre with its own authors and highly specialized audience of ""adolescents"" as defined by the ""life adjustment"" educators""

Books said to be part of that genre include *Anne Frank: The Diary of a Young Girl* and *The Catcher In The Rye*, both of which are still popular in high school curricula today. Finally, the cultural ascension of Baby Boomers might have played a role in preserving certain books in the canon out of nostalgia or familiarity, paralleling the domination of Christmas music released in the 1950s on the year-end music charts.

(This is not history, but it may be that we're in a time of another large shift in the US high school English canon, away from 1950s books. The Common Core [List of Exemplary Texts](http://www.corestandards.org/assets/Appendix_B.pdf) doesn't include any of the books you mentioned except for Fahrenheit 451. Instead, it includes a much greater number of works of literature by women and minorities, and a much greater emphasis on non-fiction texts, which it says should make up 70% of a year 12 curriculum. [One paper](https://www.ncte.org/library/NCTEFiles/Resources/Journals/VM/0211-sep2013/VM0211Young.pdf) said that these ""text exemplars will become a new canon for
literacy instruction, a kind of national reading list.” )",0
"The short answer is that no one would have helped you rebuild, and nobody would have subsidized the repairs. You would have been largely on your own.

The unexploded ammunition in your collapsed building might have attracted the interest and attention of the occupying militaries. Unfortunately, methods for disposing of unexploded ammunition in Berlin immediately after the war are not well-documented. (Anyone who finds further information on this topic, please weigh in!) Most probably, the local occupying military would have coordinated removing this. Unexploded bombs remain an issue across German cities to this day, however, so it is possible no one would have come to help you at all. But ammunition clearing is the most likely reason that authorities would have played a role in clearing out your ruined building.

Once it was clear that the building wasn’t going to explode, however, you were almost certainly on your own. One of the most famous images of postwar Berlin—and indeed, of many German cities in the postwar period—is that of the “Trümmerfrau” or Rubble Women. These were women who went around gathering up the crumbled remains of buildings destroyed by bombs and in street fighting, and carted them away to begin a city’s rebuilding process. Although a heroic image of women stepping up to help rebuild the country persists up to today in Germany, it is not the whole story. Women were only the primary source of rubble clearance in the Soviet zone of Berlin. There, they were not volunteers—instead, they were forced to clear rubble away by the Soviet occupiers, who considered the civilian German population responsible for the existence of the war, alongside the German military. (Some unemployed men were also pressed into rubble clearance in this zone, but more women than men were available, leading to a gender imbalance.)

In the other three Allied zones (the British, French, and American), penal labour was used for rubble clearance. German prisoners of war and former Nazi Party members—both male and female—were forced to clear the streets in these parts of Berlin. This was seen as an “atonement measure.” This was a direct reversal of Germany’s policy for clearing up bombing-related rubble created during the war, when foreign POWs and people held in concentration camps were used to clear up cities after Allied air attacks. The other source of rubble removal was professional – companies and individuals who had cleared rubble during the war were also contracted by Allied military administration governments after the war, sometimes using their own equipment that the Allies had confiscated and then were loaning back to them. This method was first and foremost used to clear public spaces, such as major roads and intersections.

So, in the east of Berlin, women might have helped to clear away some of the piles of rubble that spilled onto the streets and sidewalks from your building, while in the west, prisoners and former Nazi Party members would have done this work. However, any further repairs on the building would have been up to you, the owner. As Jeffrey Diefendorf (author of *In the Wake of War: The Reconstruction of German Cities after World War II*) explained: “Individuals borrowed money, often from relatives in the country, and that is where they also found building materials. It was never centrally managed.” Unlike other countries that needed to rebuild, such as Japan and the Soviet Union, there was less centralization and planning, in large part because there was not a single central government running Germany. This meant that the centre was not issuing clear and timely directives or plans, and so people were left much more on their own.

The above description is true for anyone who did not have any affiliation with the Nazi Party. If, however, you were a party member – and particularly if you were a Nazi and were active in public life (i.e. a member of the SS, the police, or a civil servant), you might not have retained ownership of your building. One of the first things the occupying Allied powers did in Germany after the war ended was start a process known as “denazification.”  This involved arrests for many (400,000 Germans were placed in internment camps and underwent case-by-case reviews between 1945 and 1950.) Others who had been lower down in the system had to fill out detailed forms explaining what they had done during the Nazi period. One of the penalties for people identified through the denazification process was being stripped of one’s property.

I say “might” not have retained ownership because denazification was unevenly and incompletely applied. Not anywhere near all Nazi Party members were caught, those who were did not usually go through rigorous investigations, and the entire denazification process ended relatively quickly. In West Germany, many former Nazis were able to reintegrate into public life within a few years. Others made surface-level changes to their identities and avoided the less-than-rigorous system of detection. And still others managed to get out of losing their property by transferring the rights to a non-Nazi Party member right before their denazification investigations/tribunals began. It was a deeply imperfect process.

As long as you had not been a Nazi Party member active in public life, then, you probably would have retained control of your building, and repairs would have been your own responsibility. On the other hand, within the Soviet zone of Berlin, in addition to former Nazi Party members, people considered political enemies were also sometimes arrested and interned. Therefore, if your building was located in this zone, it is possible you might have been targeted as a capitalist landlord. However, there were lots of people who opposed Communism in that zone, and the owner of a single apartment building may not have been rich or powerful enough to attract the attention of Soviet authorities. (Unless you treated your tenants poorly, and one or more of them reported you to the new Soviet military government to punish you. In that case, watch out! They might have investigated you.)

For anyone surprised to learn that rebuilding was this ad-hoc and individual in postwar Germany, it is worth noting two things. First, the famous Marshall Plan designed to “rebuild” Europe was primarily focused on rebuilding entire economies—not individual pieces of personal property. It operated at a government level, and individual residents of countries did not get cheques or funds from it directly. It also did not come into effect until late 1948 or 1949. As you can imagine, many people had already decided what they were going to do with their damaged buildings and homes years before that.

Second, for anyone surprised that this individualistic, deal-with-it-yourself attitude was also in place in the Soviet Zone (the eastern portions of the city that would become part of East Germany), it may be even more surprising to learn that ownership of real estate (“personal” property as the Communists preferred to call private property) remained legal throughout the entire duration of East Germany, explicitly protected in multiple constitutions written for the country. This was not in keeping with the spirit or the ideology of Communism, necessarily, but the Soviet Union had entrenched property ownership and struggled to move away from it, and as a result pushed all other Communist countries within the Warsaw Pact to follow their lead (both because they believed it was effective in keeping the population invested in the national project of Communism and to not make them, the Soviets, look bad.)

So, TL;DR – no one was swooping in to help repair war-damaged private property in postwar Berlin.

Sources: “Denazification,” Alliierten Museum, last accessed 14 May 2020, [http://www.alliiertenmuseum.de/en/topics/denazification.html](http://www.alliiertenmuseum.de/en/topics/denazification.html).

“How Did Germany Rebuild After World War II,” The Aleppo Project, 15 October 2015, last accessed: 14 May 2020,[https://www.thealeppoproject.com/how-did-germany-rebuild-after-world-war-ii/](https://www.thealeppoproject.com/how-did-germany-rebuild-after-world-war-ii/)

Betts, Paul, “Private Property and Public Culture: A Forgotten Chapter of East European Communist Life,” *Histoire@Politique*, 1, no.7 (2009).

Diefendorf, Jeffrey, *In the Wake of War: The Reconstruction of German Cities after World War II*, (New York: Oxford University Press,1993).

Treber, Leonie, “The Big Cleanup: Men, Women, and the Clearance of the Rubble in Postwar East and West German,” *Gendering Post-1945 German History: Entanglements*, ed. Karen Hagemann, Donna Harsch, and Friederike Bruhofener.

Ziemke, Earl, *THE U.S. ARMY IN THE OCCUPATION OF GERMANY, 1944-1946*, (Washington D.C.: Center of Military History US Army, 1990).",0
"In short, yes. 

And no. 

Actually, it's complicated. 

First, the most basic question:  which *edition* of Frankenstein?  The 1818 version (which is more science-horror) or the 1831 (which is the one most people read, and is more religiously-minded)?  I think I remember this passage from the last time I read the 1818 version... but I'm not sure. 

Either way, you can approach the question of whether Europeans from the 1750s to 1850s expressed concern, emotions of regret or pain, or even outrage from a number of different directions. 

1)  The Literary Angle. 

Shelleys work is a work of early Romanticism.  The Romantics (which included Shelley's husband Percy) were all about using their literary craft to excite emotion or generate passionate feelings.  That's why, in Mary's *Frankenstein*, there's so much anguish:  Victor tearing at his breast, Victor falling down and weeping, Victor swept up in the powerful scenery of the mountains he's climbing, Victor recoiling in abject horror, etc. etc.   Coming out of the more cerebral Enlightenment, the Romantics were all about *feeling.*

And one of the ways to evoke this feeling was to tap into older tropes about the misery of human history--tropes of lament that go back before the early Christians, even, to the great literary figures of Classical Rome.  Tacitus' *Germania* was in some ways a pointed critique to Romans of their own political and moral corruption of course, but it was *also* a lament for these sturdy, simple German people, who were doomed to be conquered by Rome, and assimilated by them.  (as Rome had done to so many other peoples.)

(Christopher Krebs has a good book out on Tacitus--*A Most Dangerous Book* (2011)-- where he writes about the Germani as one of the first groups to be seen as ""noble savages""... who were doomed by the wheels of history to fall under the expanding yoke of mighty Rome.) 


Interestingly, though Bartolomeo de las Casas, who wrote passionately about the destruction of the Indians (by Spanish rule in Latin America) in the 1540s,  is often described in history survey classes as a pioneer humanitarian and even a cultural relativist--because he not only cared about the Indians, but wrote respectfully of their ways and tried to argue that their ways were something other than 'savagery'--he was actually on pretty solid and well-trodden ground *literarily*, as there had existed this 1,500 year-old genre of lament for the defeated, pity for the fallen, regret for the destruction of the noble-savage foe. 

In Shelley's *Frankenstein*, when the Creature reads, he/it reads a bunch of 18th century, Romanticism-tinged histories... including Volney's *Ruins of Empire* (a lost classic) written in the 1790s, describing the fall of civilizations of the past.  This late 18th century history in turn was drawing on 2,000 years of classical lament for the defeated, the disappearing, and the dying.  

Shelley was very well-read.  By including this wide range of literary and historical works in *Frankenstein*, she was of course flexing her intellect.  But she was also digesting these books for a more popular audience.  She's sort of a fusion of Carl Sagan and Stephen King... but I digress.

So, how many readers in (say) 1830s London, as they read about the monster in tears over the fate of the Native Americans, shared the feeling?   Difficult to say.   Certainly some.  It would not have felt ""strange"" to read the words, because it was an established literary motif, both for the Americas (from Las Casas onwards) and for all suffering ""noble savages"" everywhere.  (more on that below.)  


2)  The American angle.   

This is the one that I'm not qualified to really grapple with (because my American history is so weak).  Nonetheless, while the most recent trend in American historiography has been to emphasize how the eradication/removal of Native Americans amounted to a genocide (with genocidal policies but also genocidal *motivations*), an older generation of history emphasized the ""deep regret"" with which American political leaders, religious leaders, thinkers and writers, and even soldiers and journalists confronted or grappled with the way that European Colonials/the Early United States pioneers brutally treated the Native Americans.  From Henry David Thorough to (maybe?) Laura Engles Wilder.  But I don't know enough to say more on this however. 

Regardless, I doubt Mary Shelley would have been very tuned in to this discourse.  It's too early in the century. 

First off, she would not have been that up on American culture or events (though Wikipedia tells me she met Aaron Burr... I did not know that!)  
But she would not have been reading *American* books for sure... because no one was, since the United States was such a political, economic, and cultural backwater.   A cultural wasteland, if you will.  Not worth the time.  (and has this really changed....?   but I digress...  :-) 

Alexis de Tocqueville's influential book on America, which did regret the treatment of Native Americans (though I read this decades ago, so take with a grain of salt) wouldn't be published until the 1830s... well after the second edition of *Frankenstein.*  So Shelley couldn't have read it, and probably wasn't very aware of early (early 19th c. United States) disagreements on the cruelties of ""Indian Policy.""  


3)  The Imperialism angle.  

But even the literary tropes of regret, lament, and compassion for the suffering noble savage mentioned above could have various implications and various things ""feeding"" into it. 

In the escalation of imperialism that first began stirring (or rather, reviving) around 1815 and peaked in the 1880s & 1890s, there was a growing fascination with--even obsession with--the ""dying"" or ""vanishing"" races of primitives. One of the key books here is Patrick Brantlinger's *Dark Vanishings: Discourse on the Extinction of Primitive Races* (2003), which is worth a read (though I think he's a literary scholar, not a historian??).  

Either way, there was a lot of ink spilled by scientists, explorers, missionaries, politicians, poets, and journalists about how the ""primitive"" peoples were doomed to disappear after contact with ""civilized"" races. Brantlinger calls this ""extinction discourse""... which he sees as an offshoot of imperialist and racist discourses, in that--despite their tone of lament and regret (and in some cases call for efforts at ""preservation"")--presented this physical/cultural/historical eradication as inevitable... and thus became a sort of self-fulfilling prophecy.

Obviously, Charles Darwin is a key figure here.  In his section on “the extinction of races” in *The Descent of Man* (1871) Darwin writes, 

>When civilised nations come into contact with barbarians the struggle is short, except where a deadly climate gives its aid to the native race.

But Brantlinger's work shows how this theme--of primitive races being *doomed*--predates Darwin significantly.  (In fact, the more I learn about Darwin, who published the *Origin of Species* in 1859, the more I see how he really compiled long-standing ideas, observations, and tropes rather than originating them .. but I guess that is always the case in history, there is nothing new under the sun.)  

Anyway, while Darwin's work on ""extinctions"" of human groups in 1870s was a half-century after Shelley wrote *Frankenstein*, Brantlinger shows how much this idea was in circulation much earlier, even around 1800... which the erudite Shelley would have absorbed.

And, to repeat:  while this notion of the regrettable dislocation of, vanishing of, death of ""primitives"" (the new ""noble savage"") was often one that made people feel sympathetic or even sad, it could also be deployed as one more element in the ideological arsenal of empire:

""The primitive civilizations are doomed!  This means there is no reason not to annex this island/sell this rum/beat back this savagery with volley fire...!""

""No, sir, I disagree!   Yes, the primitive civilizations are doomed... but this means that we have the moral obligation to go in and protect them, to minimize the damage, to administer their transition to civilization, to make sure it is done correctly!""

""Good Sirs, I could not disagree with you both more!   Yes, the primitive civilizations are doomed... and so we must fund a Great Museum, in which we will place on display all of the objects and artefacts collected from the primitive peoples, so at least some elements of their primitive culture will be Preserved for All Time!""

All of these themes actually work equally well with imperial expansion. 

(CONTINUED IN COMMENTS)",0
"So strictly speaking, the premise here isn't entirely correct, as it depends greatly on when, how, and where we are talking. It is important to remember that, broadly, there were *two* forms of camps run by the Nazis: Extermination camps, such as the Reinhard Camps, where the vast majority of new arrivals were murdered within mere hours of their arrival; and Concentration Camps, where large numbers of people were held for reasons that often amounted to nothing more than existing (Hybrids also existed, most famously Auschwitz, but the distinction isn't too important in this vein). In the latter case, extermination was often still the long term goal, but not before every ounce of useful labor had been eked out of the imprisoned bodies as part of the massive system of slave labor employed by the Nazis.

In bring this up for two reasons. The first is because in the extermination camps, the murder persons brought there was done as a matter of course, in mere hours, and there was no real opportunity for disease to take root and spread, although it might be worth noting that the grotesque ritual of stripping, shaving, and showering that new arrivals were told they would go through was often presented to them as delousing. Second then, in the concentration camps, widespread disease was *not* ideal. The inmates of the concentration camps were a workforce. A workforce which were planned to literally work *to death*, but morbid as it is to discuss, it was a planned death from complete and utter exhaustion at the end of their strength, not one from disease that could strike down swathes of those who still had labor to steal. 

So this now brings us to the main point, namely that in a concentration camp, attempts were made to prevent outbreaks. Available medical care was rudimentary, limited mostly to prisoners in the camps who had medical training, and were provided with only the barest of supplies by their captors, but it was there. Delousing upon entry was routine, even if it was often ineffective. Rudimentary care existed, and sick persons could be put isolation wards, but it is also important to note that killing the sick as a not uncommon method of dealing with it. For instance, the first arrivals at Chelmno in late 1941 was a group of some 5,000 Roma who had been previously interned in a makeshift concentration camp near Łódź. After an outbreak of typhus saw some 600 or so individuals infected, it was decided the best way to deal with it would be to send all of them, infected or not, to the newly built extermination camp.

The prisoner-doctors in the camps would, when able, even hide or downplay those infected, as infections could easily result in out of hand execution, and even the isolation wards of the camps were little more than 'a place to put people to die'. But in any case, the core point here is that epidemics within the camp were something seen as undesirable - for pragmatic reasons rather than concern of welfare - but undesirable nevertheless.

*But*, as I noted at the beginning, you aren't entirely off the mark here, and there are some key exceptions. Most infamous, and possibly what is on your mind, is the situation in some camps near the end of the war, the best known being Bergen-Belsen, where tens of thousands died (including Anne Frank) in early 1945 due to several concurrent outbreaks of disease, best remembered being typhus. But it is important to emphasize that this was early 1945. Nazi Germany was falling apart, with the Allied powers closing in from both East and West. Bergen-Belsen was specifically being used as a collection point for the evacuation of the inmates of camps further to the east at that point, and the number contained there increased several fold over that it had been intended for, from 15,000 in late '44 to over 60,000 by April. The cramped over crowding, combined with the complete disregard for even the most basic welfare of the camp population by the guards - there was no running water or bathroom facilities of any kind in the main enclosure, allowed the quick spread of disease, and resulted in mass death. 

At *this point*, we can absolutely point to the situation as one created by the Nazi regime, and in part driven by the total disregard for those held within the camp. Belsen was not an extermination camp, but it was by that point not one where a productive workforce was being press-ganged, and their death by untreated disease was of no real concern. There is some irony, perhaps, in that near the end Himmler *did* order the epidemic to be combatted, in the vain belief he could use the camp inmates as a chip to deal with the Western Allies, but by then of course, things had spiraled far beyond control anyways.

The prime example though, and perhaps alternatively closest to what you have in mind, is not the situation in the camps, but the situation in the closed off ghettos to which the Nazi regime forced Jewish persons to congregate in in the East prior to the establishment of the extermination camps. Similar to the situation in Belsen, the ghettos generally saw these spaces quickly filled up far beyond capacity, all Jewish families of an area now concentrated into a space which before had likely housed only a fraction of them. The cramped space, the overtaxed sanitation systems, the rudimentary support provided by outside authorities all but ensured outbreaks of disease - which of course then only confirmed the racial pseudoscience of the Nazis which believed Jews were dirty and prone to disease. 

This had been part of the public justification for forced ghettoization in the first place, even, and perhaps comes closest to what you were envisioning about causing mass death by disease. A declaration in 1940 supporting the ghettoization, by the Generalgovernment's head health official, noted:

>The Jews are overwhelmingly the carriers and disseminators of infection. Spotted fever endures most persistently in the regions heavily populated by Jews with their low cultural level, their uncleanliness and infestation of lice unavoidably connected with this. [...] There are only two ways [to deal with them]. We sentence the Jews in the ghetto to death by hunger or we shoot them, even if in the end the result is the same, the latter is more intimidating, we have one and only one responsibility, that the German people are not infected and endangered by these parasites, for that any means must be right.

And while strictly speaking, ""let's put them all here so they die of disease"" wasn't in the absolute literal sense 'the plan', the Nazis were often quite happy with the results, Hans Michael Frank, Governor General of Poland, happily remarking in 1943 how the terrible conditions of the ghetto was helping to deal with the Jewish population 'naturally'.

A number of outbreaks occurred, although it should also be stressed that despite the dire circumstances they were forced to work in, the Jewish health professionals in the ghetto did their damndest to fight back, often with marked success, although often they had to fight not only the diseases themselves, but also the active interference of the Germans who prevented their receipt of essential tools. Doctors in the Warsaw ghetto, for instance, were denied anti-typhus serum deliveries, and even saw the Germans quite literally steal their medical supplies in the middle of an outbreak. While both the Lodz and Warsaw ghetto suffered notable, extensive, outbreaks, the Vilna ghetto developed a particularly effective system of containment, so it wasn't always a fight without victories, but even then, disease was never truly defeated.

This isn't to say German authorities had *no* concerns either. They might not have cared little for the humanity of the Jews in the ghetto, but generally they didn't desire large scale-outbreaks either. But unlike the Jewish doctors and nurses, they often could be brutal in their methods of containment, and often chose methods that for all their brutality were ineffective anyways, implementing harsh, but ineffective disinfection routines, and requiring complete, isolated quarantine of over 6 weeks for a house with an outbreak, despite 2 being quite adequate. 

Further more, as in the camps, typhus infections would at times be dealt with through straight up murder. In the Kovno ghetto, for instance, a supposed outbreak of typhus resulted in the burning of the hospital... with patients and medical personnel still inside. As Baumslag aptly sums it up, ""the German methods of fighting infectious diseases were well known and feared more than epidemics"" and the result of course was incentive to hide infections. Although it was well known that lice caused typhus, German solutions mostly focused on the infected persons, and did little to attack the origin of the outbreaks.

*Now*, I have written quite a lot here to address the premise of your question, but not really what you asked, but the two are quite related. The takeaway, I hope, at this point is that even if the Nazis were not effective about it, nor doing so out of concern for the *humanity* of the prisoners, they did make at least some attempts to contain disease in the camps, although at the same time it should certainly *not* be said that they particularly cared about their failures, especially seeing as large-scale killing was one 'solution' they pursued. And lest we consider for a moment more effective control wasn't possible, upon liberation of Bergen-Belsen, the British were able to report that there were no new typhus infections within two weeks of implementing their delousing program, although a number of prisoners, far to weakened and already infected, did pass away following their liberation.

1 / 2 [...]",0
"Well, HIV/AIDS didn't end prostitution in the 1980s and early 1990s, when it was seen as a near-future death sentence; heterosexual couples have unprotected sex today in places where women still have no reliable access to birth control (or had reliable access and watched their right to it get stripped away); people get drunk and hook up with a stranger at a party...human beings are not exactly known for rational and self-preserving behavior when it comes to sex, is what I'm saying. We also should recognize that some societies, like late medieval and early modern Europe, worked long and hard to make prostitution one of a few ways that poor urban women could earn a living on their own, or turn to it as ""occasional work"" to make enough money just that one time...and get trapped in it because they got arrested or *fama* got out and they couldn't escape the reputation and get other work.

That said, human beings are also capable of less hormonal modes of being, and in other cases less desperate frameworks for actions, in which they recognize that some diseases are sexually transmitted, and try to come up with ways to limit risk. I'll focus on western Europe in the late Middle Ages in this answer, and perhaps we can hear from other people about other times and places.

There's a decent debate in scholarship concerning how severe the pool of STDs in medieval Europe was--essentially, was there syphilis or something like it before the late fifteenth century. (Syphilis being significant here as more severe and deadly than gonorrhea and chlamydia). But setting aside attempts at historical epidemiology, there are a couple of ways we can see medieval sources talk about diseases they associate with sexual transmission. 

One is a reference to a ""burning sickness,"" as the ordinances governing the (legal) Southwark stews in the fifteenth and early sixteenth centuries have it. The other major one is facial pock marks, pustules, measles spots, and so forth--often overlapping, in medieval useage, with leprosy. (You might have heard ""in the Middle Ages, 'leprosy' was really syphilis'--it's not quite that straightforward, but more like, ""leprosy"" was probably a range of diseases that included some STDs). There is a strong tradition of moralistic writing that argues prostitutes are dangerous to men's bodies because they have sex with lepers and can pass on leprosy; of course they are dangerous to men's souls for other reasons.

As a *sexually* transmitted disease, of course, there was an even stronger moral condemnation component in the Middle Ages than today. So STDs were absolutely tied to illicit, immoral sex--fornication, adultery, and above all prostitution. It is important to stress that this was a development of medieval *discourse*. We cannot think of prostitutes as Petri dishes for disease in the way other people aren't. After all, how did they get those diseases in the first place? (Hi, men!)

Thus, measures to limit STD transmission applied to legalized prostitution, and more crucially, to the prostitutes themselves, not the men who hired them. The Southwark ordinance, for example, bars women with the ""burning sickness"" from their job. Alice Dymmock was a brothel-keeper and prostitute in 15th century Great Yarmouth who, after numerous brushes with the courts for prostitution and disrupting public order, was finally exiled from the city as a ""leper.""

To shift the time frame a little, and bring us back to the original point, Laura McGough proposes in *Gender, Sexuality, and Syphilis in Early Modern Venice* an explanation for why, in fact, men continued to seek out sex with prostitutes even despite a strong cultural association between prostitutes and disease (and in the case of her study, syphilis in particular--a popular early modern ""origin story"" for syphilis involve a deadly beautiful prostitute as Patient Zero). She argues that the cultural rhetoric that developed around syphilis/the French disease/the Spanish disease focused very narrowly on immorality--and indeed, on specific patterns of immorality, especially here dangerously beautiful prostitutes and out-of-control soldiers of another country. This allowed individual men to divorce themselves from seeing a risk *to themselves*.

Less than a decade after references to syphilis start popping up in western European sources, so do treatments for it, and so does skepticism that those treatments work. Blood-letting, a special kind of wood from the West Indies, mercury (the last particularly noteworthy because that was popularized by Paracelsus writing *explicitly against* the Galenic party-line of the day). 

On the other hand, Nancy Siraisi shows, by the mid-16th century there was unanimous medical opinion that syphilis was curable. She argues that multiple diseases had become grouped as ""syphilis,"" some of which we would identify with the pathogen today. Other things that traveled under that name *were* ""curable"", or more likely went into remission or healed on their own. Thus, while syphilis itself was a deadly danger, early modern men and women did not have to believe it was *necessarily* a deadly danger if they contracted it.

None of this, of course, addresses how prostitutes dealt with the chance of contracting an STD, or how women marrying older men they assumed had visited prostitutes or had affairs along the way dealt with the risk to themselves. This is one case where, right now, we're dealing mostly with assumptions we build from silence. We know a little about former prostitutes who made a second life for themselves, including some with what are presumably STDs, but not about preventative measures beforehand. However, there is a fair amount of work on premodern attempts at birth control, and it seems reasonable to assume women having PIV sex with men they didn't trust would take parallel steps for *other* risks of sex.

Is it probably a transhistorical trait that people are capable of diverting from rational to irrational thought processes when it comes to sex? That seems likely. However, that trait still plays out through channels that are specific to times and places. In this case, in late medieval and early modern Europe, men soliciting prostitutes found ways to believe it wouldn't be their problem. And, one assumes, prostitutes found ways to convince themselves it was worth the risk--to trade an STD for a chance to survive, whether that meant money or doing what they were forced to do.",0
"You are more or less correct. Hitler came to lead the NSDAP after the events sometimes referred to as the July Crisis. The Crisis began while Hitler was away from Munich, which was where the party was based, visiting Berlin on a fundraising tour, when discussions took place between members of the German Socialist Party and the NSDAP, as well as another group that had recently appeared on the right wing political circuit, the *Deutsche* *Werkgemeinschaft*.

The DSP, the NSDAP, and the new *Werkgemeinschaft* held many of the same beliefs and aims. With their shared values, the expansion of political activities that a merge would bring, and additional popular speakers headlining their rallies, the combined party would be a significant force on the national stage. The influence of Adolf Hitler in a larger party with other popular speakers would be severely diminished, something that must have been, if not an overriding aim of the merger, a pleasant silver lining. For the majority of the committee, Hitler had too much influence over the party due to his popularity with the membership, but was the driving force behind the party's growing success. 

When Hitler himself heard of this meeting, supposedly tipped off by an ally on the committee, he returned to Bavaria and ambushed the representatives sent to discuss the alliance. He ranted and raved, denying any value in cooperation and insulting the head of the *Deutsche* *Werkgemeinschaft*, Dr Otto Dickel, who Hitler held in contempt for his philosophical approach to politics. He continued his tirade for three hours before storming out of the building. Having seen this all before, (previous attempted mergers with the DSP had failed due to Hitler's tantrums) the representatives of the NSDAP calmly continued with the negotiations and agreed to present Dr Dickel’s proposals to the full committee. Hitler resigned the same day, the 11th of July, declaring that the committee had gone against the wishes of the membership by even considering a merger, and that “I will and can not be any longer a member of such a movement.” 


While some members of the committee were quite happy to be rid of Hitler, others saw this as a fatal blow to the party that could lead to a split within the ranks. Within two days, Anton Drexler (the current Chairman)asked under what conditions Hitler would return to the party, possibly expecting him to request a ban on future party mergers. Instead, Hitler went on the offensive, demanding the position of chairman, along with dictatorial powers, guarantees that the party would remain based in Munich, that the party programme would remain unchanged, ***and*** a ban on future party mergers. A day later the committee agreed to his conditions due to his “immense knowledge” and his “unusual talent as a speaker”. Hitler returned to the party on the 26th of July, and received the near-unanimous agreement from the membership for his ascension as Chairman on the 29th, which now had the vast powers he had demanded.

Sources: 

Ian Kershaw,  Hitler 1889–1936: Hubris

Konrad Heiden, Hitler: A Biography

Alan Bullock, Hitler

",0
"The earliest origins of the Greek gods lie in prehistory. We have no written sources and very little archaeological material to go on. The Greeks descent from Indo-Europeans and so do their language and mythology. By comparing the many Indo-European languages and mythologies researchers in these fields have been able to reconstruct to considerable extent the Indo-European culture. For example, *Zeus* is the Greek instance of the Indo-European sky-god *Dyeus* and so is *Jupiter*, or *Dyeus Pater* (father).

It is impossible to determine exactly when these gods or myths first originated or when they are first recognizably Greek. The earliest Greek writing was in the Linear B script. This was used by the Mycenaeans during the Late Bronze Age, roughly a thousand years before the Classical and Roman Greek periods. (The Late Bronze Age collapse and the Greek Dark Ages divide Bronze Age Greece from the more familiar Archaic, Classical, and Roman periods.) In these Linear B tablets we find the names of a number of Greek gods that are more or less the same a millennium later. Most Greek myths, including the ones told by Homer, are set in this Late Bronze Age (Mycenaean) period.

In conclusion, the very earliest origins of the Greek deities stretch back many thousands of years deep into prehistory because they are Indo-European. They were distinctly and recognizably Greek to modern eyes certainly by the Late Bronze Age in the Mycenaean culture.",0
"A few different traditions that you find mentioned. One is that his father was a Germanic Roman Centurion named Panthera (which raises other theological issues... the origins of this is actually Jewish anti-Christian writings, but that was a minor detail). Another is that his grandparents were Galilean, and Babylonian Aryans resettled there after the Assyrian's conquered Babylon, and had been forced to convert to Judaism, but weren't *racially* Jewish. Hardly the only ones as tons of anti-Semitic thinkers of the late 19th to mid-20th century did their damndest on this, but a few examples.

Some reading I'd suggest on this would be:

Head, Peter M. 2004. “The Nazi Quest for an Aryan Jesus.” Journal for the Study of the Historical Jesus 2 (1): 55–89.

Heschel, Susannah. 1999. “When Jesus Was Aryan: The Protestant Church and Antisemitic Propaganda.” In *In God’s Name: Genocide and Religion in the Twentieth Century*, edited by Omer Bartov and Phyllis Mack, 68–89. New York: Berghahn Books.

Heschel, Susannah. *Aryan Jesus: Christian Theologians and the Bible in Nazi Germany*. Princeton University Press, 2008.",0
"There is always more than can be said and I freely admit that my research is limited by the fact I do not read French or German, but the best answer a historian can give you is likely: it depends.

There are a few things I can tell you for sure about your curriculum, provided you were teaching at a non-private, non-religious high school in the city. First, a fair amount wouldn't have changed. You would still be preparing your students for entrance into French colleges and possibly even teaching *khâgne* (also known as *classes préparatoires littéraires*) for students who were looking to enter French academic and cultural elite circles by matriculating at the more exclusive French universities and programs. You would still teach and maintain the standards for ""proper"" French language as determined by the Académie Française and would have still taught French literature, philosophy, and culture (more on that later.) Meanwhile, the nature of your curriculum would still be shaped by decisions made at the federal level. You likely had women and men colleagues and your married women colleagues continued to teach after getting married, a norm that had been established earlier in the century and was fairly unusual for European and North American education systems.

Another thing that would have remained fairly constant was having a mixed-gender class. The French government made a number of large-scale changes related to hard gender segregation in the 1920s, following policy changes made by Leon Bérard, a long-serving Minister of National Education. These changes meant that many high schools that were previously ""girls"" schools adopted ""boys"" curriculum, making them de fact ""boys"" schools, and leading to more gender diversity at individual high schools. I wasn't able to find any evidence of any changes to gender configurations during the war which doesn't mean it didn't happen, but generally speaking, it doesn't appear as if the Vichy government was concerned with those particular details.

What would have changed, though, is your sense that what you were doing as a teacher was for the good of France. Between World War I and World War II, the French education system worked through a number of long-standing issues related to the funding of public and private education. While most of this related to the church's stance on what constituted a proper education for young Catholic children, there were also issues related to the role of public dollars in funding religious schools. The French Third Republic was committed to a secular education for its citizens and after lots of verbal sparring and policymaking, just before World War II, there was a sense of closure - French public high schools would focus on civics, Catholic high schools would focus on faith. The church dictated what happened in Catholic schools and the government shaped what happened in public schools and although it was a tenuous separation, it was mutually agreed upon. In effect, that changed following the occupation of the city. From Male's 1962 history of French education:

> Under the Vichy government of the early 1940's, however, old wounds were reopened as the government gave public money to church schools and made religion a compulsory subject in the public schools. The latter produced such opposition from public school teachers that the decree was rescinded later in 1941. A decree of 1942 abrogated the Association Law of 1901 and allowed religious orders to organize legally. All this was encouraged by the German occupation forces as a means of dividing and weakening France.

At the same time, Vichy leaders such as Philippe Pétain encouraged French children to go to Church and his government required public school teachers to incorporate religious instruction into their curriculum (Sweets, 1986.) So, whereas before the war, you would have focused on civics and government when teaching French culture, you were now expected to teach Catholicism as part of that culture. This would have also meant that in addition to other wartime restrictions, your school likely had its budget cut as the government elected to send money to private Catholic schools. In addition to making changes related to Catholic schools and funding, the Vichy government also moved around the country's education system, especially the various approaches to higher education. Which this wouldn't have directly impacted your curriculum, it likely would have shaped how your students thought about their future after you. (Note: as far as I can tell, the general sentiment among those who study French education history is that the Vichy's efforts to restructure French education failed as the country reverted fairly quickly after the war.) 

All of that said, the variable that played the greatest role in shaping your curriculum was your individual identity. In Jean Guéhenno's *Diary of the Dark Years*, he describes his many sleepless nights worrying about his students, worrying about being caught saying the wrong thing, worrying about other teachers. Purges were common, especially early in the war. If you survived those early purges and maintained your classroom and your position,  you would have noticed a series of compounding small changes. You couldn't buy new textbooks, especially history ones, without approval. Whereas before the war you could teach your students to be proud of being French and put France in the center of all things artistic and creative, you no longer could. Not just because of the occupation and the hit the French took to their sense of self, but because such sentiments could be seen as anti-German. There were constant air raids. Children would suddenly disappear, sometimes just because there was no longer an adult to make sure they got to school. Your colleagues could suddenly disappear. 

Finally, you would have seen a marked change in the students who did come to school. Whereas before, you typically engaged your students in heated discussions and debates in all areas of the curriculum, after two years of occupation, most of that fire was gone. From an English translation of Guéhenno's diary, 1943:

> Every one of these young intellectuals has never thought so much about himself alone, his happiness, his joy, his career— and with such pettiness, such servility. When you talk to them of freedom or honor, they just get embarrassed. They rub their noses and lower their heads, deaf, fearing nothing so much as to be asked to do something, however little. For the past few weeks, I had the feeling they have been still more inert. ... I write these things down with great sadness: this country would be ready for servitude if its young intellectuals resigned themselves so easily to the misery of others. And then, I think I can see all too easily how the spirit can die.... In the freedom of former years, great choices were offered to these young people, and that was enough to give some of them a certain impulsion, a certain anxiety and nobility. Now they don’t even know what great choice they could make. Most of their teachers no longer dare talk about it. Discussion is forbidden. Just one kind of propaganda is permitted— nay, encouraged: the propaganda of obedience and submission.

 

_____

Guéhenno, J. (2014). *Diary of the Dark Years, 1940-1944: Collaboration, resistance, and daily life in occupied Paris.* Oxford University Press.

Male, G. A. (1963). *Education in France.*

Mitchell, A. (2008). *Nazi Paris: the history of an occupation, 1940-1944.* Berghahn Books.

Sweets, J. (1986). *Choices in Vichy France: The French Under Nazi Occupation*. Oxford University Press.",0
"There are so many, but off the top of my head: 

William Sheridan Allen, ""The Nazi Seizure of Power: The Experience of a Small German Town"" is excellent and focused.

Richard Evans is a heavy hitter for this field, check the first volume of his three-volume series on Nazi Germany, ""The Coming of the Third Reich."" 

The Oxford Readers series is an interesting approach because it gives you the first-person sources, curated with editorial comment. (""Nazism (Oxford Reader)"" edited by Neil Gregor)

And to go meta, Ian Kershaw's ""The Nazi Dictatorship"" will not only give you the history, but talk about the major schools of thought, or ""problems of interpretation"" that historians have faced in writing about the period. It got a new (fourth) edition a few years ago. His double volume biography of Hitler is stunningly comprehensive, but probably not what you want for this question.",0
"
By now, you may have noticed the surprising continuity of the *chola* dress. While hat and skirt patterns and styles have changed, the overall assemblage has changed little. Where does *that* come from?

The *pollera,* the *manta* draped over the shoulders, and the variety of hats are all directly taken from Spanish peasant clothing of the 19th-century. This is the result of a 1782 ruling by Jose Antonion de Areche, a royal *visitador* sent to get the Viceroyalty of Peru making a profit again. The *visitador* was, by then, an outdated title from back when Spain was just Spain and the king needed someone to directly check in on different regions for him. This led to conflict with the Viceroy of Peru, but that was interuppted by the rebellion of distant Inca heir Tupac Amaru II in Cusco, and the subsequent uprising in, and seige of, La Paz led by Tupac Katari. Though Amaru II was defeated within a year, Areche responded with a harsh, oppressive program to ""erase any memory of Inca-ness"" and prevent furhter rebellions. This included a ban on ""the use of national dress, which could bring to mind the ancient Inca memories"" and the promotion of Spanish-style dress.

Thus, what looks to an outsider to be ""traditional"" Bolivian dress is really a European imposition that doesn't clash nearly as much with the *bombin* as it would seem. Women were wearing European style hats well before the 1920s; a new style would not have elicited too much notice.

____

""But CoCo!"" you might be wondering at this point, ""I asked why it was mostly *indigenous* people who wore bowlers, and it seems like cholas are all mestizo!"" And, well, you wouldn't be wrong... but you're also not right? Regardless, it's the second answer to the question: the image of the chola has become so much more than just that of ""non-white woman who participates in the informal economy.""

The chola identity emerged in direct opposition to traditional images of indigenous Aymara, a process powered both by foreign and local actors. The early 20th-century photographs above are markedly Western in most every aspect. As Bolivia integrated itself into global markets at the the turn of the century, there was pressure for its white, [criollo](https://en.wikipedia.org/wiki/Criollo_people) leaders to seriously consider the liberal ideas on which Bolivian independence was supposedly founded. Yet they would not give up their oligarchy easily. Indigenous and mestizo populations increased in urban areas, as did their potential to participate in formal political and economic processes, but it inevitably came at the cost of their Aymara traditions. The families most able to integrate into urban life were those who had a member serve in the War of the Pacific against Chile. These were known to some as ""cholas decentes,"" in contrast to those who contintued to live in rural areas and visit the cities for business. Since it was almost entirely women who took produce to markets and who worked as domestic servants, it was the feminine ""chola,"" and not the masculine/neutral ""cholo,"" that developed into modern usage. These workers also faced incerasing obstacles to integration. Traditional open marketplaces were standardized, forced into claustrophic concrete blocks that matched the urban environment, or into rows of identical metal stalls. Archaic property laws were renewed, and renewed, and renewed. Public transport introduced regulations about the amount of luggage and size of clothing that was permitted (an issued that reccured with La Paz's [cable car system](https://www.opinion.com.bo/media/opinion/images/2020/02/25/2020022520441129509.jpg)). 

Meanwhile, Westerners demonstrated a perverse, patronizing fascination with chola dress and lifeways. The previously cited Mechner text is representative of contemporary remarks:

> The *chola* is a refreshing exclamation point in an otherwise drab grouping of human figures. She manages by some magic to appear spotlessly clean, although her asks take her tripping over the dirtiest of cobbles. She makes a near approach even to the North American conception of beauty, her Spanish blood erasingsome of the dull copper from her cheeks and giving her figure an almost Andalusian grace of carriage.

These were highly racialized, and directly contrasted with descriptions of the ""real"" Aymara. Those descriptions are, of course, so grossly racist that I'm not quoting any here. A relatively tame example is Arhtur Posnansky's influential, yet entirely vapid, multi-volume work on the archaeological site of Tiwanaku, which described the 20th-century Aymara living there as ""troglodytes... devoid of culture."" The chola, however, could be ""decent,"" yet exotic and exciting.

This mode of thinking diffused into the Bolivian elite. They saw in the chola a unique Andean icon of hardwork and ""heritage,"" but one that reamined subsiverent to *criollo*, male interests and deflected pressure to address the continued harms against indigenous populations. In popular literature, the chola was given an air of innate, racialized promiscuity: white men have always sexualized the ""uncivlized"" Other.  The chola had become such a figure in the collective imagination that Carnaval dancers, which traditionally wear elaborate costumes from folk Catholicism, began ""dressing up"" as cholas, as seen in [this image](https://i.imgur.com/lsW0nYA.png) from a 1924 issue of the magazine *Arte y Trabajo*. The young woman, Carmen Rosa Anze Guzmán, was a frequently-courted socialite, as attested by the gushing love poem scribbled on the back of this archived copy. ""Playing chola"" let her try on, for a day, those values and reputations. Criollos defended this appropriation by pointing to the the fundamental role of the chola in the Bolivian economy. But to quote Seligmann (1989):

> there is a difference between facilitating the operation of an economy and making it grow or change to the market women's advantage



*Pollera* as a costume would become the reality for many women. The mandated Spanish-style dress had persisted for 150 years because the position of native communities in Bolivian society had changed litte. The country's independence in 1825 had few repurcussions for rural laborers; the *hacienda* plantation system, an esentially feudal system of ownership, persisted across the Andes until the Agrarian Reforms of the 1950s and '60s. As the ancient Spanish systems dissolved in the early 20th-century, so did its racialized rhetoric. Or rather, it blended with classist and sexist into a more ""acceptable"" and ""liberal"" form of oppression. *Campesino* (peasant) and *indio* (""Indian,"" with much the same derision as the English word), were synonymous. Being indigenous- at least in terms of language- was much more of a social role than a designation of heritage: was an Aymara who lived in the city and spoke Spanish *really* Aymara, or were they necessarily *mestizo*? 

The role of cholas, however, did not neatly resolve into urban/rural, white/native, consumer/laborer divides. It persisted, instead, as its own defined role in the national imagination. *Pollera* was no longer what cholas wore, it was what one wore to present as a chola. Texts into at least the '40s continued to use ""chola"" with an explicitly racial implication, but by the '70s it had become its own category: neither explicitly native nor mestizo, but definitely not white. When did this change occur? It's hard to pin down. Residents of Wila Kjarka, town outside La Paz, interviewed by anthropologist Andrew Cannessa recall that, after the 1952 revolution that formally ended *haciendas*, *mestizos* in La Paz began wearing Spanish clothing and rural Aymara started wearing *pollera*. It's unlikely this was a society-wide trend, but probably does refelct a decision made my many families and individuals in navigating the new sociopolitical order. Indeed, Mary Wesimantel's definitive work on the ""chola"" concept argues that every time a woman puts the *pollera* skirt, the hat, and the jewelry, it is an intentional decision, and she lists many instances of women changing attire for different employers, when they move to a new house, or when they reach a certain age because of the new identity they want/need to present.

There are so many legends about the origins of the Pacena chola's bowler hat because it's so much more than a hat. It's part of well-defined costume that places the wearer within a particular ethnic, gender, and economic role.

____

It's finally time to apologize.

Nobody knows the actual answer to your question. At the time the *bombin* made its way into La Paz fashion, it wouldn't have made the same impression it does today, and its association with the chola identity means that modern interest in its origins is greater now than ever.

Why is the hat particular to La Paz? Who's to say. Regional trends in dress, language, and food diffuse unpredictably, and people make a deal of it after the fact.

But what I hope I've communicated is that even in historical questions with no academic answer, the reason for that can be just as interesting.

___

Alexander, Robert Jackson, and Eldon M. Parker. 2005. A History of Organized Labor in Bolivia. Greenwood Publishing Group.

Canessa, Andrew. 2012. Intimate Indigeneities: Race, Sex, and History in the Small Spaces of Andean Life. Duke University Press Books.

Maclean, Kate. 2019. “Fashion in Bolivia’s Cultural Economy.” International Journal of Cultural Studies, January.

Seligmann, Linda J. 1989. “To Be in between: The Cholas as Market Women.” Comparative Studies in Society and History 31 (4): 694–721.

Weismantel, Mary. 2001. Cholas and Pishtacos: Stories of Race and Sex in the Andes. University of Chicago Press.",0
"This is an answer I gave to a similar question several years ago.

So I'm only gonna be talking about the Sumerians here. The only way that we are going to have any knowledge of what the first civilizations knew of their past is through written records. Writing first appears in Sumer in around 3000 BC, this is during the Uruk period (4000-3001 BC). At this time it was very much proto-writing and did not develop into a full written language until the Early Dynastic Period (2800-2500 BC)

It is not until the end of the Early Dynastic Period (2300 BC) that we see the emergence of the Sumerian King List which documents the kings of Sumer and gives us an insight into what the Sumerians may have believed about their past.

The King List, a collection of several sources, details the rulers of the cities of Sumer from the first antediluvian rulers to the last dynasty of Isin. The first king was Alulim, first king of Eridu. He ruled for 28,800 years. Now at first glance that may seem an unreasonably long rule and that is because it is. Alulim, the first king after the kingship descended from heaven created by the god Enki is clearly almost entirely mythical. This list of antediluvian kings ends with Ubara-tutu and the coming of the great flood that wipes the world clean.

The list resumes with Jushur the first of the dynasty of Kish. He ruled for 1200 years. 20 kings and 15255 years later we have En-me-barage-si who is the first king we have archaeological evidence for. He is dated from around 2600 BC from two pieces of alabaster vases found at Nippur which bear his name. Though he ruled for 900 years, a rather long time, it can be surmised that he is indeed real. He is mentioned also in the Epic of Gilgamesh alongside Gilgamesh himself giving credence to the thought that Gilgamesh is a historical figure. The King List continues into the time of rulers that can easily be verified such as Sargon of Akkad who ruled for 40 years and founded the Akkadian Empire. Thus we see the transition of the mythical into the semi-mythical and then verifiable history. But more on that later.

The Sumerian creation myth is important to note in this discussion. It recounts that the gods Enki among them created the first “black-headed people” (the Sumerians) and settled them in the land giving them the kingship and thus the first cities were created. A large part of the story is missing but at some point the gods decide not to save mankind from a flood which strikes destroying man and cites. Later the world is presumably repopulated.

In addition there is the “Debate between Summer and Winter” a creation myth from the mid 3nd millennium. This details the creation of the land and seasons by Enlil. In it he is seen to irrigate the land “guaranteeing the spring floods at the quay” and to begin the agricultural tradition of the land “making flax grow and barley proliferate.”

Finally and most interestingly for this topic is the “Debate between Sheep and Grain” another creation myth written in the mid-3rd millennium . The myth details a time in which sheep and grain were unknown to the land. The people “went about with naked limbs in the Land. Like sheep they ate grass with their mouths and drank water from the ditches.” The myth ends with the virtues of grain being extolled “from sunrise to sunset may the name of Grain be praised. People should submit to the yoke of grain.”

Therefore we can see that early history of Mesopotamia, the Ubaid period and before, is in Sumerian text seen in a divine light. The land was created by the gods as were the people and they were given cities and kingship. Enlil gave the people the summer and the winter, he gave them wheat and irrigation as Enki gave them kingship. Only in the “Debate between Sheep and Grain” is there indicated any knowledge of a time before sedentary agriculture. This myth clashes with that of the “Debate between Summer and Winter” though it is part of the same tradition indicating the lack of a unified view of their past. It seems that the Sumerians saw their past as part of a very real mythical tradition. Their kings begin as mythical figures and progress towards the non-mythical. The mythical and the non-mythical are closely linked in the Sumerian view of themselves and their past

I would conclude that the Sumerians did believe themselves to be not just the first civilization but the first people, it is part of their creation myth. In addition there was no knowledge in the way we would think of a hunter-gatherer life preceding their urban civilization. If there is any hint it exists as another facet of the extensive and contradictory creation myth of the peoples of Sumer.

[etcsl.orinst.ox.ac.uk](http://etcsl.orinst.ox.ac.uk/)

[cdli.ucla.edu](https://cdli.ucla.edu/)
",0
"> [*Godwin's Law*]

Look, I know we joke about AskHistorians being the world's biggest storehouse of Hitler trivia, but *that's not supposed to be a good thing.*

Please don't.",0
Fuck yes. This is why I love this sub. Thank you,0
"For further reading, I would really recommend: 

[Courtesans & Fishcakes: The Consuming Passions of Classical Athens](https://www.amazon.com/Courtesans-Fishcakes-Consuming-Passions-Classical/dp/0226137430), a book by James Davidson, which has pretty significantly changed the scholarship on prostitution in Classical Greece.

This essay by Yvonne Rösch [Hunting Hares and Lovers: Socrates’ Playful Lesson in Xenophon, Memorabilia III](https://halshs.archives-ouvertes.fr/halshs-01825159/document) discussing the context of the dialogue between Socrates and Theodote

[Goddesses, Whores, Wives, and Slaves: Women in Classical Antiquity](https://www.amazon.com/Goddesses-Whores-Wives-Slaves-Classical/dp/080521030X) by Sarah Pomeroy",0
"You probably know one way to answer this question for late medieval/Renaissance Europe: the Fugger and Medici dynasties both got their start in the international textile trade, moved into money management/banking when they had the funds to do so, and greatly aided their coffers through monopolizing regional mining.

There's no question that Jakob Fugger, Cosimo de Medici, and their family ""businesses"" were stupendously wealthy. But the fifteenth-sixteenth century economic world is far removed from our own in some ways, and far removed from earlier medieval centuries too. A ""Fortuna L"", to medieval people, would consist of a lot more than what we today think of as businesses.

The first major economic force outside the traditional landed nobility were the Christian religious orders. You might have heard ""The Templars were the first international bankers""; let that be an introduction into the economic and political power of monastic and mendicant orders as a whole, and more locally, many individual convents (while others, especially women's communities, struggled desperately with actual poverty).

Religious orders were so successful first and foremost because of a factor in the medieval economy we don't think about today: the economy of salvation. When rich nobles and, eventually, burgher families donated land, money, and goods to monasteries, or paid to establish a perpetual chantry at a cathedral chapel, they were doing so in exchange for a specific transactional service: prayers for the souls of themselves and their family.

Now, we talk about ""land""; what does that mean by way of economic growth and success? The obvious first level is agricultural produce. But there were other ways to exploit land. English monasteries, for example, involved themselves in building and operating mills. Mining, or rather the sale of mining rights, could help a house prosper. And especially from the early 14th century on, cash rent from tenants was the direct, desirable way to turn land into value.

The system was not an endless spiral upwards. We often talk about the Black Death as adding fuel to the fire of a late medieval fear of/fascination with death, catalyzing even further a desperation for prayers and Masses to speed one's and one's family's souls through purgatory to heaven. Well, those perpetual chantries I mentioned above--donations to a church or convent to say Masses in perpetuity for someone's soul? They were often funded by a donation of a tract of land. And if that particular plot lost all or most of its tenants to one of the waves of pestilence, no more chantry.

What gives this discussion of monasteries as economic actors a particularly *medieval* dimension is the relationship between individual communities and the international order. The actual administration of orders varied, of course, and some houses were famously independent of major orders altogether. The *Frauenstifte* of imperial Germany are one of the most famous examples there. But even among a fabulously wealthy *order* like the Franciscans, individual communities--especially women's houses--could fall into abject poverty, barely able to support themselves; meanwhile, satirical authors from England to Germany tear apart too-wealthy friars for their love of money over God.

The complexity of defining ""corporation"" in the medieval economy (as well as defining all the elements *in* the ""medieval economy,"" as noted earlier in discussions of assembly-line production of paradise-souls out of purgatorial ones) also comes into play when we consider something more akin to businesses: the crafts or trades. Individual workshops, merchant houses, and proprieters are certainly and necessarily one level of economic actor. However, in terms of economic contribution to and political power in late medieval cities, we have to think of ""corporation"" as indicating the collective trade--that is, the craft guild. Medieval guilds were not the premodern equivalent of trade unions. They served as quality control over products and services produced in a given city, in part to maintain the local craft's reputation in regional and international trade. Guilds were also primary players in the urban economies of salvation and public life, sponsoring religious events, feasts, theatre, what have you--often *competing* with each other.

Georg Christ's *Trading Conflicts: Venetian Merchants and Mamluk Officials in Late Medieval Alexandria* shows the difficulty of untangling individual from civic ""corporation"" when considering medieval economic power. Christ studies the career of Biagio Dolfin, Venetian consul to Alexandria who in official capacity traded in gemstones (which he was allowed to do As Consul), and in unofficial capacity of course also traded in wool textiles (which he had to do as, well, himself). 

From my notes, here's a great example of balancing individual profit motives with protecting the overall Venetian merchant corps: a group of merchants complained to Venice/Dolfin that the Mamluks, controlling Egypt, were levying an extra merchandise tax which was most unfair. Of course Dolfin bucked up and negotiated with the Mamluks to reduce it--while spilling to Venice back home that individual merchants (*certainly not him*) were smuggling merchandise and money sewn into clothing and hidden in other ways, so it had become necessary to levy the tax at a different point in the sales chain. Christ had to make sure he was greasing the gears (which 13th-14th century western Europe was finally beginning to understand again!) with the sultan in order to be able to win Alexandrian court cases and aid Venetian merchants overall; he also had to advocate for Venetian needs in order to preserve his status as consul.

Two patterns that have emerged in examples so far of economic powerhouses will come to the fore when we differentiate craft guilds. Here we have a nice guide to hierarchy, since London was generous enough to future historians to establish a hierarchy of its Worshipful Companies in the early 16th century. Merchants are on top, which probably doesn't come as much of a surprise. It was much more beneficial to be a trader of other people's labor than to deal directly with laborers. Furthermore, it was obviously beneficial to be involved in an industry that could take one outside the bounds (physical and fiscal) of an individual city. While there were some luxury specialty items that garnered an international clientele for the best of the best artisans--/u/WARitter has some amazing posts on the international reputations of individual armourers, especially from my beloved Nuremberg and Augsburg--throughout the later Middle Ages, the great European trade icon comes back to textiles. It's not an accident, indeed, that the Medici and Fugger banks got their start in textile trading.

The thing about textiles is, sure, lots of places have a local wool or linen production industry. Despite the predominance of northern Europe in the various steps involved to make workaday cheaper cloth (from growing wool up to dyeing), even a city like Florence, Italy, marshalled enough ground-up cheap wool production to organize an entire guild around (the *Arte della lana*). The cheap wool and fustian was produced locally and sold almost exclusively locally.

But the real reason textiles became such an important *international* trade item is that everyone wanted better textiles from somewhere else. This isn't even just European. When white traders arrived in West Africa, they didn't actually have anything to trade that the Africans didn't already have. But they *wanted* different options for products like textiles. And so with Europeans.

Thus Florence's *other* textile merchant guild enters the picture: the *Arte di Calamela.* These merchants were not involved in managing the sheep-up production of fabric. Their traffic was in luxury woolens, in producing a finished product. For the most part, they just bought fine woolen fabric from northern Europe--everyone loves the English not-quite-finished good stuff!--and brought it back to Italy to concentrate on making it from fine to finest. Crucially, dyeing the fabric was one of the most lucrative ways to add value. In fact, when you look at ship cargo manifests from the early Atlantic trade, over and over the commodities acquired are various woods used for brighter and brighter dyes! John Munro points out that Florentine luxury wool merchants could *double* the value of their purchased wool by dyeing it the most desirable (expensive) shade of scarlet.

There is *a lot* we still don't know about the medieval European economy. Michael McCormick's famous thesis in *The Origins of the European Economy,* that slavery and slave trading helped power an international European economy through our near-blackout in sources and jumpstarted the ""commercial revolution"" has recently been cast into doubt by Alice Rio (*Slavery After Rome*), for example. Horden and Purcell's suggestion that ""invisible"" local trade up and down the Mediterranean coastline served that purpose has been difficult to investigate because, well, it's invisible in surviving sources. But two things that are definitely apparent are, first, the complicated meanings of ""corporation"" in economic analysis. And second, he fact that some trades were better than others, banking could make you the real money but first you had to get the money from somewhere, and some *corporations* in medieval Europe were rich enough to buy islands--and the papacy.",0
"Taking place over thirty years after the end of the conflict, we can say with a fair bit of certainty that there isn't [*quite* that level](https://www.youtube.com/watch?v=5v8W-QqVxFQ) of preservation of battlefields, but it is important to understand the context of the game here. I've written previously about another aspect of Civil War memory in the game, discussing the [post-war presence of Confederate affiliated criminal groups](https://www.reddit.com/r/AskHistorians/comments/b10n87/red_dead_redemption_2_takes_place_in_1899_and_has/eiio7u5/), which strictly speaking also isn't accurate that long after the war (although in-game lore makes clear they haven't existed the entire 30 years, but rather are a recent resurgence). But what both of these elements point to, as well as other inclusions in the game, is a heavy lean in on common *tropes* of the Western, as many classic films, on which RDR2 is of course drawing inspiration from, use the Civil War or else its immediate aftermath as part of the backdrop against which their stories are set. The presence of a battlefield, still strewn with debris and while overgrown, done so in a way that suggests a few years have passed rather than a few decades, doesn't reflect reality of 1899, but it does reflect popular media. For me at least, it was quite hard not to think of the scene from *The Good, The Bad, and the Ugly* where we come across the aftermath of a battle, and I'd hardly be surprised if that wasn't a direct inspiration, as it feels intentional.

Now, while it might not look exactly real, we can discuss the state of battlefields in the period. As already noted, the ones in the game are well kept all in all, and while this *is* not too far removed from the period when battlefield preservation began, they certainly didn't want to keep them in that sort of state. Early battlefield preservation was mostly focused on memorialization of the dead. Many battlefields had national cemeteries created on their grounds, or else nearby, where the Federal government interred the Union dead. Fredericksburg National Cemetery for instance was founded in 1865, Shiloh National Cemetery in 1866, and Gettysburg National Cemetery was created before the war even ended, in late 1863. Confederate dead were not placed in Federal cemeteries, but some of the earliest Confederate groups to form in the post-war were the Ladies' Memorial Associations who saw to the internment and maintenance of the cemeteries and memorials for the fallen traitors.

By the turn of the century, the battlefields themselves were beginning to be preserved as historical sites. This movement began in the 1890s, first with Chickamauga and Chattanooga National Military Park in 1890 and soon followed by Antietam National Battlefield was also in 1890. The process continued on for decades. Even before their official memorialization, many former battlefields were serving as grounds for reunions of veterans, and many battlefields were preserved by local veterans or memorial groups, such as the UCV, SCV, or UDC. This impulse especially came to the forefront in the 1890s as the preservation of such 'sacred ground' was seen as an important part of keeping alive the flame of the 'Lost Cause' that such groups in the South worked to perpetuate.

To be sure, some battlefields never saw significant efforts at preservation, but far from meaning they were left alone, more often than not that meant they became farmland again. Civilians needed to get back to their lives, and anything too broken for the armies to haul away with them, they would need to deal with instead, which could mean months of backbreaking labor by communities simply to reclaim their land for use again. 

Even battles in forested areas, by 30 years onwards, often had seen those woods cleared for more farmland. And whatever the preservation state 30 years on, in the immediate aftermath, the most accessible detritus would long have disappeared. Battles were rarely fought far from settlement, and this certainly isn't the case in the game. After the armies left, civilian populations would scour the field and recover things they thought useful, and as noted, haul away anything simply in the way. 

The military too, of course, would scour the field to remove the dead and such, a thoroughly distasteful job often forced upon the African-Americans supporting the armies, regardless of the side. At many battles, the dead were buried where they fell at first, and only later would parties come to rebury them properly, although there would of course be remains in the open that were missed. The remains were often buried under a few inches of soil, civilians commenting on the abhorrent stench that would remain for months. Quite literally buried where they lay, this often meant more work for the civilians who had to remove the bodies and rebury them elsewhere, not only to make their fields usable, but often because of makeshift graves literally on the doorstep or under their porch. Only wealthy officers would usually have the luxury of being shipped home to rest with their loved ones.

Even preserved battlefields have *tons* of material though! It just isn't out in the open like that. Trenchlines are often preserved, but natural erosion and weather means that they don't look as sharp, and wood elements such as the duckboards would long ago have rotted of course. Plenty of weapons are left behind, but by 30 years on, most would be buried, and require excavation to reveal. Even the best preserved battlefields occasionally have new finds of material that wasn't recovered the first time around. It is perhaps ironic that the *closest* thing you might find to ignoring such matters would be in the North. Few major engagements took place there, of course, but for the ones that did, while there was great concern about preserving the Northern honor, less attention was paid to the South there. At Antietam, for instance, while the Union dead were moved to the cemetery quickly, Confederate dead, buried on the battlefield itself, were mostly not dealt with for another decade, finally unburied in the 1870s, although not placed in the National Cemetery of course.

But in any case, to return to the original point, by 1899, many battlefields were well on their way to preservation as National Military Parks, and even those which didn't receive such treatment were hardly abandoned immediately after the battle to never be handled again. Even the small engagements saw efforts to clear up and memorialize the dead, and it is quite unreasonable to consider the representation of a former battlefield in RDR2 as close to reality. But it shouldn't be understood on those terms in any case, but rather understood for the Western tropes that it represents and the important place Civil War Memory has long held in the genre.

**Sources**

Adams, Michael CC. *Living hell: The dark side of the Civil War*. JHU Press, 2014.

Cashin, Joan E., ed. *War Matters: Material Culture in the Civil War Era*. Chapel Hill: University of North Carolina Press, 2018.

Janney, Caroline E.. *Burying the Dead but Not the Past: Ladies' Memorial Associations and the Lost Cause.* University of North Carolina Press, Jan 2008.

Sellars, Richard West. *Pilgrim places: Civil War battlefields, historic preservation, and America's first national military parks, 1863-1900*. Vol. 3. Fort Washington, PA.: Eastern National, 2005.

Slotkin, Richard. *Gunfighter Nation: The Myth of the Frontier in Twentieth-Century America.* University of Oklahoma Press, Dec 1992.

Stone, Richard D, and Mary M Graham. 2007. “Selective Civil War Battlefield Preservation as a Method of Marketing The Southern ‘Lost Cause.’” In *Conference on Historical Analysis & Research in Marketing*, 221–27. Durham, NC.

Sutton, Robert K. ""Commemorating the American Civil War in NAtional PArk Service battlefields"" in *The heritage of war*. Gegner, Martin, and Bart Ziino, eds., Routledge, 2011.",0
"Speaking of medieval western Europe: for leisure, no, not really. The rise of beach travel is generally associated with the nineteenth and early twentieth centuries.

The foundation of this shift is changing ideas of the ocean. While I personally think the evidence from literary sources has been overstated in scholarship, it's pretty clear that people in the Middle Ages viewed the ocean as dark, forbidding, and dangerous, even apocalyptic. A whale washed up on a Dutch beach in 1522, and Martin Luther believed it was a sign of the apocalypse. Near Eastern and European travel accounts are filled with stories of sea monsters eating giant chunks out of ship hulls. The biblical story of Jonah being eaten by a very big fish was equated, in Christian preaching, to people going to hell.

The rehabilitation of oceans and large seas (Mediterranean, North, &c) in the western cultural imagination came in the late 18th and throughout the 19th century. Scholars such as John Walton and Lena Lencek find the roots of the transformation in, of all things, spa culture.

Bathing in hot springs had long been justified as medicinal. (Public baths also had...other...reputations in medieval and early modern Europe). Changing ideas about medicine and hygiene in the beginnings of the modern era increased the importance of the *water* aspect of hot springs--including the utility of cold water, which was accessible to northwest Europe in the non-polluted form of the sea. (A lot of Walton's work is about the northern coast of Spain.) Another important factor in transforming the sea embraced the primitive: the ""back to nature"" attitude, the idea that the untouched and never-civilized *wilderness* is something beneficial to people.

But this covers the *sea* part of things. As to the beaches? Well, Samuel Coleridge (1772-1834) has you covered on the difference, with a good dose of the swirling mists of the capriciousness and importance of nature for humanity:

> God be with thee, gladsome Ocean!

> How gladly greet I thee once more!

> Ships and waves, and ceaseless motion,

> And men rejoicing on thy shore. 

> [...]

> Fashion's pining sons and daughters,

> That seek the crowd they seem to fly,

> Trembling they approach thy waters;

> And what cares Nature, if they die? 

...of course, not everyone ""revisiting the seaside"" is so in touch with the raw wilderness of the sea and the sky and the human soul as Coleridge. He wryly points out that a whole lot of people are there essentially to See And Be Seen. And according to scholars, he wasn't wrong.

Now, all of this is not to say that medieval people didn't know how to have fun in *water* or alongside the water. Fifteenth-century German friar (basically a monk who is not restricted to a monastery and ministers to lay people in the world) Felix Fabri made two pilgrimages to Jerusalem, and wrote rather entertaining accounts of it. Fabri's text has to be interpreted through the genre of 15C travel narratives, which (among other things) stress or invent the exotic in order to entice readers (among other reasons). He also has, or pretends to have, a religious agenda in teaching his readers spiritual lessons or affirming the preeminence of his religious order, the Franciscans. Nevertheless, he tells some great stories--and they were stories that his readers would have to understand as plausible.

So let's talk about the Jordan River. Because Fabri did.

This was an important pilgrimage site for those taking the extended journey beyond the city of Jerusalem itself. Fabri tries to lend the whole visit a spiritual veneer--that he really was there to stand in the same waters where Jesus was baptized and reflect on the moment--but this is one place where his literary efforts kind of fall apart. No matter how much he talks about saying Mass and quotes Latin prayers, he can't hide from:

> So we stood in the water with
great delight, and jestingly
baptized one another.

But swim across the river? Oh, the Muslim guides tried to warn all the Christian pilgrims off of *that*. People who thought it was fun to play underneath the water often got sucked down into the mud and never came back. Creatures from the Dead Sea might swim up the river and eat you. (Sea monsters, anyone?)

Of course, pilgrims did it anyway. Including Fabri.

At least, male pilgrims. Fabri notes that while the men were daring each other to go further and further out into the river, the women waded in, indulged in mock baptisms, and...hung out on the shoreline.

It's not quite a beach party. And the overall story makes it clear that the pilgrims need a justification for playing in the river. While traveling in medieval fashion--pilgrimage--""going to the beach"" was not legitimate. But seizing an opportunity to splash along the shoreline or hang out on the beach and gossip with your friends a little--sure. Just make sure that sea monster doesn't seize *you*.

Further Reading:

* Susan Anderson and Bruce Tabb (eds.), *Leisure, Water, and Culture in European History* (2002) - most of the essays deal with 20C, but there are some good, relevant ones in there
* Lena Lencek and Gideon Bosker, *The Beach: The History of Paradise on Earth* (1998) - not the only history of beaches and beach travel out there, but a solid and well-written one
* In 100% seriousness, for more information on this exact topic, check out my book [How to Slay a Dragon: A Fantasy Hero's Guide to the Real Middle Ages](https://www.simonandschuster.com/books/How-to-Slay-a-Dragon/Cait-Stevenson/9781982164119), where you can learn how to *survive* that sea monster's jaws using stories of medieval people who did just that. (Also how to survive cannibals, pirates, and *cannibal pirates*. The Middle Ages are truly living their best life.)",0
"The exact number of men who died while digging trenches in World War 1 is difficult to determine. However, trench construction was indeed a hazardous task that resulted in numerous casualties. Trenches were often dug under extreme conditions, including heavy shelling, sniper fire, and exposure to harsh weather. The construction of trenches involved manual labor, with soldiers using shovels, picks, and other hand tools.",1
"I think you kind of answered your own question already: it all depends on what specific situations we are talking about here. There is no generic answer to the question, ""What did someone -- anyone -- take with them when they left the house in ancient times?"" Hence my example of the spear in Archaic Greece, and that one would bring a hat and supplies for a longer journey, and so on.

There simply is no one-size-fits-all answer. Wealthy Romans were sometimes transported in palaquins, like Cicero, as a symbol of their status, like a wealthy person today would be driven around in a fancy car; a returning Roman general would be driven around in a chariot and wearing a laurel wreath; a regular Roman farmer going to town to get supplies would probably be accompanied by a slave; and so on. Archaic and Classical Greek vase-paintings suggest that women could tie small *aryballoi* (perfume bottles; they contained scented olive oil) around their wrists, so they would presumably bring those with them, and they also wore various types of jewellery (again, depending on their personal status/wealth).

As regards religious ""relics"" (technically a very particular kind of object) would not be brought outside except on special ocassions, like religious processions. But ancient people did wear objects that were believed to hold some kind of magical power, most notably amulets, many of which have been unearthed in tombs; pendants in the form of scarabs from ancient Egypt are a good example.",0
Your answer about Satan ruling hell is one of my favorite answers I’ve seen on this sub!,0
"The linked post by [/u/kieslowskifan](https://www.reddit.com/u/kieslowskifan/) will answer most of your questions, but I'll have a quick stab at the last bit. I'll include some references as much of this is paraphrased from an essay I did on this for Uni last year.

>Did he try to distance himself from other fascist regimes in order to  appear more ""moderate""? I hear he is often considered a ""semi-fascist"",  did that play into it as well?

Franco & Fascism is a big topic. Some key points are:

* There were some real Fascists in Francoist Spain, the ""Falange"". They were the only permitted political party\[1\] and much like the regimes in Germany and Italy, the party became *big.*
* However it wasn't quite so active in carrying out large projects (like the ""Final Solution"") as the Nazi party was, and became more of a social institution that everyone joined in order to get ahead socially & in their careers.\[2\] Paxton argues that Franco worked to weaken the Falange and subjugate them to his own personal control by making them the sole political party and thus forcing the large conservative, monarchist (importantly both were *very* Catholic, with the church being much more powerful in Spain than in Italy or Germany)\[4\] groups to join the Falange and thus change its makeup.\[3\]
* Payne argues that as early as 1942, Franco was working to remove Fascism from his regime.\[5\] He had accepted their help in the Civil War (and indeed a large amount of the ""first-rank"" Falangist leadership were 'lost' in that time)\[6\] but this can be considered strategic. They had an ideological similarity in that both Franco and the Falange were radically anti-Bolshevik and at least somewhat yearning for an idea of Spain before the Republic\[7\] (the Republican government could be construed as ""anti-clerical"", and thus anti-status-quo, and indeed was by Conservative elements)\[8\], but the original Falange *did not like Franco*. In 1940, they even tried to assassinate him.\[9\]
* Franco accepted help in the Civil War from the ""Falange"" within Spain but also from Hitler and Mussolini. This was not something he reciprocated in any meaningful way.\[10\] He was opportunistic and ruthless, no doubt, but he was never a committed member of any kind of international Fascist alliance, he was an ally of ""convenience"" more than anything. He supposedly (I can't find the reference, sorry) offered Hitler that he would join the war in return for some terms that were completely unacceptable to Hitler (large amounts of money, weapons etc).
* Not to labour the point too much, but, *Catholicism*. There is really no way for Fascism and a strong religious institution to co-exist, and Spain had/has a very strong religious institution. This is the main ingredient that is missing, as Franco absolutely fills many other criteria (Authoritarian strongman, 'ultra'-nationalism, some limited 'palingenesis' A.K.A 'national rebirth', and a 'stab-in-the-back' myth a la the Republican government + Bolshevik partisans towards the Catholic church). Franco was (much like his Portuguese contemporary Salazar) a dictator from the far right of the 'political spectrum', which is certainly similar to Fascism but lacks the fervour, radicalism and indeed expansionism of the very particular ideology of Fascism. Franco was much more ""anti-\[Liberalism, Bolshevism, Trade-Unionism, and so on\]"" than he was ""pro-"" anything (except maybe Catholicism).

I'm not sure I answered your question perfectly well, and I didn't want to produce anything too verbose and inaccessible, so please ask if there's anything you want me to clarify/expand on and I'll do my best.

&#x200B;

\[1\]  Thompson, W., 2011. Fascism. In: *Ideologies in the Age of Extremes: Liberalism, Conservatism, Communism, Fascism 1914-1991.* London: Pluto Press, pp. 85-103 , *Fascism*. P102

\[2\] Payne, S. (1999). *Fascism in Spain, 1923-1977.* Madison: University of Wisconsin Press.  p284-285

\[3\] Paxton, R. O. (2004). *The Anatomy of Fascism.* New York: Alfted A. Knopf. P150

\[4\] Blinkhorn, M. (2003). *Fascists and conservatives: the radical right and the establishment in twentieth-century Europe.* Abingdon: Routledge. P144

\[5\] Payne, S. G. (1996). *A History of Fascism, 1914-1945.* Madison: University of Wisconsin Press. P435

\[6\] Blinkhorn, *Fascists and conservatives*. P133

\[7\] Campos, I. S. (2004). Fascism, Fascistization and Developmentalism in Franco's Dictatorship. *Social History*, 324-357. P346

\[8\] Payne, *Fascism in Spain*. P7

\[9\] Payne, *A History of Fascism*. P431

\[10\] Payne, *A History of Fascism*. P239",0
"The former, other than the leather scene and the drag scene, there wasn't any outlet for one's sexuality or individualism in American culture to my knowledge. There were bathhouses and cruising culture (the colored hankies, established ""cruising areas), but AIDS also created a *heavy* stigma around this activity because....AIDS. The movie *Stonewall* gives a really good look at the community feminine gay men got to be a part of, which mostly consisted of dressing up and going to the gay bar, to later be beaten by a bunch of police officers.",0
"An addendum here (which I'm sure you know well, but for anyone reading): it is important to note that experiences vary widely depending on what *kind* of slave we're talking about. Slaves working on latifundia (Roman plantations) would by the nature of their work and location have a different relationship with their owner, and in fact the social dynamic among slaves on a latifundium at least superficially mirrors that of a chattel plantation in the North American South. A slave working grain or olives would have less potential for social advancement than a villa rustica (plantation house) slave might, and in turn that villa slave would be limited in their own advancement compared to one working in the city domus (a home located in a city or large population center) or even a villa urbana (a villa located outside of a city but close enough to allow for easy travel into the city). 

Ultimately a potential for freedom through technical skills was something reserved for the most prized members of a masters slave stock - like you mention skilled artisans, tradesmen, those that could handle the books, and tutors. These individuals would have accounted for a minority of any given patrician or equestrian's human property, the vast vast vast majority relegated to unskilled and often highly dangerous manual labor on the aforementioned latifundia, in quarries/mines, as oarsmen for a vessel, or laborers on construction projects. For these slaves there was little hope of freedom, for most death as a slave with the hope that their child might be able advance beyond their station was the best they could achieve.

I'd also like to mention that chattel slavery in the Americas can often lead individuals to misleading conclusions with regards to Greco-Roman slavery. Whereas in America the narrative of freedom is often understood as something a slave might have to work out on the side, often under the radar of their master for fear of a moving goalpost, the relationship between a master and a skilled slave in Rome would have been very different. As you mention a freedman never truly leaves the retinue of their former master, instead their status is elevated to a member of the family (not a *full* member of course, that would be quite the scandal - save for the most exceptional of slaves). Beyond the financial and service requirements a client (the freedman) would owe to their patrons (a dynamic that runs through every rung of Roman society - everyone was a client to someone, except the Emperor of course), that client would never cease to contribute to their patrons status. Everything they do, all of their accomplishments, would contribute to the reputation of the family and the pater familia (father of the family - their patron).

This status - or gravitas as they would have called it - isn't some abstract construct placed on Romans by later historians, this was the very real ever present social capital driving society. Romans of every stripe were in an unspoken competition of achievement, who could be the most Roman. Not only with each other, but with their ancestors too. If one did not at least meet their achievements of their forefathers it could spell doom for a family, a signal to rivals that the house was in decline. We could spend all day discussing how gravitas and Romanitas (""Roman-ness"") impacted Roman society, but the point I'm getting at here is the achievements of a skilled slave, elevation to freedman status, and their accomplishments as a free citizen all contributed to a master's gravitas. Often elevation would have been the culmination a choreographed plan instigated by the master almost immediately upon acquisition. I say this to stress that manumission was not a gift of benevolence, but rather one of many tools in the arsenal of Roman elite to elevate their status and the status of their family. Rome was **the** slave society of antiquity, and there was no aspect of the system that was not intended to serve those in power first and foremost.

Sources: Gonna be honest much of this is derived from my lecture notes I took in a class on Roman slavery - but some great sources on the subject that much of this is pulled from are


Plautus; ""The Prisoners"", *The Pot of Gold and Other Plays*. Penguin Classics, 1965. (along with kind of all of the plays in the collection, but The Prisoners especially)

Shaw, Brent; *Spartacus and the Slave Wars: A brief History with Documents*. Bedford,2001.

Wiedemann, Thomas; *Greek & Roman Slavery*. Routledge,1981.

And of course the histories of Livy, Plutarch, Polybius, etc.",0
"This is a great point and something that dogged Mussolini's regime from the get-go.

Unlike in Germany, the enthusiasm for Italian Fascism never quite achieved that ""totalitarian ideal"" that the Nazis achieved.\[1\] Paxton writes that “Nazi Germany alone experienced full radicalization”, citing the 'Total War' they engaged in as the catalyst for this extremity.\[2\] The extremity of the war the Nazis engaged in has to be the primary cause, however we can't ignore that both the Italian Monarchy & widespread Catholicism persisted through any attempt at Fascist-ising Italians. Yuval Noah Harrari (an excellent author) has made the point that in Italy, ""family"" was the institution that Fascism could not overcome.\[3\] This is a great perspective especially when we consider *just how invasive* Nazism became in the institution of the family. Mussolini was not able to replicate this, and I think this is inseperable from Catholicism (and Christianity generally) and its ideas of the family & its place in society, i.e. the primacy of the family structure even above the needs of the nation.

I would point out that the Monarchy & Church tolerated Mussolini and his Fascists whilst he was winning back territories in Africa and keeping social order, but they were not there to defend him when the tides of war turned; indeed they have survived where Mussolini has not. Whereas Germany engaged in a brutal fight to the last man in 1945, Italy capitulated relatively easily.

Really importantly for analysing Fascism (and this comes back to my notes on Franco), is 'The Party'. In Nazism, the Nazi Party became a formidable institution in its own right, rivalling the state and more or less replacing it in many regards. This is what is referred to as a ""parallel institution/power/structure"", i.e. an institution that rivals a specific function of the state (or for Nazism, eventually almost every function).\[4\]\[5\] A function in this context being something like providing policing, food, jobs, or for Mussolini the ""Blackshirts"", who originally sold their services to business owners looking to break up protests.\[6\] These are really key for Fascism and to cut a long story short: Nazism found more success with its parallel institutions than the Italians did.

We must also consider the rise to power that Fascism takes. As Paxton tells us in his *Five Stages* (a worthwhile read by any account), Fascists need the backing of conservative political forces to get to power.\[5\] This is a compromise that will either bring down the Fascist regime when the consent is withdrawn (as in Italy and, to slightly muddy the waters: Spain)\[7\], or will solidify a Fascist regime if it is made reduntant (as in Nazism). The phrase goes ""In the countryside where fascism starts""; that is to say, rural and ferverntly conservative elements in society are the core demographic of at least early Fascism. To answer your question: Hitler was able to beat the compromise, Mussolini became stuck in it. By the time the war was becoming unwinnable for the Axis, Mussolini was still engaged in a balancing act of powers that he had failed to win, and he paid the ultimate price for that.

This should be enough to answer your question, but I have written slightly more that is a bit more wooly about the ideologies if you're interested. Not everything below here has a source, and it isn't meant to be taken as firmly as my first section.

====================================================================

Interestingly, there is something to be found in the ideological genealogy of Nazism and Italian Fascism that explains how they turned out. Italian Fascism 'came first', for want of a better phrase. Whilst it is slightly reductionist, Hitler and the Nazis can be considered to have built upon Mussolini's success. They obviously went in a *unique* direction (read: horrific), and they specifically included a lot more antisemitic and racial theories that Mussolini did not. This is to say, even though Italy & Germany are widely considered to both be the original ""Fascists"",\[7\] they are actually fairly distinct. This is a whole topic in itself, which I won't digress into too far, but the key point to answer your question about how Italian Fascism coexisted with Catholicism is to do with the genealogy of Italian Fascism (and how Nazism was able to move away from that and become even more radical).

Italian Fascism, the rise of which ultimately has to be 'credited' to Mussolini, was the culmination of a fair variety of ideologies & ideas that were in the contemporary far-right milieu (and interestingly ideas seen now as far-left, making the traditional political spectrum somewhat difficult for understanding Fascism). Mussolini actually started out on the far-left, and was interested in creating a coalition of Veterans from the left and right.\[8\] Put bluntly: this didn't work, and eventually he found himself backing the ""Squadristi"" who were by all accounts quite far right. The original ideology that led him to thinking a left-right coalition could work can be traced to a movement called the ""futurists"". Mussolini was pretty into them, and the ""Futurist Political Party"" was absorbed by him into his new Fascist party in 1919. The movement was full of some weird stuff, but it essentially was concerned with modernisation and nationalism. This is pretty different from how Hitler found Fascism (essentially through militaristic, ultranationalistic and antisemitic veterans) and so I think Mussolini came at Fascism from the get-go not realising that an absolutist party and dictatorship (with no rivaling social institutions) was the only way for his regime to survive. Even if he did realise this, he probably would have needed a lot of time and effort to overcome the influence of monarchy & Catholicism in Italian society and this was not permitted by the war.

Essentially my argument is that the Italian regime was fundamentally a lot more fragile than Nazism, precisely because of Monarchy and Catholicism. Counter-factuals are difficult, but if the war did not break out it would probably still have been very difficult for Mussolini to assert his authority in the same way Hitler did. By most accounts, the war in Ethiopia was a political ploy to solidify Mussolini's domestic position. The absolute fight-to-the-death that Germany engaged in against the USSR, USA, UK + allies was never going to happen in Italy because of this fragility. 

Fascism cannot coexist with a strong religious institution, and I suspect, given enough time, either Mussolini or the Catholic church would 'blink' and Italian society would have been either reverted to a pre-Fascist state, or have been irrevocably altered.

N.B. I have leant on Paxton fairly heavily here. I believe he is an excellent & readable source and that is why I have used him so much, but I understand this may not be satisfactory for everyone. If you want other sources for anything, I can absolutely try and provide them.

\[1\] Heywood, A., 2017. Fascism. In: *Political ideologies: an introduction.* London: Palgrave Macmillan, pp. 194-219. P194, P207

\[2\] Paxton, R., 1998. The Five Stages of Fascism. *The Journal of Modern History,* Volume 70, pp. 1-23. P20

\[3\] Harari, Y. N., 2018. *21 Lessons for the 21st Century Kindle Edition.* 1 ed. London: Vintage Digital - Jonathan Cape. location 4426 

\[3\]  Paxton, *The Five Stages.* P14

\[4\] Paxton, R. O., 2004. *The Anatomy of Fascism.* New York: Alfted A. Knopf. P149

\[5\] Paxton, *The Five Stages*. P13

\[6\] Paxton, *The Five Stages*. P14

\[7\]  Blinkhorn, M., 2003. *Fascists and conservatives: the radical right and the establishment in twentieth-century Europe.* Abingdon: Routledge. P11

\[8\] Paxton, *The Five Stages*. P15",0
"#Was Gunpowder Regularly Consumed?

I would not go so far as to say that it was regularly consumed. But it certainly was not unheard of. 

##Gunpowder and Saltpeter

Gunpowder consists of sulfur, charcoal, and saltpeter (potassium nitrate). The latter of these three had been used to cure meat since the Roman times. Reportedly, ""by around 200 BC, the Romans recognized that salt from some sources contained contaminants that contributed a reddish-pink color and flavor to cured meats"" [1]. This contaminant was none other than saltpeter. 

By the mid-17th century saltpeter was widely used as a ""relatively [available]"" meat preservative [2]. At times gunpowder would be similarly used, on the basis of its saltpeter content. Rabbi Henry Cohen, writing in 1900, relates the tale of a US military expedition to confront American Indians on the frontier, as they sought to oppose the expansion of the railway onto their lands. Following a brutal battle, the American garrison's supplies began to run low. Cohen explains that after four days under siege by the Indians, ""the meat [...] began to get putrid, and it was sprinkled with gunpowder, in the hope that the saltpeter in the powder would aid in preserving the meat or make it a little less unpalatable"" [3]. (Alas, it was not effective). Thus there was clearly widespread knowledge of the use of saltpeter as a preservative, and under extreme circumstances, gunpowder might be used as a source thereof.

##The Link between Gunpowder and Rum

With more relevance to Blackbeard's drink, the mixture of gunpowder and rum would have been fairly well known. Not for consumption's sake, but to establish the rum's proof. Sailors would mix their rum ration with a bit of gunpowder, and expose it to a flame. If the gunpowder caught, that was said to be ""100% proof"" that it had not been watered down (57.15 was the necessary alcohol by volume for it to catch) [4]. That said, there's no indication Sailors mixed the two for purposes other than this.

----

#Gunpowder and Rum in Jamaica

However, the mixture of rum, gunpowder, and other substances *does* appear in various Caribbean rituals and religions, most notably in Jamaica.

The ritual consumption of rum and gunpowder stems from Tacky's rebellion of 1760. Erskine explains that Tacky's ""warriors prepared for war by mixing rum with gunpowder and grave dirt,"" adding a bit of blood from each participant, and subsequently sharing the drink amongst one another [5]. 

The ritual persisted past Tacky's rebellion, and became a part of the Jamaican religion *Obeah.* The ritual and other elements of the religion ""were passed like heirlooms between successive generations of freedom fighters."" The drink marked the rebels' pledge of loyalty to one another and to their cause [6].

The tradition lasted at least until 1865, when Paul Bogle, a Baptist minister and one of Jamaica's National Heroes, performed it as part of his own freedom movement [6]. We find mention of it in British Governor Edward John Eyre's 1866 report before a royal commission. Eyre describes that after men swore an oath to the Bogle's rebellion, they were ""then given each a dram of rum and gunpowder to drink"" [7]. Eyre's account stems from the testimony of rural constable James Foster, who describes [7]:

> Paul Bogle spoke to the men in a language I did not understand. The men then took the oath; they kissed a large book, the Bible. Paul Bogle gave each of them a dram of rum and gunpowder, which they drank. I saw the rum and powder mixed myself in a large bottle.

---

#Other Questions

##Was it Dangerous?

In small doses, no. However, Thomas Trotter, a British naval physician from 1779-1802, comments on the occasional consumption of gunpowder by sailors for the purpose of feigning illness [8]:

>When at Plymouth in October last, a paper, taken from a sailor was put into my hand by Dr. Mein of the Caton hospital ship from which it appeared that deception was become a complete and scientific system. Among other articles to produce spitting of blood, gunpowder or salt petre is taken in large doses; haemoptysis quickly follows from the blood being overcharged with oxygene gas which these ingredients readily afford.

##What was its Flavour?

Charcoal tastes bad (multiple pediatric studies of activated charcoal indicate children often refuse it) [9]. Sulfur is flavourless. Saltpeter has a ""cooling, saline pungent taste"" [10]. Thus, the combination of all three would not taste particularly good, but any flavour would likely be drowned out by the rum.

---

**Citations**

1. Bedale W, Sindelar JJ, Milkowski AL. Dietary nitrate and nitrite: Benefits, risks, and evolving perceptions. Meat science. 2016 Oct 1;120:85-92.

2. Edwards ME. Virginia Ham: The Local and Global of Colonial Foodways. Food and Foodways. 2011 Feb 9;19(1-2):56-73.

3. Cohen H. A Brave Frontiersman. Publications of the American Jewish Historical Society. 1900 Jan 1(8):59-74.

4. Rogers A. Proof: the science of booze. Houghton Mifflin Harcourt; 2014 May 27.

5. Erskine N. The roots of rebellion and Rasta theology in Jamaica. Black Theology. 2007 Feb 1;5(1):104-25.

6. Barima KB. Cutting across space and time: Obeah's service to Jamaica's freedom struggle in slavery and emancipation. Journal of Pan African Studies. 2016 Jul 1;9(4):16-32.

7. Papers Laid before the Royal Comission of Inquiry by Governer Eyre. British Parliamentary Papers. 1866. 

8. Porter IA. Thomas Trotter, MD, Naval Physician. Medical history. 1963 Apr;7(2):154-2.

9. Skokan EG, Junkins EP, Corneli HM, Schunk JE. Taste test: children rate flavoring agents used with activated charcoal. Archives of pediatrics & adolescent medicine. 2001 Jun 1;155(6):683-6.

10. National Center for Biotechnology Information. PubChem Compound Database; CID=24434, https://pubchem.ncbi.nlm.nih.gov/compound/24434.",0
"This is an excellent question, and I'm going to complicate it by asking, ""what do you mean by power?"" The Inca had a complex interweaving of social, sacred, economic, and political power, and the history of the Andes had examples of women attaining power of various stripes long before the Inca. To give two brief examples predating the Inca:

Archaeological excavations in coastal Ecuador at the site of Capa Perro recovered the burial of a shaman, or perhaps an initiate, of a young woman, some 14-18 years old. She was interred with numerous accoutrement depicting her role in society: a *coquero* (perhaps antique for her having been passed down), which is a small lime pot used to activate coca leaves' alkaloids while chewing; greenstone pendants (green being a collor of fertility and preciousness throughout the Americas) and a fruit bat mandible found in the abdominal region; and a fragmented ceramic figurine that was found in the disarticulated remains of a feline snout. [More recent work in Bolivia](https://www.forbes.com/sites/kristinakillgrove/2019/05/06/ancient-bolivian-fox-snout-pouch-contained-cocaine-and-other-hallucinogenic-drugs/#7aedc1bd2b8a) has recovered a fox-snout pouch with residues of hallucinogenic drugs, and it's possible we are looking at another example of one of these Andean ""bags of holding"" for shamanic paraphernalia. This young woman dated from Terminal Valdivia times, some 1800-1500 BC - so at this early time within Andean social complexity, some societies held women as authorities in religious matters.

Let's turn the clock forward a bit more to the Moche of the north Peruvian coast. There, at the site of El Brujo, Peruvian archaeologists recovered a female mummy now known as the ""[Lady of Cao](https://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Reconstitution_de_la_Dame_de_Cao_%28Mus%C3%A9e_du_quai_Branly_-_Jacques_Chirac%2C_Paris%29.jpg/375px-Reconstitution_de_la_Dame_de_Cao_%28Mus%C3%A9e_du_quai_Branly_-_Jacques_Chirac%2C_Paris%29.jpg)"". As you can see from the picture link I just attached, the lady of Cao was interred with a remarkable array of gold, including a sizable hammered-gold headdress and two massive golden war clubs (ceremonial). The Lady of Cao has been posited as a ruler or dynast of El Brujo, itself a sizable Moche settlement - so in the early centuries AD among the Moche there is evidence of women within the ruling class not only in a religious sense, but politically, with implications for military strategy and legitimacy.

With some of this historical background in mind, let's look at the Inca. The position of Sapa Inca, in the chronicles and dynastic lists compiled, were always men. However, their queen, the *Qoya*, was also imbued with immense power. Being of Inca stock herself (by blood - think Targaryens or Habsburgs, so sisters or first cousins), the Qoya provided counsel to her husband and acted as a chief political advisor and ally for the emperor's policies. Being the primary Qoya carried immense weight, as queen or queen mother. However, the Sapa Inca married secondary wives as well, often to cement political alliances with ethnic groups to be brought into the imperial fold. Wayna Qhapaq had something like a dozen wives, and some fifty children, which is what precipitated the fraternal civil war of the 1520s, when he and his chosen heir Ninan Kuychi died unexpectedly. The Qoya, being part of the Inca extended royal family, held especially great power in determining how these heirs were selected; this is because a Sapa Inca, rather than strictly taking his father's name and kin group, founded his own kin group or *panaqa* (some have called these royal corporations, or dynastic lineages). Each new panaqa, while claiming legitimacy from their father going back to Manqo Qhapaq (the mytho-historical first Inca), seated their power in practice to the ruler's mother. This meant they were often the primary advocates for their sons' political fortunes, and commanded immense wealth which they had full autonomy to distribute. (Unfortunately, much of this courtroom intrigue has been lost to history.)  As in the cases of Inca mummies of the Sapa Inca, the Qoya's mummy and image also maintained this power after death. However, this heightened power also meant higher stakes - Wayna Qhapaq, upon taking the throne in the typical coup of legitimacy, banished the failed heir to the throne but executed his mother.

This is all in addition to the institution of the *aqllakuna*, the Chosen Women commonly referred to in surveys of the Inca. These high-status retainers selected by the Inca, and secluded in *acllawasi*, were essentially given a cotillion of sorts, with training in textile production, political and military strategy, religion, and history; their destiny was that of being consorts to the Inca elite (or their mummies), or else being tactfully deployed wives for marriage to potential allies and subjects. As you might expect, however, our knowledge of women in Inca society, as elsewhere in history, is limited when compared to men, and often colored through male gazes and voices.

EDIT: Whoops, sources:

D'Altroy's Incas has a good section on the queen for more information, that's what I consulted for this.

Zeidler, James A., Peter W. Stahl, and Marie J. Sutliff
1998 Shamanistic Elements in a Terminal Valdivia Burial, Northern Manabí, Ecuador. In *Recent Advances in the Archaeology of the Northern Andes: In Memory of Gerardo Reichel-Dolmatoff*, edited by Augusto Oyuela-Caycedo and J. Scott Raymond, pp. 109–120. Cotsen Institute of Archaeology, UCLA, Los Angeles.

Ziolkowski, Mariusz
1996 La guerra de los wawqi: los objetivos y los mecanismos de la rivalidid dentro de la elite Inka, siglos XV-XVI. Ediciones Abya-Yala, Quito.",0
"Speaking from our perspective as mods, we too have also heard some unhappiness from contributors and users about these questions and that this has developed into a ""thing"". We even talked about it [in the latest Friday-Free-for-All thread](https://www.reddit.com/r/AskHistorians/comments/682ta1/friday_freeforall_april_28_2017/dgvyq80/).

These questions have now become a thing that will most likely disappear again soon (though I have to say, I am a bit disappointed I haven't had to remove [I am a hot blooded foreigner and I've got a fever of a hundred and three. Baby, do you do more than dance?](https://www.youtube.com/watch?v=5c1m2BAg2Sc) yet) but the ""I am a ..."" questions have long been, if not controversial, at least a point of minor disagreement among user and contributors on this site. Some people think they are immensely annoying (and every time one comes around, there is at least on person thinking they are super original by posting ""No, you are not"") while others like them.

Because we are historians, I think it's fair to talk about this question format and its relation to historiography and generally how we ""do"" history.

I'm generally more on the pro-side of the ""I am a ..."" questions. The reason behind this is that they are a chance not only to highlight the history behind the question but one of the major factors we grapple with in the study of history pretty much all across the board: The difficulty of accessing the lives of the people who make up the vast majority of all history: the ordinary people, the farmers, wage laborers, artisans, butchers, bakers and candle stick makers; the poor, the downtrodden, the women, and the slaves – in short, all the people who were not kings, generals and politicians and who made the whole place hum and work. Questions like what they did for fun, what their aspirations and every day concerns were, how they perceived themselves and the world around them is an immensely interesting subject, especially given that unlike kings, generals, and other ""great man"" figures, they comprise the majority of the people in history.

The problem here and it is a problem that has been famously addressed with the question [Can the subaltern speak?](https://www.reddit.com/r/AskHistorians/comments/384ty2/monday_methods_can_the_subaltern_speak/), meaning that how do we address writing about people who themselves have left no writing but have only been written about by others. Where we can address them and their perspectives – such as in the case of *The Cheese and the Worms* or *The Return of Martin Guerre* (if you haven't, check out the book or at least the movie. Seriously, it'll be worth your time) – the ""I am a ..."" question gives us the chance to share what can be gleaned from these works of scholarship and from the study of the historical perspective of the ""ordinary"" person. Plus, it is immensely refreshing, in my opinion, that through the thick of all the questions about what Adolf Hitler liked to have for breakfast and what he thought about hat fashion, there are questions that take into account that history is more than just the story of men like Hitler, Churchill, Stalin etc.

On the other hand – and here's where we get to the criticism of these questions – many who don't like them assert that questions of similar content and with a similar focus could be asked be better and the ""I am a ..."" format always includes both elements of presentism, meaning that they project things we do today back in the past like ""going out with your mates"", and are still pretty limiting, as for example the latest wave of ""hot blooded questions"" for the most part completely left out women of the equation (save a few which have not been upvoted much).

And they are right in their criticism: Many of these questions imply certain behaviors as anthropological in the sense of eternal, natural, and present throughout history as well as again limiting the scope of questions to the historical subjects the asker can imagine while those who are usually not afforded much of a subjectivity in current social narratives are also left out of these questions.

So, indeed, these questions can be formulated and asked better (as can [the Hitler questions btw.](https://www.reddit.com/r/AskHistorians/comments/4cewq8/on_adolf_hitler_great_man_theory_and_asking/)). At the same time, this is something contributors can address (as has /u/Tiako in the Roman version of this questions that started it all) and is also among the reasons why we as mods would not disallow these questions as we won't disallow the Hitler questions (be warned though, there is a limit to what we are willing to accept in terms of these questions in the name of common decency. ""I'm a member of the Einsatzgruppen. How can I unwind after killing 30.000 Jews?"" will not fly here. Period.)

Our general philosophy in terms of questions is to approach it very light handedly. After all, people are here to have their questions answered and we give questions a lot of leeway, including allowing those with false premises. We are here for education after all and limiting questions heavily to the ones we want to be asked would not really serve that purpose at all.

So, maybe it would be time for us to re-run and/or re-write some tips on how to ask better questions (or as Zhukov put it once to make not-stupid-question not-stupider), including a specific address of these kinds of questions.

",0
"There's an implication of a misconception in your question that I think is addressed by the appendix to [William Page Wood's 1859 speech](https://books.google.com/books?id=KzJcAAAAQAAJ&pg=PA37#v=onepage&f=false) on the topic:

>The state of our law is singularly misunderstood, not only ""out of doors,"" but by many members of Parliament. It is supposed that because the marriage with a wife's sister was *voidable* only, and not *void* until Lord Lyndhurst's Act, there was a species of half sanction to such unions. Now, the fact is, that no such marriage was ever in the smallest degree sanctioned; but the Courts of Common Law would not allow any proceeding in the Ecclesiastical Court to set them aside after the death of either party, so that after the death of husband or wife there was no mode of obtaining a judicial decision, and of course all marriages actually solemnized are good till such sentence is given. The best mode of making this understood is to call attention to the case of marriage with a man's own sister or mother, being in precisely the same position, and in the same sense voidable only, not void.

That is, while the 1835 act made it illegal to contract such a marriage, it had always been considered invalid under English common law for a man to marry the sister of his wife - and for a man to marry his brother's wife. It's just that it was previously more like grounds for annulment, someone would have to say, ""hey, by the way, judge, my marriage was made on an illegitimate basis, please invalidate it,"" if they wanted to end it. After the act - or, actually, in Scotland from 1567 on - it simply couldn't happen/wasn't considered a real marriage. Ginger Frost cites the specific 1859 case of Fenton v. Livingstone in *Living in Sin: Cohabiting as Husband and Wife in Nineteenth-Century England*: the first wife of Thurstanus Livingstone (yes) died and in 1808 he married her sister, subsequently having a son with her; in 1832, his second wife died without ever going to court to have the marriage voided. After Livingstone died, his son inherited land from him in England and Scotland, but the Scottish courts refused to allow him his inheritance because, by their standards, his parents had never been married and he was illegitimate.

The basis for the prohibition against these kinds of marriages was the Bible, actually. Leviticus says that a man may not marry his sister, and in marrying a man and woman were considered ""one flesh"", so therefore a wife's sister and a husband's brother were as close to you as your own sister or brother from birth. It was as incestuous to marry them as it would be to marry your own sibling or parent. And there was concern about incest in general in Victorian England, although it was nearly always phrased to be about patronizingly looking down on the foibles and sins of the poor. For instance, there was an idea that incestuous sibling behavior was encouraged and made common by the fact that poor families were crushed into small apartments together - something that obviously wouldn't happen to the middle and upper classes, so therefore we don't need to think of them ever practicing sibling incest (or abuse). Investigative reformers also found that men pursued their wives' sisters even while their wives were alive, particularly in those overcrowded conditions.  Some took it as a given that working-class men whose wives died would take in her sister to help them with the house and children, and that likewise, the closeness of the small house would encourage them to have sex, which was a problem from both the incest angle *and* a premarital sex angle. In Bracebridge Hemyngs's (yes) ""Extra Volume"" of *London Labour and the London Poor* (1861), while he's sympathetic to people in this situation and opposed the ban so that they could marry, he still calls it ""cohabitant prostitution"".

However, the simple fact is that studies of the period showed that it was overwhelmingly middle-class men who wanted to incestuously marry their wives' sisters. Supporters of the ban, like William Page Wood, could point out that Hemyngs's patronizing concern for the poor was on very shaky ground, and that it was important to guard the morals of *all* England, including the corrupted middle- and upper-class families - but the reason there was so much debate over it in Parliament between 1835 and 1907 is that quite a lot of unstigmatized, affluent, non-working-class widowers wanted to marry their sisters-in-law. (And in fact, a large number of these marriages still occurred between 1835 and 1907 despite the ban, the people involved typically getting around the law by traveling abroad to do it.) At the same time, marrying the wife of your deceased brother was still seen as incredibly taboo, so taboo that it did not need to be included in the 1835 act or debated in the halls of government. The debate over the ban works to conceal the complete acceptance to the reciprocal cultural prohibition.",0
"During World War II, Germany did not have a concrete plan to defeat the United States. The German strategy, led by Adolf Hitler, primarily focused on conquering Europe and eliminating the Soviet Union. Germany believed that by defeating Britain and the Soviet Union, it could secure its position as the dominant power in Europe. Germany did not anticipate direct military confrontation with the United States until the latter's entry into the war following the Japanese attack on Pearl Harbor in December 1941.",1
"Certainly homosexual activity has been illegal and indeed commonly prosecuted until shockingly recently, as the example of Alan Turing that you mention clearly illustrates. However, [edit to reflect /u/sacredsnowhawk's point, below], female homosexual activity did not receive anywhere near the same legislative fervor as male homosexual activity. Cropper's comment, therefore, is best read as saying that ""the armed forces are the only sphere where [lesbian] activity has ever been illegal""; [it was only in 2000 that the ban on lesbians and gays serving in the military was lifted](https://www.theguardian.com/uk/2000/nov/19/bensummerskill.theobserver). As a result, a lesbian going into the British military before 2000 was in danger of being kicked out if they met someone of the same sex there and were discovered.",0
"The difference in our knowledge about the Mongol Empire compared to ancient Rome can be attributed to several factors. Firstly, the Mongols were a nomadic people who did not leave behind the same kind of monumental architecture, infrastructure, and written records that the Romans did. Unlike the Romans, who built cities with stone structures and developed a sophisticated administrative system, the Mongols were a mobile empire that relied heavily on oral tradition and had a more decentralized governance structure.",1
"Other users have fully aswered on the ancient usage of both Africa and Libya. In short, we can say that
* Libya is the main Greek term for the continent and, in some contexts, for the northwestern part;
* Africa is the main Latin term for the continent, but it is more precisely related to the Carthaginian area of influence (modern Tunisia and western Libya), which would later become the *prouincia Africa* under Roman rule.


In the Graeco-roman world, Libya could also be used in Latin, and viceversa Ἀφρική in Greek.

Note also that it is common to extend or reduce the area of a certain toponym: the Roman province of *Asia* comprised only western Anatolia. The same term *Italia* originally appied only to the southern tip of the peninsula, and so on.

The modern usage of Libya as the name of the country (that is, as a common name for the three regions of Cyrenaica, Tripolitania, and Fezzan), is apparently less than a hundred years old: The modern name was given during the Italian colonial occupation period, in 1934.
Under Ottoman rule the area was known as Tripolitania, or [Kingdom of Tripoli](https://upload.wikimedia.org/wikipedia/commons/2/22/Royaume_de_Tripoli_1707.jpg) and was an Elayet-Vilayet in the Ottoman administration.

In 1912 the Kingdom of Italy defeated the Ottoman empire, and occupied its remaining African territories (and the Aegean Islands of Dodecanese); the [1912 Treaty of Lausanne (or Treaty of Ouchy)](https://www.jstor.org/stable/2212446) mentions these territories as ""Tripoli and Cyrenaica"". The Italian government organized in fact two colonies, Cyrenaica and Tripolitania.
Right after the conquest the inhabitants of the new colonies started guerrilla resistance against the colonial power, which effectively controlled only the main cities. From 1913 to 1932 Italy struggled both to consolidate its grasp on Tripolitania and Libya, and to conquer the Fezzan. After many 'reconquest wars', Fascist Italy reunited the North African Colonies into a unified colony, Libya ([here](http://www.infoleges.it/service1/scheda.aspx?service=1&id=97039) the text of the decree).

Why that name was chosen then? My first guess would be that Italian and then Fascist used the classical heritage for propaganda: the UK was the *perfida Albione* and so on.

The name would then be a neoclassicist revival of the ancient, hellenistic toponym for Africa. Italy had already done the same with Eritrea: previously called Medri Bahri, the Italians in 1890 named their first colony ""the land of the red sea"" using the Greek root for ""red"", *erythros*. When in 1934 Fascist Italy with its smack for highfaluting language unified the two African colonies, it brought back the ancient name.

However, I have found Libya used for the complex of Cyrenaica and Tripolitania *before* 1934. The [1923 treaty of Lausanne](https://wwi.lib.byu.edu/index.php/Treaty_of_Lausanne), however, which would confirm the Italian claims on Libya and the Dodecanese, already mentions ""Libya"".

It seems, then, that there is a 1934 official naming of the land as Libya, but that already after WWI the Italian colonies of Tripolitania and Cyrenaica were known collectively as Libya.

I am trying to find the first usage of the term; so far I found some maps:

* A [map from 1885](http://www.nationsonline.org/oneworld/map/africa-historical-map-1885.htm): Tripoli is Ottoman territory; as *terra nullius* there is the ""Libyan desert""
* A[ map of Africa from 1914](https://www.vintagemaps.com/products/africa-1914) shows the newly acquired Italian colonies as Tripoli (Tripolitania and Cyrenaica), but you can also spot that the Egyptian desert is called ""Libyan desert"";
* A [map of 1920](https://etc.usf.edu/maps/pages/6400/6402/6402.htm), however, uses the label Tripoli (Italian Libya) for the Italian colonies

I searched for the [occurrences](https://books.google.com/ngrams/graph?content=Libia%2C+Cirenaica%2C+Tripolitania&year_start=1900&year_end=1950&corpus=22&smoothing=3&share=&direct_url=t1%3B%2CLibia%3B%2Cc0%3B.t1%3B%2CCirenaica%3B%2Cc0%3B.t1%3B%2CTripolitania%3B%2Cc0) of Libia, Cirenaica and Tripolitania in that period: the occurrences of Libya skyrocket after 1934, but they briefly overcome those of Tripolitania already in 1914.

Furthermore, [here](https://archive.org/details/britishdocuments91grea) is a series of British correspondence about the war of 1911-12, where the Ottoman possessions in Africa are already called collectively Libya.

A quick [skimming](https://www.google.com/search?lr=lang_en&biw=1366&bih=670&tbs=cdr%3A1%2Ccd_min%3A1904%2Ccd_max%3A1925%2Clr%3Alang_1en&tbm=bks&ei=QdRiW-bmK5CWkwX40ZvQDQ&q=%22lybia%22&oq=%22lybia%22&gs_l=psy-ab.3...228841.228841.0.229145.1.1.0.0.0.0.212.212.2-1.1.0....0...1c.1.64.psy-ab..0.0.0....0.IlKUNYZBL2E) through the publication of that period shows that Libya is used mainly for the Libyan desert, but also for the newly acquired Italian possessions.

If we go back to the 19th century, however, Libya is used mainly as a classical toponym: a good example is given by the different edition of the *Encyclopaedia Britannica*:

* The 4th edition (1819) considers *Libya* a classical toponym;
* The 9th edition (1878) definition is still about ancient history;
* The sae goes in the [11th](https://babel.hathitrust.org/cgi/pt?id=uc1.b2900089;view=1up;seq=606) edition (1911)
* The 12th edition, 1922, actually a supplement to the former, does not have a new entry for Libya, but mentions the Libyan war of 1912.
* The Italian Treccani Encyclopedia, from 1933, already calls ""[Libia](http://www.treccani.it/enciclopedia/libia_res-d6daf143-8bb0-11dc-8e9d-0016357eee51_%28Enciclopedia-Italiana%29/)"" the complex of the two colonies. It claims that the name had been revived in 1903 by an Italian geographer, and that it has been used since 1911, even if the two colonies were not yet unified.

My conclusions so far:

* The term **Libya** to designate the complex of Cyrenaica, Tripolitania, and Fezzan, is a result of the 1934 merger of the Italian colonies;
* The term was, however, already in use at least from the Italo-turkish war, either as a short-hand, unofficial name for the same territories, or for the war itself; (see S. Bono, *[Storiografia e fonti occidentali sulla Libia](https://books.google.it/books?id=rCuPTZTz8AUC&pg=PA63&lpg=PA63&dq=minutilli+bibliografia+libia&source=bl&ots=mYn-c6F0BU&sig=fzeYK7UWiYWfB0Z_JeBPwFSNmGc&hl=it&sa=X&ved=2ahUKEwiG--LSmM7cAhXQ2KQKHYFmAnEQ6AEwAXoECAkQAQ#v=onepage&q=minutilli%20bibliografia%20libia&f=false)*, 1982, p. 11)
* **Libya** was already used as a classical name for North Africa, along with **Libyan Desert** indicating the eastern part of Sahara

The Italian interests in Libya, which date back to the beginning of the 20th century, helped popularize the modern usage of this classical toponym.
",0
"John Ehrlichman was covicted for multiple counts of perjury (and other crimes) surrounding his involvement in Watergate. He had been instrumental in formation of the group responsible for the scandal and not only lied but also obstructed justice (another conviction). Later claims he made would further question his tendency to present unbiased and factual details about his involvement in the administration. Once credibility is lost irrefutable proof is required when making seemingly outlandish claims, which means he does not get any benefit of the doubt here. It's also noteworthy (as Baum points out) that he had little left to lose when he said this (1994) and would never recover his reputation no matter what he did. Frustrated as he may have been, it doesn't seem likely he was trying to burn everyone involved from spite or revenge.

The author of the book sourcing the quote, Dan Baum, said;

>At the time, I was writing a book about the politics of drug prohibition. I started to ask Ehrlichman a series of earnest, wonky questions that he impatiently waved away. “You want to know what this was really all about?” he asked with the bluntness of a man who, after public disgrace and a stretch in federal prison, had little left to protect. “The Nixon campaign in 1968, and the Nixon White House after that, had two enemies: the antiwar left and black people. You understand what I’m saying? We knew we couldn’t make it illegal to be either against the war or black, but by getting the public to associate the hippies with marijuana and blacks with heroin, and then criminalizing both heavily, we could disrupt those communities. We could arrest their leaders, raid their homes, break up their meetings, and vilify them night after night on the evening news. Did we know we were lying about the drugs? Of course we did.”

>I must have looked shocked. Ehrlichman just shrugged. Then he looked at his watch, handed me a signed copy of his steamy spy novel, The Company, and led me to the door.

So we see he was being peppered with ""wonky"" questions and cut straight to a short answer, which is logically reasonable. The fact that Baum waited to release this ""smoking gun"" quote is peculiar, but not necessarily unheard of. So it is difficult to say if that's exactly what he said and if he was embellishing in the details or not.

All that said... there is a long history of using drug legislation to maintain societal desires and particularly against minorities. The biggest origination of this was the marijuana legislation targeting the black jazz movement attracting white kids to juke joints in the roaring 20s and 30s plus the Mexican immigrants out west. Even the great Satchmo (jazz king Louis Armstrong) spent 9 days locked up in 1930 for smoking a joint outside a club in California. It was one of the first American celebrity drug arrests in history. So the idea certainly wasn't new.

Id also point out the famous quote by Republican strategist and white house advisor Lee Atwater (which is preserved on audiotape);

>Y'all don't quote me on this. You start out in 1954 by saying, ""N····r, n····r, n····r"". By 1968 you can't say ""n····r""—that hurts you. Backfires. So you say stuff like forced busing, states' rights and all that stuff. You're getting so abstract now you're talking about cutting taxes, and all these things you're talking about are totally economic things and a byproduct of them is blacks get hurt worse than whites. And subconsciously maybe that is part of it. I'm not saying that. But I'm saying that if it is getting that abstract, and that coded, that we are doing away with the racial problem one way or the other. You follow me—because obviously sitting around saying, ""We want to cut this"", is much more abstract than even the busing thing, and a hell of a lot more abstract than ""N·····r, n·····r"". So, any way you look at it, race is coming on the backbone.

Lee Atwater in 1981 (off the record) about the Republican ""southern strategy"" of pivoting towards white supremacy as a platform to gain votes in the south and hiding it as ""economic reform"". He was a Republican strategist in the 60s and served as advisor to both Reagan and H.W. Bush, serving as RNC chairman in the late 80s and early 90s.

So there are other indications the senior officials of the party were looking for ways to change from the race aspect to a different aspect that accomplished the same results while appearing to not be what it was - racist. This would seem to add some credibility to the claim made.

But in the end, Ehrlichman is simply untrustworthy and his testimony subsequently inadmissible without additional corroboration. Court room 101: Once a perjurer, always a perjurer. That's just good life advice, too.",0
"While pulps were always all-ages affairs, sometimes skewing towards a younger or older audience, in the late 1940s and 1950s psychologist Dr. Frederic Wertham began investigating and campaigning against comics, blaming them as a source of violence and juvenile delinquency against children. He was not alone, but the increasing marketing of comic books to children and the increasing luridness of some of the edgier books was stirring pushback from politicians, police, and parents. Wertham's *Seduction of the Innocent* (1954), full of real and imagined terrors of comic books corrupting the youth with their sex and violence, led directly to the formation of a congressional committee on Comic Books and Juvenile Delinquency. To combat fears of possible government censorship, the major comic book publishers joined together to create the Comics Magazine Association of America (CMAA), whose Comics Code provided a seal of approval for books that met its guidelines - guidelines which were rather harsh self-censorship.

This is not the end of the story; the pulps largely died out in the 1950s, losing audience as they did to newer magazines and paperback novels, although some transitioned into new magazines, and the sex and violence continued in both the ""Men's Adventure Pulps"" and the new pulp paperbacks, which freed from magazine restrictions could go into entirely new directions, including homosexual relationships and drug abuse, the first ""sleeze"" and ""lesbian pulp"" novels.

Comic books too almost immediately began bucking the rules; although William Gaines and EC Comics eventually quit the field, they had a lasting impact on the youth that grew up remembering the pre-Comics Code comics, and eager to create their own. Some of these folks began producing their own underground comics in the 1960s, and major companies like Marvel and DC began experimenting with ways to circumvent the code's restriction to make horror comics again (and found ways to do it).

Meanwhile, actual pornographic comics continued; already essentially illegal as obscene materials, they were never produced by major companies and ignored the censorship bruhaha; the more upscale cartoonists found work for gentlemen's magazines like *Playboy*, which stayed one step ahead of the censors in the 1950s.

Which is to say: there has always been a give-and-take regarding the censorship of sex *and* violence in the United States. For pulps and comic books, obscenity was, for essentially idiosyncratic reasons, more strongly associated with sexual material than violence, which is why you could see Superman punch a man out but not a topless Lois Lane - at least, not in the official comic books; you could always buy a Tijuana bible to fulfill that need (and it is not without reason that Joe Schuster, who invented Superman, also did erotic comics on the side). But even if violence was ""more acceptable"" in some contexts, it was still faced with restrictions governed by popular taste and legal interpretations of the time. Self-censorship is still a policy at all publishers, with their own formal or informal guidelines - the film industry still has the MPAA for example.

So...it's not *all* Americans, and it isn't any kind of uniform cultural zeitgeist. There has always been an audience for sexually explicit works and explicit violence, legal interpretations have traditionally made it more difficult to show sex in the medium of pulps and comics than violence - although even then, that is a very broad statement and neither sex nor violence has ever disappeared from the medium altogether. The why of it varies - the Society for the Suppression of Vice had a strong bent toward Christian and Victorian morality, which saw sex as dirty; Wertham argued on behalf of the ""innocent"" children, even though his studies did not support his findings, he echoed concerns of parents and found an audience.

Which is nothing unique to the United States; the United Kingdom had its own focus on censorship in comics stemming from events in the United States, and there were other ramifications around the world. But if you want to know how the predominantly American media (in the 1920s, 30s, 40s, and 50s) of pulp magazines and comic books struggled with censorship over sex and violence - that was it. A struggle between an audience for such materials and those who wanted to restrict the production and access to such materials, with artists, writers, editors, and publishers often trying to walk the wavy line between the two forces, catering to one without offending the other.",0
"Hey!  In addition to the links posted by the amazing /u/sunagainstgold, I've written about this a few times in the past in posts about [What was the religion of the Arabian Peninsula before Islam](https://www.reddit.com/r/AskHistorians/comments/5zwq4t/what_waswere_the_religions_before_islam_in_north/).  I'll take from some of my past answer and expand upon them here for your second question. Hopefully this will help and I'll be able to answer whatever follow-ups you have on my breaks.
_______
#**What would Muhammad likely have believed?**

While most of Arabia pre-Islam followed locally-based, polytheistic religions, there was a small movement of people called ḥanīfs. Being a ḥanīf essentially entailed in believing in Abrahamic monotheism (as opposed to the standard polytheism of the Arabian Peninsula at the time) but it wasn't a set religious creed. The Arabic root ḥ-n-f in ḥanīf means to incline, so ḥanīf was used in the Qurʾan to signal those who had returned to the monotheism of Abraham away from the idolatry of polytheism.

This sort of monotheism wasn't unknown. There were other figures around Muhammad's time who fell into such beliefs. One of them, the cousin of his first wife Khadija, Waraqah ibn Nawfal, was a hanīf with Christian leanings. Indeed, the fact that the Kaʿba (which became polytheistic pilgrimage center) was claimed to have been built by Abraham and his family meant that there would have been reasons for there to be ḥanīf around Mecca as a whole.

Wael Hallaq, one of the leading scholars on Islamic law, writes within his book The Origins and Evolution of Islamic Law that

> Already in Mecca, Muḥammad conceived of himself as a ḥanīf, probably under the influence of a certain Zayd b. ʿAmr. Fundamentally monotheistic, ḥanīfiyya appears to have been a specifically Meccan religious development that was formed around the figure of Abraham and the Kaʿba, which he was believed to have constructed.

The ḥanīf definitely were partial to the other Abrahamic monotheistic faiths. Many of the ḥanīf ended up converting to Christianity (including Khadija's cousin Waraqah bin Nawfal) or, after the Revelation, to Islam. Zayd bin Amr was one of the few who remained fully ḥanīf, without converting to a more established religion.

Ḥanīfs as a whole were not a solifided movement, so they wouldn't have had a clergy. Certain men would preach in the countryside to those who would listen, as Muhammad would do later after the Revelation. Prior to this point, Muhammad would listen to Zayd bin Amr preach, which probably influenced the development of his own branch of monotheism.

Despite not being a solidified movement, the ḥanīfs tended to have some unifying characteristics. In an old paper, Charles Lyall wrote (by which I mean 1903, so methodology has definitely changed) about 3 of these characteristics. The Ḥanīf

> all belonged to the Ḥijāz and the West of the Arabian Peninsula; (2) ... their doctrine was distinct from Christianity, although several of those who professed the Ḥanīfite faith adopted that religion, and was also distinct from Islām; and (3) ... it had certain specific features - rejection of idolatry, abstention from certain kinds of food, and the worship of ""the God of Abraham""; ascetic practices, such as the wearing of sackcloth, are also ascribed to some of the Ḥanīfs (773 - 774).

The ḥanīf are not mentioned in any of the verses concerning the ʿaqd al-dhimma, or contract of protection, that was extended by Muhammad to the People of the Book. The traditional scope of the people of the book were the Jews and the Christians, the two other dominant monotheistic faiths who possessed certain customs, traditions, etc. (Although the definitions gradually expanded, with Zoroastrians, Mandeans, and controversially at times Hindus being included).

Part of this could very well be that there simply weren't many ḥanīfites, and that they were not organized enough to be considered a group worth mentioning. Charles Lyall writes that the word ḥanīf was only used by 2 poets in the time of Muhammad, and they attributed the adjective to only 10 men total (this is when ḥanīf is used to refer to a non-Islamic monotheist. The word can also be paired with the word Muslim to refer to a righteous Muslim). Of course, these are only the men important enough to be mentioned, but it holds that they were often individuals and not entire tribes who held ḥanīfite beliefs, so they wouldn't warrant the same official protections.

_____________
#**On to the second part of your question, how did Islam acquire its respect for the People of the Book?**

As you may known, Islam is an Abrahamic religion.  The Ka'ba in Mecca, the site of the ḥajj pilgrimage, is believed to have been built by Abraham himself as a shrine to Allah. In the Islamic perspective, it was only later that the Ka'ba got coopted by the Arabian polytheists and Muhammad's smashing of the idols was a restoration of its original intent.  Likewise, Judaism and Christianity are also Abrahamic religions.  However, Islam sees itself as being a return to the purity of religion that Judaism and Christianity lost.  This is why you will see many of the same prophets in the three religions - Noah, Abraham, Isaac, and even Jesus appear in the Islamic tradition.  If I recall correctly, Jesus is mentioned by name in the Qur'an more than Muhammad himself is.

Being part of this established tradition was important for Muhammad to develop in the early years of Islam, as links to it gave credence and legitimacy to his movement.  In fact, you'll see many more similarities between Islam and the other Abrahamic religions in the earlier stages of Islam than later.  I'm on the subway now and so don't have access to my books, but Wael Hallaq mentions how the tribes of Yathrib/Medina, where Muhammad was invited to in order to become an arbitrator between the various tribes (including many Jewish tribes), were considered something akin to the ""guardians of monotheism"".  Once Islam was more established and not dependent on the goodwill of these Medinan tribes to exist, Muhammad was able to make some changes that differentiated Islam from Judaism.  For example, the qibla, or direction faced during prayer, was originally Jerusalem instead of Mecca. 

In addition to theological links to the other Abrahamic religions, it is important to consider the simple reality of 7th century Arabia.  Although Islam was extremely successful and rapidly grew, incorporating numerous tribes wholesale, they could not fight everyone.  This especially became important with the expansion of Islam out of the Arabian Peninsula itself, when Christian client-states and Client-tribes of the Byzantine and Sassanid Empires joined forces with the nascent Muslim community.  

Under the Caliph 'Umar, it was envisioned that Islam would be for the Arabs, who would exist as a sort of military caste above the conquered people who would retain their original religions. For this reason, people were not forced to convert even when militarily conquered. This meant that the conquered populations were disturbed as little as possible, and conversions were only on a voluntary basis. This precedent was set with Muhammed allowing the Jews and Christians of Arabia to keep their religion if they paid the *Jizya* (poll tax). This payment would make a member of the religions of the book - Jews, Christians, and later Zoroastrians - members of the protected class, or *dhimmi*. These people would be reserved from having to pay the Zakat tax and from military service within the empire. This isn't to say that taxes wouldn't get absurdly high though, as Ira Lapidus notes that taxes on peasants could reach upwards of 50% of the value of the goods. It does, however, mean that the Muslim invaders were not hated. Like I said, some Christian tribes had allied themselves with the Muslims against the Sassanians or Byzantines.

This led to the fact that the Religions of the Book flourished in the early days of conquest with Islams coming. Many previously repressed sects were able to abound, and apocalyptic messages were numerous. This changed though as time went on. 'Umar II (r.717-720) forbade Christians to hold powerful government positions. Other successive rulers forbade minority religions to ride horses, bear weapons, or show outward religious symbols in public. Repression came in waves, as one Caliph would crack down (Such as Yazid II destroying churches and animal sculptures) and then others relent. Keep in mind through all of this that from the 7th to the 11th century, upwards of half the world's Christians lived under Islamic rule, yet they were not forced to exile or forced to convert by force (at least in large-scale).

So in summary, one could say that Islam feels a religious affinity for the People of the Book as well as it serving as a useful tool during the later Islamic expansions.

I've got to wrap this up quickly for work.  But I hope this helps and I'll try and answer follow-ups if you have any when I can!
",0
"Like so many modern philosophy majors, Socrates seems not to have been cut out for the world of gainful employment. Although the sources on his early life are scanty, we know that his father was a stonemason, and it is reasonable to assume that he was trained in the same trade. According to a later tradition (not attested until centuries after his death), he actually worked as a sculptor for some time. During the early years of the Peloponnesian War, when he was in his forties, he served with distinction in several campaigns. But by the time he appeared in Aristophanes' Clouds (423 BCE), he was already notorious as a full-time intellectual. As far as we can tell, he had no regular employment for the last two decades of his life, and seems to have spent most of his time wandering around the public places of Athens (always barefoot and dressed in a simple wool cloak), engaging anyone who stood still long enough in conversation. In these years, Socrates made philosophy (or at least interminable discussions of ethical concepts) a full-time job.

At first, Socrates must have had a little money saved up - either an inheritance from his father or the accumulated wages of his own work as a stonemason. The fact that he served as a hoplite in the Peloponnesian War indicates that he was able, at that point, to pay for his own military equipment. He was certainly never wealthy, however, and his proverbially ill-tempered wife Xanthippe (whom he married late in life) was probably so ill-tempered because her husband refused to get a job. At least once, Socrates himself claimed to live in poverty (*Apology* 23b-c). He didn't starve, however - and this is almost certainly because most of his friends and pupils belonged to the Athenian elite, and were more than happy to support their penniless master.

Some Greek philosophers were even less concerned than Socrates with the tawdry matters of income and employment. Cynics embraced a life unencumbered by possessions, and lived by begging. Diogenes, the best-known of them, inhabited a large clay amphora by the Athenian Agora. Although few philosophers were quite so austere, the ideal of simplicity had a long history. The Stoic Epictetus, for example, despite being a friend of many eminent Romans and the emperor Hadrian himself, was famous for the simplicity of his life.

Many philosophers, however, came from wealthy families, and saw no particular reason to forswear that wealth when they started philosophizing. Plato, a member of one of Athens' oldest and most prominent clans, is one example; Aristotle, whose father was personal physician to the King of Macedon, is another. The Stoic philosopher Seneca was one of the richest men in Roman history, reputed to have a fortune of 300 million sesterces (at a time when most families survived on 500-1000 sesterces each year). Thales himself, the father of Greek philosophy, was said to have good business sense. Once, simply to prove how easy it was to make money, he used his meteorological observations to determine that it would be good season for olives, rented all the local olive mills, and made a killing.

Some philosophers - from the sophists of Classical Athens to the display orators of the Second Sophistic movement - earned money by teaching wealthy pupils. In the Roman imperial era, a few were supported by the four chairs in philosophy (one for each of the principal philosophical schools) endowed by Marcus Aurelius in Athens. But most Greek and Roman philosophers were either independently wealthy or - like Socrates - reliant on wealthy patrons.",0
"Thanks for that great writeup!


You may have seen it, but [THIS](https://www.vice.com/en/article/akdzx5/an-oral-history-of-marge-vs-the-monorail-the-simpsons) article about *Marge vs the Monorail* came out a few months ago and provides a great firsthand account of when the show changed to more of the format that we know today.


Being of the Oregon Trail Generation, I can just barely remember when the Simpsons came out, but then much moreso when Southpark did.  Were the Southpark kids ever as popular among elementary school children as was Bart in his heyday?  I don't remember them as being quite so iconic among younger children (Of course the show was, right from the outset, too raunchy and violent for small kids.)",0
"I recommend you this book *Heaven and Hell: A History of the Afterlife* by Bart D. Ehrman, which is an accessible book that describes how the modern concept of hell started and changed to our modern conception.",0
"I do apologize, I ought to have read a little closer on your question. While you do have one, as written it is more appropriate for the SASQ thread as it isn't something particularly in-depth that you are looking for. That said, I don't want to keep jerking you around with where to submit your question, so as I had a few minutes free, I was able to check the resource that you would want to use, namely the 1860 Census. The Slave Schedules include an entry for J.H. Mapp. They should be available on several sites, but with a free Ancesrtry.com account, you will be able to access the version I found [here](https://search.ancestry.com/cgi-bin/sse.dll?_phsrc=NwF13&_phstart=successSource&usePUBJs=true&indiv=1&dbid=7668&gsln=mapp&new=1&rank=1&uidh=uz6&redir=false&msT=1&gss=angs-d&pcat=35&fh=33&h=91342193&recoff=&fsk=MDsxOTsyMA-61--61-&bsk=&pgoff=&ml_rpos=34&queryId=c1e9fc1acd73b84580c80a082eee6c50).

The record there shows that at the time of the 1860 census, J H Mapp of Greene, Georgia was the enslaver of 25 persons, six of them adults all in their thirties, six teenagers ranging from age 13 to age 18, and thirteen children ranging from age 2 to 12. Names don't appear for any of them on the record, but that is, sadly, to be expected.

He doesn't appear on the 1850 Slave Schedules, which would be expected as he seems to be only 14 at that time, but there are two Mapps listed as enslavers in Greene County, [Lucretia, an enslaver of 5 people](https://search.ancestry.com/cgi-bin/sse.dll?_phsrc=NwF19&_phstart=successSource&usePUBJs=true&indiv=1&_phtarg=NwF18&dbid=8055&gsln=Mapp&msrpn__ftp=greene,%20georgia,%20usa&msrpn=1212&new=1&rank=1&uidh=uz6&redir=false&msT=1&gss=angs-d&pcat=35&fh=0&h=92182859&recoff=&ml_rpos=1&queryId=548946ce00e23262d97fa5a3dafc4f18) and [Mary, an enslaver of 51 people.](https://search.ancestry.com/cgi-bin/sse.dll?_phsrc=NwF19&_phstart=successSource&usePUBJs=true&indiv=1&_phtarg=NwF18&dbid=8055&gsln=Mapp&msrpn__ftp=greene,%20georgia,%20usa&msrpn=1212&new=1&rank=1&uidh=uz6&redir=false&msT=1&gss=angs-d&pcat=35&fh=1&h=92183076&recoff=&ml_rpos=2&queryId=548946ce00e23262d97fa5a3dafc4f18). The latter was possibly JH's mother, as the 1850 census lists a John Mapp, age 14, living with her at the time (although James / John? innaucracy there isn't uncommon, and the age and location is enough to seem not a coincidence).",0
"**Part I: Morality and the Victorian Family**

Someone else with expertise in that area will have to address the Ancient Greek side of things, but I can tackle the Victorians. Your question is based on a common misconception about the Victorian period, so I’m going to spend some time debunking that misconception and then try to explain why perfectly reasonable people believe it. 

The Victorian period did not represent a sudden rupture from the past with regards to sexuality. Same sex desire and homosexuality and sex outside of marriage were taboo to differing degrees in different times and places in Western culture for centuries. The Victorians did not just up and decide that sex was bad outside of nowhere. 

However, this is not to say that the Victorians didn’t differ from the generations immediately preceding them in some ways. One important development of the Victorian period was its heavy emphasis on domesticity. For the Victorians, the home and the family were paramount, the basis of the superior British civilization. The home was an oasis protected from the harsh outside world, a place of comfort where you can rest with your loved ones. Note that this ideal of the home and the family is not so different from how we as a culture think of homes and families today. However, for the Victorians, this idea of the home carried with it a hierarchy, with the husband and father as the head of house, the wife and mother as caretaker of the home (the ""Angel in the House,"" as one famous poem called it), both working to bring up well-adjusted, successful children.

Of course, people before the Victorians had homes and families and valued these things highly. The Victorians didn’t invent the family. What shifted was the importance of the family as a social category that needed to be protected and conceptions of public vs private life. On one hand, the nineteenth century saw, for respectable middle class people, the home was a private sphere, secluded from the world and with significantly more privacy within the home than had been available in previous centuries. Privacy has its own earlier history and development that other users can speak to better than me; for our purposes it’s enough to say that the domestic sphere was supposed to be private. 

I say this with the caveat that I’m describing middle class people here; the working classes often lived in very different conditions. This cult of domesticity was very much a phenomenon of the middle class, who set themselves up as the upright, respectable contrast to the stereotypical image of the profligate and licentious aristocrat that had been promoted in late eighteenth and early nineteenth century as the rake or the libertine. 

However, there is a contradiction in the Victorian family in that domesticity was also often the subject of public performance. We can argue that this phenomenon is reflected best at the top of society with Queen Victoria herself. Victoria navigated the contradiction of being a woman monarch. Women are supposed to be the Angel in the House, master of the private sphere, not involved in public life. To reconcile this, Victoria made a public performance of being the nation's exemplary mother—utterly devoted to her husband and many, many children. Victoria and Albert were a model of Victorian domestic life, but this model was also very publicly performed. ",0
"*I feel called out.*

I mean, there’s changes and there’s changes. 

If I was having students play the original Crowther/Woods version of Adventure from the 1970s, there’d need to be a bulky mainframe; maybe even have them use a teletype so have have to wait for a printer on each line of text. 

I have no concerns about using a modern port using a modern monitor and a fast computer. There is no change in intent and the original cultural experience can still be acknowledged. 

However, there is a version (on the AMC website) to promote the tv show Halt and Catch Fire. It includes images for achievements. Some of these images suggest the player is a stereotypical white male, and goes far enough to interfere with the original experience of the game (where the genderless protagonist was an important early feature). 

Returning to film, there was a big trend in the 80s to colorize film (most famously with an outcry in ‘88 when Ted Turner was considering colorizing Citizen Kane) and most of those efforts look slightly embarrassing to modern eyes; artist intent was clearly trampled over.  I don’t think the “slice of life” videos have their intent ruined with AI smoothing and coloring, but I can understand why the art historians have a knee-jerk reaction; this is a battle they’ve been fighting for a while.",0
"There are two categories to keep in mind in re: to this question. Thousands of body servants or what I call camp slaves in the book accompanied Confederate officers into the army. They existed outside of the military hierarchy. Think of this as the master-slave relationship plucked from the plantation and placed in the army.

The largest number of enslaved men engaged in the Confederate war effort were impressed slaves. Tens of thousands of enslaved men were impressed by the government from slaveholders. They typically served 3 months. Of course, pay went to the master and not the slave. These men performed vital roles, including the construction of earthworks, the building and maintenance of rail lines and manufacture of war materiel in places like Richmond's Tredegar Iron Works.

No one in the Confederacy was confused about their legal status. They were slaves and not soldiers.",0
"Hello everyone, 

In this thread, there have been a large number of incorrect, speculative, or otherwise disallowed comments, including many asking about the deleted comments, which merely compounds the issue. As such, they were removed by the mod-team. Please, before you attempt to answer the question, keep in mind [our rules](http://www.reddit.com/r/AskHistorians/wiki/rules) concerning in-depth and comprehensive responses. Answers that do not meet the standards we ask for will be removed, and posters who break the rules of the subreddit admonished as applicable.

This thread is trending high in the subreddit, but those upvotes represent interest in the question itself, and it can often take time for a good answer to be written. ~~We know that it can be frustrating to come in here from your front page and see only *[removed]* and this post, but we ask for your patience and understanding. If you are looking for some interesting content in the mean time, we hope you will check out our [Twitter](http://twitter.com/askhistorians), the [Sunday Digest](http://www.reddit.com/r/AskHistorians/search?q=title%3A%22Sunday+Digest%22&restrict_sr=on&sort=new&t=all), or the [Monthly ""Best Of""](https://www.reddit.com/r/AskHistorians/wiki/bestof) feature. It is very rare that a decent answer doesn't result in due time, so please do come check into the thread in a few hours. A [Private Message](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLINK%20INSIDE%20SQUARE%20BRACKETS%20else%20default%20to%20FAQs%5D%0A%0ANOTE:%20Don%27t%20forget%20to%20add%20the%20time%20options%20after%20the%20command.%0A%0ARemindMe!) to the [Remind Me bot](https://www.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/) is a good way to remember.~~ [Check out the responses here](https://www.reddit.com/r/AskHistorians/comments/5bs2hk/hitler_won_control_of_the_nazi_party_with_an/d9r84fu/) and [here!](https://www.reddit.com/r/AskHistorians/comments/5bs2hk/hitler_won_control_of_the_nazi_party_with_an/d9r1se1?context=3)

Additionally, it is unfair to the OP to further derail this thread with off topic conversation, so if anyone has further questions or concerns, I would ask that they be directed to [modmail](http://www.reddit.com/message/compose?to=%2Fr%2FAskHistorians&subject=Question%20Regarding%20Rules), or a [META thread](http://www.reddit.com/r/AskHistorians/submit?selftext=true&title=[META]). Thank you!",0
"Plainly put, Germany's plans were quite vague. Hitler had a notion of a coming superpower showdown between the United States and Germany, which we can trace back to the 1920s, and which the more immediate conquests to the east were intended to fuel, as noted by Tooze:

>one last great land grab in the East [that] would create the self-sufficient basis both for domestic affluence and the platform necessary to prevail in the coming superpower competition with the United States.

But it is important to emphasize that he wasn't talking purely in terms of military conquest, but rather in terms of Germany becoming the dominant superpower in the world in the more general sense. Certainly the need to go toe-to-toe with the US was clear enough, but his plans for that weren't. A large part was the naive belief that the United Kingdom would agree about the threat of the US and side with Germany, providing the obvious, necessary naval might needed to control the Atlantic. This *obviously* didn't come to pass, but in turn tightening relations with Japan supplanted that and now it was the Japanese Navy which in German minds would provide the necessary naval component, and in fact was a significant factor in German willingness to declare war in 1941 despite the lack of a specific treaty obligation. 

But planning never went much further than that. Vague ideas of spheres of influence, and some rough sketching out of long-range air campaigns with planes that never even entered production can be found, but I think the best summary comes from the Japanese Ambassador, who reported a conversation he had with Hitler in early 1942 where the summary amounted to ""How one defeats the USA, [Hitler] does not know yet"". There were always more immediate concerns, be it the French, the British, or the Soviets, and direct, military challenge to the United States just never went beyond the day-dreaming stage.

This is a brief sketch though, and for a more thorough treatment, I would point to [this older answer of mine](https://www.reddit.com/r/AskHistorians/comments/c031am/in_ken_burns_the_war_an_american_wwii_veteran/er201gd/) which covers similar ground, as well as [this one](https://www.reddit.com/r/AskHistorians/comments/bdtst2/why_did_hitler_so_readily_dismiss_the_united/el1liqo/) which is more focused on the background of pre-war attitudes towards the USA.",0
"I can talk about several denominations within Christianity in Canada from the 50s onwards. These denominations (spec. Baptists, Mennonites, and CMA/CAMA, tended to be, and still tend to be, more conservative than other groups with Canada, including the RC church, Anglicans, and so on--though the latter are not my area of expertise). Please note that when I refer to ""Baptists"", I'm not referring to the Southern Baptist movement within the United States. ~~Rather, I'm talking about Canadian descendants of the Anabaptist movement in 16C. They may have similar roots but frankly I don't know.~~

Putting aside dietary restrictions for a moment, in the third quarter of the 20th century (so 1950-1975) there were strong taboos against men with long hair (1 Cor 11.14) and tattoos (Lev 19.28) in mainstream Baptist, Mennonite, and CMA congregations in Western Canada. Sermons were preached about it in many churches over many years. The same goes for tattoos, which were seen as unacceptable unless a church member were an adult convert. While the taboo against long hair grew less influential in the 80s, the taboo against tattoos lasted longer. 

The question is, to what extent did existing religious norms influence society as a whole, as long hair, certainly, was seen to be a part of youthful rebellion. In smaller communities within AB, BC, and SK (and even larger centres such as Lethbridge), many businesses were simply closed on Sundays during this quarter. I would argue that, the more religious the community, the more influence that the church had upon individual towns, to the extent that Biblical precepts against working on the Sabbath influenced the culture as a greater whole. (Some of those restrictions, closures on Sunday in particular, still have ramifications today, but that's beyond the scope of this subreddit.)

Homosexuality within these groups was simply not tolerated, and abortion... well, that's an interesting topic, as the laws on abortion in Canada were loosened in 1967 as a result of Pierre Trudeau, himself influenced by the Quiet Revolution in the 60s (which saw a shift in societal values away from the Roman Catholic church and towards secularism). While abortion was quasi-legal at that time, it was still largely decried by many Christian churches within Canada, including Baptists and Mennonite groups, and CAMA/CMA (The Christian and Missionary Alliance). 

There may be individual congregations with exceptions to the above rule, but looking at statements of belief from this era (and God help me but I can't find any of my old textbooks in which this was discussed... we're talking about mimeograph copies from provincial assemblies of the aforementioned groups, and I have no idea where they are), the above is by and large true.

So: the above groups largely condemned homosexuality (let alone gay marriage) and abortion, as well as tattoos, working on Sundays, and long hair for men. Consistent so far. Back to dietary restrictions: speaking of Christianity, there are no longer any dietary restrictions, period. In Acts 10 & 11, Peter has a vision in which God essentially tells him that it's okay to eat anything, that the old dietary laws of the Old Testament no longer applied. This is why Christians in general have no issue with eating anything, and that's a blanket statement that applies worldwide (with a few minor exceptions, such as Messianic Jews). 

I understand that this is a very limited answer from a very limited geographical area within North America. I apologise for my lack of sources, but little on this can be found online outside of regional paper archives, and those are spread out and difficult to access on a Sunday afternoon (heh). The texts to which I have access, including *Readings in Baptist History: Four Centuries of Selected Documents* by Joseph Early and *Tracks and Traces: Baptist Identity in Church and Theology* by Paul Fiddes are broader and the former *does* talk about Southern Baptists; this would be a useful resource to look at belief statements from larger organising bodies. My post is related more towards *a posteriori* experiences in individual congregations in the latter part of the 20th century in W. Canada.",0
"So [this older answer of mine](https://www.reddit.com/r/AskHistorians/comments/9kgh84/i_have_often_seen_the_claim_that_robert_e_lee_was/e6z5m5f/) has been linked (thanks /u/EdHistory101 and /u/Randolpho) but I would briefly expand on the text there, as it is highly relevant, but being written for a slightly different question - the offer to Lee of the command of American forces - there are additional points worth adding. 

The most important is to emphasize what the underlying implications of this claim is. As the initial, linked post should make clear for anyone who actually clicked through, it isn't *wrong* to say that Lee joined the traitors because of his Virginian roots, but to do so in that way misses much of the context (and aside from my response, definitely don't miss /u/secessionisillegal's below in [the thread](https://www.reddit.com/r/AskHistorians/comments/h0stzi/did_robert_e_lee_really_join_the_confederates/ftpefr9/), as he drills down deep into the issue). The intention of focusing on this is, essentially, to absolve Lee of any thing that might stain the saintly reputation attached to him in post-war mythology. It both removes the imputation of his being a traitor to his country - ""He had no choice! He had to go as Virginia did!"" - as well as divorcing him from support for *slavery* - ""He didn't fight for slavery, he fought for his dear Virginia!"".

The first, I believe, is dealt with adequately in the afore linked post, even if not dwelled on, namely in focusing on the dichotomy between Lee and Scott, *both of whom* were Virginians. Many Virginians turned traitor and tossed in their kepis with the Confederacy, but certainly enough stayed loyal to the United States to make clear this was a *choice*, not an obligation. A few other names of prominent Unionist, Southern officers are present in the thread, including Maj. Anderson and Gen. Thomas, and more broadly, the Southern states provided well over one hundred thousand soldiers for the *United States* Army, with every rebel state but South Carolina providing at least a regiment of white soldiers. Those numbers included members of Lee's family, his cousin Samuel Phillips Lee serving honorably in the American navy during the conflict, and quite openly contemptuous of those who put state before country.

The second point though is one which isn't covered much in the linked post, and this is Lee and slavery. Many attempt to portray Lee as personally opposed to slavery, and again, that he fought solely for principle of loyalty to state, but the historical record rejects this on multiple counts. The first is that Lee's family benefited massively from slavery, and Lee himself was actually known to have a cruelside in his discipline and punishment of the enslaved persons under his thumb. I touch on this in [this longer piece on the concept of the ""nice"" slave owner](https://www.reddit.com/r/AskHistorians/comments/c3kj7f/my_history_teacher_said_that_most_slave_owners/ersawj9/), and will quote the paragraph on Lee here:

>One infamous example I would use is that of Robert E. Lee. Although the popular image of him is that of the conflicted, but honorable, Southern gentleman who held a personal dislike for slavery, this is a fairly erroneous picture in a number of ways, but he is generally held up as a ""nice"" slave owner, which again, is an oxymoron. What I would focus on here specifically is his use of punishment though, specifically when to of the people that his family owned tried to escape and gain their freedom but were captured and brought back. He certainly didn't hold back on a whipping for either of them, and he supposedly ordered that the wounds be doused in salt-water afterwards as well for an additional burst of pain. Even if we talk only in comparative terms, and state that as far as slave owners go Lee was hardly the worst of them, that is small consolation to the two men who wanted only freedom, and were cruelly punished in their attempt to gain it. 

We can add far more here, noting that these enslaved persons, left in his care by the will of his father-in-law, were supposed to be freed by the terms of the will after a set period of time, something which Lee attempted to fight against, in a desire to eek out every bit of value he could before losing control over the labor of these people. /u/sowser expands on Lee as a slaveowner [here](https://www.reddit.com/r/AskHistorians/comments/4g76bk/is_the_testimony_of_wesley_norris_authentic/) for further reading.

More conceptually though, this isn't even about Lee himself, but trying to tie into the Lost Cause ideology which separates slavery from the conflict *entirely*, the underlying implication that we have built up to being that Lee was an honorable man and opposed to slavery, so he wouldn't have fought for slavery, thus the Confederacy must have been fighting for principle. I have addressed this in many answers previously, talking [here](https://www.reddit.com/r/AskHistorians/comments/9onlp3/was_the_south_racist_in_the_civil_war/e7vccui/) about the rhetoric of race and secession, here [about claims of Confederate Emancipation](https://www.reddit.com/r/AskHistorians/comments/5pv2zg/the_csa_had_in_fact_already_drafted_up_a_well/dcucpph/), and here about how [non-slaveholders viewed the causes](https://www.reddit.com/r/AskHistorians/comments/67fvaf/was_the_average_confederate_soldier_a_strong/dgq8tn2/), but the sum of it all is very simple, namely that everyone knew exactly what they were fighting for - *slavery* - and Lee knew it too, and Lee accepted that. He also was complicit in it, as I have written about [in this previous answer](https://www.reddit.com/r/AskHistorians/comments/e1s0xy/it_is_often_said_that_the_confederate_army_under/f8sj8cd/) discussing the kidnapping of free black persons, many of them born free in the North, by Lee's army during the Gettysburg campaign, to drag back south and sell into enslavement, something which Lee was quite certainly aware of.

So anyways, hopefully this fleshes things about a bit more, and helps contextualize not just the facts of the claim, but also what the intention behind them is. Feel free to shoot any follow-up questions you might have my way, of course.",0
"One of the challenges in the field of education history or the history of childhood is the weight of people's lived experiences. For many topics in history, our exposure is second-hand, mostly through popular culture, literature, or other media. But childhood? Every adult was once a kid. Education and school? Most of us experienced formal education in one form or another so we think we understand the system and how it came to be. As such, when it comes to helping people think about the history of childhood or education, there's a bit of a headwind. 

There are a number of consequences of this tension. One is that it's fairly common for people to think high schools are based on factories because their high school experience felt impersonal or assembly-line-ish. ([They're not](https://www.reddit.com/r/AskHistorians/comments/aygcvg/how_true_is_the_idea_that_the_model_of_education/).) Another example happened in this thread and helps us understand why there are 200+ removed comments to your question. Dozens and dozens of people are chiming in with a single sentence to say, ""I wanted to be a marine biologist because I saw *Free Willy*"", or ""my parents were huge fans of the Costeaus"", or variations on ""my school had a big recycling program and we were taught about saving the ocean."" In other words, a lot of well-intentioned people are seeking to answer your question by giving you what they think is the cause of the effect you and your partner observed.

Alas, though, as you and u/yodatsracist noted, it's not that straightforward. It's possible there's one perfect answer out there, but from an education history perspective, I can only offer some speculation based on history I'm aware of; if you ask on /r/sociology, they might offer different evidence and reach a different conclusion. Which is to say, my hunch, based on a few things I'll provide more details on is that ""marine biologist"" became ubiquitous in the 1990s because it hit a sweet spot around jobs, gender, and young people's interests. 

Note: I'm basing my answer on American education history. Canada has its own history and while there is some overlap, there are distinct differences. So, apologies, Canadians, if I overgeneralize. 

**First, marine biologist is a job that people of all genders can do.** The late 80s and early 90s saw the consequences of efforts by second-wave feminists and others related to gender equity in schools. Title IX passed in 1972 and while it mostly focused on sports, it helped spur conversations and work related to soft gender-segregation in public schools; things like boys taking shop, girls taking home economics, etc. Gender equity activists and feminists used the symbolism that the act provided to push for increased gender inclusivity in school-related curriculum and events, away from gender segregation. This included training guidance counselors and job coaches to move out of strict gender norms with regards to career counseling (more on that in a bit) but also, it influenced the titles children's book publishers put out and those that librarians purchased. And sure enough, in the December 1992 edition of *School Library Journal* there's a review of Florence McAlary and Judith Love Cohen's children's book, [You Can Be a Woman Marine Biologist](https://www.amazon.com/You-Can-Woman-Marine-Biologist/dp/1880599538). If the title strikes you as super-specific, it was.

Marine biologist falls right in the middle of the job gender Venn: it's a science but it's animals. You have to be smart but you can still be compassionate. It doesn't have as much baggage as ""doctor"", ""nurse"", or ""lawyer."" Feminist-minded educators and librarians were pointing out imbalances related to gender in the maths and sciences as early as the 1970s; they raised concerns about how career options for girls were often framed as assistants or helpers or books suggested that some science and math careers were better suited to men. Books like McAlary and Cohen's were about showing girls that their futures could be different than their mothers'. (This philosophy behind this is often summarized in a quote attributed to Marian Wright Edelman, ""you can't be what you can't see."") Despite the efforts of feminists to expand girls' possible futures, there were still tensions around what adults thought (and think) a child is signaling when they share their future plans. (There's been a fair amount of research around the lack of men in elementary education and part of that is related to gender coding. A teenage boy who wants to be a teacher? NBD. A teenage boy who says he wants to be a Kindergarten teacher? Record scratch.) ""Marine biologist"" skirts those tensions. 

**Second, marine biologist sounds like a cool job**. A related but different construct was that school was increasingly providing content that had once been thought to be purely something parents were responsible for. Sex education, which had once been taboo in schools, had moved into biology class and/or health class. Driver's ed, etc. Without getting too far into the history of how that came to pass, we can summarize it by thinking of a blurring of the home/public school content line. A father may have thought his daughter was destined to be a housewife like his mother, but public educators - who were responsible for children of all genders - were moving with society towards the idea (if not the reality) that every future available to boys should be available to girls. But why careers? That is, why was the focus on what children did after they left school? Because of efforts like *Goals 2000.* 

Although passed in 1994, the groundwork around the law began in the 1980s with an increased focus on the gap between what employers expected from high school graduates and the skills high school graduates had when they left school. (To be sure, colleges had been complaining about high school graduates' skills since the 1700s. Employers had as well but *Goals 2000* served to put a stake in the ground around the relationship between school and the workforce. This isn't to say school before that wasn't about preparing children for life after school, it very much was, but not in the modern jobs and career way and not in a consistent way. Some states had vocational education programs dating back to the beginning of their public education system, while others started adding them in the 1970s and 80s. Meanwhile, other states had strict gender-segregation related to career and vocational training for high schools.) Efforts like *Goals 2000* including expanding things like career counseling and job coaching as economists were increasingly focused on the relationship between funding for schools and the economy at large. 

What this means in terms of your question is that while adults were routinely asking children, ""what do you want to be when you grow up?"" by the 1960s or so, but by the 1990s, schools were asking the question not to start a conversation, but because it mattered to policymakers and school administrators. (Note: Unlike say, German schools, Canadian and American schools do not track children in high school based on anticipated post-high school plans. If/when children participate in vocational education as part of their HS experiences, they and/or their parents opted-in.) So, let's say someone asked you what you wanted to be and you weren't sure, ""marine biologist"" is right there. 

**Third, kids thought marine biologists were cool**. In my first paragraph, I explained how librarians ordered books to expose students to more and different futures, but also they ordered books because kids asked about them. To a certain extend, ""marine biologists"" slots into the same space as slap bracelets and jelly shoes. It was a cool thing kids liked and then it wasn't. For some, it may have been about a TV show (or a movie, or a series of books, a relative, Sea World marketing, etc.) Their parents may have been big fans of sea life themselves (as u/Kelpie-Cat describes [here](https://www.reddit.com/r/AskHistorians/comments/kp0w4x/why_are_whales_associated_with_cosmos_so_much/)) It has the appeal of being like a vet but underwater! It's easier to say than *zoologist* and *oceanographer* but more adult sounding than *fish doctor*! Adults have done a great job teaching children to answer a particular set of questions (more on ""what's your favorite color?"" [here](https://old.reddit.com/r/AskHistorians/comments/hng4kq/what_is_the_history_behind_what_is_your_favorite/fxd8tfr/)) and for a whole bunch of adults in 2021, we remember a spell in the 1990s where our answer to the question about our future career was ""marine biologist.""  


____


Tyack & Hansot, Learning Together: A History of Coeducation in American Public schools, 1990.

Sandberg, D.E., Ehrhardt, A.A., Mellins, C.A. et al. The influence of individual and family characteristics upon career aspirations of girls during childhood and adolescence. *Sex Roles* 16, 649–668 (1987).

Terzian, S. Science World, High School Girls, and the Prospect of Scientific Careers, 1957-1963, *History of Education Quarterly*; Urbana Vol. 46, Iss. 1,  (Spring 2006): 73-99.

DeFleur, M., & DeFleur, L. The Relative Contribution of Television as a Learning Source for Children's Occupational Knowledge. *American Sociological Review*, 32(5), 777-789.",0
"I would expect that they were independent traditions and that they could have arisen simultaneously with limited contact between the two. There is no evidence for trade as far east as China prior to the Roman period most clearly, and so if any such trade was happening between the regions it was likely to have been so far removed as to also likely remove cultural beliefs and implications. 

This is somewhat similar to an argument that people have put forward on the idea of the origins of writing altogether. An Assyriologist and linguist named Ignace Gelb once attempted to argue that writing was only ""invented"" once and that all other inventions of writing had happened by means of people learning of the other types of writing and creating their own. This theory has been entirely abandoned as Mayan writing was scarcely understood in Gelb's era and he believed the hieroglyphs were art and not writing in the pure sense, so now that we recognize Mayan hieroglyphs as true writing it is impossible to make an argument that could explain how the idea of ""writing"" was transferred from Mesopotamia to Mesoamerica. It was already a stretch doing so for China, but it was impossible to go further.

Today it is generally accepted in academia that writing was ""invented"" so to say probably 4 times entirely independently, in Egypt, Mesopotamia, China and Mesoamerica. This was likely in ways true for astrological traditions and beliefs as well. The Egyptian tradition and through it the Greek and Roman traditions certainly borrowed large amounts from Mesopotamian astrology, but there are almost certainly no connections between that tradition and the Chinese or Mesoamerican traditions. For the Mayans we know that astrology was likely a very important part of their written and oral culture, and the four genuine Mayan codices that we have still remaining today all detail astrological information. The different astrological traditions would have most importantly used different ways of dividing the sky into signs, which we call the ""zodiac.""

The zodiac itself was not invented in Mesopotamia until likely around the 4th century BCE. We have a few astronomical diaries written in cuneiform, which detail the movement of the planets and moon throughout a year (among a number of other things), that do not use the zodiac as a reference system from the 5th century BCE, but by the 4th century BCE the zodiac becomes the prominent reference system. The zodiac in Mesopotamia was basically the same zodiac that we have today in western astrology, with the only real change being that ""Aries"" was ""The Hired Man"" instead of a ram (something that is already changed by the Greeks). As I am sure many people have looked up, the Chinese zodiac is quite different in its constellations, and I expect the Mayan reference system for tracking the stars would have been equally as unique. This is a result of the fact that the stars one sees in the night sky will change depending on location. Each culture would need to create its own tradition for dividing up the stars in the sky in such a manner that they could be referenced and discussed in writing, and this is the most basic aspect of what astrology was in ancient cultures.",0
"A comparison that at least one scholar, Hebert Berg, has made is between Nation of Islam's relationship to Islam and Mormonism's relationship to Christianity. Both  groups expand considerably on traditional interpretations of religion. Both groups also had charismatic leaders who taught innovative new religious ideas.

**Founder's Influence**

NOI's unique theological views were shaped by its leader Elijah Muhammad, who led NOI from it's foundation in the 1930s until he died in 1974. Muhammad, born Elijah Poole, was converted to Islam in Detroit from the teaching of a rather mysterious figure named Wallace Fard.  After Fard disappeared in 1934, Muhammad began to teach that Fard was God, and Elijah Muhammad was his messenger. Elijah Muhammad would later explain the rest of NOI's cosmology.

**The Gap Between NOI and other Islamic Groups**

NOI's vast difference with traditional Islam led to real tensions with other Islamic groups. The Washington D.C. Islamic Center, funded by various Arab states, put out statements that NOI was not a legitimate form of Islam, and the Federation of Islamic Associations in the United States and Canada had a similar statement in 1963. Even a number of Ahmadiya Muslims, a relatively marginalized group that was often seen as heretical by Muslims, argued that NOI's teaching was false and harmful.

**Return to Islam?**

Elijah Muhammad's son Warith Deen Muhammad (born Wallace Dean Muhammad) was troubled by how little NOI resembled other forms of Islam. When he assumed leadership after the death of his father, he engaged in a reform campaign to make NOI resemble Sunni Islam. Whites were allowed to join in 1975, the calendar was aligned with the rest of the Islamic world (particularly important for the celebration of Ramadan), and terms like ""Temple"" or ""minister"" were dropped for ""Mosque"" and ""imam."" Warith Deen Muhammad ultimately rejected NOI's theology, including the idea that Fard was God, and in 1975 changed the name of the organization to the World Community of al-Islam in the West (today called the American Society of Muslims). Most of the membership of what was once NOI became indistinguishable from other Sunni Muslims.

The result of the reforms was schism. Louis Farrakhan, a minister of NOI's Harlem Temple, objected to Warith Deen Muhammad's reforms and asserted his own leadership of the organization. Farrakhan and a sizable minority of the membership continued to assert they were the true Nation of Islam. This branch of NOI kept Muhammad's teachings about Yacub creating the White race and Fard being God. Farrakhan justified his leadership with claims to divine revelation, especially by claiming he had been taken onto a spacecraft called the Mother Plane, which would rescue all Muslims at the end of time. Farrakhan taught that Elijah Muhammad was alive aboard the Mother Plane.

**Summary**

NOI's theology was the result of the teachings of it's longtime leader Elijah Muhammad. When he died, most members of NOI followed Muhammad's son into Sunni Islam. A smaller minority following Farrakhan still called themselves the Nation of Islam and kept the distinctive beliefs of Elijah Muhammad.",0
"It would differ from time, place, and circumstances.

Courtesy of /u/Iphikrates, the ancient Greeks, at least in the Classical period, repatriated their war dead for public burial. The exception was the Spartans, who buried them on the battlefield. You can read about that [here](https://www.reddit.com/r/AskHistorians/comments/e7hzew/a_spartan_soldier_returns_home_after_a_victorious/fa18xx7/). You can also read about how the Persians buried their own and the Greek dead after Thermopylai, but after Plataia the Greeks left the Persian dead to rot [here](https://www.reddit.com/r/AskHistorians/comments/f7484x/since_the_persians_fought_on_enemy_soil_what/fi9uj99/).

Tacitus describes Germanicus arriving at the scene of the battle of Teutoburg forest six years after the event [thus](https://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.02.0078%3Abook%3D1%3Achapter%3D61):
>Germanicus upon this was seized with an eager longing to pay the last honour to those soldiers and their general, while the whole army present was moved to compassion by the thought of their kinsfolk and friends, and, indeed, of the calamities of wars and the lot of mankind. Having sent on Cæcina in advance to reconnoitre the obscure forest-passes, and to raise bridges and causeways over watery swamps and treacherous plains, they visited the mournful scenes, with their horrible sights and associations. Varus's first camp with its wide circumference and the measurements of its central space clearly indicated the handiwork of three legions. Further on, the partially fallen rampart and the shallow fosse suggested the inference that it was a shattered remnant of the army which had there taken up a position. In the centre of the field were the whitening bones of men, as they had fled, or stood their ground, strewn everywhere or piled in heaps. Near, lay fragments of weapons and limbs of horses, and also human heads, prominently nailed to trunks of trees. In the adjacent groves were the barbarous altars, on which they had immolated tribunes and first-rank centurions. Some survivors of the disaster who had escaped from the battle or from captivity, described how this was the spot where the officers fell, how yonder the eagles were captured, where Varus was pierced by his first wound, where too by the stroke of his own ill-starred hand he found for himself death. They pointed out too the raised ground from which Arminius had harangued his army, the number of gibbets for the captives, the pits for the living, and how in his exultation he insulted the standards and eagles.  
[And so the Roman army now on the spot, six years after the disaster, in grief and anger, began to bury the bones of the three legions...](https://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.02.0078%3Abook%3D1%3Achapter%3D62)

To give some examples from my area, Takeda Shingen was laying siege to Shiga castle in 1547 when the *Kantō Kanrei* sent an army to relief the castle. Shingen sent a detachment to face the relieve force, and the Takeda won. Afterwhich the Takeda took the heads of the 3000 dead from the relief force and laid them out for the Shiga castle garrison to see. In general though they would've been buried. After Sekigahara, for instance, Ieyasu paid 1000 *koku* to the samurai in charge of the area and had him take care of burying the dead and fixing any damaged temples. After the Siege of Ōsaka, a merchant camp follower by the name of Okamoto Saburōemon, known by his shop of Yodoya, volunteered to take care of burying the dead bodies, in return for the right to strip and sell the valuables he find. In Japan it seem to be custom to bury the dead and build at least a shrine for the dead and give offerings, like [this one at Sekigahara](https://www.google.com/maps/uv?pb=!1s0x6003cb3a4fbea5e9%3A0xbfc6b14d3fef3243!3m1!7e115!4shttps%3A%2F%2Flh5.googleusercontent.com%2Fp%2FAF1QipMyM6fvUuTfimiI9ysoW_KHkkIifog8obZrdN7s%3Dw213-h160-k-no!5z6KW_6aaW5aGaIC0gR29vZ2xlIOaknOe0og!15sCgIgAQ&imagekey=!1e10!2sAF1QipMyM6fvUuTfimiI9ysoW_KHkkIifog8obZrdN7s&hl=ja&sa=X&ved=2ahUKEwjZvtL9mejrAhWjyYsBHRwYB_kQoiowCnoECBkQBg), or at least [a little plaque](https://www.google.com/maps/place/%E5%8D%83%E4%BA%BA%E5%A1%9A/@35.2476715,137.2702652,3a,75y,90t/data=!3m8!1e2!3m6!1sAF1QipNJ1ayZ8DcTIGS_pBqIxn5G-7p5RqPCx46HYVpY!2e10!3e12!6shttps:%2F%2Flh5.googleusercontent.com%2Fp%2FAF1QipNJ1ayZ8DcTIGS_pBqIxn5G-7p5RqPCx46HYVpY%3Dw203-h151-k-no!7i2288!8i1712!4m8!1m2!2m1!1z5Y2D5Lq65aGa!3m4!1s0x0:0x239e27903621e251!8m2!3d35.2475818!4d137.2703934) for the dead of a supposed 1467 battle, so the dead don't haunt you.

For the longest time, the people of Aizuwakamatsu held a grudge against the people of Kagoshima (Satsuma) and Yamaguchi (Chōshū) for invading them in 1868 during the Boshin War. One of the widespread stories is that the defeated locals wanted to bury their dead, but the government forbade it. This has been proven false in recent years, as documents came to light [that after Aizu surrendered, the government ordered four Aizu samurai to organize the burial of 567 dead](https://www.asahi.com/articles/ASKB24DYRKB2UGTB004.html) showing the dead were actually buried. In this case the victors had the defeated take care of the burial, and the legend show what the people thought should (and in this case, actually did) happen.",0
"Hi, I'm a beer sommelier and not a historian, so I'm not versed in the art of sourcing, but I can answer your question. 

In short, it wasn't. It wasn't until Louis Pasteur first demonstrated the process of contamination, and subsequently studied alcoholic and lactic fermentation in beer, wine and milk, and thus we could identify the microorganisms involved in those processes, that we developed the techniques required to isolate and multiply specific species and strains of yeast *in vitro*. 

Before this time, there was not much more than the cultural knowledge that fermentation *happens*, and despite the ignorance of the actual mechanism, there were thousands of years of accumulated empirical knowledge on how to begin and, to a certain extent, control fermentation. This also can explain well the long cultural relationship between bread and fermented beverages and religion, because honestly if you don't know germs exist and all your grape juice and wet grain is turning into a drug that makes you euphoric and uninhibited, you'll probably come to the conclusion that there's something magical happening here. But I digress. 

Traditionally there have been many ways to cultivate yeasts without knowing what they are, and they all revolve around the same principle of keeping a bit of what's been fermented to be used the next time. Breweries would often have a ""blessed"" wooden stirring paddle that (we now know) had become infested with yeasts from previous brewing, and was used to stir the next batch, thus beginning fermentation. Sometimes they would keep a piece of wood made for this specific purpose, or harvest some of the trub(the layer of residue left in the bottom of a fermenter, which, we now know, is composed mostly of dead and live yeast cells) and dump that in the next batch. Bakers would do similarly by using some stale bread, which would also carry live yeast cells, to start the next batch of bread. It's worth noting that the main yeasts used for bread and beer are the same species. With yogurt also, to this day many people make homemade yogurt by using the previous batch to start the next. Sometimes, most notably with wine and what we now call wild ales, cheese and so on, there was no direct inoculation of any form, instead the makers would simply prepare the base product and let nature do its magic. This is because yeasts and fermenting bacteria like lactobacili exist all around us, in minute quantities, so given enough time and exposure, anything that has fermentable material will ferment. Of course, this means a lot less control over the final product, but also a lot more complexity, as instead of one cultivated strain acting on the food, you will have a myriad of different yeasts and bacteria competing, to different results. Beer is still made this way in some places in Belgium (Lambics), and now the US and Brazil are making some too since it's delicious. Wine is also made this way, the skin of grapes naturally gathers yeasts, so grape juice pretty much always turns to wine, unless it's pasteurized. 

Hope I was clear enough. Book sources for this include Randy Mosher's Tasting Beer, and The Brewmaster's Table and The Oxford Companion to Beer, both of these by Garrett Oliver. If the mods feel I need to provide more sources or reformat this in any way, please let me know. 

E: one piece of trivia: sometimes you'll see advertisements for Budweiser claiming that they use Beechwood in their process, and that somehow enhances flavor. This is marketing bullshit. They use chunks of washed Beechwood precisely because it *doesn't* affect the beer flavor, and wood provides an excellent medium for yeast to attach itself to. Other breweries do just fine without this, and others still actually use wood for flavor in many ways, but not in the fermentation process. ",0
"I'm glad you didn't specify any particular place or time because I'd like to add some info about Africa here. I haven't studied sleep specifically, but you do run into it in anthropology. Particularly, there is a remarkable study [1](https://repository.kulib.kyoto-u.ac.jp/dspace/bitstream/2433/128939/1/ASM_31_107.pdf) by Hewlett & Hewlett comparing the sexual habits of two neighboring communities of Aka foragers and Ngandu (Bantu speaking) farmers in central Africa. This paper could only be done because the two researchers had known these communities for 20+ years, only then were they able to pry at some fascinating questions about touchy subjects, ones which most people wouldn't tell a random inquisitive outsider. Some of these responses might be quite shocking to our delicate sensibilities...Both societies observed a ""post-partum taboo"" which meant that for a year after birth the couple shouldn't have sex. For the Ngandu men and women interviewed, 100% believed in the taboo and a staggering 90% of the men said they simply left the household to go to another town to have sex with *other women* during this time! One man said:

> No way I will wait one year. I will search for another woman, masturbation is not an option. If I search far away I will use a condom. I am not searching for children, only pleasure.

Even though this is ""accepted behavior"" it is still not *good*. If you do this, your child could catch ekila dibongo (taboo disease of the knees) which can be fatal. So to safeguard their philandering, men then use medicines (on themselves) to fight the disease. I'm in the United States, and I don't think I could possibly convince my wife to agree to this situation for the first year of our baby's life. In fact, if I said this in all seriousness, I'd probably get an angry response or even slapped; a reminder of how different social worlds can be sometimes. 

But you're asking about sleep patterns...So both Aka and Ngandu people woke up multiple times during the night for a bit, to do things like have sex, maintain the fire, take care of an infant, to talk, or to eat. And to be clear, people weren't having sex every night, but would have a session for one night and then rest for a few nights. This averaged 2.1 days of resting for Aka couples and 2.7 days of resting for Ngandu couples. On average, Aka couples aged 18-45 had sex 3 times a week and 3 times a night each session. For Ngandu couples aged 18-45, it was 2 times a week and 2 times each session. Each ""time"" was sex until male orgasm, as men reported (Aka men formally and Ngandu men informally) that they had an orgasm each time. But for women, both groups said they had an orgasm ""at some point"" or only once the whole night (though some had more). 

This was the average, so sometimes it was 5 times a night. As one Aka man said, ""...If I do not do it five times my wife will not be happy, because she wants children quickly."" Also, some Aka men have second wives, although this socio-sexual triangulation (and how it relates to their sleeping patterns) was not questioned by the researchers. An Aka man said, ""My father is dead and I need to make a big family. My first wife found my second wife for us because she was also looking to have many children."" This sounds like quite a lot of *work*, and ""work"" (the Aka word - bila) is precisely how the Aka and Ngandu define sex sessions. As Aka people said, ""The work of the penis is the work to find a child,"" and ""Getting food is more difficult, but both are lots of work. Sex life is not as tiring as work during day; the work at night is easier because you can make love then sleep."" Although some Aka younger couples reported having sex in the forest, we can see here how strongly their social values about *work* and *creating a family* are attached to their domestic sleep patterns. In fact, how Aka people build their houses is related as well, as they are designed specifically for a place to do these night-time activities (and not too much else). As Boyette and Hewlett [2](https://www.google.com/search?sxsrf=ALeKk024n6m30sYnAmx62t9NJAzoYQlCQQ%3A1582912199402&ei=x1JZXraUGPGrytMP9u68OA&q=Hum_Nat_boyette_hewlett_reC5Fss&oq=Hum_Nat_boyette_hewlett_reC5Fss&gs_l=psy-ab.3...34499.34499..35035...1.0..0.153.401.2j2......0....2j1..gws-wiz.......35i39.o65Sdlp1wIk&ved=0ahUKEwj2k-y65_TnAhXxlXIEHXY3DwcQ4dUDCAs&uact=5) mention, ""Aka houses are used only for sleeping, are placed 0.3 to 1.22m apart on average, and have enough room for a hearth fire and a bed (Hewlett et al. 1998).""

So generally if you're in a society such as this, where your house is small and of perishable materials, then you'd have developed/inherited a sleep pattern which lets you (or someone in your house) wake up to tend the fire. Some societies don't do this, and so sleep through it and awake to the fire being smoldering embers. But for those societies who do, their sleep patterns are changed: 

> Fire also produces steady, irregular (in volume, frequency, and quality) noise that some ethnographers report as being subliminally monitored in sleep: continual small noises are reassuring, loud pops are arousing, and the absence of sound wakes the sleeper concerned with fire maintenance. The presence of the flicker and the faint glow from the fire is reported as comforting and soothing or hypnotic, conducive to sleep during periods of nighttime insomnia, and facilitative of reassuring visual scans of the sleeping space.
- Toward a Comparative Developmental Ecology of Human Sleep, Worthman & Melby [3](http://anthropology.emory.edu/home/documents/worthman-lab/Ecology%20of%20Human%20sleep.pdf)

That lengthy paper by Worthman & Melby has more information on looking at how peoples around the world sleep, but generally it's going to be quite different from what you are likely to have experienced (if you are European or Euro-American). While Euro-Americans allow their pets such as dogs and cats to sleep with them, they often don't let children sleep directly with them. This is the opposite in the rest of the world, where pets are more often ejected and babies allowed. ""It's pretty much universal that babies don't sleep alone. They either lie right next to their mothers, or nearby on a mat, or in a cradle or a sling,"" notes Carol Worthman [4](https://esciencecommons.blogspot.com/2012/01/some-eye-opening-thoughts-on-sleep.html). This is called co-sleeping and is found in many parts of the world, being done after infancy as well. As Hewlett & Roulette [5](https://psycnet.apa.org/record/2014-09498-010) summarize: ""...[it] may be common in the high fertility mortality small-scale cultures that characterized most of human history."" But specifically, the Hewlett's and others [6](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3049106/) have focused on Congo basin central African foragers and farmers. These societies simply have a lot of ""emotional proximity."" 

> Forager camps are generally very dense, often occupying a space the size of a large dining and living room in the USA or the space of one or two [Bantu speaking] farmer houses. When hunter-gatherers sit down in the camp, they are usually touching somebody. At night, foragers sleep very close together and usually sleep with someone; our study of co-sleeping found that forager children and adolescents never slept alone.""
- Hewlett et al. [6]

> ...the number and composition of sleepers in the small (about 2 meters in diameter) leaf huts of Efe foragers vary, but virtually no one sleeps alone, and one may routinely find two adults, a baby, another child, a grandparent, and perhaps a visitor sleeping together in the small space. Two or three sleep along the back of the hut, one of either side of the fire, and another one or two around the edges. Degree of physical contact is high, with full body contact and frequent entwining of appendages of two or three sleepers, along with periodic arousals associated with rearrangement movement of others, noises (cries, sniffs, snores, etc.), and traffic associated with staggered bedtimes and occasional elimination...
- Worthman & Melby [3]

This is a historically documented practice as well, the Khoi pastoralists of southern Africa are shown co-sleeping in a large group [7](https://archive.org/details/uncivilizedraces00wood/page/246/mode/2up) as seen in a racist popular anthropological overview from 1878. As mentioned on [page 233](https://archive.org/details/uncivilizedraces00wood/page/233/mode/2up), he notes their ability to sleep at any time, ""it is almost impossible to place him [a Khoi person] under conditions in which he will not sleep."" He notes that this curled up and co-sleeping arrangement is different compared to nearby Bantu speaking farmers, who sleep in their houses as families on laid out beds. While unverifiable, he notes that a Khoi person will sleep if they are hungry and have to wait til they can get or make food later.",0
"In short, the answer to your question is no. Raccoons were not eaten among high society, either in the 1920’s or at any point during the 1900’s. It is also unlikely that raccoons were fancy food in American culture during the 19th century.
 
This is why President Calvin Coolidge felt uncomfortable eating Rebecca the Raccoon for Thanksgiving dinner in 1926. He was playfully ridiculed [by at least one newspaper](https://chroniclingamerica.loc.gov/lccn/sn83045462/1926-11-27/ed-1/seq-1/#date1=11%2F08%2F1926&index=5&date2=12%2F01%2F1926&searchType=advanced&language=&sequence=0&words=raccoon&proxdistance=5&state=District+of+Columbia&rows=20&ortext=raccoon&proxtext=&phrasetext=&andtext=&dateFilterType=range&page=2) for having cosmopolitan tastes and pardoning the Thanksgiving mammal. The raccoon was originally sent to The White House by [Vinnie Joyce from Nitta Yuma](https://chroniclingamerica.loc.gov/lccn/sn83045462/1926-11-26/ed-1/seq-1/#date1=11%2F08%2F1926&index=0&date2=12%2F01%2F1926&searchType=advanced&language=&sequence=0&words=RACCOON+raccoon&proxdistance=5&state=District+of+Columbia&rows=20&ortext=raccoon&proxtext=&phrasetext=&andtext=&dateFilterType=range&page=2), Mississippi, a still unincorporated community along the Mississippi River. Rebecca Raccoon was then adopted by second lady Grace Coolidge and kept lovingly for several years. She was a wild animal (the raccoon, not the first lady), supposedly bit the President, and did not enjoy large meetings, [the Easter Egg Roll in 1927 for example](http://www.loc.gov/pictures/resource/npcc.16723/). After the Cooledges’ left in 1929, Rebecca was sent to the Rock Creek Zoo in DC where she died shortly after. When Herbert Hoover took office in early 1929, Billy Possum claimed squatter’s rights on Rebecca’s prior home. He was shortly [apprehended by Officer B. B. Snodgrass](https://www.whitehousehistory.org/photos/raccoons-at-the-white-house-photo-3) before being granted permission to live in Rebecca’s prior home for the rest of his life with few changes.
 
But it is worth noting that Rebecca Raccoon was sent to the White House with the intention of accompanying the Thanksgiving turkey (Or even replacing the turkey). Raccoons were eaten extensively during the 1800’s to the point that Mark Twain wrote about “possum” and “coon” as being one of [the true American meals](https://www.gutenberg.org/files/119/119-h/119-h.htm) he missed on his tour of Europe. They were, and still are, classified small game within hunting laws, and are sometimes given special considerations as to handling or hunting. There were mentions of them being cooked in a traditional African style by many black communities and raccoons were widely eaten throughout Native American societies prior to European settlement by being hunted rather than farmed. Further, there are only a few examples of raccoons being farmed, namely in the 1920’s. However, this was mainly for fur specifically for [raccoon fur coats](https://commons.wikimedia.org/wiki/File:The_Saturday_Evening_Post,_November_16,_1929.jpg), rather than for meat, and there have been no industrial attempts to farm raccoons in the US since then. Quite simply, the history of raccoons as a food source is the history of hunting in the US, and specifically of small game hunting within the United States.

Part 1

Edit: Hoover came to the White house in 1929 not 1927",0
"

Finally then, did these outbreaks in the ghettos, or in certain camps, result in large scale infection of the guards or non-prisoner personnel? In short, no. Almost all of these diseases are in fact quite treatable. Typhus, for instance, can be vaccinated against, and the vaccine was even on hand there at the time, but you can fairly safely assume *who* was going to receive it and who wouldn't. Likewise, Germans had access to effective delousing, a process which, humiliatingly, was often done by forced laborers who themselves had no such protections. It ought to have been little surprise that the Jewish workers forced to do delousing of German soldiers quickly saw typhus infections among themselves. 

This isn't to say that infections were *never* possible. Even the vaccine wasn't a total guarantee, and at least one outbreak of typhus I found mention of occurred among Lithuanian staff in a German hospital who all did have the vaccine, but on the whole, even with some chance infections, there was little risk of widespread outbreak within the German ranks. Even without the vaccine, and even without the better delousing procedures, they were also a population of mostly well-fed, healthy young men and women, who in the case of infection, had access to decent medical care and would be kept in proper medical isolation in situations where it was called for. 

So while, hopefully, I've provided some better insight into the nature of disease in the camps, and the at best half-hearted attempts by the Germans to do anything about it, the end result, and the answer to your *specific* query is a fairly muted on. Not that I can say, with absolute certainty, there isn't a source out there which mentioned an incident, on the whole, it is very safe to say that large outbreaks of disease among the Nazis who ran the camp system, or enforced ghettoization, were not an issue faced, and while some isolated incidents can be pointed to, the circumstances for large-scale epidemic simply weren't present.


**Sources**

Baumslag, Naomi. *Murderous Medicine: Nazi Doctors, Human Experimentation, and Typhus.* , Praeger, 2005.

Cesarani,  David  (2006) ""A Brief History of Bergen-Belsen,"" *Holocaust Studies,* 12:1-2, 13-21

Rees, Laurence . *The Holocaust: A New History.* Penguin Books Ltd, 2017.",0
"^^^Holy ^^^crap, ^^^I ^^^can ^^^actually ^^^contribute ^^^to ^^^something ^^^here! ^^^This ^^^is ^^^your ^^^moment, ^^^buddy, ^^^don't ^^^screw ^^^it ^^^up!

Mary Del Priore published a book on imperial letters between the Brazilian Emperor, Dom Pedro I, and his mistress, the Marchioness of Santos, Domitila de Castro Canto e Melo. Quite an interesting book, honestly, exploring their life and the Emperor's marriage with Empress Leopoldina, his first wife.

The Emperor was know, even at the time, for being a womanizer. His attempt to secure a second wife faced some serious problems for it. He liked to exercise, had a fit body and Leopoldina described him as ""like an Adonis, shadowed by a brown curly hair and beautiful black eyes"". He was also a gregarious, charming man and, well, he was the Emperor. Being successful with the opposite sex wasn't that hard to begin with.

The Empress, however, was not really know to be a pretty woman. Overweight, short, and regarded by the Emperor as someone to do ""marriage love"". Reportedly, she considered sex to be ""disgusting and unfit to a lady"". Sex was to be performed ""in silence, laid in bed, and thinking of the empire."" Like a proper Austrian princess should behave, like her mother-in-law told her she should behave. Fun, huh?

She proved herself quite a competent ruler, was loved by her people and is still regarded as an important figure in Brazilian art history. But her marriage life was probably not ideal, despite her obvious dedication to her husband and nation.

Before declaring the Brazilian independence from Portugal, the Emperor met Domitilia de Castro Canto e Melo, from São Paulo, daughter of a high ranking military officer, recently divorced. How they met is somewhat mysterious, but she was a woman that wouldn't ""pollute the pleasures of the flesh with prayer"", like Leopoldina would. A delicate mouth, green eyes, tall, dark hair, and that Brazilian body. São Paulo also had a shortage of men at the time, witch caused the female populace to become more independent and strong willed. 

Dom Pedro I gave her a mansion in the capital, Rio de Janeiro, a title, as Countess first and eventually as Marchioness, and (why the hell not) placed her as a Lady-in-waiting for the Empress.

According to Mary del Priore, Dom Pedro I ""never confused his verbs: marriage and love. Nobles of blood should only marry nobles of blood, never nobles of bed."" She describes the Emperor's life in Domitila's house as somewhere he could experience all the pleasures of the flesh and social life, where he would laugh with his friends and receive members of the court.

Now that introductions are done, the *hotter* parts of the book talk about letters between Dom Pedro, who signed them as (seriously the best translation I could come up with) Hot Little Fire, to ""Titilia"". In those letters, he would usually include drawnings of his own penis, sometimes ejaculating, or add pubic hair. Domitila would always answer in kind.

In the letters, both Dom Pedro and the Marchioness, talk openly about their sexual encounters. Mary del Priore details that the Emperor liked to send kisses to her ""thing"" (Mary's word) and to use a clear language: he wanted to orgasm, both on her and with her. Domitila played along, asked what he wanted her to wear, how she wanted to receive Dom Pedro again in her house and helped to plan soirées in her house.

Eventually their relationship ended. Dom Pedro went to back to conquer the Portuguese throne and Domitila married a rich farmer. The Empress Leopoldina, however, died aged 29, leaving 7 children and a bundle of depressing letters to her family in Europe and friends. He never saw the end of the relationship or her son take the throne.



-------------------------------------------------------------------------------



This is my first post here, hope I'm not breaking any rules. Also, I'm not a historian, so let's hope someone can expand on this.

It is a single example, but it was the only one I could provide literary evidence to support. Dom Pedro had many women in his life, it's sometimes hard to see what is actual a proven fact and what is not.",0
"[1/2] (or [1/3, thanks to the majestic /u/hillsonghoods])

It's true that no one would ever use the term ""medieval surveillance state."" However, the problem with that phrase is *state*, not surveillance.

You might be familiar, in media discussions, of the idea that we live (or in these accounts, lived) in a narrow band of history where there could be an expectation of privacy in the Western world. The typical image invoked is the village small enough that everyone knows everyone else's business. But that's still a fairly recent view.

Rewinding to the Middle Ages, we meet a concept called *fama*. This is a Latin word that means reputation or word on the street or rumor, some combination of those--and in medieval courts, *fama* was a legal principle with concrete implications.

Bad *fama* was used to discredit witnesses or reject their testimony altogether. According to 13th century French legal texts, in a lawsuit between someone with *bonne renomee* and someone with *mals renome*, the first person would receive the benefit of the doubt automatically. In some cases, bad *fama* would cause a person's lawsuit to be dismissed out of hand, or permit them to push for charges of fraud. 

F. R. P. Akehurst citing civil jurist Philippe de Beaumanoir gives this exemplum of the power of *fama*--and who had control over it:

> An innkeeper with a good reputation could avoid charges of having stolen property from his guests, but if his reputation were not very good he would be the most likely suspect, even if there were signs of forced entry and broken chests. In such a case, the reputation of the innkeeper would be determined by a judicial inquiry. 

> The only kind of evidence such an inquiry would turn up would be oral: what people *said* about a person could make or break him. The inquiry also delved into what other people thought of a person, what they remembered of him.

While Beaumanoir is writing a prescriptive text, Akehurst compares the procedures listed favorably in terms of reflecting contemporary practice. So we should take seriously what Beaumanoir is saying here: forensic or physical evidence did not determine the case; other people's opinions of a person close to the crime did. Gossip made reality.

There is also, for the late Middle Ages into the early modern era, the question of the sacrament of confession. This has a vast and contentious historiography, so in advance, I want to be clear that we have to distinguish between ""the population at large"" and ""some individuals here and there""--that is, not everyone has the same experience or depth of exposure/intensity/care.

In 1215, the Fourth Lateran Council very famously (well, okay, very famously to medievalists, which is not really very famously at all) decreed that all Christians of both sexes must confess their sins to their parish priest once a year. The actual point wasn't confession itself, of course--it was that lay Christians must receive the Eucharist once a year, and confession was necessary to cleanse one's soul before what was central enough to be just called ""the sacrament.""

In practice, however, the confession-Eucharist connection amounted to a strong focus on *both* sacraments in religious instruction: the Eucharist, that it was the genuine body and blood of Christ and reception was necessary for salvation; confession, what sins were and what was moral behavior and the necessary contrition-confession-penance triad. Oh, yeah, and that really you needed to confess to a priest and receive sacramental absolution; just shouting at the sky was insufficient for salvation purposes.

Now, this doesn't mean that in 1216, every Christian in the medieval West was confessing their sins on Palm Sunday *just like that*. However, participation ramped up over time; by the fifteenth century there were dioceses mandating confession more than once a year, and others reporting it was offered more frequently to certain groups (""women and students"" being my favorite example). 

Medievalists have absolutely called confession an attempted tool for social control or discipline. It's not an accident that even into the 15th century, German-language (not Latin!) texts on awareness and avoidance of sin divide wrath into murder, war, and arson--these are real issues people struggle with. 

And as Pierre Payer pointed out, instructional manuals for *confessors* focus on sexual sins at a rate from twice as often as anger and greed (Robert Grossteste) to seventy-six times as often (Robert of Sorbon, *we know who you are in the dark*). The Church had long made the definition of marriage and attempt to control sex a centerpiece of its play for power over the Church on Earth to make sure it became the Church in heaven as well.

So looking at late medieval guides to confession, for priests and for lay people alike, scholars like Steven Ozment and Jean Delumeau argue for the late Middle Ages as a period of immense social anxiety over confession, over having to scrutinize every inch of your soul for every possible sin lest you miss a tiny thing that punts you to purgatory or even hell. The problem is, to this end they cite almost exclusively post-Reformation Protestants, *especially* Martin Luther. A significant chunk of whose theological game was that terror over confession and penance and never being good enough was part of the spiritual crisis and temptation of the devil that pushed him towards the 'breakthrough'. These are, in other words, absolutely not objective accounts.

Looking at medieval sources, we find a much more diverse picture. Standards for behavior/recognition of one's sins to the point of emotional self-mutilation became a hagiographical trope for women ""living saints"" like Dorothea von Montau and Elisabeth Achler. They're confessing every day, confessing every sin of their childhood over and over, etc etc. 

And usually their hagiographer (also their confessor) is noting that these women should be examples of spiritual excellence, NOT role models to follow. There is also evidence from 15th and 16th century sermons that some theologian-priests were preaching that the *desire* to be saved, along with the sacraments, was enough even if one couldn't live up to behavioral standards. 

Unfortunately, we can't do what we really want, which is to get down in a confession session between a priest and penitent and find out what confessors *actually* demanded. Did they scroll down the list of Latin questions about sins and translate to the vernacular on the fly? (These include things like ""did you kill anyone"" to ""did you throw snowballs at someone passing by your house"") Were they ""one and done""-ing assembly line offering services? Of course, there was probably a variety of severity...and by the 15th century, lay people were gradually winning the right to choose their own confessors.

Which brings us, in fact, back to *fama*.

Confession today brings up mental images of ""the confessional,"" the closed little *private* box, hushed voices. The confessional is an early modern invention. While priests were required to keep the so-called seal of the confessional, the actual practice of it would be the penitent standing next to the priest with a long line of their neighbors standing right there--easily in a position to overhear, popular literature attests.

I've illustrated, I think, the immense difficulty of securing emotional (not necessarily physical) privacy in the later Middle Ages, far beyond 'nosy neighbor' nostalgia for early 20th century Main Street, USA or 19th century prairie towns. I've also shown that people reacted with different intensity to aspects of this culture. That's not to say anything about 'mental illness' at all, you understand; I just want to start by breaking down a monolithic medieval Christian society in terms of responses to what we might see as ""popular surveillance.""

I am in general going to let the psychologists talk about schizophrenia and its history as a disorder, but I want to make a few remarks on how historians approach neuropsychiatric disorders. A lot of things about the Middle Ages scream ""superstition!"" to us today--fear of the devil, belief in mystical visions can easily read as delusions and paranoia. We need to distinguish what was quite normal to most medieval people from what can read as sliding into paranoia today.",0
"It's not ""code"", but such claims could be met with suspicion. Hunting accidents were both a natural and common occurrence, but also a good cover. 

Let's look at one of the more suspicious hunting accidents - the death of king William II of England in 1100. He was out hunting with two men - Walter Tirel and Henry (William's brother and heir to the throne) - when he was shot with an arrow after Walter mistook him for a deer. Obviously, one has to wonder how the hell someone mistakes their king for a deer. Other versions of the story have Walter know where the king is relative to the deer, and is simply unlucky with his shot. This is all difficult to believe because Walter Tirel was known for his skill with a bow and being a keen hunter - that's why the king had invited him to go hunting in the first place! Walter immediately fled to France and denied that he had ever gone hunting with William II. It was also recorded that William II had received a message that morning from a monk who had dreamt last night that the king would die if he took part in the hunting trip, which sounds a lot like a coded warning, but could also just have been a coincidence or an invention to help explain why God had chosen to kill such a sinful king, as in other versions of his death it was William himself who had the dream. It also just so happens that the only other person on the hunt, Henry, was then crowned as king of England and other members of the wider hunt benefited enormously from Henry's rule. If it was an accident, it was incredibly convenient for Henry. 

But the strange thing in this case is that people generally accepted it was an accident despite obvious problems with the story. Here's the most detailed account from William of Malmesbury: 

> “The sun was now declining, when the king, drawing his bow and letting fly an arrow, slightly wounded a stag which passed before him… The stag was still running… The king, followed it a long time with his eyes, holding up his hand to keep off the power of the sun’s rays. At this instant Walter decided to kill another stag. Alas, [Walter's arrow] pierced the king's breast. …Walter immediately ran up, but as he found him senseless, he leapt upon his horse, and escaped with the utmost speed. Indeed there were none to pursue him: some helped his flight…


Although there were rumours that it was an assassination rather than an accident, such voices were not the majority. The fact is, hunting accidents were normal. For prey like deer, a well placed arrow was usually sufficient for a kill, but sturdier animals like boar (a favourite of the aristocracy) had to be fought up close with spears and swords. Getting up close was the manly way to hunt, and that could be dangerous. Another of William II's brothers, Richard, had died in the same forest after being knocked down and trampled by a stag. The great Byzantine emperor Basil I died after his clothes were caught in the antlers of a deer and he was dragged by his belt for miles as it bolted. King Fulk of Jerusalem had the most brutal of accidents - his horse tripped at speed, leading Fulk to fall off and be followed by the tumbling horse. The saddle crushed his skull, but didn't kill him immediately. His injuries were so bad that William of Tyre records how his brains were partially forced out, and he died comatose three days later. Even accidental shootings could occur, such as with Valdemar of Denmark who, despite being very popular and having no enemies who would want him dead, was killed by a stray shot whilst hunting. 

So hunting was dangerous, and hunting accidents were a common cause of death that not even kings and emperors could escape. When we hear of some noble dying in a hunting accident, there's a good chance that it actually happened. This is especially true of accounts where it was an animal, not a person, that killed them. Where someone was killed by an arrow, that's when the suspicions start. Accidents could and did happen, but that's what made it a good cover story for a murder. At the very least, a hunting accident created plausible deniability - despite the clear problems with William II's death being an accident, it remains genuinely possible that it was so and the evidence of an assassination can all be dismissed as circumstantial. Given that William II was  unpopular, it may have been more convenient to go along with that story than to raise questions. We have no way of knowing whether William II's death really was an assassination. 

But generally speaking, 'hunting accident' was not code for 'quietly murdered', even when we have good reason to believe that someone was indeed assassinated.",0
"Focusing on Europe and North America:

Distilled liquor starts to gain more than a microscopic presence in Europe at the very end of the Middle Ages. However, at this point, it's mostly in the hands of apothecaries prescribing it as medication. 

It's also very limited in consumption in ways very important to our purposes. The strength of hard liquor did not go unnoticed by city and imperial authorities, who (a) instituted wildly high prices to discourage significant purchase (b) actually limited amounts that could be purchased. In particular: in some places/at some times, customers of apothecaries and grocers were limited to purchasing brandy in-store, for immediate consumption standing at the counter.

But here's the really interesting thing. ""Shot"" as an alcohol-related word, in the 1500s, still refers to the *bar tab* (and in fact, for the group, not even for an individual). But ""dram,"" another modern word for...well, a shot--*dram* seems to refer to a doseage of alcohol-medicine by the end of the 16th century. The OED has a written reference from 1590, suggesting that a dram had been better than a damn years before.

So we're still at liquor as medicine, but the idea of drinking a shot and *calling it* by a word that will later come to mean our modern shot, is already present.

But as people are gaining experience with quickly drinking small amounts of distilled liquor, a second practice is becoming quite popular. One that involves swift consumption of an alcoholic drink in ways that would hopefully limit how much you would drink at once.

Around 1500+, the practice of ""pledging healths"" becomes not just wildly popular but seen as a genuine social problem. Which is to say: drinking toasts, but turning it into a drinking game and a competitive sport. People would go around the table pledging a health to which everyone would have to drink. At the beginning of the night, the point was to pledge the most clever health (a backhanded compliment to your friend's wife, perhaps?); at the end of the night, ""most clever"" probably had a...rather more relaxed meaning.

Now, pledging healths in 1517 is not drinking shots in our modern sense. But it's a step towards that sort of practice.

Different scholars will point to different points in the next 200 or so years when drinking distilled liquor becomes truly popular. Ann Tlusty lays out laws from Augsburg and elsewhere in the Holy Roman Empire at the end of the sixteenth and early sixteenth centuries that get increasingly shrill about trying to limit the drinking of hard liquor, especially brandy. (Although much of this also involves the sale of watered-down or low-quality liquor, which would either be less likely to be drunk as shots for the obviously reason, or more likely if it tasted gross).

Jack Blocker, on the other hand, puts the time frame just a slight bit later, and suggests it is especially with European colonialism in the West Indies and North America where white people become especially fond of drinking rum. Yes, very weak alcohol consumption was extremely popular in homes at this time--but it was beer. Rum and other drinks were more limited to public houses, and apparently these were pretty seedy on the whole. (In other words, you're not going to have men in robes with pipes discussing international affairs while sipping on gin and tonic, with their pinkies in the air.)

By the early 18th century, ""dram"" (at the very least) seems to signify a shot in the modern sense. In one of his satires, Alexander Pope has a character mistake someone saying drama for *dram*, and explain apologetically that there is no jenever left. I don't drink, but I am informed that jenever is a form of gin typically consumed in quick, small amounts.

So by your 1880s ball, my gentlefriend, you would definitely not be throwing away your shot.",0
"I wrote that imprecisely. 

[Here's a seperate article on Redoshi](https://www.nytimes.com/2019/04/03/us/transatlantic-slave-trade-last-survivor.html) 

At the time of Cudjoe Lewis's death in 1935, he was believed to be the oldest surviving person who had been part of the transatlantic slave trade.  That was generally believed up until about 2018. 

Sally Smith (Redoshi) was brought over as a 12 year old and married as a child bride to another slave.  Presumably, because of her situation, her origin was not well documented and had to be pieced together through detective work.  Although she survived lewis by two years, her existence as such was not ""known to historians""  until the last few years when a novelist doing research stumbled across her story and consulted a historian. More research was done and was published. 

In the NYT article above it's also worth pointing out, Redoshi was interviewed shortly before her death for a department of agriculture video released in 1938.  That footage is apparently the only known footage of a female survivor of slavery.",0
"Don't know if the mods are going to remove this whole line of discussion, but in case it sticks around:

The problem is that when you put ""nationalist"" in front of ""socialist,"" the former cancels out the latter. Each ideology has a fundamental disagreement with the other regarding what people's core identity is, and how political and social relations should look.

Socialism says that everyone has an economic identity: the workers of the world are common with each other, not to those of other classes who share their ethnicity and nation. And relationships should be egalitarian and non-hierarchical: everyone gets a voice.

Nationalism/fascism says that everyone has an ethnic identity: the members of the nation are common with each other, not those of other nations who might be in the same class position. And relationships within the nation are hierarchical: you follow orders from the top and give orders to those below you.

That's all ideology and not practice, in which it does get complicated. But even if you're wanting to grant them the idea that they started out socialist: 1) it's pretty clear they were appropriating a popular term for their own use, 2) their party program was ideologically incoherent in the early years because they wanted to be all things to all people, 3) after taking power they purged their quasi-socialist faction and went all in on a capitalist war economy (Night of the Long Knives).

To your final point, I'm not sure you can make a distinction between wartime and peacetime economy under the Nazis. They were preparing for war from Day One, and even those social efforts you describe were often based on a sort of internal imperialism of looting Jewish businesses, political opponents, ""Gleichschaaltung"" or shifting everything into Party control. (See Adam Tooze's ""Wages of Destruction"" for a definitive take on the Nazi economy.)",0
"***How is RBG related to the ECOA***

Ruth began working with he ACLU (American Civil Liberties Union) and their Women’s Rights Project, where she worked (among other things) on litigation strategies for furthering Women’s rights in court, and participated in 34 cases from 1971-1980, six of them as lead or co-counsel in oral arguments for the supreme court.

The legal background for this period was some of moderate progress, for example the  with the 1963 *Equal Pay Act* and the 1964 *Civil Rights Act Title VII*, but at the same time there was *no* supreme court case of sex discrimination protection under the constitution, and you had cases like *Hoyt v. Florida (1961)* that accepted a Florida state law where only men were required for Jury duty, and accepted that an all-male jury was not problematic^(3).

A landmark legal change was the case *Reed v. Reed (1971)*, which was the first case such supreme case Ginsburg and the ACLU cooperated on, with Ginsburg and Mel Wulf writing the brief. The details of the case aren’t super important, the key is that it was the first case establishing fourteenth amendment protection (*equal protection clause*) against sex discrimination. The supreme court ruled that the Idaho law that gave automatic preference to men, without regard to the individual abilities:

>“cannot stand in the face of the Fourteenth Amendment's command that no State deny the equal protection of the laws to any person within its jurisdiction (…)  
>  
>The Equal Protection Clause of that amendment does, however, deny to States the power to legislate that different treatment be accorded to persons placed by a statute into different classes on the basis of criteria wholly unrelated to the objective of that statute. A classification must be reasonable, not arbitrary, and must rest upon some ground of difference having a fair and substantial relation to the object of the legislation, so that all persons similarly circumstanced shall be treated alike.""”

A further important case is *Fronterio v. Richardson* *(1973)* where differentiated benefits to the families of former female and male service members was a violation of the *equal protection clause* under the fourteenth amendment.

The newfound *equal protection clause* protection against discrimination spurred both legislative and judicial action and put a lot of existing laws or practices in an unclear legal position. The cooperation on this case was probably the basis for Ginsburg joining the ACLU and their women’s rights project. Going forward, they identified some core areas of discrimination that could be combated both in the legislative and judicial area, the discrimination in credit was one of those areas.

So the *credit Discrimination* was a part of the wider womens movement at the time, and other organisations had focus on it as well. The issue especially gained popularity after an editorial in the 1972 Ms. Magazine of a womans process in trying to get a credit card and the responses to that. The National Organization for Women (NOW) formed a credit task force, the Center for Women’s Policy Studies received a grant to undertake studies and the National Commission on Consumer Finance held hearings on the availability of credit to women, part of whose conclusions I have cited above.

The literature seems to indicate this feminist push for credit equality as the driving force behind ECOA, that of course doesn't mean the ACLU or Ginsburg were solely responsible, NOW seems to be an extremely central organisation in this push specifically, but the ACLU was a part of it and the legal groundwork it had laid should not be ignored, even if the effect is indirect. Ginsburg did not alter credit discrimination in a concrete case on either side of the bench, so the quote does seem a bit exaggerated legally speaking^(4), but it certainly was one of the issues she has a record of fighting for women's equality in. As a jugde from 1981 and onwards she of course also had cases upholding and applying the ECOA and in that sense combating credit discrimination, but i don't have a record of all ECOA cases on which she has presided.

^(3. This case was overturned in Taylor v. Louisiana (1975, illustrating how much happened in one and a half decade))  
^(4. I also struggled finding the actual source for this claim to understand what they meant. I assume it's just a case of listing both things she has been an personal or professional advocate for and things she has achieved in terms if court results.)

***Sources***

Campbell, A. (2002). Raising the bar: Ruth bader ginsburg and the aclu women's rights project. *Texas Journal of Women and the Law 11(2)*: 157-244.

Gates, Margaret J. (1974):  Credit Discrimination Against Women: Causes and Solutions *Vanderbilt law review 27(3)*: 409-441

Hyman, Louis (2011): Ending Discrimination: Legitimating Debt: The Political economy of Race, Gender and Credit Access in the 1960s and 1970s *Enterprise & Society 12(1)*: 200-232

Joslin, C. G. (2018). Discrimination in and out of marriage *Boston University Law Review 98(1):* 1-54.

Trumbull, Gunnar (2014): Consumer Lending in France and America *Cambridge University Press*

Edit: Added a tl;dr and a summary, mostly to clarify a few misunderstandings that seem to have arisen.",0
"The short answer is, ""Yes.""

The medium answer is, ""Obviously yes, humans are no different today than they were thousands of years ago.""

The long answer (or at least a demonstrative example thereof) is as follows:

Lu You (陸游) was a scholar and poet of the Southern Song Dynasty \[1127-1279 CE\]. Born only a little more than a year before the tearing apart of the already weakened realm, and the permanent expulsion of the Song from all territories south of the Huai River, Lu grew up knowing very well what it was to be ***""down on one's luck.""***

As with any good son of a relatively well-positioned family, Lu You received a good education and support, and grew up to be very patriotic - assured of his country's right and ability to retake what had been so egregiously stripped from them: both the North, and their pride. Such a victory would prove rather less than forthcoming.

At age 29, he passed the infamously difficult Imperial Civil Service Examination, on his second try (his first at 19 being a failure)... this time, he was the valedictorian of the Lin'an Region (Hangzhou, the S  Song capital). Sadly, because such a prominent position threatened the ascension of one of the rising stars of the time (the grandson of the infamous traitor Qin Hui), he and scores of other candidates were therefore disqualified from the National Examination the following year. 2 years later, his first wife - fed up -  divorced him.

In spite of his (unfair) exclusion from the Civil Serive Exam, Lu You was nevertheless able to score a government job, seemingly through little more than sheer moxy. Sadly, due to his fervent belief that the Song fight fight to reclaim their northern homelands - against the prevailing sentiment of the times that it was a lost cause not worth spilling more blood over - he was shortly dismissed from his initial government position. He would be hired on in 1172 as a military adviser, further spurring his patriotic sentiment and fervor, but it would all prove to be hopelessly out-out-tune with the sentiments of the era, and largely went nowhere.

By 1175, largely defeated by age, time, and the apathy of his entire society, the 50-year-old Lu You had given himself over to indolence, drink, and hedonism... and poems. Oh yes, of course, poems. Always a hobby of the literati class, Lu You had spent his career waxing poetic about the virtues of patriotism and fighting the good fight... and they'd gotten him less-than-nowhere.

but then...In 1183, now 58, Lu You's life irrevocably changed...

>**POEM FOR MY CAT #1**  
>  
>裹盐迎得小狸奴，I got a little kitty-servant with a bag of salt,  
>尽护山房万卷书。He'll be the protector of my house's countless books.  
>惭愧家贫资俸薄，What a shame that my salary is so small,  
>寒无毡坐食无鱼。He has no blankey to sleep on, or fish for dinner.

Thus would begin Lu You's spiraling love affair - and voluntary enslavement, to the mouser that would take over his life...

>**RATS WERE DESTROYING MY BOOKS SO I GOT A CAT AND NOW THEY'RE ALL GONE!**  
>  
>Military conscription has left the house empty  
>Only my cat keeps me company.  
>It's so soft to touch and warm to hold in bed!  
>Such a brave mouser! So capable in destroying the rat's nest  
>As valiant as the soldiers on the battlefield!  
>I regret that I don't have much fish to give it, but it doesn't mind  
>It doesn't even waste time catching butterflies among the flowers

So, *that* went well! Some 8 years later, Lu You (now 66) adopts another kitteh... this one he calls 雪儿 (Xue'er), meaning ""Snow Child,"" or just ""Snowy:""

>**I GOT A CAT FROM A NEARBY VILLAGE THAT I'M NAMING SNOWY!**(1191)  
>  
>He looks like a tiger and climbs trees,  
>He thinks he's a horse, but can't possibly pull a cart!  
>He vanquished those pesky rats  
>But asks for nothing in return - not even fish!  
>Every once in a while, he gets drunk on catnip,  
>But every night, he snuggles up cozy on his rug.  
>I swear, he must've been my own child from a past life,  
>Reincarnated now to keep this old man company!

He can't stop now. He's officially a cat-person...

>**A POEM FOR PINK-NOSE**(1193)  
>  
>Night after night you used to massacre the rats,  
>Guarding our grain so ferociously!  
>So why do you act now like you were palace-born and bred  
>Demanding your daily fish and claiming my bed for your own?

It becomes increasing unclear who is the master, and who is the slave...

>**POEM FOR MY CAT #3**  
>  
>I do not scold you failing to catch mice,  
>Your fish arrives on-time!  
>Every day I see you sleeping without care  
>Why, then, do you scamper about here and there?

Reality dawns: ""I've Made a Huge Mistake...""

>**SOME THOUGHTS**  
>  
>The cat is sleeping on my bed, totally oblivious to the rampaging rats  
>My books are gnawed to ruination, the birds wake me at dawn every morning!
>I can't believe it - was this really all just a ruse to get food from me?!
>He's so lazy! Warm and safe now, he just doesn't care!
>Impossible to train with a full stomach!
>Oh, how naive I've been! I'm totally stressed out now...

But he accepts his fate as Cat Person with alacrity:

>**RAINSTORM ON 11/04**(1192)  
>  
>Wind sweeps the world and rain darkens the village,
>Thunder rolls off the mountains like ocean waves churning.
>But the furnace is soothing, and the blanket is warm.
>Me and kitty, we're not going anywhere today.

So, as you can see... pet owners (or, perhaps more accurately... owned) have always been thus. In fact, in modern China it's commonplace that cat ""owners"" refer to themselves and one another as 猫奴 (""cat slave"") and 铲屎官 (""Officer Turd-Scooper"").
__________

Giles, Herbert. *A Chinese Biographical Dictionary.*Zhao, Xiran Jay, [Thread](https://twitter.com/XiranJayZhao/status/1299167601403162624?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1299167601403162624%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=http%3A%2F%2Frecursion.org%2F2020%2F10%2F29%2Flu-you-cat-poems).

________
Addendum: Please consider checking out my podcast on China's amazing history: [The History of China Podcast](http://thehistoryofchina.wordpress.com)! We're entering the 1390s - the final decade of the first ruler of the Ming Dynasty, the Hongwu Emperor!",0
"A free ride? 

Paul worked a job to support his preaching. The one recorded sermon we have largely seems to be met with a yawn by the audience. People just ask what in the world he is talking about, and decide he's proclaiming a new deity.

Christians themselves were total nobodies, a sect of a sect of a minority ethnic group under Roman rule. When they're eventually noticed, it's because people say they have orgies and drink blood, and that by refusing to make token sacrifices to the Roman gods (a sign of fealty to Roman rule), they keep getting themselves executed. 

Even then no one really cares about them until Nero blamed the Great Fire of Rome on them 67 CE, kicking off the first systematic persecution of Christians specifically. 

They also don't join the Jewish revolts that wipe out a few Legions before being slaughtered in turn and their temple at Jerusalem destroyed, making it very clear  that they're not Jewish any more. That's in 66 CE, one year before the Great Fire. 

There's a few footnotes mentioning then in passing or asking for guidance on how to deal with them in some Roman sources, but until they become the scapegoats they have no standing in society. 

In fact, they're rather famous for recruiting the dregs of Roman society until they manage to start poaching Stoics into the fold, a philosophy that was well represented in the elite. (A lot of the theological language in the New Testament is shamelessly appropriated Stoic philosophical jargon.)

Paul himself reports being shipwrecked a few times, imprisoned a few others, and just generally having a wretched go of it. 

Then some Jewish leaders accuse him of blasphemy and other unspecified crimes, to which he exercises his right as a Roman citizen to appeal directly to Caesar (a move that apparently surprised his accusers, who may not have known he was a Roman citizen).

But things don't go very well, and much like Pilate and Jesus, he ends up the victim of internal political wrangling and Roman concessions to Jewish intransigence, and he's beheaded in Rome, and dies.

There is a lot that can be said about Paul, good and bad. But saying he was getting a 'free ride' on some kind of Christian gravy train and therefore had an incentive to lie where his bread was buttered just isn't true. 

No doubt Paul would say his life was better for becoming a Christian. But a purely secular account of the consequences of his conversion would imply he got the very raw end of the deal.  ",0
"First, a few caveats to your assumption.  For one thing, a lot more of the silver that ended up in China came not from Latin America but from Japan instead (although a lot of it did come either through Manila or around the cape).  Secondly, although there was a reversal of the silver trade in the 1800s, with the Opium Wars and the Taiping Rebellion, China's silver demand resumed shortly thereafter (the ""Mexican Dollar"" essentially being the operative trade currency of China, after the Mexican war of independence).  

The problem with the silver lies primarily in the form of the Great Depression.  The reliance of many countries on the Gold Standard turned into disaster when in the face of the massive financial shock in 1929, the inability of countries to provide economic stimulus through monetary policy because of the gold standard caused some countries to abandon it entirely.  This in turn would force other countries to abandon the gold standard, and so on, creating a cascading financial crisis that did much economic damage.  The US was no exception to this, with severe restrictions on the gold trade coming on in 1933 and the Gold Reserve Act of 1934 allowing the government to devalue the gold dollar, increasing the money supply.  

You may ask why this would affect China, as China was a country that was essentially run on a silver standard, rather than a gold one.  Indeed, China managed to ride out the early stages of the Great Depression without much incident.  Though the Qing Dynasty had long fallen and the nascent Republic of China and its instable, authoritarian Kuomintang/Guomindang (KMT) government had risen in its place, the silver situation had not changed too much aside from being formalized by the KMT.  However, things would change, thanks to the 1934 Silver Purchase Act.  

Silver miners in the US were a powerful industry force, and they pushed the US government to boost silver demand by buying silver at a higher price than current market prices.  Led by Key Pittman, a senator from the silver producing state of Nevada, they pushed the US government to pass the Silver Purchase Act of 1934, which caused the US government to purchase silver from the public at a rate substantially higher than the market price.  

Naturally, speculators with at least two brain cells quickly realized that there was an opportunity to make money through arbitrage.  They bought lots and lots of silver, especially from China, in order to sell it to the US government at a higher price.  [This can be seen from inventories of Chinese banks, especially foreign ones.](https://imgur.com/L6Omd6f)

The Chinese government, to their credit, had already seen this outcome when the legislation was first proposed.  They failed to block the legislation despite lobbying, however, and now the Chinese were facing a currency crisis as all the silver was now being drained out of China.  In an effort to prevent total financial collapse, China forced itself off the silver standard in 1935, and attempted to back their new fiat currency by purchasing foreign currency to use as reserves.  However, this forced them to sell even more silver to do so, and the outbreak of the Second Sino-Japanese war in 1937 certainly did no favors to the Chinese government's ability to conduct fiscal and monetary policy.  Though it triumphed over the Japanese, its monetary situation was in dire straits, and a last ditch attempt to reform the currency failed in 1947, an outcome which Milton Friedman argued led to the fall of the KMT government in mainland China and the rise of the CCP.  

Sources: Franklin D. Roosevelt, Silver, and China, Friedman, Milton, JPE, 1992.  
The Shanghai Capitalists and the Nationalist Government, 1927-1937.  Coble, Parks, 1986.",0
"Roman slaves only held personal property (peculium) by their master's sufferance. The nature and value of this property varied with the nature of the slave's duties; herdsmen, for example, might ""own"" a few of the cows they tended. But the only slaves who could become anything like wealthy were the relatively privileged minority who worked in cities, and in close association with their masters. 

Although they had certain rights over their property, it legally belonged, like themselves, to the master. A master who wanted to strip a slave of his savings could do so. But custom weighed against this, as - in many cases - did economic logic. From a master's point of view, a slave was nothing more and nothing less than an investment. To get the most out of that investment, he needed to motivate the slave to work as hard and as well as possible, ideally without supervision. Allowing a slave to earn money and buy his own freedom was a powerful motivating tool. 

Masters freed their slaves on different terms - sometimes in their wills, sometimes when a slave reached a certain age, and sometimes when a slave purchased his own freedom (normally at a price more or less equivalent to his own market value). In the city of Rome, slaves seem to have usually earned this money by operating - in an independent or semi-independent capacity - some business enterprise on their master's behalf (they might serve as doctors, or sell high-value craft goods from ""private"" storefronts). Masters allowed them to keep a portion of the profits, and to apply this portion toward the purchase of their freedom, for the simple and sufficient reason that slaves working toward freedom tended to work much harder than slaves laboring without hope. In these instances, masters refrained from plundering their slaves for the same reason: it was bad for business. 

Roman slaves, of course, seldom became rich (though they might be put in charge of valuable property - including other slaves - in the interests of the master's enterprises). It was freedmen - former slaves - who accumulated really tempting fortunes. One thinks of Trimalchio, the freedman in Petronius' Satyricon who is so rich that he has hundreds of his own slaves, and only knows a tenth of them by sight. Or of the emperor Claudius' freedmen, who became some of the wealthiest men in Roman history. Why didn't masters swoop in and claim the riches of their former slaves? 

Manumission did not sever the relationship between a Roman master and his slave. It simply changed the terms of that relationship. A freed slave took his master’s praenomen and nomen as his own, and remained part of the family (in the Roman sense of familia - i.e. the master’s household). He became, in effect, a sort of son. He continued to have obligations to his former master, which sometimes included unpaid labor and always entailed social deference. And the erstwhile master, for his part, became the freedman’s patron. These ties lasted as long as the freedman lived, and even beyond (freedmen were sometimes buried in their masters’ family tombs). The relationship obviously favored the former master; but it also placed him under real social obligations. 

Masters did have a legal claim on their freedmen’s wealth, in the sense that they (or their heirs) usually had to be included in their former slaves’ wills. Roman slaves could not make wills; but freedmen were citizens, and had the right - within certain limits - to pass wealth on to their heirs. The property of a freedman who died intestate automatically devolved to his master (or master’s heirs). Freedmen who made wills still had to leave half their property to their former masters if they had no natural heirs; and even if they had children, only three or more entitled them to leave their masters out entirely.

But although they maintained rights over their former slaves’ estates in death, and might demand valuable services from them in life, former masters were not motivated to openly pillage the wealth of freedmen who made it big (though freedmen were expected to help masters who had fallen on hard times). The pseudo-familial terms on which their relationship was conceived militated against blatant seizure of wealth. To most masters, in any case, a wealthy freedman was valuable in his own right - an advertisement and extension of the master himself. Social capital, in short, usually mattered more.

[Edits: added some context, cleaned up typos. My thanks to the commentators on this response, who raised many thought-provoking points.]",0
"The history of Hugo Boss's involvement in producing uniforms for the Nazi regime is indeed a complex and controversial topic. After World War II, Hugo Ferdinand Boss, the founder of the clothing brand, faced denazification proceedings but was ultimately classified as a ""follower"" rather than a ""supporter"" of the Nazi party. This distinction allowed him to continue operating his company. It is important to note that the Hugo Boss brand, as we know it today, was largely shaped in the post-war era.",1
"I second u/timidasfuck here, what a great lesson we’ve just had in a very interesting and curious part of world history. 
I typically come to reddit and am met with some brainless bullshit I have no space for in my brain. But this was just great and made me so curious about learning more of this story.cheers to the OP for posting the question and you for taking the time to answer.",0
"Hi!  I wrote about this a while ago, [here](https://www.reddit.com/r/AskHistorians/comments/69atc7/after_the_reign_of_augustus_were_there_ever_any/dh5n0an/).  The TLDR, Tacitus was writing about a century after Augustus died, and that's where we get the idea that the Republic fell, but it's a problematic idea, because what a republic was meant very different things to different people, and some people (Cicero for instance) thought the Republic was falling all the time.  The idea stuck in the modern conception, I argue, largely because of Hitler.

I'm happy to field further questions, if any pop up.",0
"The fact that Hitler, Trotsky, Tito, Freud, and Stalin all lived in Vienna during the same period was indeed a historical coincidence. Vienna at the turn of the 20th century was a vibrant city that attracted a diverse range of intellectuals, artists, and political activists. It was a hub of cultural and intellectual life, known for its universities, coffeehouses, and artistic movements. Each of these individuals had their own reasons for being in Vienna, unrelated to each other.",1
"Thank you for everything you do. I'd rather get no answer than something blatantly wrong.  

EDIT: I'm now receiving PMs from people answering the question, still with no sourcing and completely anecdotal. I'm not a historian, but this sub is not just for me, it's for everyone. If you want to answer the questions, please cite your sources and post them in the thread. By sending it to me I have no other users able to corroborate and fact check.   ",0
"After the fall of the Nazi party and the end of World War II, Germany embarked on a process of denazification and de-radicalization. The goal was to dismantle the structures and ideologies that had supported Hitler's regime and to prevent the reemergence of radicalism. Denazification involved removing former Nazi officials from positions of power, purging Nazi ideology from educational institutions and the media, and prosecuting individuals responsible for war crimes and atrocities.",1
"# Bibliography

Aron, Robert. *The Vichy Regime, 1940-44*. Beacon Press, 1958.

Fishman, Sarah, ed. *France at War: Vichy and the Historians*. Berg Publishers, 2000.

Burrin, Philippe. *France under the Germans: Collaboration and compromise*. New Press, 1996.

Jackson, Julian. *France: The dark years, 1940-1944*. Oxford University Press, 2001.

Paxton, Robert O. *Vichy France: old guard and new order 1940-1944*. Columbia University Press, 1972.

Rousso, Henry. *The Vichy syndrome: History and memory in France since 1944*. Harvard University Press, 1991.

Sweets, John. *Choices in Vichy France: The French Under Nazi Occupation*. Oxford University Press, 1986.

Taylor, Lynne. *Between Resistance and Collabration: Popular Protest in Northern France 1940-45*. Springer, 1999. 

Temkin, Moshik. ""Avec un certain malaise': The Paxtonian Trauma in France, 1973—74."" *Journal of Contemporary History* 38, no. 2 (2003): 291-306.

Vinen, Richard. *The Unfree French: Life under the Occupation*. Yale University Press, 2006.",0
"It is important to note, however, these phrases and terminology were not invented but adopted by the Klan. America First was a common progressive slogan that was eventually used as the name of a foreign policy doctrine by Woodrow Wilson. It is still mainstream enough that it's the name of a mid-sized bank. Make America Great Again was also commonly used by the mainstream. In fact, President Clinton said he wanted to make America great again when he announced his run for President. [Here is the clip.](https://www.c-span.org/video/?c4600782/bill-clinton-make-america-great-again) So while it was a common slogan among white supremacists, it was also a common slogan among non-white supremacists and originated outside the movement.

Later claims, such as David Duke's claim that he invented the phrase 'Make America Great Again', are attempts by white nationalist organizations to claim cultural force they do not have. They effectively adopted slogans invented by somebody else and then attempted to claim credit for them. It is important not to entertain their delusions.",0
And shouldn’t it be phrased that 81.8% of *applicable* voters since women weren’t allowed to vote back then?,0
"The answer to this is *probably* no.

We know Gladiators were reasonably chunky to display ""show wounds"" since cuts to fatty areas wouldn't be debilitating and would also bleed a lot. They also were less likely to get infected.

Roman soldiers were different. They trained *constantly*, just like the modern military, and when they weren't training they were performing maintenance, logistics, etc. Or being employed in construction. They were short, stocky, and mostly solid muscle.

How do we know they (probably) didn't fatten up for campaign?

Well, for one we know they ate extremely healthy diets but high calorie diets. Foods high in fat were recorded to have caused intestinal distress among the soldiery on campaign. Lentils, Olives, Raisins, Dates, Nuts, and Bread (Grain, barley, oats, spelt, rye) were the staples of the legionary diet, usually with a salt ration and a pork or fish ration, and then other things they could occasionally pick up like Carrots, or Pepper (if they could afford it), or the like. They also all had worms, and this diet probably helped make that less problematic.

It's also largely backed by studies on the bones of deceased soldiers and animals, which showed that although they ate a variety of meats ranging from fish, poultry, and pork (the most common) to everything from vole to otter to wolf. They also probably broke bones open for the marrow to make stew with. But it's also worth noting that analysis of bones also showed a primarily grain-based diet, which probably composed more than 60 to 70% of their meals.

Another reason, which isn't usually talked about, is how *small* the finds of armor are. The Corbridge hoard segmentata's original dimensions will not fit pretty much any reenactor except the ones who are like 5'4. It's not the only case, either. The surviving muscle cuirasses are also too small for modern reenactors in their original dimensions. Tunics are also indicative of this - most of the surviving adult tunics were made for people slightly over 5 foot in height. Although tunics don't give much evidence in terms of weight.

Finally, it's worth noting that while ""fattening up"" for campaign makes sense for other cultures who would have to bring all their food with them at the beginning of the campaign and rely heavily on foraging and looting, the Romans had continuous supply chains with granaries (*horrea*) placed at specifically distanced intervals purely for the purpose of having a continuous, uninterrupted supply. In major campaigns outside of the empire, they would stock up ahead of time and carefully guard baggage routes to transport food and supplies constantly. Julian was relying on barges to transport grain down the Tigris and Euphrates to supply his campaign to Ctesiphon in 363.

So it seems extremely unlikely that soldiers would do this based on what we know, but there's a lot we don't know and we can't rule it out entirely. But overall my judgement is ""most likely no.""

(**EDIT:** Okay since I'm getting a TON of questions about this in particular, [Intestinal Worms were rampant in the Roman Empire](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/S0031182015001651). This was because most provincial baths were never drained when the plumbing clogged, they used human fertilizer on all their fields without using complex composting to kill the eggs, and they loved *garum* which is a fermented fish sauce that spreads marine worms, most of which don't infect humans but some which do, namely tapeworms. As a result, worms were *far* more prominent than in the iron age or middle ages.)

(**EDIT 2:** Yes people in ancient Rome were shorter. Vegetius states the ideal height for a soldier was about 5 foot 8 inches, but in reality the average height for a male in ancient Rome was about 5 foot 5 inches based on skeletal studies. This is largely due to diet. The modern diet is much higher in protein, etc. which allows us to grow to much larger builds. They weren't much different from the worldwide average.)",0
"just gotta say that flag is badass

it is correct to assume that rebels are being included as targets alongside infidels because of some presumption that rebels are defying god? the king being chosen by god, it would make sense to consider rebels as defiant to god in some manner or another.

also: i've always considered warfare to de facto include raping and pillaging (even up through most modern wars), which makes me think this is largely superfluous. was this behavior *not* the norm, or would the dragon banner just be more of a righteous justification for purposes of morale and politics?",0
"I mean, you can [read the linked-to article](https://www.wired.co.uk/article/history-colourisation-controversy) that explains their position pretty clearly. They basically say that they believe that it separates one from the original experience. 

I have no idea if they are really prominent (Art History is not my field), but I will say, I am sure if you looked you'd find historians who also thought this sort of thing was fine. Historians are of many opinions. It makes for a better article to focus on those who say, ""this popular thing is actually bad,"" but that doesn't make it at all representative. They are entitled to their views; so are the people who like these things.

I think of it sort of like textual primary sources. When I am assigning something to students, I prefer to give them a scan of an original source, rather than a re-type version of it. Struggling through the ugliness of the original source, or its awkward typography (including such oddities as the [long S](https://en.wikipedia.org/wiki/Long_s)), gives one a much better feeling of communing with the past in all its messiness, as opposed to a retyped version typeset in clean, crisp word processing software. I feel that it does engage one's ""historical instincts"" a bit better — you have to struggle a bit more, and that struggling is part of the work.

But I'm also aware that this is somewhat bullshit. It's essentially Barthe's ""reality effect"" applied to textual analysis — as if it makes it ""more historical"" to read it in handwriting rather than Times New Roman. It doesn't, really. Struggling to read a text can give one the experience _of a historian_ but it does not actually bring one closer to historical truth or even historical empathy. That is a separate process. The only advantage to the struggling is that it makes you very cognizant of your struggling to grasp the past. 

That is basically what these art historians are saying, as I read it. Seeing an upscaled, colorized version of the past is seeing a constructed, mythical, modern, ""easy to process"" version that keeps you from having to do the hard work of really extending your empathy backwards in time. OK, I guess, maybe. And they are unhappy that some people don't realize they're modified. One says students are submitting colorized pictures in essays without realizing they are colorized. I mean, that's annoying, sure, but there's a lot of fake stuff out there, and the only answer is going to be to teach students to be more critical of these things. And what a wonderful ""teachable moment"" it is to point this out, eh? And let's be honest — if ""students can get things wrong"" is a criteria for stopping something, shit, we're gonna be stopping _a lot of things_. :-)

Ultimately the audience for these creations isn't other historians. It's not people who are actually making that hard struggle. It's the everyday people who find the past hard to relate to, and are agog when they can suddenly relate to it. Personally, I think that's a great impulse to encourage, even if it's artificial. Who knows where it may lead? I can't see the harm of it, to be honest. It seems like a very abstract objection. 

I loved _They May Not Grow Old_. I have studied World War I, I have read historical works on it, I have struggled to empathize with the past, etc. And yet there _is_ something transformative and magical about Jackson's work, something that extends it beyond reality and makes it into something new. It's not a perfect reproduction of the past. It's an artificial past, one partially created by the present. But not _wholly_ created by the present: when done well, there is still a wonderful core of historical reality in there, and the increase in historical empathy can be palpable. You stare into those boys' faces and you think, ""shit, those are just kids... kids at war. What the hell?"" And yeah, you could shame people into not feeling that instinctively, or not seeing the same thing you see when you look at an image (after how many years of post-graduate education?), but really, _what is the point_? One of the historians in the article did not like the film and thought it was gimmicky — that's a perfectly valid opinion! But to jump from ""I found it gimmicky"" to ""it is actually causing people to misunderstand World War I"" strikes me as quite a leap.

Again, they are 100% entitled to their opinions and to tell the world what they think, and _Wired_ is 100% allowed to report on them. What would academia be without cranky academics? Not academia. But I also think we should recognize that alleged ""harm"" here feels pretty low. 

I could go on a rant about the hyper-criticism that is common in history and the humanities (and its ultimately uselessness — if you only tear down, you never build anything up), but that seems tangential to the above, though it comes to mind. I will just end with the note that many historians (and other humanists) are prone to lecture people on how other (non-historians) will understand something, and why it will lead them astray, and so on. But in my experience none of them have ever actually done any empirical research to find out _if that is true_. I have taken to pointing out, whenever I can, ""that is an empirical question"" when such things come up — _do_ people understand it in mode X, rather than mode Y? We could _actually find out_ with some good social science research. We maybe _should_ if we believe it is important. But this is a mode of thinking very alien to historians; I am only infected with it because I work with an excellent social scientist on how people understand various forms of risk communication (relating to nuclear matters) and have to come to appreciate that the way academics perceive things can be _dramatically_ different from how everyone else perceives things (""I am not the target audience"" is how I like to put this).",0
"I guess that is where this suggestion comes into play:

> The publication itself warned potential patrons that Stonewall was not safe (don't give out your real name or address; the bar owners and workers are not on your side, either; the cops will raid periodically)

It's hard to blackmail someone whose identity you don't know -- especially in a time without widespread databases and automated facial recognition.",0
"As someone who hasn't really done a ton of reading into the subject, why can we discount the Suge Knight theory as cow poop? Is there a lot of evidence against it? Until I read this post, I was actually under the impression that was the leading theory.",0
"Well this depends on who you are talking about as there were many culturally and linguistically distinct peoples in Britain. 

 Among Anglo-Saxons, they just had one name and maybe a nickname or more rarely a descriptor attached (think Eric the Red).  Names tended to be a little more unique (this was before English towns were made of people named mostly Thomas, Mary, and John), so there was less of a need to be more specific with names.  When you did you would just describe the person, either where they lived, what they did, or something about them (hence the origin of surnames).  But these firmly remained descriptions.  

Among Celtic tribes, they very rarely used places attached to people's names.  Being much more ""clan"" conscious, they distinguished people primarily by lineage and so described and specified people by their familial relations.  Modern surnames are often the name (like O'Brien) or nickname (O'Doherty = the destroyer, Finn = fair/blonde haired, Kennedy = ugly head) of a famous ancestor, or a frozen patronymic (like McCarthy).",0
">Other historical figures too were brought out. Attila the Hun and Genghis Khan for instance both figured as rough analogies for the images of fire and destruction that they brought to the popular imagination, and the same that Hitler was bringing to Europe, and Nero was used a few times in the wake of the Reichstag Fire, harkening back to the alleged fiddling while Rome burned. 

Wasn't Wilhelm II also compared to Attila during WWII by British propaganda?",0
"I can't answer for Americans, but I can for British: some absolutely were.

People generally complied with the letter of the law, with some black-market activity, but would often skirt its spirit. Expensive restaurants didn't need ration cards at the start of the war, it was possible to be sent care packages from relatives abroad, and wealthy housewives had a much easier time skirting the labour-board mandates of 1941 than poorer women. Bear in mind that, as of Dec 1941, women under 50 were (in theory) conscripted into some form of war work. In general, wealthy housewives were able to get out of working or volunteering outside the home for more than a few hours a week, and some were even able to keep employing servants. The party season was severely curtailed for the upper and upper-middle classes, but New Year's 1942 still featured a gala at Grosvenor House that apparently would not have looked very out of place aside from meager portion sizes pre-war.

There were also a number of sacrifices that, while in theory were equal, affected groups very differently. Clothes rationing was hard on everybody, but Utility garments tended to wear out much faster than more expensive ones, which meant the poor tended to go through their ration cards faster.

Mass Observation diarist #5427 specifically mentioned that she was considering getting her car retrofitted to use a different kind of fuel, costing the equivalent of $6,000 today, *just* to get around the gas restrictions; unfortunately for her, the gas restrictions came with a ban on most types of non-essential driving, so she dropped the idea.

Finally, many of the restrictions, like with COVID, were only recommendations and not law. There were recommendations, at least in the winters of 1941-2 and 1942-3, to use minimal fuel to heat fires in houses. Some people delayed heating their homes in October for weeks longer than others, and some continued to heat multiple rooms (central heating was pretty rare - rooms were heated individually) during the winter.

The biggest example of a recommendation that failed was the evacuation of children. Millions of children were evacuated from the cities into the countryside early in the war, but it generally went poorly. Urban (mostly poor) children suffered severe culture shock and often just wanted to return home, and parents often wound up wanting to take their children back. Most children gradually found their way back home within a few months or a year, and did not re-evacuate when air raids started.

Sources: several Mass-Observation diaries (contemporary diaries written by normal people - there are people doing this for COVID too, and I look forward to reading them someday.) plus

Calder, Angus. The People’s War: Britain, 1939-1945. New York: Pantheon Books, 1969.

Hinton, James. The Mass Observers: A History, 1937-1949. Oxford University Press, 2013.

Hylton, Stuart. Their Darkest Hour: The Hidden History of the Home Front, 1939-1945. Sutton, 2001.

Nicholson, Virginia. Millions like Us: Women’s Lives in War and Peace 1939-1949. London: Viking, 2011.

Smith, Harold L. War and Social Change: British Society in the Second World War. Manchester University Press, 1986.

Summerfield, Penny, and Gail Braybon. Out of the Cage: Women’s Experiences in Two World Wars. London: Routledge, 2012.

Zweiniger-Bargielowska, Ina. Austerity in Britain: Rationing, Controls, and Consumption, 1939-1955. Oxford, New York: Oxford University Press, 2002.",0
"My major source for this post is *A Guide to the Harry Potter Novels* (2002) by Julia Eccleshare, who was children’s books editor for *The Guardian* until 2016.

*Harry Potter and the Philosopher’s Stone* was published in the UK in June 1997 by Bloomsbury Publishing, a then-small company founded in 1986 that had only started publishing children’s books in 1994. Joanne Rowling’s agent, Christopher Little, had shopped the manuscript unsuccessfully to a number of publishers (nine? twelve? thirteen? depends on who you ask) over the course of a year or so before Bloomsbury obtained the rights in August 1996 and paid Rowling an advance of £2,500. On her publishers’ advice, Joanne adopted the *nom de plume* J. K. (she has no middle name; the K comes from her grandmother, Kathleen) on the theory that it made the book more appealing to boys. *Philosopher’s Stone* was published in an initial run of just 500 copies, a standard size for a debut children’s book from an unknown author.

At around 90,000 words, the book was seen by many publishers as too long (much like this post I expect lmao). Several other factors made *Harry Potter* in some ways an unlikely candidate for a future smash hit. Michelle Smith of Deakin University argues that in the British children’s publishing world, by the 1990s, “nobody wanted to touch fantasy stories — they were seen as old-fashioned.” Additionally, *Harry Potter* can be seen as inheriting from a long tradition of popular British boarding-school stories such as *Tom Brown’s School Days* (1857) or various series by Enid Blyton such as the *Malory Towers* series (1941-1945), a tradition that had largely fallen out of fashion after WWII as children’s librarians sought more “modern” material to engage a wider variety of young readers. Eccleshare writes that “children’s book publishing since the mid-1970s has concentrated, with some notable exceptions, on social realism and emphasized brevity and simplicity of language.” In bucking this trend, “Harry Potter was not unique but it was published against what was seen as the most flourishing strand in children’s books.” However, there was at this time some degree of opportunity for a disruptive force in British children’s literature. Since the death of the bestselling British author Roald Dahl in 1990, sales of newly published children’s books in the UK had dominated by American imports such as R. L. Stine’s *Goosebumps*; Eccleshare argues that “an heir to Dahl had long been looked for” by the British children’s publishing industry.

As usual with any new book, Bloomsbury sent copies to booksellers, librarians, professional reviewers, and so forth in hopes of generating some positive press; otherwise there was no marketing campaign to speak of, as the print run was too small to justify it. Favorable early reviews appeared in smaller media outlets such as *The Scotsman*, *The Herald*, and various specialist children’s literature periodicals; Eccleshare writes that “reviews of children’s books in the UK are rarely other than positive, so it is their existence as much as what they say that is important.” Likely drawing on these early reviews, further positive coverage appeared in major national papers such as *The Guardian* and *The Sunday Times*. Several reviewers compared Rowling’s work to Dahl’s.

In addition to (and indeed perhaps itself influencing) the positive media coverage, the early success of *Philosopher’s Stone* was due in no small part to grassroots, word-of-mouth buzz among children, who genuinely just liked the book a lot, and said as much to their parents, teachers, and librarians. Nigel Newton, chairman of Bloomsbury, gave the first three chapters to his eight-year-old daughter Alice, who read them eagerly and demanded more, saying, “Dad, this is so much better than anything else.” Beverly Lyon Clark, in *Kiddie Lit: The Cultural Construction of Children's Literature in America*, writes:

>Part of the popularity of the Harry Potter series, especially initially, results from word-of-mouth recommendations, child to child. Reviewers and other critics cite librarians who stress how avidly children take to it, how much it has done to encourage children to read. Jim Trelease, the author of *The Read-Aloud Handbook*, has claimed, “‘Harry Potter' is the best thing to happen to children's books since the invention of the paperback.”

A panel of adult judges shortlisted *Philosopher’s Stone* for the 1997 Nestlé Smarties Book Prize, which the BBC described as “one of the UK's most prestigious prizes in children's literature.” (Incidentally, Eccleshare chaired the prize from 2001 to 2007.) The three shortlisted books were then voted on by British schoolchildren; *Philosopher’s Stone* won “by a healthy margin.” Per Eccleshare, this was the critical event in the book's trajectory: just being shortlisted was a major coup for a debut novel, but winning outright “gave it a high profile within the first six months of publication, very different from most children’s books which take years to become established as successes.” Elsewhere Eccleshare writes: “Children’s books are traditionally slow performers since it takes time first for adults and then for children to adopt and so promote them.” *Philosopher’s Stone* would soon go on to win a number of other British awards voted on by children.

Arthur Levine at Scholastic, who had already acquired *Redwall* and *His Dark Materials* for American publication, bought the American publication rights to *Philosopher's Stone* at auction in April 1997—two months before its first British publication—for the unusually high sum of $105,000, which *Vox* asserts was ""10 times more than the average foreign rights sale at the time."" Levine's love for the book, and his past success, convinced Scholastic execs that it was worth the gamble. This in turn necessitated a substantial marketing push in the US in an effort to recoup the publication cost, and also led to media curiosity about such a big sale. Upon its American publication in September 1998, aided by its British reviews, prizes, and sales numbers, *Harry Potter and the Sorcerer’s Stone* was an immediate success and won a number of American prizes as well. *Vox* argues that it was the huge sale of *Philosopher's Stone*'s American rights that really put it on the path to the stratosphere.

So what about the movie? Film producer David Heyman was in the market for a children’s book to turn into a film, but couldn’t secure the rights to his first choice, Diana Wynne Jones’s *The Ogre Downstairs*. According to Melissa Anelli’s *Harry, A History*, Heyman focused on filming books “because they had a higher chance of being produced than other types of projects,” and collected various candidates for his staffers to read. Heyman’s secretary, Nisha Parti, read the pre-publication proof of *Philosopher’s Stone* (which Little sent along after another staffer had read about it in a trade journal) and told Heyman to read it. Heyman “fell in love” with the book and got in touch with Warner Brothers, who eventually secured the film rights in October 1998 for “a seven-figure sum.” This was over a year after the publication of *Philosopher’s Stone* and three months after the publication of *Chamber of Secrets*; by that point the series was already well on its way to becoming a global phenomenon.",0
"Dante does have a major influence on the depiction of Hell, but his view of the underworld isn't just about fire. In fact, the very bottom of it (where the worst people are) is extremely cold. This is a good reminder that Italian *inferno* just means 'below' (from Latin *infra*).

One of the most influential depictions of the underworld is found in the first-century BCE *Aeneid* of Vergil—whom Dante chose as his guide for the first 2/3 of the *Divine Comedy* in part for that reason. In Book 6 of that poem, the hero Aeneas goes to the underworld and sees people punished for their wrongs, grouped into certain places by type of wrong, and the good people experiencing a great afterlife. 

Vergil inherited some of this from previous authors (the punishments of Sisyphos, Tantalos, etc. go back at least as far as Book 11 of Homer's *Odyssey*), but he seems to have innovated a lot. Even after the introduction of Christianity, Vergil was one of the main authors people would study in Latin, so his influence is immense. And it wasn't a big jump to adapt his view of an underworld with people justly compartmentalized to a Christian framework.",0
"This whole thing isn't a debate anymore, and we kind of suffer because the book at this point is a bit aged.

It must be noted that Benedict Anderson with his Imagined Communities is mainly aiming to make his personal critique of the concept of nationalism (and, in fact, his idea of the 'imagined community' as the nature of the nationalistic group is still rather central to much of modern social sciences). Regardless of your personal view on the benefits or dangers of nationalistic mindsets, it is important to keep in mind that Anderson aims to criticise them in the book and, with his footnote, wants to ridicule the Japanese monarchy as the central component of Japanese (ethno-)nationalism that it was (and is). The aim of the footnote is primarily to discredit the legitimacy of Japanese nationalism and, by extension, the various horrors committed under its banner by Japanese forces, particularly between the 1890s and the 1940s.

My personal verson is the Second Edition of 1991, so there might be minor differences between mine and yours.

But, and this is interesting, I also have the footnote you describe. Precisely, it is on page 96. 

>While the Japanese spoken in Kyushu was largely incomprehensible in Honshu, and even Eda-Tokyo and Kyoto-Osaka found verbal communication problematic, the half-Sinified ideographic reading-system was long in place throughout the islands, and thus the development of mass literacy through schools and print was easy and uncontroversial. Second, the unique antiquity of the imperial house (Japan is the only country whose monarchy has been monopolized by a single dynasty throughout recorded history), and its emblematic Japaneseness (contrast Bourbons and Habsburgs), made the exploitation of the Emperor for official nationalist purposes rather simple. ***[29]*** Third, the penetration of the barbarians was aprubt, massive, and menacing enough for most elements of the politically aware population to rally behind a programme of self defence conceived in the new national terms.

Footnote 29 reads as follows:

>But I have been informed by scholars of Japan that recent excavations of the earliest royal tombs suggest strongly that the family may originally have been - horrors! - Korean. The Japanese government has strongly discouraged further research on these sites.

So, while I cannot personally confirm it, I would make the guess that the First Edition has this very same footnote in the very same passage, putting the date of the claim all the way back to the year 1983, when the book was originally published. This is significant because that means that the book was written during the Showa Era, the reign of the Japanese emperor Hirohito - the very same Hirohito who led his country into World War 2 through, at the very least, his passive tolerance of the actions of his military leadership, if not his active encouragement of the wars of aggression waged upon Korea, which has so often become a victim of the power plays and rivalries between its two big neighbors, Japan and China. So the context in which the book is written is one where the Japanese monarchy was far more conservative and reactionary than the monarchy we know today (and what a date to write about this, as today the emperor Akihito has abdicated in favor of his son).

The new Emperor, Hirohito's son Akihito, on the throne since 1989, plays into this story because he accepts the ties between the Japanese monarchy and Korea.

>I, on my part, feel a certain kinship with Korea, given the fact that it is recorded in the Chronicles of Japan that the mother of Emperor Kammu was of the line of King Muryong of Paekche.

* Akihito, December 2001 (see also [this report](https://www.theguardian.com/world/2001/dec/28/japan.worlddispatch) by the Guardian and [this one](https://www.nytimes.com/2002/03/11/world/japan-rediscovers-its-korean-past.html) from the New York Times).

So, what we are dealing with here is a classic case of outdated information. At the time than Anderson initially wrote the book, the Japanese narrative would not have allowed for any ties to Korea. But the deeply flawed Hirohito has made space for his much more open-minded and approchable son (and now, a grandson whose character will show itself in the upcoming decades). 

Akihito, by openly embracing the ties between his bloodline and the Korean people, as well as his relaxed public image and his personal apology offered to the President of South Korea Roh Tae-Woo in 1990, has made admirable progress in mending the historical animosities between Japan and Korea that were driven to the extreme by the actions of his father.

So, no, no more research is necessary. The Japanese emperor admitted his Korean heritage.",0
"TL;DR: I'm going to, perhaps unsatisfyingly, argue that the concept of a girlfriend both very much was, but also perhaps very much wasn't, a thing in Classical Greece. Indeed, while we might find a temptingly natural translation for the concept in hetaera (literally companion), we would perhaps struggle at least as much to explain what you really mean when you reference the status of 'girlfriend' to Solon the Lawgiver as we do today understanding what he would have really meant by 'hetaera'. THIS COMMENT CONTAINS FRANK DISCUSSION OF SEXUAL VIOLENCE IN THE CLASSICAL WORLD.

I think that it is worth noting both how astonishingly easy and how astonishingly difficult it is to meaningfully address your question. It is oddly easy because we have a remarkable amount of documentation, particularly from Athens, addressing the subject in the form of contemporary plays dramatizing it, judicial records fighting about it, as well as rich dudes directly musing about it - trying to build models to understand it. That history of privileged dudes trying to dissect the status of women in Classical Greece stretches from the orator Appolodorous to Descartes to the present day. However, it is also notably difficult in that there was a continuity of human experience in how both men and women navigated sex and reproduction that also stretches from Appolodorous to Descartes but which has perhaps since been broken to a remarkable degree in relatively recent history.  Indeed, the continuing abolition of slavery, increasing access to competitive labor markets for women, state institutions keeping comprehensive records on the statuses of their citizens, and the introduction of accessible and effective birth control have radically changed the contexts of these labels. If the past is a foreign country, the context that your Classical Greek everyman navigated was very foreign indeed. However, as I'll return to, it is also incredibly present to varying degrees for many of us opening this thread.

For many Classical authors, there were very clearly two kinds of women and an unambiguous distinction between the two. There were the wives who were wedded according to the laws who, for those who could afford the expense involved, would be as unseen by the rest of the polis as possible. Under [the laws of Draco in ancient Greece (621BCE)](http://en.wikipedia.org/wiki/Draco_(lawgiver\)), where we get the term draconian today, [any man who caught another man sexually violating (adultering, moicheía, μοιχεία) his wife could legally kill that man with the same legal immunity as an athlete who accidentally killed someone in competition ^(DEM23.53)](http://www.perseus.tufts.edu/hopper/text?doc=Dem.+23+53&fromdoc=Perseus%3Atext%3A1999.01.0073). Consent, or even any action or feelings on the part of the woman in question, were perfectly immaterial to moicheía, a crime that one man committed against another. Indeed, in addition to being able to just get some friends together and safely jump him while he was indisposed on the toilet Pulp Fiction style, [Draco also allowed the aggrieved](http://perseus.uchicago.edu/perseus-cgi/citequery3.pl?dbname=GreekFeb2011&getid=1&query=Dem.%2059.67)[ man to](http://perseus.uchicago.edu/perseus-cgi/citequery3.pl?dbname=GreekFeb2011&getid=1&query=Dem.%2059.67) [capture the adulterer and inflict whatever tortures he imagined so long as he didn’t use a knife ^(DEM59.66)](http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0080%3Aspeech%3D59%3Asection%3D66). While we know that in practice this extraordinary immunity often resulted in a private extortion of exorbitant amounts of money from the adulterer in exchange for publicly forfeiting that immunity, it also formed the basis for some really fascinating trials exploring their distinction between these 'two kinds of women'. Indeed, [Under the laws of Solon](http://www.perseus.tufts.edu/hopper/text?doc=Perseus:abo:tlg,0007,007:23) (594BCE) as well as later codes, [this legal vengeance only applied to wives (as well as concubines kept for the purpose of producing free children) and explicitly not to women available for sale^(DEM59.67)](http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0080%3Aspeech%3D59%3Asection%3D67). This means that we have court records of those accused of adultering wives aggressively defending themselves by declaring the objects of their attentions to be the second kind of woman – while very precisely and luridly defining that as a woman available for sale to any john, particularly if at a fixed price.

This second kind of woman, the pornēs or those like them such as flute players, two-obol women, bridge women, alley walkers, or ground beaters, were typically owned by a pimp in a highly regulated commercial marketplace. The logical consequences of the trade in pornēs were seen as an uncontroversial part of life, and indeed a public good. Tradition even held that Solon the lawgiver opened a brothel in Athens himself as an act of public service.^((Philemon, Frag. 3; Athenaeus, Deipn. 569d)) The systematic rape of the vulnerable that the institution represented was regulated by cities in the same way that roads were, as a lucrative and essential public utility. Indeed the task of overseeing it in Athens was given to the [Astynomoi](http://onlinelibrary.wiley.com/doi/10.1002/9781444338386.wbeah13026/abstract), a board of citizens [who were entrusted with tasks associated with maintaining thoroughfares, such as ensuring the reputable disposal of feces and abandoned corpses from the streets](http://www.perseus.tufts.edu/hopper/text?doc=Aristot.%20Ath.%20Pol.%2050.2&lang=original). For example, [they established a price cap of two drachmas to protect ‘consumers,’ while the same officials who enforced that cap would also adjudicate disputes over women (by the drawing of lots to avoid price competition that the woman involved could benefit from).](http://www.perseus.tufts.edu/hopper/text?doc=Aristot.%20Ath.%20Pol.%2050.2&lang=original) These disputes were notoriously rough and perhaps commonplace as Aristophanes makes fun of in a scene in Wasps where a father and son play a twisted game of tug-of-war with a naked flute player stolen from a symposium. Sex traffickers were given licenses to minimize these issues and to ensure quality ‘product’, as well as districts to operate in (generally near docks or city gates) to manage the noises, smells, and brawls over women that were inherent to the whole business. [The ‘trade’ was also clearly large and a large part of life](http://www.amazon.com/Goddesses-Whores-Wives-Slaves-Classical/dp/080521030X). While it is very unclear what the exact percentage of women could be described as pornēs would be in any western society before the advent of the modern census, it is clear that in successful cities in Classical Greece it was at least astonishingly large – particularly after victorious military campaigns when cities were flooded with more unfortunate captives than they could assault at any price. It is also important to consider that every free woman in that era had the threat of being sold into porneia hanging over her head, as women who lost the social status granted to them by a man for whatever reason could always be sold or abducted for a sort of ‘scrap value.’ This would have been true to varying degrees whether that status was as a ‘wedded wife according to the laws’ kept as part of a relationship with her father’s family and/or for the purpose of producing heirs, by virtue of more tenuous or ambiguous sexual relationship(s), or by virtue of being maintained as a still unmarried daughter or sister or cousin. Losing that connection through shifting political winds, aging into sexual disinterest, familial indifference, or through military defeat could mean the beginning of a short life of deprivation and exploitation.

(EDITED FOR LINK ROT)",0
"There are a couple of pieces to this question. The first is where the image of a spherical bomb with a lit fuse came from, and the second is how that image became entrenched in the visual lexicon of American cartoons, and from there became so ubiquitous that we can all recognize it.

Unfortunately, I can't answer the second question, but I can talk about the first: the spherical bomb is in essence a mortar bomb or case shot that was a common type of projectile of artillery in use in the early to late 19th century. Case shot was a hollow cast iron case filled with black powder from a small opening. Artillerists would close the hole with a plug, which held a fuse. Plugs and fuses came in a variety of shapes and types, but a common fuse was the ""quick match,"" a type of coated cord that burned at a consistent rate. It's important to point out that artillery fuses were highly unlikely to be a piece of quick match, as the fuses were generally quite complicated to avoid the shell exploding in the barrel, *but* quick match in various forms as well, and was a precursor to another explosion-based cartoon classic, the electric pump-handle primer.

Simple and somewhat crude, case shot was intended to explode above formations of men at ranges far longer than the even simpler canister or grape shot could reach. In order to have any hope of accuracy, a gunner would have to accurately estimate the range, the projectile's time to reach that range, and prepare the fuse to burn for exactly that length of time. Once prepared, it would be combined with the plug, screwed or wedged into the case's opening, and loaded with the rest of the powder. The fuse would ignite when the cannon was discharged. A mortar bomb was more or less the same, but mortars tended to be far larger than most field artillery.

The classic cartoon bomb is a combination of a couple of things; the case, and the fuse. While an artillerist preparing a shot for their cannon wouldn't ever have a long string of quick match sticking out the top of the case, the difficulty of timing things exactly right could lead to unexploded shells lying or rolling on the battlefield or inside forts with their fuses still lit. 

In any event, the combination of spherical iron case and lit, trailing fuse was widely understood in the mid 19th century and would have been recognized even by non-military audiences. A cartoon from 1863 shows Jefferson Davis, the president of the Confederate States of America, staring down at a [classic bomb](https://www.loc.gov/resource/ppmsca.52034/) while confronted with Confederate setbacks. The imagery was fairly commonplace in political cartoons, so it's likely that this is the path by which it made its way into the popular American cartoons of the early to mid 20th century. I can't speak exactly to that path, however. I hope someone can, because I'm interested in the answer as well.

_______

Details of ordnance can be found in the [The hand-book of artillery :
for the service of the United States](https://catalog.hathitrust.org/Record/006511322) by Joseph Roberts.",0
"The ability to attribute pelvic scarring to prostitution is far from certain. I suggest that the statement regarding the pelvic scarring be met with similar skepticism. The source mentioned by Toldinstone only refers to a ""hypothesis"" about prostitution regarding the case of two skeletons that shared characteristics with the pelvis of ""a modern American prostitute."" And no footnotes. 

I don't want to seem rude; the pelvic scarring issue is a weird one.

Regarding the issues of pelvic scarring and attribution in Pompeii, I am referencing Estelle Lazer, Resurrecting Pompeii, 2009. 

 [https://www.google.com/books/edition/Resurrecting\_Pompeii/hXWBAgAAQBAJ?hl=en&gbpv=1&dq=pelvic+scarring+pompeii&pg=PA121&printsec=frontcover](https://www.google.com/books/edition/Resurrecting_Pompeii/hXWBAgAAQBAJ?hl=en&gbpv=1&dq=pelvic+scarring+pompeii&pg=PA121&printsec=frontcover)",0
"According to the proposed scenario, he went to the South Vietnamese and asked them to pull back from peace negotiations with the promise of something better during the Nixon administrations. That would place him in the position of acting as a private, unelected citizen who was negotiating on behalf of what he hoped would be his future administration.

Of course, the shit storm that would have occurred if information along these lines had surfaced would not begin or end with the Logan Act (which would have been largely ignored). Politically, the devastation of any proof that Nixon played such a role would have ended his presidency. 

Keep in mind that nearly half the names on ""The Wall"" represent US deaths during the Nixon administration. He owns those deaths even thought he was campaigning in 1968 to have a ""secret plan to end the war."" 

If it is true that Nixon worked with a foreign government to engineer his election, then his crimes are enormous. After all, what presidential candidate would conspire with a government that was clearly an enemy of the US so that he could become president? Such a president would need to be removed from office as quickly as possible. Right???

edited with a correction thanks to the astute /u/eisagi.",0
"I don't know that I have much on the rise of sweet culture from historical sources (though I will probably start looking into it more). This area is very complex and would likely require a few books to understand properly. 


My understanding is as the cost of sugar went down, alongside the introduction of other artificial flavors, artificial sweeteners, and even alternative sugar sources like corn syrup led to companies ramping up production of sweets. The introduction of government subsidies for ""cash crops"" made sugar prices even lower. Lower cost of production created more incentive to produce goods. (I don't have good sources for this and hopefully someone could provide better insight, I only know of this from growing up in a farming community.) 


This coincided with the start of the FDA and all the work they put in to take our food system in America from incredibly unsafe to one of the safest in the world. This drastically increased public trust in our food system (more on that can be found in the book ""The Poison Squad""). 


We also experienced decades of ""demonizing fat"". This led to many years of Dietitians trying to convince people that too much of anything can be bad(as every mentor I've had has discussed. This also created a cultural belief that as long as its low fat it is ""healthy"" (more on this in the great research article 'How the ideology of low fat conquered America'). 


As to why sweets are so appealing to us, I can't say  definitively, some have proposed that sweet foods were a rarity throughout much of human history, which created our increased desire for it. But I have also seen counter arguments for this point. 

Vanillin came around at the right time in history, right before a big boom for cheap treats and while vanilla beans were popular  across the world.",0
"There’s probably three major parts to Freud’s prominence in the popular imagination. 

Firstly, Freud made waves in Anglophone pop culture for the argument that much more of our behaviour is unconsciously motivated in part by sexuality than had previously been appreciated. This has been enormously influential on Western intellectual circles in a variety of ways that go well beyond psychiatry - a figure like Slavoj Zizek would make some quite different arguments without Freud, and Freud’s had an enormous influence in literature studies. (And with this kind of stuff Freud is often more of a philosopher talking about big ideas of human nature than a scientist making specific hypotheses - and so taken as such, he’s not necessarily discredited at that philosophical level, even if his scientific practice was poor by modern standards).

Secondly, he basically was the pioneer of the ‘talking cure’ - i.e., the idea that some sort of structured/focused discussion with a therapist was useful in treatment of mental illness; before Freud, treatment simply did not look like it does today. So anybody who has found it useful to sit in a room with a professional and tell the professional about their problems might not be doing Freudian psychoanalysis but is doing something that was influenced by Freudian psychoanalysis along the way. By all accounts he was not a particularly effective therapist himself, but versions of therapy based on his ideas are still used today by some psychotherapists with some success in specific situations. 

Thirdly, the period when Freud’s ideas (or, more specifically, somewhat modernised versions of them) were most popular was the period between basically the 1940s to 1970s. This was when they were the dominant treatment technique used by American psychiatrists (who since the 1970s have moved away from these techniques, focusing more on the promise of pharmaceutical treatments). 

Many of the tropes of American pop culture were formed in this period (which saw the rise of television, for starters). The website TV Tropes which chronicles common tropes in pop culture has a page titled ‘All Psychology Is Freudian’ detailing the overwhelming prevalence of Freudian psychotherapy on TV. Prominent pop culture figures related to psychotherapy - Woody Allen on the couch or Frasier Crane on *Cheers* and then *Frasier* - have really set the tone in terms of how therapy is portrayed, and this tone is most often Freudian psychotherapy.

Of course, a lot of the time, a TV writer is probably basing the therapy scene on what happened in another TV show they liked rather than on their own experiences. But also, Freudian therapy is *dramatic*, about characters coming to realisations about themselves, their pasts, and their actions, with therapists who challenge their clients. I suspect writers feel like it plays better on TV as a dramatic device in a story than Rogerian therapy, where the therapist seems like they’re just repeating back what the client says, or CBT, where the therapist isn’t particularly interested in the childhood of the client.",0
"**Sources / Further Reading**

Boyarin, Daniel. *Dying for God: Martyrdom and the Making of Christianity and Judaism*. Stanford: Stanford University Press, 1999. 

Brown, Peter. *The Rise of Christendom: Triumph and Diversity*, Revised Edition. Malden, MA: Wiley-Blackwell, 2013. 

Cameron, Averil. *Christianity and the Rhetoric of Empire: The Development of Christian Discourse*. Berkeley: University of California Press, 2005.


Luijendijk, AnneMarie. *Greetings in the Lord: Early Christians and the Oxyrhynchus Papyri*. Harvard: Harvard University Press, 2008.

MacCulloch, Diarmaid. *Christianity: The First Three Thousand Years.* New York: Penguin Books, 2011. 

Pelikan, Jaroslav. *The Christian Tradition, Vol. 1: The Emergence of the Catholic Tradition*. Chicago: University of Chicago Press, 1975. 

Robinson, Thomas A. *Ignatius of Antioch and the Parting of the Ways: Eary Christian and Jewish Relations*. Peabody, MA: Hendrickson Publishing, 2009. 

Stark, Rodney. *The Rise of Christianity*. New York: Harper, 1996.

Tanner, Norman P. *The Councils of the Church: A Short History*. New York: Crossroad Publishing Company, 2001. 

“The Letters of Ignatius, Bishop of Antioch” in Richardson, Cyril C., ed. *Early Christian Fathers*. New York: Touchstone, 1996.

Wilken, Robert Louis. *The Christians as the Romans Saw Them*, 2nd Ed. New Haven: Yale University Press, 2003.

Wilken, Robert Louis. *The Spirit of Early Christian Thought: Seeking the Face of God*. New Haven: Yale University Press, 2003.",0
"No, the admiralty didn't have any fixed roster of seamen or anything like that -- most likely she would learn in a letter from the ship, either written by the captain or another member of the crew (literate men would often write dictated letters for other men, either for money or alcohol/tobacco/some other form of currency), or when the ship came back to pay off.",0
"Speculative building - putting up an apartment building or houses in the hope of finding and fleecing renters or buyers - is far from a modern phenomenon. I don't think that we can say when speculative building began; in some sense, the practice is probably almost as old as urbanization. We do know, however, where speculative building first flourished on a modern scale: the city of Rome. 

From the mid-Republican period through the fourth century CE, Rome was the biggest, most notorious, and most dangerous city on the face of the earth. As the center of a Mediterranean-spanning empire, Rome's appeal was obvious. Juvenal famously complains about the hordes of Syrian immigrants, and a rich array of inscriptions in Greek, Syriac, and Hebrew attests to the teeming diversity of Rome's streets. To new migrants, the city's dangers may have been less evident: the thieves who waited in dark alleys at night, the grinding poverty that overtook those who failed to find a reliable occupation or patron, and - worst of all - the hyperendemic malaria that killed thousands every year. Rome's population was never able to reproduce itself; but the constant stream of migrants ensured that it remained more or less stable until the fourth century, when imperial patronage migrated elsewhere. 

There was, in short, constant and reliable demand for housing in Rome. And there were plenty of aristocrats ready and willing to erect, derive rent from, and signally fail to maintain said housing. The vast majority of Romans lived in the apartment buildings called *insulae* (""islands""). These were multistory buildings, usually arranged around a central courtyard, with shops on the ground floor, ""good"" apartments on the story above, and increasingly unlivable units higher up. Although a series of emperors attempted to limit the height of *insulae*, the regulations applied only to street frontage, and unscrupulous contractors propped precarious additional stories on the roof. Some *insulae*, like the famous Insula Felicles by the Circus Maximus, must have approached the 170-foot height of the Colosseum. 

*Insulae* (along with that safest of premodern investments, agricultural land) were recognized as a lucrative source of income by Roman investors. Cicero seems to have been something of a slumlord, and complains about the collapse of several of his units in a joking letter to Atticus. His tenants, one imagines, were less amused. The most famous real estate magnate in Roman history was doubtless the triumvir Crassus, who (as Plutarch tells us) liked to buy buildings when they were actually on fire (thus getting a much-reduced price from the despairing owner), send in highly-trained teams of slaves extinguish to the blaze and repair the structure, and ""flip"" the reconstructed *insula* as a rental property. Many less famous Romans got in on the rental game; the poet Martial reports that one wealthy notable derived a whopping three million sesterces each year from apartments in Rome and market gardens in the vicinity. 

When a wealthy Roman decided to build an apartment building, he (or more likely, a trusted freedman) would contact one of Rome's contracting firms. These firms seem to have usually been relatively small, consisting of the contractor himself and a dozen or so skilled slaves or freedmen. Once the investor supplied the money, the contractor would hire as many unskilled laborers as he needed and set to work. *Insulae* \- if we may extrapolate from the many well-preserved examples in Ostia (Rome's harbor) - were usually built (like most large Roman buildings) with walls of brick-faced concrete. Besides enabling the construction of soaring domes like the Pantheon's and vast vaults like those of the Baths of Caracalla, Roman concrete was a time-saver, allowing a few skilled workers to oversee the rapid construction of thick and solid walls. As long as the money kept flowing, a four- or five-story *insula* could thus be ready for occupation in a few months.

We know regrettably little about rents in Rome, save that they were usually regarded as extortionate. Rent was due once a year (on July 1), a time of great distress for poorer tenants; Martial devotes an epigram to a poor man evicted from his room and forced to live under a bridge. Like their modern counterparts, Roman apartments had managers (the same man, often a freedman, would manage all the buildings owned by a given investor), doormen (*insularii*), and remorseless rent collectors. Buildings were often known by the name of the owner, but were sometimes given alluring names (like Felicles - ""Happy Isles"") to allure renters. 

The towering, tottering apartment blocks of Rome are merely the most modern-seeming of many premodern ventures into speculative construction. The basic phenomenon is as old as rent.",0
"Despite seeming like something out of a fantasy novel, the dragon banner does have a historical basis. It was not a black flag with a red dragon on it, as depicted in *Outlaw King*, but rather was shaped like the [Roman draco](http://www.fectio.org.uk/articles/draco5.jpg), a sort of windsock with an openmouthed metal head with red fabric attached to it. When turned towards the wind, the fabric would inflate and writhe. I have also found no evidence of those accompanying the banner wearing the same dragon on their tabards, as the Prince of Wales and some of his knights do in the film.

The banner was first used by the Anglo-Saxon kings simply as a royal standard. In 752 it was used by the king of Wessex in a battle against the Mercians, in 1016 against Cnut, and in 1066 at Hastings, [as seen on the Bayeux Tapestry](https://images.theconversation.com/files/140459/original/image-20161005-14227-1e7ovyz.jpg?ixlib=rb-1.1.0&rect=0%2C11%2C1491%2C997&q=45&auto=format&w=496&fit=clip). It was only after the Conquest that it seems to have had a wider significance relating to the conduct of war. From Richard I (the first English king to revive the tradition) onwards, the banner appears to have been used exclusively against those considered to be rebels or infidels, associating it with *guerre mortelle* or Roman war, war without limits where no mercy had to be shown to enemies. The only legitimate targets of such war, according to medieval theorists, were rebels and infidels (though some, like Thomas Aquinas, considered rebels to be exempt from this). The banner was used by Richard I in 1191 against Saladin at Arsuf, Henry III in 1245 and 1257 against the Welsh (who he referred to as rebels), 1264 against the barons, Edward I against the Scots (who he claimed sovereignty over), and Edward III at against the French in 1346 (whose throne he claimed).

Though some historians have been dubious that the dragon banner declared mortal war, several of medieval chroniclers certainly saw it as such. John of Oxnead (d. after 1293) described it as 'a sign of death and great retribution', Matthew Paris (d. c. 1259) said that its use by Henry III in 1257 'threatened the total extermination of Wales', and William Rishanger (d. after 1312) said it was 'a royal banner that portends a sentence of death'. The confusion may arise because the kings often intended it more as a threat than a promise. Edward III only raised the banner at Crecy after the French first unfurled the Oriflamme (which had similar connections with signifying no quarter would be given). Henry III's 1245 campaign ended with a treaty and the 1257 expedition was delayed.

I wrote a [short piece about this for a Scottish newspaper just last week](https://www.thenational.scot/news/17262475.raising-the-dragon-outlaw-king-accuracy-under-the-microscope/), which has a bit more detail on the dragon's various appearances in the record.

Sources:

*Calendar of the Close Rolls preserved in the Public Record Office*, 47 vols (London, 1892-1963), Henry III 1242-47

Rory Cox, ‘A Law of War? English Protection and Destruction of Ecclesiastical Property during the Fourteenth Century’, *English Historical Review*, CXXVIII, 535 (2013), pp. 1381-1417.

Henry of Huntingdon, *Henry, Archdeacon of Huntingdon, Historia Anglorum, The History of the English People*, ed. and trans. Diana Greenway (Oxford, 1996).

John of Oxnead, *Chronica Johannis de Oxenedes*, ed. Henry Ellis (London, 1859).

Matthew Paris, *Chronica Majora*, ed. Henry Richards Luard, 7 vols (1872-84).

William Rishanger, *The Chronicle of William de Rishanger, of the Barons’ Wars. The Miracles of Simon de Montfort.*, ed. James Orchard Halliwell (London, 1840)
",0
"Umm, very? People alive in the 1950s would find such a person pretty bizarre, as it's the kind of physique that literally couldn't be created prior to the invention of anabolic steroids in the 1930s and their popularization in bodybuilding circles in the 1960s and 1970s. It also takes an extremely specific diet to reach that level of musculature and another very specific diet to cut down to that level of body fat. Early 20th century strongmen and weightlifters were notably smaller and less cut.

To a great degree, this is a better question for an archaeologist. I am not an archaeologist, but I did read a few articles on the subject while staying at a Holiday Inn Express last night. However, I'm not sure archaeology can tell us all that much about body fat percentages or muscle mass. Textual and artistic evidence are probably more useful (or at least, it's what I'm familiar with), but they inform us more about diet and forms of exercise than body type.

People have a lot of misconceptions about the physiques of medieval people. They were not pygmies; the average height of a northern European man throughout most of the period was around 5'8"", which is only an inch or two shorter than the average modern Briton or American. They were not consistently half-starved; the average medieval farmer ate a nutritious, if bland, diet. But it was a diet that was heavy in carbohydrates, especially gruel and weak beer, and relatively light in protein, which is good for keeping you on your feet during hours and hours of heavy labor, but not ideal for building a hugely muscular or low-fat body. Likewise, when they had strenuous work to do almost every day, going on a long cutting cycle would not have been practical. The common body type of a farmer (that is, 90% or so of the population) would probably be most similar to a modern manual laborer: strong, fit for long and hard toil, but not tremendously muscular. They could probably walk the bodybuilder to death, however.

Aristocrats typically consumed more meat and did far less labor, so it may well be that they were different in build. They seem to have been very slightly taller, about on par with a modern person. Hunting from horseback, dancing, swimming, and other physical activities were very common diversions, but they're not really great ways to build huge amounts of muscle. We have evidence that those who pursued a military vocation (at some times and places, almost all able-bodied males) undertook more rigorous exercise, such as fencing, jousting, wrestling, and lifting stones, but there's also no indication that they were especially bulky. Certainly the art of the time depicts warriors, rich and poor, as relatively slender and athletic. Art from the Middle Ages was highly stylized, but in combination with other evidence, I think it's a safe bet that they looked more like early 20th century football and hockey players than Dwayne Johnson.",0
"This is a very interesting question. In an environment of fiercely hostile states competing for supremacy, it doesn't really make sense for a successful general to write down his knowledge for all to read. And any ruler who got hold of such a document would find it in his best interests to surpress it. So how did a text on military methods end up as one of the most famous and widely read texts from Ancient China?

In many ways, the 孫子兵法 (*Sun Zi Bing Fa*, ""Master Sun's Art of War"") is not the most promising place to look for an answer. The origins of the work are shrouded in mystery; the author may or may not be historical; the manuscript tradition is fraught with difficulties; the text itself was summed up by Robin Yates as ""terse and obscure"" and by Victor Mair as ""extraordinarily terse and maddeningly obscure"". We'll have to dig into these issues a bit if we are to get any sense of what the *Sun Zi* was meant to achieve.

First, the origins. At first glance, this question seems easy to answer: in his 史記 (*Shi Ji*, ""Records of the Grand Historian""), the Han Dynasty scholar Sima Qian tells us that the *Sun Zi* was written around the early 5th century BC by a famous general called Sun Wu. Sima Qian offers some delightful biographical details of this Sun Wu, including the famous story of how he trained the concubines of the king of Wu in heavy infantry drill. The notion that there was indeed a historical Master Sun, who wrote his treatise on warfare at the end of the Spring and Autumn Period, is still held by many. If we adopt this view, the answer to your question is relatively simple: a fugitive from the northern state of Qi (according to one version of the story), Sun Wu needed to impress his new master the king of Wu, and apart from his display as a drillmaster, the compilation of his Thirteen Chapters (the *Sun Zi*) would have provided evidence of his abilities. The biographical tradition helpfully notes that his advice and leadership allowed Wu to expand and conquer its neighbour, Chu. 

The problem with this account of the genesis of the *Sun Zi* is that there is no earlier account of the life of a general named Sun Wu, and that the very practice of assigning authors to works is not something the Chinese actually did until after the end of the Spring and Autumn period. The find of a collection of texts on strips of bamboo - the traditional recording method - in a group of graves dating to the late 2nd century BC has confirmed that the *Sun Zi* existed more or less in its currently known form at that time, but there is no evidence that would support a date as early as the 400s BC. Moreover, if it did date that far back, it would be *by far* the earliest Chinese military treatise, with a gap of well over a century between it and its oldest successor. 

Indeed, the text itself has been shown to reflect the military practice of the Warring States period (c. 475-221 BC), not the Spring and Autumn period (c. 771-476 BC) in which it was supposedly written. The well-organised, well-drilled infantry armies envisioned by Master Sun do not belong to the era of chariot-borne warlords and their loose entourages that was the Spring and Autumn period. Its references to crossbows must place it in the period after these weapons were invented (sometime in the 4th century BC). In other words, the text is far more likely to be a product of the 4th century BC than the early 5th; as far as we can tell, the spread and recognition of the work as a military classic belongs to the early Han Dynasty.

It is not surprising, then, that Chinese scholars as early as the 13th century AD have wondered whether the legendary author of the *Sun Zi* ever really existed. Many modern experts have been persuaded by Jens Østergard Petersen's argument that the details of Sima Qian's biography of Sun Wu are generic, and that all its elements can be found in biographies of other generals too. Sun Wu's name (effectively meaning something like ""wandering warrior"") is considered too convenient, and his historical context too vague, to take him seriously as a character of the late Spring and Autumn period. In fact, there is robust evidence of the existence of a general and military expert with the surname Sun - but his first name was Bin, and he lived well over a century after the alleged time in which Sun Wu flourished, and he is credited with writing an entirely separate treatise on military matters, which was rediscovered in the same collection that gave us the earliest extant traces of the *Sun Zi*. It seems all too likely that the family name Sun, famously associated with military expertise, became attached to a growing collection of martial wisdom compiled over the course of the Warring States period. The opening of each chapter of the text with the formula ""Master Sun said:"" is paradoxically an indication that there was never a ""Master Sun"" - only a succession of authors legitimising their wisdom through his reputation. Predictably, Sun Bin was eventually credited with being a descendant of the legendary Sun Wu, whose wisdom clearly ran in the family.

The fact is that the way of recording Ancient Chinese texts, using the strips of bamboo mentioned above, is extremely conducive to this sort of textual tradition. Single strips of bamboo contain no more than one or a few lines of text; the strips are held together with string. As the string breaks or decays, texts become jumbled, with later editors often totally unable to restore them to their original order. From our point of view, it is shockingly easy for lines to be added or lost. One of the frustrating things about the *Sun Zi* is that much of its metatext - its ""next""s and ""therefore""s - don't actually make any sense, and that not enough serious philological study has been devoted to restoring the text. Given its many uncertainties, we are on far safer ground to assume that the work is the product of many hands over a long period (roughly the 4th and early 3rd centuries BC) than that it is the work of a single, legendary military genius.

So where does that leave us with regard to your question? Why was the *Sun Zi* written? Who benefitted from writing it and spreading it around?

Here we find a useful parallel in the tradition of Ancient Greek literature. In Greece, as in China, military treatises did not develop in a vaccuum, when a particular military mastermind decided to commit his wisdom to the ages. Rather, they began to appear in an era that saw a general shift in notions of learning, in which oral tradition and abstract thought began to gave way to practical written works that allowed knowledge in discreet fields of expertise to be replicated and taught. In Greece, as in China, the art of war was swept up in this movement by the conditions of the age: in Greece, the hegemonic wars of the Classical period (of which the Peloponnesian War seems to have been especially important as a driving force), and in China, the prolonged and devastating Warring States period. In both cultures, the earliest written works of philosophy had appeared some time before, and kickstarted a tradition of recording advanced thought for the benefit of others. It is not surprising to find Chinese catalogues grouping the *Sun Zi* in the Daoist (that is, philosophical) tradition. It and the many other military treatises that appeared soon after all fit into the same new approach to the spread of knowledge. The purpose of the text was to teach, so that future generations facing military problems would not have to reinvent the wheel.

---

**Sources**

* R.D.S. Yates, 'New Light on Ancient Chinese Military Texts: Notes on Their Nature and Evolution, and the Development of Military Specialization in Warring States China', *T'oung Pao* 74.4/5 (1988), 211-248

* J.Ø. Petersen, 'What's in a name? On the sources concerning Sun Wu', *Asia Major* 5.1 (1992), 1-31

* V.H. Mair, *The Art of War: Sun Zi's Military Methods* (2007)

",0
"Oh boy is this a question, and dont worry you arent the first to wonder. Hell even current and former soldiers have thought, how the Hell did this happen, especially when less than 2 years later he would be dismissed(intentionally on his part) from West Point for misconduct. 

So lets lay down the basic facts, in late May 1827 Poe lies about his age and identity to enlist in Company H, 1st Artillery in Boston. He would be promoted to Sgt Major in January 1829, and was released 2 years into a 5 year enlistment that April in order to work towards acceptance to West Point which he would enter the following year, and then Spring 1831 would see him dismissed and his military career over for good. 

In October 1827 the Company(along with the HQ of the 1st Aritllery) was sent from Boston to Ft Moultrie outside Charleston, and later Ft Monroe at the tip of the VA Peninsula guarding Hampton Roads. And Col House, CO of the regiment would be the one to promote Poe, and was personally familiar with him as they had been colocated for most of that time. 

And despite his later personal demons, Poe actually made a fine soldier, was regarded as energetic, reliable, and bright. He also had something of a family legacy to live up to as a grandfather had been a quartermaster in the Continentals. 

We can also note that about a year in Poe was promoted to Artificer, essentially the equivalent of a modern day Specialist. In that role he had responsibility for the care and condition of the artillery stores, and especially those new varieties of temperamental explosive shells! 

So in the relatively flat enlisted org structure Poe was one of maybe 20 or so senior enlisted already with his company, which was itself only led by a 1st LT, and the CO of the regiment was present with him at the time. 

It was the in late 1828 that Poe began to desire to leave the ranks, he revealed his deceptions to his Company Commander, a LT Howard. Howard then had Poe speak to Col House(and told Poe to write to his foster father), and shortly after Poe was reassigned to work with the regimental Adjutant as part of headquarters. Then just a few weeks later House exercises his authority in the matter and makes Poe his Regimental Sgt Major in January 1829.

Remember too that the Army was routinely short of men and had trouble filling the authorized establishment, or saw attrition to illness and desertion, with 75% authorized strength present not being a crazy baseline. An artillery company of the 1820's only had a max authorized force of 42 privates, 3 artificers, 4 corporals, and 3 sgts. The HQ then only had the Sgt Major and Quartermaster Sgt at least officially. So with such a small group, with the companies of the regiment rarely concentrated more than 2-3 to a post, it was easier for one to standout. 

We honestly dont have hard explanations one way or the other. Poe was clearly a good soldier, and could now be honest about his slightly notable family history, was already a budding writer, and had some education from his stint at UVA. Those are traits that are certainly welcome in a senior enlisted in the peacetime artillery branch pre war. At least one source also mentions House was something of a lover of literature so there is another point of commonality for the two. 

It was also about this time that Lt Col Worth rejoined the regiment from a stint at West Point and it could also be that his experiences there, and history of making the jump from enlisted to officer, are what pushed Poe over the edge to not just want to get out, but then go to the USMA. Their post at the time, Fortress Monroe may also have played a role. It was the closest thing to a home the artillery had, as it was home to the Artillery School. When possible artillery companies would be concentrated there for a year or so and trained up as a group, and new West Point grads would also spend a year or so there if they were destined for the artillery. So Poe would have had plenty of exposure to recent grads. 

Worth seems to have been onboard with the idea as he helped advocate for his appointment as a cadet. And so with his relationship patched for the time being with John Allen, and his commanders in his corner, he was released from his contract that Spring. 

Some easy reading on the matter! From the Army Space Journal. https://apps.dtic.mil/dtic/tr/fulltext/u2/a525759.pdf

Organizational history of the field artillery! Though Poe's old unit is actually now the 1st Air Defense Artillery Regiment, also having been designated the 1st Coast Artillery regiment, encapsulating the three pronged specialization the artillery branch eventually developed. https://history.army.mil/html/books/OH_of_FA/CMH_60-16-1.pdf",0
"Thanks for your detailed answer. I can't speak to the Thirteen Colonies, but in New France, families were rewarded for each child brought into the world. The ""Filles du Roi"", the king's girls or daughters (most of them were jailed for one reason or another), were specifically sent to New France to marry the male settlers and multiply. Some women gave birth to 15-20 children in their lifetimes. They needed a labour force to work the  fields. It was recently calculated that the optimal number for a successful settler family was 9-10 children. Any more than that, and the family would not be able to afford educating or feeding their family. It seems the Catholic Church might have understood that too many children would affect the betterment of the society (or their own coffers), so families were required to give some of their children to the Church as benediction and tithe.

As an additional question for you, would the slave trade also factor into those numbers in the Thirteen Colonies? Were slaves even considered in the census?",0
"Eddie, I'd contend that what you should be paying attention to is an event that happened less than one month after Hitler was appointed chancellor on Jan. 30, 1933. I'm referring, of course, to [the Reichstag fire](https://www.ushmm.org/wlc/en/article.php?ModuleId=10007657).

On Feb. 27, 1933, the German parliament building, the Reichstag, burned down. 

This is a key moment in the rise of Nazi Germany and one of the points at which Germany shifts from the Weimar Republic to a totalitarian state.

The Reichstag had been the heart of democratic Germany, a representative body that ─ even in 1933 ─ still held the belief that it answered to the people, not the state. The Nazi party held a minority of seats in that body. It had even lost 34 seats in the November 1932 election while the Communist Party gained 11 (for a total of 100). 

The Reichstag was by no means perfect. The Weimar government had shown its flaws, and the rise of Nazism had shown the Reichstag's conservative members to be willing to go along with the radical Nazis. The conservatives in the Reichstag believed Hitler was a buffoon, someone controllable even if he got into power as chancellor. They were not captivated by his speeches (in general) and believed he was less extreme than he actually was. With Hindenburg as president, the conservatives believed that Hitler would be checked, and the Nazis could be brought into the fold safely. Things worked out differently, of course.

When the fire took place, Hitler and his closest advisers saw that it was a golden opportunity. A young Dutch Communist had been arrested at the scene, and so they were quick to declare the fire an arson, a communist plot designed, as Goebbels wrote in his diary, ""to sow confusion in order, in the general panic, to grasp power for themselves.""

Many people have claimed in the years since the fire that it was entirely orchestrated by the Nazis as a pretext to seize power. Many others have claimed that there's little evidence of a conspiracy, and that the Nazi leadership was simply opportunistic. [Richard Evans' 2014 review of *Burning the Reichstag* is a wonderfully detailed breakdown of the arguments](http://www.lrb.co.uk/v36/n09/richard-j-evans/the-conspiracists). 

In the end, I'm not sure whether it matters all that much. In either case, the Nazis called the fire an act of terrorism, and in order to fight terrorism, they needed to improve security. In order to improve security, they needed new powers for the state, and they needed to make arrests. Nazi party members were enrolled as auxiliary policemen, and overnight they arrested hundreds ─ if not thousands ─ of communists and left-wing politicians and political organizers. Disregarding parliamentary immunity, the Nazis seized the leaders of the Communist Party and the Social Democratic Party.

The morning after the fire, the German cabinet ─ which, like the Reichstag, had a non-Nazi majority ─ drew up an emergency decree designed to ensure security. It abolished freedom of speech, freedom of the press, freedom of assembly, legalized phone-tapping, suspended the freedom of some of the German states, and in general meant the end of free government in Germany.

Using the terrorist attack at the Reichstag as a pretext, the Nazis pushed through the emergency decree, which was signed by a wavering Hindenburg. 

Now, with unprecedented power as Chancellor, Hitler was able to brow-beat his opponents. Less than a week after the emergency decree passed and the Nazis had been able to unleash a firestorm of violence against their opponents, a new federal election was held. The Nazis still tallied only 44 percent of the vote, despite their actions, but it was enough.

Two weeks after the election, the Nazis were able to gain enough votes from non-socialists to pass the Enabling Acts. These acts gave Hitler and his ministers exclusive legislative power. The president and the Reichstag were sidelined.

By the summer of 1933, all opposition was crushed. More than 100,000 people had been sent to concentration camps. Thousands more were murdered. All independent political parties were dissolved. Only the Nazis remained.

It happened that quickly. There were less than four months between Germany's last free elections, in November 1932, and the passage of the Enabling Acts in March 1933. If you want to learn more about this, read Richard Evans' *The Coming of the Third Reich.* It's widely published and easy to understand.

***

Now that I've told you what I think, let me actually answer your question.

On the night that Hitler became Reich Chancellor, Goebbels organized a torchlight parade in Berlin with some 60,000 participants. Some observers noted in their journals that Goebbels had the marchers go in a circle, so as to pass the reviewing stand at least twice in order to create the impression of greater numbers. There were still plenty of cheers regardless.

Though the Nazis put on a show of force with the parade and other events, they were careful to stage the production as a show of support for Hindenburg, to say that the parades were a ""tribute to Hindenburg"" and that they were not truly disrupting the traditional order. There were marches in other cities as well, and occasional violent clashes with Communists. There was a shootout in Spandau, shots fired from a house in Charlottenburg at a march. Copies of the communist party newspaper were seized and burned.

Generally, however, the leftist parties and left-center parties tried to keep a low profile, fearing a government crackdown on their operations. There was local opposition, but nothing organized at a national level.

Four days after taking the Chancellorship, Hitler made moves to keep the leadership of the German Army neutral. He feared a coup, and to reduce the chance of that, he spoke to Army leaders and pledged to do many of the things they favored ─ fight the Treaty of Versailles, restore conscription, and destroy Marxism.

On Feb. 4, the cabinet issued a decree allowing the government to detain for up to three months (without trial) people who used weapons to breach the peace. It was targeted at people resisting Nazi stormtroopers. 

Before Hitler became Chancellor, the Prussian police had been keeping an eye on Nazis and other paramilitaries who caused trouble. The police might not have been able to act against these armed groups because of political considerations, but they still investigated them. Hitler ordered those investigations to stop.

In the middle of February, Hitler also created an auxiliary police force made up of Nazi paramilitaries, in effect putting Nazi violence under the protection of the police.

In the beginning of February, the Nazi minister of the Interior, Wilhelm Frick, and Hermann Goring (the minister of the Interior for Prussia) began banning Social Democratic newspapers. The Social Democrats responded by suing, which had some success.

As the month went on, the Social Democrats began to join the Communists as targets of violence from Nazi paramilitaries. In response, the Social Democrats tried to stick to a legalistic defense. They did not want to respond to Nazi violence with violence of their own, an act that would encourage a heavy-handed response from the federal government.

In Wurttemberg State, the president, Eugen Bolz, declared the new government to be an enemy of freedom. Hitler responded that Bolz had no room to talk, since he hadn't protected the Nazi Party when it was persecuted in the 1920s.

""Those who made no mention of our freedom for 14 years have no right to talk about it today. As Chancellor I need only use one law for the protection of the national state, just as they made a law for the protection of the Republic back then, and then they would realize that not everything they called freedom was worthy of the name,"" Hitler said in a speech.

While the Nazi paramilitaries were making enemies of the Social Democrats and the Communists (and to a lesser extent, the Centre Party), there were plenty of people in Germany who delighted in what he was doing. Remember, this was the Great Depression, and there were many in Germany who enjoyed his actions.

Evans quotes the diary of a woman named Louise Solmitz:

""I’m delighted at Hitler’s lack of a programme, for a programme is either lies, weakness, or designed to catch silly birds. ─ The strongman acts from the necessity of a serious situation and can’t allow himself to be bound.""

",0
"> Spartiate numbers were always dwindling

Would you mind elaborating on this?

I know that Sparta had a relatively small military compared to its rivals, and that maintaining a large slave population exhausted a lot of resources, but I'm not familiar with the dwindling population. Was it just losses in warfare, or something more complex?",0
"For those who would like more context on how male same-sex relationships were conceived of in earlier centuries of Ottoman history, I can also offer [a previous answer of mine on the topic](https://www.reddit.com/r/AskHistorians/comments/awmwkv/was_homosexuality_seen_as_normal_in_the_middle/eho3x55/).",0
"[A repost of my previous answer that's the one in the FAQ linked by someone else](https://www.reddit.com/r/AskHistorians/comments/7rvynq/what_did_people_think_about_static_electricity/):

I'm going to assume that by 'prior to the discovery and understanding of static electricity' you essentially are asking about the pre-'Scientific Revolution' time period - e.g., before William Gilbert's 1600 book *De Magnete* was using the word 'electricus' to refer to electrical phenomena, before Benjamin Franklin flying his kite and all that (after all, there's a sense in which electricity was first discovered by the first person to get a static electricity shock), and certainly before electric lights or Tesla cars.

It’s worth remembering that, in antiquity and the middle ages, the workings of the natural world were fundamentally mysterious. At a fundamental level, we modern types - living in a world with computers and search engines and university science faculties - generally assume that the natural world is orderly, based on natural laws. Because of this, the world is, for us, in principle usually going to be solvable. Things we don't understand are probably understandable through further research or experimentation (or perhaps have already been solved by people much smarter than us). Ancient and medieval people simply didn't have this mindset. As a result, their explanations of the natural world were - understandably - *magical* ([see this previous comment of mine for more background on the mysterious, magical world of the ancients](https://www.reddit.com/r/AskHistorians/comments/6pf6vw/preenlightenment_science_has_a_popular_reputation/dkpx2oy/)). After all, if you think that the world at a metaphysical level is fundamentally unknowable, the presence of spirits and souls in objects doesn’t seem entirely silly. Even Aristotle, that most practical-minded and scientific of Ancient Greek philosophers, thought that inanimate substances contain meaning/purpose (*telos*), and talked about souls, in ways that seem odd to people who grew up in a world that assumes the universe is fundamentally physical.

Writers in antiquity and the middle ages often thought in terms of sympathies and antipathies; particular objects would have *natural* attractions and repulsions from others. So, for example, it was believed that rubbing garlic on a magnet would stop it from working, while rubbing lambs’ blood would make it work again. These beliefs about magnets were eventually shown to be incorrect, but even Scientific Revolution figures like Descartes believed some of them. In general, medieval writers weren’t particularly inclined to argue with ancient authors except where religious doctrine intervened, so it took until after Columbus had ’discovered’ America for these ideas about garlic and lambs blood affecting magnets to eventually stop being considered common knowledge. All of which is to say that ancient people thought very differently to us; they were not basically modern people who just hadn't figured out electricity yet.

With that in mind, let's look at what some ancient writers said about electricity. The first figure to discuss static shocks is usually considered to be Thales of Miletus, though we only know *that* he discussed static shocks because his writing has not survived. Our main source on Thales discussing electrical phenomena is a biography by Diogenes Laertius, written close to a millennium later. According to Laertius, Thales was ""the first, some say, to discuss physical problems. Aristotle and Hippias affirm that, arguing from the magnet and from amber, he attributed a soul or life even to inanimate objects.""

The wording here perhaps suggests that amber's electric effect after being rubbed (e.g., your hair might be pulled towards it) was well-known to the Greeks before Thales; certainly there's plenty of amber jewelry in the archaeological record. Laertius seems to me to be implying that Thales was the first to *theorise* about it, arguing that there was a sort of animateness in seemingly inanimate objects. But that quote is the sum and total of everything he said about it. It should be pointed out that the surviving writings of Aristotle - writing centuries after Thales and centuries before Laertius - only mention Thales arguing from the magnet rather than from both amber and the magnet. And because the text by Hippias that Laertius mentions is also lost, we can't tell whether that discussed Thales theories about electricity in more detail. Nonetheless, Thales probably argued that there is something *animate* in what are on the surface inanimate objects such as amber; otherwise the effect of static electricity would not occur.

In contrast to Laertius' brief mentions of Thales, the Roman writer Pliny The Elder in *The Natural History* - a sort of encyclopaedia of received wisdom on a variety of topics - discusses amber at length. Pliny points out within his discussion that ""[when a vivifying heat has been imparted to it by rubbing it between the fingers, amber will attract chaff, dried leaves, and thin bark, just in the same way that the magnet attracts iron](http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.02.0137%3Abook%3D37%3Achapter%3D12)"".

Here Pliny makes a link between the electrical effect of amber and the magnetic effect of certain metals/stones ([I discuss pre-modern science conceptions of magnetism here](https://www.reddit.com/r/AskHistorians/comments/6kj0yv/the_compass_was_invented_far_before_the_discovery/djn2yf4/)); Thales probably made the same link, if Laertius's sources are accurate. Elsewhere in his writing, [Pliny The Elder also discusses electrical discharge in relation to fish](http://www.perseus.tufts.edu/hopper/text?doc=Perseus:text:1999.02.0137:book=9:chapter=67): 

>The [torpedo](http://www.iucnredlist.org/details/161397/0) is very well aware of the extent of its own powers, and that, too, although it experiences no benumbing effects from them itself. Lying concealed in the mud, it awaits the approach of the fish, and, at the moment that they are swimming above in supposed security, communicates the shock, and instantly darts upon them.

Pliny The Elder makes no attempt to explain the powers of the common torpedo here; he merely describes them as a prelude to pointing out how good torpedo liver tastes.

A few other ancient writers also discuss the torpedo. Plutarch says in *De Sollertia Animalium* that:

>	You know, of course, the property of the torpedo: not only does it paralyse all those who touch it, but even through the net it creates a heavy numbness in the hands of trawlers. And some who have experimented further with it report that, if it is washed ashore alive and you pour water upon it from above, you may perceive a numbness mounting to the hand and dulling your sense of touch by way of the water which, so it seems, suffers a change and is first infected. Having, therefore, an innate sense of this power, it never makes a frontal attack or endangers itself; rather, it swims in a circle around its prey and discharges its effluvia as if they were darts, and thus poisons first the water then through the water the creature which can neither defend itself nor escape, being held fast as if by chains and solidified.

There is also a lost text by Diphilus discussed briefly in Athenaeus’s *Deipnosophisticae*:

>	In his commentary on Nicander’s *Theriaca*, Diphilus of Laodicea says that not the entire body of the animal produces the numbness but only a part of it. He alleges that he has arrived at this conclusion on the basis of many experiments.

(Plutarch’s reference to ‘some who have experimented further with it’ may also be a reference to Diphilus)

Generally, as can be seen above, ancient writers don't really try to *explain* occurrences that we’d now understand as electricity. Our surviving references to electrical phenomena mostly just report them and marvel at them (I’ve quoted the ancient writers’ entire statements on electricity above - that’s as much detail as they went into). But perhaps we can get a window into how they implicitly thought electrical phenomena worked by looking at how they used their knowledge of amber and torpedos. And, well, what Pliny says about how to use torpedos does quite clearly suggest that he believed that their electrical abilities were essentially magical - there wasn't any obvious mechanism by which the torpedo could stun other fish, so it was probably magic. 

Specifically, the *Natural History* mentions various remedies that use the magical powers of torpedo. One hair removal remedy that Pliny discusses is ‘the brain of the torpedo applied with alum on the sixteenth day of the moon’. Additionally, if a torpedo is caught while the moon is in Libra and kept alive in the birthing room of a pregnant woman, Pliny believes that it makes childbirth easier. Note the way that both remedies involve the movements of the heavens in ways that sound like astrology to us. But remember that, from their point of view, if rays like the torpedo can affect other creatures at a distance in the water, why shouldn’t the movement of the moon also affect creatures at a distance? 

Finally, Pliny claims that excessive sexual craving can be dulled if the ‘gall of the torpedo, while it is still alive, [is] applied to the genitals’. Well, that one doesn't rely on the movement of the heavens, and...well, I can see how applying electricity to the genitals may cause some people to rapidly lose interest in sex. Some people, anyway.

Sources:

* *The Shocking History Of Electric Fishes: From Ancient Epochs To The Birth Of Modern Neurophysiology* by Stanley Finger and Marco Piccolino (2011)

*  *A History Of Electricity And Magnetism* by Herbert W. Meyer (1972)

* *The Experimental And Historical Foundations of Electricity* by Andre Koch Torres Assis (2010)

* *The Invention Of Science: A New History Of The Scientific Revolution* by David Wootton (2015)",0
"The answer to the question is very related to the follow up question from u/JustZisGuy. No, not all immigrants were the same, and there were lots of tension between immigrants from different countries, different times, different classes, and different religions.


America didn’t really start off as an Immigrant country. The first colonies that were formed at Virginia and Massachusetts were formed in 1607 and 1620 respectively. Given that the modern-day USA was born in 1776, that’s 156 years of life in the continent, and enough for your grandchildren to naturalize in any country by today’s standards. Even by 1770 when Massachusetts was declared under martial law instead of more lenient taxing, most of the colonial leaders hoped to “reconcile with the British Government” rather than declare independence. By this time, the colonists weren’t colonists or immigrants anymore, but the people who were born on the east coast who built this country from almost scratch, including all the good and bad things that has happened.


These original colonists didn’t come here for no reason at all though. Catholicism was restricting freedoms throughout Europe and some immigrants came to escape this religious persecution to practice Puritanism (This is a completely different subject that requires a different research). Most of them came as indentured servants, slaves for pay for a predetermined amount of time, because the price to sail was too steep. These servants ranged from white Europeans to black West Africans. By the time of the Civil War, there were a continuous influx of people from across the Atlantic to the states, which all came either as slaves or indentured servants from all walks of life. 


All the way until the declaration of independence and the civil war, the US was known in the world as a place with “class mobility”, where you can work your way into the upper class and become rich and elite. The US promised freedom to practice your own religion, your own language. The colonial Pennsylvania is a good example of how it’s founder, William Penn envisioned a utopian society where diversity would beget tolerance. 


Migration was a part of the colonial American life. Americans themselves migrated every 10 years to different colonies, including people like Bejamin Franklin, so up until the beginning of the 18th century, immigration was considered a part of American life, and even the naturalized born and raised Americans were migrating. As the number of people who came to America on their own slowly diminished, and the slaves and indentured workers were transported accordingly, the states delved into more important matters, like declaring independence and fighting for it, and the age of colonialism in North America came to an end as a baby nation with all the turmoils of making one arose in its stead, full of Protestant and Puritans who were promised liberty and riches, but received varying levels of these promises. These people were in a time where immigration didn’t have the connotations it had today. An immigrant was a self-made man who fought for what he wanted and didn’t take no for an answer. An immigrant was an opportunist, a hard worker, a strongman who took care of his family in the most ideal fashion. 


Years of life in the states slowly eroded this image of the immigrant as the settlers settled, cities grew and the economy and jobs expanded. With the constitution came law, but not yet order. Americans owned and ran American properties, and new slaves and workers shipped across the Atlantic did the blue collar work. 


This was the type of country the US was when the conditions across Europe worsened as the US’s economy grew. Shipment of people to the states were around 60000 for more than 50 years until the famine in Ireland and political turmoil in Germany, which boosted these numbers dramatically. In 1851, there were 380000 people in the US ports of entry, a very dramatic increase in the consistent influx of humans. 2.7 million new prospective citizens entered the country in the next 7 years, and most of these people were Catholic in a time where Catholicism was hated in the US. There were stark opposition to Catholic churches and schools, but these immigrants had bigger problems. They drew hostility because of the diseases they brought with them from the old world. They were poor, just like the original immigrants but instead of improving the forests of Massachusetts into a sprawling city, they diminished its features with the slum housing they stayed in, the increase in crime rates, alcoholism and other misdemeanors. The American-Born protestants thought their English heritage was true Americanism and despised the Irish and the German. These people were called “nativists”, who believed opposition to the Catholics was necessary to protect America. The Know-Nothings, a political organization that was created by these nativists managed to become the second most powerful political organization in the nation, electing 5 senators and 43 representatives. After the civil war, nativist activity declined dramatically.


So to finally answer your question, anti-immigration sentiment arose from immigrants themselves because they viewed new immigrants as fundamentally different from themselves or their families. America undoubtedly was a melting pot, but this did not exclude people from making the distinction between cultures. Blacks were slaves, Irish were poor, Brits were true Americans etc. and anti-immigration is a very broad term for everyone who came to the US. Immigrants were slaves, immigrants were cheap labor, immigrants were Catholics, Immigrants were nation builders, and different groups had different expectations from these immigrants. Given all of these distinctions, it isn’t inherently illogical to say that immigrants are anti-immigration without including themselves. It is important to realize that actions have different consequences in different contexts, and some immigration is inherently more useful than others.


America, for most people, is the melting pot of the “correct” cultures. 
_________________________________________________________________________________________________________



Just as an extra, here are some examples of anti-immigration that happened in the US throughout its history:


“Yet as industrial revolution transformed the United States in the postwar years and attracted a vast new influx of immigrants, the antialien animus rose again. In the 1870s more than 2.7 million newcomers arrived at U.S. ports.”


“more than eighty thousand immigrants from China arrived between 1870 and 1875, brought to America by companies that had contracted to supply cheap labor to mines, railways, and other enterprises needing unskilled labor. With 30 percent of California’s workforce unemployed following the panic of 1873, many workers attacked these newcomers as “coolies” willing to work for slave wages. Outbreaks of violence against the Chinese spread throughout the West, from Los Angeles to Seattle to Denver. In 1882, Congress responded to anti-Asian nativism with the Chinese Exclusion Act, which suspended immigration from China for ten years.”


“A striking number of new nativist fraternal groups were formed, the most important being the American Protective Association (APA). Founded in Iowa in 1887, the APA had attracted a membership of 500,000 by 1895.”


“By the end of the nineteenth century, the APA had disappeared. Nativist activism did not flourish in the first decades of the twentieth century, the years of the Progressive Era. It rose again in the form of the post-World War I Red Scare in 1919, and in the powerful but short-lived Ku Klux Klan in the 1920s. It was in the nineteenth century that antialien movements had their greatest impact in American history.”


Sources:


https://www.gale.com/binaries/content/assets/gale-us-en/primary-sources/newsvault/gps_newsvault_19thcentury_usnewspapers_immigration_essay.pdf
https://www.history.com/topics/immigration/u-s-immigration-before-1965
https://history.state.gov/milestones/1776-1783/declaration
https://philadelphiaencyclopedia.org/archive/immigration-and-migration-colonial-era/
https://www.battlefields.org/learn/articles/brief-overview-american-civil-war
http://immigrationtounitedstates.org/548-history-of-immigration-1620-1783.html

Kettner, James H. The Development of American Citizenship, 1608-1870. Williamsburg: Omohundro, 1978. Document.


",0
">> CW
>
>What's a CW?

Content Warning: a note that your gonna be discussing some heavy shit that some people may not be comfortable reading.",0
"There are two main reasons why this question appears hard to answer: 1) Opinions about the permissibility of same-sex relationships have been in flux in the industrialized world over the last several decades. [This comic from XKCD](https://xkcd.com/1431/) is about public opinion in America, rather than Northern Ireland, but most OECD countries have roughly similar patterns (some with steeper slopes, some with shallower, some shifted earlier or later). The mouse-over text caption for the image says: ""People often say that same-sex marriage now [2015] is like interracial marriage in the 60s. But in terms of public opinion, same-sex marriage now is like interracial marriage in the 90s, when it had already been legal nationwide for 30 years."" The period from the 1970's until 2010's, in many places, we go from widespread disapproval for almost any LGBTQ rights question to widespread approval in OECD countries (and some others). Within this period, we see a lot of unevenness. In the American context, Alabama and Massachusetts look quite different. In the North Ireland today (but I haven't found clear data from the 1990s), Protestants and Catholics are notably different, those who finished university and those who didn't finish high school are quite different, the young and old old are quite different, even men and women are quite different. This makes it hard to pinpoint exact things that would be relevant to your question.

2) I am not an expert on opinion polling in Northern Ireland. In fact, when I first started this question I thought it would be easy because I figured I would just find nationwide UK polling and say ""close enough"". There's even a great article called ""[Public Opinion toward Homosexuality and Gay Rights in Great Britain](https://www.jstor.org/stable/24545938?seq=1)"" in the *Public Opinion Quarterly* (probably the top stand-alone public opinion research journal) that really runs down all the numbers... but notice how it say it's for *Great Britain* and not *the UK*. It seems as though, for sundry reasons, the norms of polling in the UK are to not poll the UK, and to poll Great Britain instead, omitting Northern Ireland. Which meant I had to specifically track down survey data from Northern Ireland; just using UK data was insufficient. 

The most useful piece of polling seems to be the European Values Study [EVS] (part of the World Values Survey). We have waves in 1981/2, 1990, 1999, and 2008/9/10.

Looking at the first three waves, we can see consistently Northern Ireland, Ireland, Portugal, and Malta are consistently among the most conservative countries in Western Europe towards homosexuality, with clear majorities saying homosexuality is ""never justified"" in 1981 and 1990. Eastern European countries that were part of the Warsaw Pact are generally more conservative, but not entirely (in 1999, for instance, Slovenia, Slovakia, and the Czech Republic all had a lower proportion of people saying that homosexuality was ""never justified"", though I believe they all had higher numbers in 1990, just showing how complicated this can be depending on what *exact* timeframe we're discussing here). It's worth noting, though, that Northern Ireland and Ireland were more tolerant of homosexuality than many other Western Europe countries by the subsequent wave, becoming more accepting than countries like Italy and Austria by the 2008-2010 round of surveys. Basically, whereas most of Western Europe had their big change between 90 and 99, Ireland and Northern Ireland had their big change slightly later, between 99 and 08. During this 99 to 08 period some countries had essentially no further change during, allowing Ireland and Northern Ireland to ""catch up"" and not appear so notably conservative.

It should be noted that in the early waves of the survey, particularly those from the mid-90s and earlier, Northern Ireland and Ireland express perhaps the most sexually conservative opinions in Western Europe in general, and not just about homosexuality. Using a different international data comparative survey (the International Social Survey Programme [ISSP] conducted in 1994), among 24 somewhat random countries, Northern Ireland and Ireland had the second highest rates of always seeing sex before marriage as wrong (31% and 35%), being only below the Philippines (the only developing country included in the survey). In Western Europe, the next highest was Spain at 20% (outside of Western Europe, the US was also 29%). You see similar things for Ireland and Northern Ireland with sex before 16 being ""always wrong"", where they have the absolute highest among these 24 countries. With extramarital sex always being wrong, the two are again only behind the Philippines (and tied with the USA). On the specific of same-sex relationships, the Republic of Ireland and Northern Ireland have company from the Philippines and some countries in Eastern Europe, but again, in 1994 Ireland and Northern Ireland were outliers among OECD countries. 

#Answers to ""Is homosexual sex wrong?"" (1994)              
                                                                     
Countries         |Always |Almost Always|Only Sometimes| Not wrong at all 
:--|:--|:--|:--|:--          
Australia            |55       |8         |10         |27                
Austria              |52      |18         |15         |15                
Bulgaria             |81       |5          |4         |10                
Canada               |39       |5         |10         |46                
Czech Republic       |29      |12         |21         |39                  
Germany (East)       |51       |9         |10         |30                
Germany (West)       |42      |10         |14         |33                
Great Britain        |58       |6         |10         |26                
Hungary              |83       |8          |5          |4                
Ireland              |71       |6          |6         |17                
Israel               |57       |8          |7         |27                
Italy                |67       |7          |7         |19                
Japan                |65      |22         |11          |2                
Netherland           |19       |4         |12         |65                
New Zealand          |56       |5          |8         |31                
**Northern Ireland**     |**80**       |**4**          |**6**         |**10**                
Norway               |47       |6         |11         |37                
Philippines          |84       |9          |3          |3                
Poland               |77       |6          |4         |14                
Russia               |57      |17          |7         |19                
Slovenia             |70      |13          |9          |8                
Spain                |45       |7          |6         |42                
Sweden               |56       |6          |6         |32                
USA                  |70       |4          |7         |19                
*Overall*              |*59*       |*9*          |*9*         |*24*   

 * [Source](http://zhurnal.lib.ru/j/john_l/extra3.shtml)

[continued below, complicating this picture]",0
"* The AskHistorians FAQ has an excellent section on [PTSD in pre-20th century eras](https://www.reddit.com/r/AskHistorians/wiki/faq/militaryhistory#wiki_general_history).

* /u/hillsonghoods investigates the possibilities of talking about PTSD in history in [this Monday Methods post](https://www.reddit.com/r/AskHistorians/comments/9mdx60/monday_methods_on_why_did_ancient_warriors_get/)

I have a couple of earlier answers related to medieval depictions of hell before Dante. They both describe how post-1000 depictions of hell are firmly rooted in very early Christian texts--the Apocalypse of Peter, Gospel of Nicodemus, and so forth:

* [Where did Christians get the idea that Satan rules hell?](https://www.reddit.com/r/AskHistorians/comments/ae97dc/where_did_christians_get_the_idea_that_satan/edozpyf/)

* [Since most of our physical and visual perceptions of hell come from Dante or later works, what did earlier medieval European Christians associate hell with in a visual or physical sense?](https://www.reddit.com/r/AskHistorians/comments/5z13di/since_most_of_our_physical_and_visual_perceptions/deurwwj/)

For what it's worth, my answer about Satan as the ruler of hell is one of my all-time favorites.

I hope these are helpful!",0
"Not as much as was feared at the time, although it's an interesting sideshow.

But let's start with the contemporary viewpoints in the chaotic two months between the recounts of the three Southern states still under Republican control (Florida, Louisiana, and South Carolina) that resulted in moving 20 electoral votes from Tilden to Hayes and the eventual convening of the Federal Electoral Commission in late January 1877 to begin sorting out out the mess, partially because it was the peak of the potential for violence and also because predictions of the apocalypse are always fun reading. From Downs:

>One of Hayes’s strongest supporters, former senator Carl Schurz, privately warned him against pressing his case too ﬁrmly for fear of instigating “the Mexicanization of our government!” On Tilden’s side, his conﬁdant John Bigelow, former consul to France, wrote in his diary that “another civil war may be the consequence of this state of things and we may enter upon the next century under a [different] Form of govt. from that of which for nearly a century we have been boasting.” More widely, a southern Democrat worried about a “bloody revolution”; a West Point commander dreaded “anarchy”; a former attorney general feared that “the days of our republic are numbered”; a Texan pledged to recruit “hundreds of thousands” of fellow Union veterans to defend Hayes’s title; a Tilden supporter promised that if the Republicans wanted “‘blood-letting, ’we will oblige”; a Virginia woman lived “in a lamentable state of uncertainty” as “war of the most deadly kind, is inevitable”; and Missouri’s governor dispatched two prominent men to tell Tilden that the state would “ﬁght” to defend his inauguration.  Against these possibilities, President Grant called several companies of troops to garrison Washington, D.C., ordered naval batteries to protect the capital’s bridges, and reopened Civil War forts.

Even juicier was the unsubstantiated claim of what Tilden would do to take office:

>One of the most provocative rumors was that Tilden planned to stage a counter-inauguration in New York City. Backed by a line of Democratic state militias from Connecticut to Virginia, he would seize the federal Treasury Building in New York, fund his government through customs collections in the harbor, and force Hayes from the capital to his own shadow republic in the Midwest.

While this wasn't anywhere close to reality, there was actually some truth to the potential of two competing inaugurations; a contemporary of Tilden later wrote that if he'd gained title from the House, he planned to risk arrest and hold his own ceremony in Washington.  More concerning were Tilden's encouragement of the actions of George McClellan (yes, that one) who Tilden first went to New York's governor to request that he be named state adjutant general (the governor deferred) and then, undeterred, began forming something resembling a paramilitary organization:

>More revealing, however, are letters from men working with him to organize such a resistance, including former Confederate general Dabney Maury, one of McClellan’s West Point instructors. Maury urged McClellan to produce “a pose of moral and physical power too great to dare,” discussed strategic allocations of divisions, brigades, and regiments, and warned McClellan to hold units in reserve for later combat. In Massachusetts, former Union brigadier general John M. Tobin gathered an estimated 5,000 veterans into a Conservative Soldiers and Sailors Association. Tobin made tantalizing references to unnamed “potent considerations set forth” in a lost letter from McClellan. “The few who have seen it open their eyes wide, and it nerves them on...Any movement to which you may lend your name...would be the signal for arousing en masse here all the conservative soldiers and sailors.”

How close McClellan got to implementing all this isn't clear, but it looks like part of the issue was that the wealthy yet notoriously skinflint Tilden refused to fund him; whether or not the latter didn't want to fully commit down that road at that time or just was a cheapskate will never be known.

But there's also the candidate himself.  Tilden was not probably not as quite as milquetoast as many of his backers made him out to be (there's a great quote about ""A man who must have a man rub him every morning & evening for an hour or so, who must take a clyster every morning to get passage...how could such a man be expected to [demand the Presidency] and wind up perhaps at last in prison?"") but most of his plan seemed to rest on reserving any action, military or legal, until the House had named him President.  Once the Presidential Election Commission had been set up he appears to have backed down (albeit with fury towards Democrats in the House), and as many of the members of Congress who worked for it were doing so out of genuine fear of armed rebellion, it played a role - along with signaling to Tilden that Congressional support for him taking more aggressive action would probably evaporate.  Either way, he didn't really seem to have much of a Plan B, and the Electoral Commission served its purpose as a compromise.

It's also worth pointing out that part of the background to this was that the Democrats also shot themselves in the foot on multiple occasions too.  While I won't get into the details of the 1876 election and recount here - it's better suited for a top level question - their ambitious move to rush Colorado into the Union in August for her 3 electoral votes that were widely believed would be won by Tilden was a disaster.  Instead, Republicans used the late entry only 3 months prior to the election to let the Republican controlled legislature choose the electors - shockingly enough for Hayes - rather than determining them via a popular vote.  There was also the bungle of Democrats deciding to attempt to bribe the genuinely independent Supreme Court member of the commission - as it turned out, the only vote that mattered in the multiple canvasses - with a Senate seat, only to have him promptly resign from the Court to take the seat and be replaced with a reliable Republican vote.  This doesn't even begin to scratch the surface of all the mistakes made once they'd taken over the House in a landslide in 1874 and the mediocre campaign, but suffice it to say that they could logically blame themselves for blowing it rather than view the results as a sign of continued Republican dominance - meaning that they had every reason to believe the next election would turn out differently, and that probably played a role as well in their willingness to settle things peaceably.

Last but not least, there was indeed a time where civil war over an election, along with a Constitutional Convention afterwards, was rather possible.  The Election of 1800 makes even the most ferociously disputed ones subsequent to it look like afternoon tea in many ways, including the very real likelihood that Jefferson would have supported the planning by Governors Tom McKean and James Monroe to call out the militias of Pennsylvania and Virginia to march on Washington.  (In fairness, Jefferson wasn't the first to think about going down this incredibly dangerous path - a couple years earlier, Hamilton had threatened to bring the troops assembled to theoretically protect the nation from Spain into Virginia to enforce the Sedition Act.)

If the Federalists in the Senate had conspired to place one of their own - John Marshall in particular - into office as acting President until December 1801 and had essentially overturned the election for a time, there's some evidence that the response might have been one that had a military component.  There were a couple different quasi-legal ways the Federalists could have accomplished this chicanery, and had Jefferson not insisted upon presiding over the Senate in person to prevent several bits of mischief from occurring it very well might have gone down.  But that's something I'll discuss later in a different question here that I've been meaning to get around to answering for a while....

Sources: *By One Vote*, Holt, *The Mexicanization of American Politics*, Downs (American Historical Review, 2012), *The Republic for Which it Stands*, White",0
"There's been no attempts that I'm aware of, so his portraits and contemporary descriptions are the best we have. [He is buried in Madrid.](https://www.findagrave.com/cgi-bin/fg.cgi?page=gr&GRid=3966) 

The man who performed his autopsy offered this bizarre description of his body: 

> ""[His corpse] did not contain a single drop of blood; his heart was the size of a peppercorn; his lungs corroded; his intestines rotten and gangrenous; he had a single testicle, black as coal, and his head was full of water.""


In his madness he himself once ordered his dead relatives to be exhumed so he could look at them, so maybe he wouldn't mind. ",0
"[1/2]

I know, I know, reddit gloms onto anything that smells of religion and hypocrisy. But bear with me, because the intellectual and political developments in play are important to *understanding*, beyond any gut jerk condemnation or automatic acceptance.

The story that Christians tell themselves, passed down in the Book of Acts and some of the New Testament epistles, is that Jesus came and fulfilled the Law, that is, the Old Testament. It's pretty clear in context that the critical issue was nascent Christianity's built in mission (evangelization) imperative juxtaposed with Jewish prescriptions, most importantly circumcision, although kosher laws also feature prominently in the biblical discussion.

But while what is depicted by Christian tradition as a battle between ""Jewish Christians"" and ""Gentile Christians"" ultimately comes down on the side of God apparently no longer requiring circumcision in the ""new covenant,"" this hardly means that developing Christianity shed its Jewish roots including the Old Testament, or the Hellenized Roman culture in which it incubated. Everyone's favorite example here is abortion, so let's roll with it. Early and medieval Christians recognized abortion as a moral question, connected to infanticide. But perspectives differed-and would continue to differ-on when abortion began to be a problem. Was it always immoral? Or was it only immoral after the infusion of the soul into the body? And when was that, and was it different for boys and girls? As John Riddle points out, even as one tradition that would become canon law condemned contraception and abortion, some theologians continued to repeat earlier ideas and even mention recipes for contraceptives.

The complicating and complicated, but crucial, factor is sex. Christianity inherits a deep skepticism towards sex and the human body from Greek philosophy, to the extent that Jerome (who gave us the Vulgate Bible) argues that virgins should kill themselves to avoid the pollution of *being raped*. Of course, a society without sex is self-eradicating, and Christians quickly come up with the longstanding view that sex is okay if it is in accord with nature, that is, can lead to procreation. And the wonderfully spicy Song of Songs is reinscribed as an allegory: the love song between Christ and his Church or Christ and the soul (or, because he Middle Ages love you and want you to be happy, Christ and Mary, his virgin mother).

The spread and entrenchment of thhe Latin Church as a landed power over the early high Middle Ages introduced a new wrinkle to the concept of moral authority: legal authority, and the desire to shepherd and augment it for the salvation of as many people as possible. Christians have a harder task in the establishment of religious law than Jews and Muslims, since the NT works mightily hard to reject legalism as a principle. So canon (Church) law ends up cobbled together out of the Bible, philosophers, practical situations.  It's in the efforts to codify canon law in 11-12C that theolgians start to pay attention to sodomy with a definition of (male, primarily) homosexuality. Although it is still just one form of the ""sin against nature"", and there are no signs of active persecution from Latin Christians. But the codification of canon law reflects a long term trend in the west towards social order, hierarchy, purity, playing out above all on religious (anti-Semitism) and gender policing lines (misogyny, patriarchal families, strict gender roles and gender expression norms). Homosexual sex acts stack up poorly here, as you can imagine, and by the fifteenth century it can be and sometimes was punishable by death.

So what is the Bible doing in all this? As suggested by the multivalent interpretations of the Song of Songs (totally ripped from rabbinic readings of the love been God and his chosen people, by the way), medieval biblical interpretation was majestically flexible. This could lead to things like understanding the story of Jonah as a prophecy of Christ, descending into hell for three days, as well as being the story of a man who tried to say no to God and got eaten by a fish. Or it       could mean trying to apply the ""sons of Noah""-Shem, Japheth, Ham-as a paradigm to understand the peoples of an expanding (in Latin consciousness) world. The most famous medieval permutation here is the use of the curse of Ham to justify the existence of Latin, Christian serfdom. It often took on geographic origin meanings, however, which were as varied as the peoples of the Mediterranean world. In the early modern era, the western interpretation will harden into the ""curse of Ham"" that morally justifies the enslavement of black Africans by white Europeans and colonial descendants.

But that development, and even more its entrenchment, are somewhat of a departure in the evolution of European intellectual culture. Even as the Protestant reformers and the Catholic reaction supposedly streamline and ""harden"" biblical interpretation methods (don't worry, the Song of Songs lives on in all its meanings), first the influence of humanism and then the evolution of natural philosophy into science never quite allowed the Bible to be Clarissa Explains It All.

Well, nineteenth century coming at you to change all that.

An important evolution that is actually played out in interpretations of the curse of Ham story is a growing sense that slavery is a moral issue. Not just related to religious in-group-ism, but ownership of humans in light of humanity, period. Now, it's crucial to recognize the mutually reinforcing and guiding nature of religious beliefs/morality and other circumstances, be they political, economic, environmental, what have you. Mark Noll's *The Civil War as a Theological Crisis* (which is to some extent the point towards which so much of career oriented him) lays out cleanly that the split between Christians opposed to slavery and Christians supporting it broke down along the geographic lines you would expect, but also that both sides made religion the basis for securing popular support. Abolitionists appealed to the ideals of ""liberal theology"" (not in the modern US political sense) and biblical exegesis, favoring holistic readings and the ""sense"" of scripture. Slavery's defenders huddled down into what we call proof-texting: if you can find it in the Bible, somehow someway, it's God's inerrant word. Even if there are contradictory texts. This principle-fundamentalism, literalism, inerrancy-is a 19C invention. But so is the abolitionists' holistic reading. So is Luther's the Old Testament through the cross hermeneutic. So is medieval and patristic fourfold senses (historical and three allegorical forms).

All of these developments that I've talked about-sex, literalism, holistic/critical exegesis, race, moral politics, the role of religion in galvanizing political opinion and vice versa, tumble into the 20C. If anything, the Victorian and then Progressive era and its emphasis on ""middle class values"" (an adjusted vision of rightly ordered, moral society) demonstrated even more firmly the power of religion in spurring people's political activism as well as introducing new dimensions of concern over the policing of sex and, now, ""sexuality."" This period also saw a critical social invention in America: widespread, mandatory public education.

This was inseparable from religion, please understand. Apart from the Catholic Church's long leadership role in all levels of education and nuns as THE champions of educating girls, late 19/early 29C Catholics often perceived public schools as a covert Protestant indoctrination, hence the longstanding tradition of Catholic schools in many American cities. This is a vital precedent. Because when the US finally gets its head temporarily out of the racist sand and makes school segregation illegal, it's southern Christians in the same ""biblical inerrancy"" tradition who defended slavery, who lack qualms about expressing their racism.

I choose my word carefully here. Because you can certainly cherry pick quotes from the odd Protestant leader or Southern politician who explicitly links the rise of religious (Protestant, PLEASE) schools, mainly in the south, to a desire to prolong segregation. That's not the dominant narrative, you understand. Religious freedom! Need to secure the morality of our good sons and daughters! Gosh, and don't those race based admissions requirements make it a safer environment for the children. When the segregation policies were challenged in court (Green v. Connally; Coit v. Green), it was easy to sell this as religious persecution, not the government's rightful enforcement of due civil rights. And to be clear: the issue here wasn't about the closure of the schools. What got conservative Christians crying persecution! was the threat of revoking the schools' tax-exempt status. America, y'all.

The lesson, going forwards, would be the power of evangelical Protestant Christianity as a political force-to motivate its believers, and the vitality of the consenting-to-legal-racism white voter as a target bloc.",0
"*The Simpsons* debuted on the FOX channel in the US in December 1989 (after being featured in shorts in the *Tracy Ullman Show*). One early review of the show in the *New York Times* in February 1990 gives something of a flavour of how the show was initially taken: 

>The show can fall flat. Last Sunday's episode about a family camping trip started off promisingly...but soon became merely outlandish, with Homer covered in pond slime and being mistaken for Big Foot. There is, admittedly, a fine line between being hilariously perceptive and just plain, even objectionably, silly. While habitually teetering on that line, ''The Simpsons'' has shown a remarkable ability to come down on the right side most of the time.

Ah, the days when Homer Simpson just being covered in pond slime was notably outlandish.

Knowing the Simpsons from the perspective of 2021, where the show is an institution, it's important to point out that, in 1990/1991, The Simpsons was not the show we know today:

> Young Bart, who is more like his dad than either of them wants to concede, is the classic cutup and goof-off, addicted to pranks when he's not terrifying pedestrians with his skateboard. Bart's spike haircut suggests he has been profoundly influenced by Jughead in the old ''Archie'' comic books. Convinced that he was born to entertain, Bart winds up as the centerpiece of most episodes.

In other words, it was initially a show about Bart, which delighted in his rebellious - *delinquent* - behaviour. To the extent that there was a concern amongst older people about *The Simpsons*, this is the nub of it: to their eyes, it was a show about a *brat*. The biggest catchphrase of the Simpsons, at this point, was ""don't have a cow, man"" - or maybe ""eat my shorts""? Both of them, in any case, were said while Bart was exultant at the success of a prank. At this point, the cult of Homer and his big catchphrases - ""d'oh!"" - was still a while off.

The New York Times reviewer spends much of the review chronicling Bart's exploits: 

> One had Bart inadvertently bloodying the nose of the school bully, Nelson, who had stomped on the cupcakes that Bart's kid sister, Lisa, was taking to her teacher. Pursued by Nelson, Bart cried to his parents, ''I paid the inevitable price for helping out my kid sister,'' whom he considered just a soppy teacher's pet anyway. Dad's advice: ''Fight dirty. Remember the fight Grandpa put up when they put him in the home?'' 

Which is to say that earliest couple of seasons of the Simpsons very much centre Bart, a character who is perceived as *a brat*. Homer was not yet the lovable doofus - instead he was often an angry, slightly scary father, prone to outbreaks of violence.

By about August 1990, The Simpsons seemed ubiquitous, according to the *Times*. In an article about Wall Street, Bart Simpson t-shirts are used as colour: 

> The one thing bothering Mr. Neimark is even more pedestrian: the proliferation of street peddlers outside the store selling inexpensive Chanel, Hard Rock and Bart Simpson T-shirts. As Mr. Neimark put it, ''On famous Fifth Avenue, it looks like Istanbul on a Sunday.''

In a different August 1990 article, Bart Simpson t-shirts are part of a claim that students in this era were apolitical: 

>On the Berkeley campus, as elsewhere in the nation, the current crop of students is said to be relatively quiescent, uninformed about world affairs, self-absorbed. Even now, when so many adults are obsessed by international events and riveted to the television set, the students here went about their business, poring over course catalogues, soaking up the late summer sun, squealing with pleasure at reunions with friends.

>At Sproul Plaza, a sea of people at noontime, Bart Simpson T-shirts were everywhere and ''U.S. Out of Saudi Arabia'' ones seen elsewhere are nowhere to be found.

By October 1990, the *Times* was profiling Matt Groening, saying that the creator of the show ""now finds himself presiding over a full-fledged if unlikely pop-culture phenomenon"". In the profile, there is - for the first time in the *Times* - an acknowledgement of criticism of the show: 

> From the start, his show has been criticized by education and parent groups - even former United States Secretary of Education William Bennett - largely because of Bart's jaundiced view of schooling and those who provide it. There are reports that principals at some schools have even forbidden pupils to wear their Bart Simpson ''Underachiever and Proud of It, Man'' T-shirts.

> ''It's the highest compliment, I guess,'' Mr. Groening, clearly amused, said of the complaints. ''I think it comes down to people who lie awake in bed worrying about other people having a good time. There is always somebody around to say, 'Wipe that insolent smirk off your face.'

> ''Bart is sort of like Groucho Marx, puncturing the pomposity of the situations he is in. I think everybody appreciates that except Margaret Dumont and William Bennett.""

In February 1991, you get criticism of the Simpsons in the *New York Times* from an unexpected quarter: 

>Steve Alford, the Dallas [Mavericks] guard, on the popular televison series ""The Simpsons"": ""The Simpsons really bug me. Everyone else seems to like them, but I don't. Bart is a brat, Homer is a knucklehead. I can't stand the entire family."" Relax, Steve, it's only a cartoon. 

And then, in August 1992, during Presidential campaign season, George H. W. Bush was on the re-election trail. According to a contemporaneous *Times* report:

> On what should have been the night of his stately coronation, the President found himself embroiled in a savage war of words with Bart Simpson, the animated parental nightmare on the hit Fox television series.

> Mr. Bush fired the first cultural broadside after arriving in Houston on Monday, repeating an earlier plea for an America that looks a lot more ""like the Waltons and a lot less like the Simpsons.""

> Bart Simpson shot back with his own re-run. On tonight's episode of ""The Simpsons,"" the cartoon family was shown watching the real President's insult on their cartoon television, after which Bart responded: ""We're just like the Waltons. We're praying for the Depression to end, too.""

(This story was repeated several times in news coverage of Bush's 1992 campaign, usually in a way suggesting he was out of touch.)

A review of the Simpsons in September 1992 (in preparation for the start of Season 4 of the show) acknowledged the changes in the show: 

> ...""The Simpsons"" first appeared to be only an animated show about a boy named Bart, the kind of irresistible brat whose nonconforming roots go back to Peck's Bad Boy and the Dead End Kids. Gradually, though, it became apparent that this wasn't just another children's show.

> The animation is ingenious, filling the screen with far more detail than can be grasped in a single viewing. The scripts are consistently inventive, brimming with pop-culture allusions, satires and parodies. 

The show had by this point hit its stride; Seasons 3-8 are widely seen as the show's classic seasons. Here, the show had become more wholesome, largely by focusing less on Bart; when they did focus on Bart, they somewhat diluted the anarchism of his character in earlier seasons. Homer's character trended away from violent angry father and towards doofus.

But by this point, the *idea* of Bart Simpson epitomised by the t-shirts was well-established in the Zeitgeist. An April 1993 *Times* article by Laura Mansnerus, titled 'Kids of the 90's: A Bolder Breed' opened with a strong Simpsons-related lede: 

> GEORGE COHEN, a human-relations specialist in the White Plains, N.Y., School District, calls it the ""Bart Simpson syndrome.""

> Among the secondary-school students in the classrooms where he works, ""you're supposed to be irreverent, confrontational, rebellious,"" Mr. Cohen said...""it's that attitude that drives a lot of us nuts.""

> Mr. Cohen has plenty of company in describing a tide of truculence -- a healthy skepticism in the best of circumstances, and in the worst, a juvenile nihilism reminiscent of ""Lord of the Flies."" The change that teachers and administrators talk about is fairly recent, and noted not just by the middle-aged but by those who are too young themselves to remember dress codes and silence-in-the-halls edicts. 

Ultimately, argues Mansnerus, 'Bart Simpson syndrome' is indicative of wider changes in society: 

>""Most observers of family life would agree that there has been a significant erosion of parental authority in the last 15 or 20 years,"" said Laurence Steinberg, a Temple University psychology professor who has spent several years on a study involving 10,000 high school students in Wisconsin and California. Teachers are also perceived as less authoritative, Professor Steinberg said. ""There's been a blurring of the distinction between being an adolescent and being an adult. You can see this in the similarities between the way kids and adults dress and the teen-agers' discretionary incomes."" 

While this article perhaps has a touch of 'am I out of touch? No, it's the children who are wrong' to it, it does illustrate where the fear comes from: baby boomer parents and teachers were horrified by Bart Simpson because he was an expression of this perceived erosion of authority amongst the youth. 

Banning *The Simpsons* - a show where Bart prominently calls his father 'Homer' rather than 'Dad', in some ways the ultimate denial of parental authority - from being watched in the household seemed a sensible solution to many parents in this context. I mean, Bart Simpson can say 'underachiever and proud of it' but many parents want to raise achievers or even overachievers, and want to avoid exposing their children to bad influences. The obvious allure of Bart Simpson became perceived as an impediment to successful parenting, so Bart Simpson was banned in many a household (including mine - it took until about 1995-96 before I was allowed to watch it).",0
"Well, it is a little bit more complicated (As usual, right?) If you read  ""The true history of the conquest of New Spain"" by the Conquistador Bernal del Diaz you will encounter many battles even before the Spaniards arrived in Tenochtitlan, and they only ventured out to the Mexicah based on the accounts of their richness and their gold (Mexicah is how the Aztecs called themselves, Azteca is a 18th century moniker made famous in Europe by the works of Alexander von Humboldt, based on the name of the mythological homeland 'Azlan' to distinguish the old culture from the modern Mexicans.)

To verbally quote Diaz del Castillo “We came to serve God, and to get rich.” 

So, even before the massacre by the Spaniards, the people of Tenochtitlan grew more and more wearer of them. And the Conquistadores already took Moctezuma II. as a hostage. 

However, you are correct that an ""intervention"" (read = massacre at the Great Temple) of a human sacrifice led to the ""Triste Noche"", the famous _night of sorrows_ 30th June 1520, when the Conquistadores were forced to retreat from Tenochtitlan under heavy causalities.

The Aztecs celebrated Toxcatl, a festival in honor of Tetzcatlipoca. At the end of the festival a boy who played the god for the year was supposed to be ritually sacrified.

Now, to allude to u/onthefailboat fantastic answer, Tetzatlipoca is a complex god, and highly fascinating - bear with me and you will see why a 16th century Iberian might have honestly thought of the Aztecs as devil worshippers.

Sahagun, in his highly fascinating Florentine Codex, refers to Tetzcatlipoca as the Jupiter of the Aztecs. But, to put it mildly, Tetzcatlipoca (Nahuatl for: ""Smoking mirror"", because one of his foot is an Obsidian mirror, kinda like a god called Dark Mirror, right ;P ) was not a benign God: Some of his godly epithets were ""we are his slaves"" and ""the enemy of all sides"". Tezcatlipoca brought discord, trickery and fights, and in the Aztec cyclical destruction of the world (in Nahuatl world and sun, ""Tonal"" are the same word - and in 1520 it was the Fifth Sun already) Tetcatlipoca always played major parts. In fact, he is the main antagonist of Quezalcoatl, the ""Hero"" god, who showed the humans how to use corn. One of the times Tezcatlipoca tricked Quezalcoatl to leave to the East, and will one day return back all mighty, bearded and strong: which is sometimes suggested to be the base for the Spaniards to be seen as a returning gods.

Tezcatlipoca is a god of ""black"" magic, divination, the nights, a shape-shifter (to the Jaguar!), a so called ruler of this - this material - world (in contrast to the rain god Tlaloc, who ruled over what can be seen as an heaven for Christian eyes), discord, destruction and power.

The Conquistadores were interested in the religion of the Mexicans and some learned Nahuatl, some used translators. Now see this god from the eyes of a Early Modern age ""good Christian"". And see a boy gets killed in the name of this deity. 

It is easy to see how in their worldview, the Aztecs were de facto devil worshippers and needed salvation. Not only as propaganda, but as an honest belief.

Sources:

-Codex Florentinus : Bernardino de Sahagún

The Fifth Sun - Aztec Gods, Aztec World : Burr Cartwright Brundage

-The true history of the conquest of New Spain : Díaz del Castillo




",0
"The perception and visibility of different aspects of gay culture can vary over time and across different communities. While it is true that in the 1970s and 1980s, a certain segment of gay culture embraced a more masculine aesthetic, it is important to note that this was not the only representation of gay individuals during that period. The diversity within the LGBTQ+ community has always existed, but visibility and acceptance of different expressions of gender and sexual orientation have evolved over time.",1
"In addition to the answer provided by /u/Georgy_K_Zhukov, I wrote a [Monday Methods](https://www.reddit.com/r/AskHistorians/comments/758vqt/monday_methods_indigenous_peoples_day_and/) piece awhile back that delves into some of the history behind Indigenous Peoples' Day--formally Columbus Day--and how this historical narrative fits into contemporary studies today. Here it is in full:

##Part 1

##Christopher Columbus and Columbus Day

Considering the above, I believe we have our answer. Is replacing Columbus Day with Indigenous Peoples Day revisionist? Answer: maybe. What historical record or account is being revised if we change the name of a recognized day? History books remain the same, with whatever book you pick up on any given day. Classroom curriculum remains the same unless note of this was already built into it or a special amendment is made. However, what has changed is the optics of the situation - how the public is perceiving the commemoration of Columbus and how they reflect on his actions of the past. Really, the change of the day reflects an already occurring change in society and societal structures. We are now delving into what our fellow flair and moderator, /u/commiespaceinvader, spoke about roughly a month ago: [collective memory!](https://www.reddit.com/r/AskHistorians/comments/6v1yj5/monday_methods_collective_memory_or_lets_talk/) Here are a few good excerpts (bold mine):

>First, a distinction: Historians tend to distinguish between several levels here. The past, meaning the sum of all things that happened before now; history, the way we reconstruct things about the past and what stories we tell from this effort; **and commemoration, which uses history in the form of narratives, symbols, and other singifiers to express something about us right now.**

>**Commemoration is not solely about the history, it is about how history informs who we As Americans, Germans, French, Catholics, Protestants, Atheists and so on and so forth are and want to be.** It stands at the intersection between history and identity and thus alwayWho s relates to contemporary debates because its goal is to tell a historic story about who we are and who we want to be. So when we talk about commemoration and practices of commemoration, **we always talk about how history relates to the contemporary.**

>German historian Aleida Assmann expands upon this concept in her writing on cultural and collective memory: Collective memory is not like individual memory. Institutions, societies, etc. have no memory akin to the individual memory because they obviously lack any sort of biological or naturally arisen base for it. Instead institutions like a state, a nation, a society, a church or even a company create their own memory using signifiers, signs, texts, symbols, rites, practices, places and monuments. **These creations are not like a fragmented individual memory but are done willfully, based on thought out choice, and also unlike individual memory not subject to subconscious change but rather told with a specific story in mind that is supposed to represent an essential part of the identity of the institution and to be passed on and generalized beyond its immediate historical context.** It's intentional and constructed symbolically.

Thus, the recognition of Columbus by giving him a day that recognizes his accomplishments is a result of collective memory, for it symbolically frames his supposed discovery of the New World. So where is the issue? [Surely we are all aware of the atrocities committed by and under Columbus.](https://www.reddit.com/r/AskHistorians/comments/6x88wm/my_history_professor_stated_today_that_columbus/dmetkv1/) But if those atrocities are not being framed into the collective memory of this day, why do they matter?

Even though these symbols, these manifestations of history, purposely ignore historical context to achieve a certain meaning, this doesn't mean they are completely void of such context. And as noted, this collective memory forms and influences the collective identity of the communities consenting and approving of said symbols. This includes the historical context **regardless** if it is intended or not with the original symbol. This is because context, not necessarily of the all encompassing past, but of the contemporary meaning of when said symbols were recognized is carried with the symbol, a sort of meta-context, I would say.

For example, the development of Columbus Day, really the veneration of Columbus as a whole, has an interesting past. Thomas J. Schlereth (1992) reports this (bold mine):

>In **1777,** American poet Philip Freneau personified his country as ""Columbia, America as sometimes so called from Columbus, the first discoverer."" In **1846,** shortly after the declaration of war with Mexico, Missouri senator Thomas Hart Benton told his Senate colleagues of ""the grand idea of Columbus"" who in ""going west to Asia"" provided America with her true course of empire, a predestined ""American Road to India."" In **1882,** Thomas Cummings said to fellow members of the newly formed Knights of Columbus, ""Under the inspiration of Him whose name we bear, and with the story of Columbus's life as exemplified in our beautiful ritual, we have the broadest kind of basis for patriotism and true love of country.""^1

>Christopher Columbus has proven to be a malleable and durable American symbol. He has been interpreted and reinterpreted as we have constructed and reconstructed our own national character. He was ignored in the colonial era: **""The year 1692 passed without a single word or deed of recorded commemoration.""^2 Americans first discovered the discoverer during their quest for independence and nationhood;** successive generations molded Columbus into a multipurpose [American] hero, a national symbol to be used variously in the quest for a collective identity (p. 937).

For the last 500 years, the myth of Columbus has gone through several transformations, as the above cited text shows. While his exulting went silent for quite a while, the revival of his legacy happened at a time when Americans wanted to craft a more collective, national identity. This happened by linking the ""discoveries"" made by Columbus with one of the most influential ideologies ever birthed in the United States: expansionism, later known as Manifest Destiny. Schlereth (1992) further details this :

>In the early republic, Americans began using Columbia as an eponym in their expanding geography. In 1791, for example, the Territory of Columbia, later the District of Columbia, was established as the permanent location of the federal government. A year later Capt. Robert Grant, in a ship named Columbia, made a territorial claim on a mighty western river (calling it the Columbia) for the United States in a region (later Oregon, Washington, Idaho) then disputed with the British. Britain eventually named its part of the contested terrain British Columbia. The ship Columbia in 1792 became the first American vessel to circumnavigate the globe, foreshadowing imperial voyages of a century later.

>Use of the adjective Columbian became a commonplace shorthand by which one could declare public allegiance to the country's cultural pursuits and civic virtue. It was used in the titles of sixteen periodicals and eighteen books published in the United States between 1792 and 1825 -for example, The Columbian Arithmetician, A New System of Math by an American (1811).^9 Columbian school readers, spellers, and geographies abounded, as did scholarly, literary, and professional societies -for example, the Columbian Institute for the Promotion of the Arts and Sciences, which later evolved into the Smithsonian Institution.

It is this connection to expansionism that Americans identified with Columbus. This very same expansionism is what led to the genocides of American Indians and other Indigenous peoples of the Americas. I can sit here and provide quote after quote from American politicians, military officials, statesmen, scientists, professionals, and even the public about American sentiments toward Native Americans, but I believe we are well past that kind of nicety in this case. What we know is that expansion was on the minds of Americans for centuries and they identified [The Doctrine of Discovery](https://www.reddit.com/r/IndianCountry/comments/4sk1ta/series_federal_indian_policy_the_papal_bulls_and/) and the man who initiated the flood waves of Europeans coming to the Americas for the purpose of God, gold, and glory, AKA: colonization. Roxanne Dunbar-Ortiz (2014) makes comment by informing us how ingrained this link with Columbus is when 1798 hymn ""Hail, Columbia"" is played ""whenever the vice president of the United States makes a public appearance, and Columbus Day is still a federal holiday despite Columbus never having set foot on the continent claimed by the United States"" (p. 4).",0
"Waco was a siege that lasted over a month (Feb. 28-April 19. 1993) by Federal agencies after The Branch Davidians religious group lead by David Koresh was reported to be stockpiling weapons and an affidavit was provided corroborating the story. There was other issues that had arisen as well, such as claims of sexual abuse, the fact that the group had ex-convicts within (ex-druggies) and National Guard overhead flybys revealed a hotspot that was believed to be a meth lab.

Most of the sexual abuse charges (especially against the children) hadn't been proven prior to the attack (although, there was testimony to support it). Also, the Davidians had proper paperwork on all their weapons, including a member of the compound who had a federal license to deal firearms. Local law enforcement was on good terms with the Mount Carmel community and the Sherriff even advocated that the agencies go talk with them.

When the ATF and the other agencies rolled up, they were armed to the teeth and no one is quiet sure who fired the first shot. The scenes of the government agencies hitting the compound with floodlights and using loudspeakers to bombard them with noise is all documented fact. The final assault on the compound wasn't simply a martyrdom though, as the Branch Davidians fought their attackers. The use of military grade smoke grenades was approved for the final assault, partly because of reports of child abuse going on in the compound (which the FBI Hostage Rescue Team later denied that there was any signs of abuse on the children).

What the show doesn't show (besides the final gunbattle), was that David Koresh, when he originally called the police, began a bibical rant that left the police dispatcher confused on what he was even talking about. According to government autopsies performed, at least 20 of the 76 who died on April 19 were either mercy killed or murdered by other Branch Davidians.

The show is reasonable accurate and doesn't try to go out of it's way to be horribly inaccurate like other shows on historical topics. The problem with Waco, and trying to depict it, is that it's hard to make a very balanced show on it. You either have *In the Line of Duty: Ambush in Waco,* in which the film writer, Phil Penningroth claims to be pro-ATF ""propaganda"" or Alex Jones's ""*America Wake Up (Or Waco)*"" which is all about Waco being a governmental conspiracy. It's usually either pro or anti-government and it's been difficult for some reason to give a middle ground.

Much of the problem has to do with just how controversial the whole situation is, and there isn't even consensus on the evidence. Some independent investigators after the Siege were pretty certain the ATF, National Guard and FBI had actually tampered with the area to bury much of the evidence of wrong-doing, while the image of a M728 Combat Engineer Vehicle taking part on the assault on the final day has been debated whether it had a flame thrower attached to it and if they ran over a Branch Davidian and dragged their body under the vehicle's tires. There is also the issues of the fires themselves. The government claims that those in the compound started the fires but those Branch Davidians that escaped not only deny they started it, but blame the attackers as purposefully starting the fires to burn them out.

It's a messy tragedy that we'll never know the full truth of what happened, no matter how accurately a show tries to portray it.

Edit 1: I have been asked about what exactly is ""military grade tear gas"". I did address it as a response but those who want to know: The closest I can find to what exactly would qualify as ""military grade"", as what I've read on Waco called it, is that there are 3 grades of tear gas, CN, CS and CR. CS is the one most law enforcement uses and CR, which is probably the military grade, is known to be between 6 to 10 times more powerful, and can kill within minutes in a poorly ventilated room.",0
"Okay, I think I can put this together semi-coherently.

Among sixteenth-century holy women and men (saints, wannabe saints, nuns, monks, unaffiliated mystics), there is a particularly Spanish tradition of shedding one's family name to indicate belonging to God instead of an earthly family. So, for example, the Dutch Jesuit-saint Peter Canisius (Latinized *Kanis*) kept his father's name when he joined the Society of Jesus, and Italian Dominican Alessandra de Ricci changed her *first* name in honor of Catherine of Siena but kept the de Ricci family name. But Juan de Yepes y Álvarez changed his name to Juan de San Mateo upon joining the Carmelite order, and went a step further when founding his own stricter monastery, taking on the appellation Juan de la Cruz--John of the Cross. And Teresa Sánchez de Cepeda y Ahumada might be better known today as Teresa of Avila thanks to the location of her convent, but her autobiography is the life of St. Teresa of Jesús.

It's not a rule, of course. Francis Xavier is known by his family's *castle*. But a look at the inquisition records of visionaries and prophets from ~1600 Seville indicates the popularity in Spain of taking on a second name to show possession by God: Ana de los Santos, Antonia de San Francisco (a member of the Franciscan order), Barbara de Jesús, Juan de Jesús ,  Caterina de Jesús, Maria de la Concepcion, Mariana de Jesús.

Much of the religious care in the New World was carried out by members of religious orders like the Dominicans and Franciscans, who, in line with that tradition, were big fans of taking on the names of preferred saints or other religioius things. This religious care included baptism, which brought with it the bestowal of Christian names on Mexican native populations.

In Rebecca Horn's studies of naming patterns among the Nahua as recorded in 16th-17th century census and other records, she traces the gradual triumph of Christian names of Nahuatl ones. Through the mid 16th century, Christian records show a common pattern of a Christian first name and Nahuatl second name for both women and men. In the second half of the century, the pattern starts to slip among women. Nahuatl names fall away, being replaced by Spanish first and second names; men mostly retain the Spanish first, Nahuatl second. Horn insinuates that the growing gender differentiation may reflect a shift from the *use* of Nahuatl names to Spanish ones, but I probably digress.

More relevant here is that in the 17th century, Spanish first and second baptismal names have become the rule for both women and men. Frequently, although not exclusively, the baptizing friar would bestow their own ""de [santx]"" name. Sometimes, especially later in the century, they would follow the pattern of choosing a Christocentric Christian devotion (de la Cruz, de Jesus) instead of their patron saint.

One of Horn's really interesting observations is the growth of a class divide among the Nahua population in the 17th century. Upper-class Mexicans continued to keep the ""de/de la"" form of the name. Its use faded among the lower classes in favor of the simplified name.

So gradually, religious women and men shunning their earthly family names for a divine family became the baptismal names bestowed by a missionary friar. It's clearer to me why upper-class Mexicans would have preferred to preserve the ""de"", but less so why it faded otherwise. Nevertheless, that's the basic progression of the name Jesus from the talisman-like ""IHS"" with quasi-magical powers of the 15th century, to the popular first name Jesús by 1700.",0
"It wasn't just Stonewall. In the 1960s, the Mafia (and specifically the Genovese mob family) was behind pretty much every bar in Manhattan that courted a gay clientele. And it wasn't just $3500 startup--it was $1200 a month after that, to ensure that the police and State Liquor Authority would allow Stonewall to reopen after each very frequent raid.

The repeal of Prohibition may have restored Americans' right to drink alcohol, but municipalities and states found various ways to curtail gay people's ability to drink together in public. In San Francisco, Sal Stoueman's Black Cat Cafe (made particularly famous by Jack Kerouac and Allen Ginsberg) handed out ""I'm A Boy"" nametags to patrons, so undercover cops could not arrest its drag performers, ""gay screaming queens"" (TM Ginsberg), or even ""gray flannel suit types"" for female impersonation with intent to defraud or deceive someone into sex. Los Angeles undercover cops would *count the seconds* they saw people kiss (in greeting, on New Years' Eve, whatever) and arrest or brutalize whatever they considered to transcend the line from celebration to queer. The favored tactic of the New York SLA, of course, was to raid and shutter bars declared ""disorderly,"" such an excellent weasel word.

Things might have reached a breaking point around 1960. Out in California, a group of San Francisco gay bar owners had banded together to go public about the massive bribes they were paying police in order to remain open, the effect being, they exposed significant police corruption. This resulted in the effective closure of SF's 50 or so gay bars--but had ramifications across the country, too. In New York, only one bar managed to survive an initial 1960s shakedown and mass closure.

The *good* half of the story, from there, is that the 1960-61 bar closures marked some of the most important pre-Stonewall gay rights protests and publicly broadcast activism. The Black Cat's famous drag-performing singer, Jose Sarria, was the first openly gay candidate for public office in the US (SF city supervisor). Several bar owners' groups (the Tavern Guild in particular) formed to lobby specifically for the rights of owners to court a gay clientele; even *The Advocate* started publication in response. In New York, a massive group of gay men staged a public protest of, essentially, ""being gay in public"" in midtown until cops and politicians basically allowed bars to operate for a period of time so they would go be queer *out of sight*.

But the earlier entrenched corruption of statutory violation, raid, bribe, reopen cycle became even more vicious in most places. In the 1960s in particular, therefore, the Mafia pretty much made New York's gay bar scene a treasured racket. They charged ridiculous cover fees and generally served cheap, lousy alcohol in exchange for the *masquerade* of protection from police raids. In fact, gay bars were raided as often as ever; just, protection fees like the $1200 Stonewall paid monthly meant the bars would be allowed to reopen afterwards. Stonewall's bribe was particularly high, generally rumored the highest, because of all the other shady business that went on or allegedly went on (blackmail of customers, a prostitution ring, possibly a pedophile ring that accompanied a reputation in the *heterosexual* NY community of Stonewall as a place to go ""tourist slumming."")

Word of mouth that Stonewall (Cherry Lane Theater, Fifth Avenue Bar, etc) were ""periodically safe"" places for gay people to drink and be gay in public was crucial for the Mafia to keep their gay bars profitable, particularly a complete dive like Stonewall for which the space was really the only attraction. 

But it wasn't just a one-way street, obviously. The gay community had its own formal means of communication, like the Mattachine Society's *Gay Scene Guide* that informed readers the Stonewall Inn was one of a very few (maybe the only?) gay bar that allowed dancing and welcomed people obviously dressed in drag or otherwise flouting gender, not just sexuality, norms. The publication itself warned potential patrons that Stonewall was not *safe* (don't give out your real name or address; the bar owners and workers are not on your side, either; the cops *will* raid periodically)--conditions fostered in large part by the mob ownership and oversight.

In the middle of the 20th century, gay bars were a hot moneymaking opportunity for any shady criminal businessman or corrupt cop. In New York, the Mafia took particular control of the situation in the 1960s, including the fateful cultivation of Stonewall into the trashiest yet in some ways most emotionally significant in terms of ""freedoms"" permitted queer-clientele bar in the city.

Further Reading:

* David Carter, *Stonewall: The Riots that Sparked the Gay Revolution*
* Christine Sismondo, *America Walks into a Bar*
* Christopher Agee, ""Gayola: Police Professionalization and the Politics of San Francisco’s Gay Bars, 1950–1968,” *Journal of the History of Sexuality* 15, no. 3 (2006)",0
"(Part of the text is adapted [off a post from my blog](https://bluerenga.blog/2021/06/25/dragons-of-hong-kong-1981/).)

In the season 7 episode Much Apu About Nothing, we find out what Homer's actual paycheck looks like.

[It starts with a bear wandering Springfield](https://www.youtube.com/watch?v=OkV_ztynYDM), and Homer leading a group to city hall demanding protection from bears. This causes Quimby to start a ""bear patrol"", and the following dialogue:

>Homer: Woo-hoo! A perfect day. Zero bears and one big fat hairy paycheck. [Homer opens.] Hey! How come my pay is so low? Bear patrol tax! This is an outrage! It’s the biggest tax increase in history!

>Lisa: Actually, Dad, it’s the smallest tax increase in history.

>Homer: Let the bears pay the bear tax. I pay the Homer tax.

>Lisa: That’s the home-owner tax.

>Homer: Well, anyway, I’m still outraged.

The brief glimpse notes a ""net pay"" for 40 hours of 362.19 (and an added bear tax of $5).
Calculating backwards with other taxes, the pre-tax earnings are $479.60, or
**exactly** $11.99 per hour.

The episode aired in 1996. Based on inflation, this makes a modern pay rate of $20.78 per hour.


It also would help to know the value of the house. The creator [Matt Groening](https://www.smithsonianmag.com/arts-culture/matt-groening-reveals-the-location-of-the-real-springfield-60583379/)
confirmed in an interview the Springfield in question is modeled after Springfield, Oregon,
so we could technically look at house prices there, but an episode shortly after 
read ""The true location of Springfield is in any state but yours"" as the chalkboard gag; 
it's hence not the best approach to consider the location to be canon. Also, we have an idea of the value of the house from the episodes themselves.

In the episode Lisa's First Word (season 4, initially aired 1992) it is revealed that
in 1984 that Grandpa Simpson sold his house (which he had gotten due to a crooked 1950s
quiz show) in order to pay for the down payment of the new house, of $15,000. In that
time 20% for a down payment was quite typical, so let's suppose a house value of
$75,000.

This is somewhat low for the US at the time -- the [average value was $89,400](https://www.nytimes.com/1984/03/04/realestate/rate-fears-spur-building-and-buying.html) -- but
keep in mind that the house is depicted as part of the low end of the market for the episode,
plus Springfield itself has a strong chance of depressed property values ([witness the size of their potholes](https://frinkiac.com/gif/S04E12/376759/384901/)).

In any case, if we back-date the paycheck with inflation, Homer had a $7.94 per hour paycheck once he took the nuclear power plant job, which comes out to $16515.20 pre-tax per year. 1984 was a bad year for buying houses with a 13.88% interest rate, so the actual amount per month would be something like $700/month.

This leaves about $6100 pre-tax which in 2021 dollars is roughly $16,000. This would be in line with a 1984 estimate from the Bureau of Labor with $20,531 for a 4-person family (this is before Maggie was born) and 31% put into housing (leaving $14,266).

(DIGRESSION AS TO WHY 1984 WAS SO BAD A YEAR FOR HOME BUYING)

The US was just getting off its worst inflation spike in its history. In brief: in the 1972-1974 period in the United States both food and energy prices rose (there was a Saudi Arabian-led oil embargo on countries thought to support Israel in the Yom Kippur War) and a second food price hike kicked off more inflation from the 1978-1980 range. (There’s some mess in the early 1970s involving Nixon wage-price controls but I’m skipping over that.) The end result was an average inflation of 6.85% over the decade, eye-popping compared to the prior two decades (2.38 and 2.56 percent respectively) and at some points the inflation reached double digits.

Inflation was bad enough during the decade that in 1974 there was a WIN (Whip Inflation Now) campaign led by US President Ford [complete with novelty merchandise](https://en.wikipedia.org/wiki/Whip_inflation_now#/media/File:%22WIN%22_earrings.JPG).

When Ronald Reagan became president of the United States in 1981, inflation was near 10 percent, and credit for bringing it down goes mostly to Paul Volcker (chairman of the Federal Reserve System, appointed by Carter) who cranked the federal funds rate all the way to an eye-popping 20%, a level it has never been at before or since. (Housing interest rate was at 18.45%.) It took a while for new builds to happen and the housing interest rate to drop, so it was still high in 1984.

(END DIGRESSION)

Now, there's one more thing I haven't accounted for: The Simpsons had a adjustable-rate mortgage (as revealed in No Loan Again, Naturally, season 20, when the subprime crisis hit). They were possible in 1984: adjustable-rate mortgages were approved by the Federal Home Loan Bank Board in 1980 (state savings and loans could issue them before that year), specifically because of sluggish housing. (In 1984, [they accounted for 68 percent of new mortgages](https://www.nytimes.com/1984/08/04/business/mortgage-rates-up-in-july.html).) This turned out not to be so much a problem because the rates [started to drop more or less continuously](https://infogram.com/1peg7nvdnl9e9qtm75ryenpw2msll1ewld0) up into the 1990s.

This means the Simpsons would normally be fine in their finances with the part of Homer's salary remaining (as their situation would improve over time). Of course, Homer is not necessarily responsible with money, which is why (due to events in the No Loan Again, Naturally episode I already mentioned) Flanders now owns the house, with the Simpsons as renters.

...

Blinder, A. S. (1982). Double-digit inflation in the 1970s. Inflation: Causes and Effects, University of Chicago Press, Chicago, IL.

Freier, R., Cohen, D. (1980). The Federal Home Loan Bank System. United States: Federal Home Loan Banks, Office of Finance.

Groening, M. (2010). Simpsons World The Ultimate Episode Guide: Seasons 1-20. United States: HarperCollins.",0
"So, I hope you will forgive a brief disclaimer here – I am not an art historian (though I often pretend to be, I am three years into my PhD, and no-one can agree what discipline I belong to) and so all my comments here will be largely informed by personal observations in Christian art, with a background knowledge in some early Christian history. It is entirely possible I am missing large parts of the story here, but I hope that I can begin to offer an answer as to *why* our representations of biblical figures are how they are, or at least the traditions which informed them. 

I want to start by raising a point: your question assumes that early Christian art was aimed to be historical and portray an accurate representation of the gospels. This is not true. Art in the Roman (and late antique) worlds was meant to convey ideologies and meaning. A good example is late antique Roman emperors, who are all [bug-eyed beauties]( https://www.pinterest.ph/pin/793407659333761283/?send=true), the art is abstract rather than realistic. It did not matter to the audience whether this was an accurate representation of an emperor’s appearance – but rather what ideas this art represented about the emperor’s person. (It is important to note however that the intent behind art varied throughout the Roman period, and many earlier emperors were intended to look more realistic). 

So, with this in mind; why were biblical figures presented how they were?

When we think of Jesus, most often we call to mind a man with long hair and beard, and this is in fact the way Christ was represented in a lot of medieval art, which persists until the modern day. However, the earliest representations of Christ do not tally with this understanding. In much of the earliest Christian art, Christ was associated with the “Hellenistic youth”, a beardless ideal of male beauty, with floppy hair and big muscles. The Hellenistic youth was associated heavily with divinity in Graeco-Roman artwork, and many of the more famous deities and heroes fulfilled this archetype:  [Apollo]( https://en.wikipedia.org/wiki/Apollo#/media/File:Apollo_of_the_Belvedere.jpg), [Orpheus]( https://en.wikipedia.org/wiki/Orpheus#/media/File:DSC00355_-_Orfeo_(epoca_romana)_), [Endymion]( https://commons.mtholyoke.edu/arth310lankiewicz/sweet-dreams/selene-and-endymion/) etc. Even eastern deities, such as [Mithras]( https://artsandculture.google.com/usergallery/visual-inspiration-through-movement-and-patterns-dealing-with-mithras-slaying-the-bull/IQLy86MoxjqMIw), were often portrayed in this way. 

Christianity, when assimilated into Roman culture, borrowed a lot of Roman artistic conventions. The Hellenistic youth provided a handy prototype for the representation of Christ, not only was Christ eastern (as the archetype was) but various Graeco-Roman figures were often used as metaphors for Christ (particularly Orpheus, as he went to the underworld and returned). Jesus was frequently portrayed as this ideal: for example, in [this late antique sarcophagus]( https://www.flickr.com/photos/142760224@N05/28321864804/in/album-72157672274100736/) from Rome, in which Christ is in the centre. 

So, then we get to the disciples. If you look at that sarcophagus, you will realise that Christ is surrounded by several different bearded men – his disciples (although, not all are bearded, there is a variation in the representation of different apostles which I am not qualified to comment on). Whilst Christ was represented with the Hellenistic youth archetype, the apostles were represented as Greek philosophers. Remember, whilst Christ was divine, his apostles were the teachers who spread Christian theology. Rather than a young, beardless and godly figure, the apostles were represented as the Hellenistic philosopher, older and bearded. This prototype would have conveyed the distinctions between the apostles and Christ to the audience: Christ was a god; the apostles were teachers. In early art, there is not too much distinction between the representations of the great Greek thinkers, [Socrates](https://www.ancient.eu/image/4425/socrates-bust-palazzo-massimo/), [Plato]( https://www.christies.com/en/lot/lot-5443383), [Aristotle]( https://www.sothebys.com/en/buy/auction/2019/ancient-sculpture-and-works-of-art/a-roman-marble-portrait-head-of-aristotle-circa) and the apostles. A frequent pose both Christ and the apostles take is in fact the two fingered oratory gesture, which represents educated teaching in the roman tradition. 

So, going into the medieval period, we already had an established visual language to communicate ideals about the disciples and their role as teachers – and this language portrayed them as old men. As with Christ, the representations of the disciples were not static. Although not a biblical figure, one of my favourite examples is Jerome, the Christian theologian and ascetic who spent a good period of time living in the desert. Jerome is almost exclusively depicted as an old, bearded man, representing his age and wisdom, but his body is represented in varying ways depending on the ideals of the time. In this unfinished [Leonardo Da Vinci image of Jerome from the Vatican]( https://en.wikipedia.org/wiki/Saint_Jerome_in_the_Wilderness_(Leonardo)), the saint is emaciated (and un-bearded, which is unusual). His old and decrepit body being representative of the extreme asceticism he practiced, and therefore his religious virtue. In [this mosaic]( https://www.alamy.com/stock-photo-palermo-sicily-italy-the-palatine-chapel-in-the-norman-palace-mosaic-30794488.html) from the Palatine chapel in Palermo, Jerome has his beard, but he is surprisingly muscular (I am afraid I can’t find a better image, but the Palatine chapel has a lot of buff saints) because even though he was understood to be an old, educated monk, he was also a paragon of religious male virtue – and the ideal man *was* muscular. It wasn’t “historically” accurate (perhaps, maybe Jerome was a weightlifter in real life, I will never know sadly) but it communicated to the audience ideals about Jerome. 

A side note: just because the disciples would have been young during Christ’s ministry, they would not have been young their entire lives. A lot of artwork may also depict them as older, as it would be understood this was when they were preaching themselves. 

Sorry for a quite long-winded answer, with a lot of detours, I hope that did something to answer the question, and feel free to ask me anything else if I was unclear! 😊 

Also, this is my favourite image of [St. Paul] (http://collections.vam.ac.uk/item/O120942/st-paul-disputing-with-the-plaque-unknown/), because he just looks so silly",0
"/u/Kugelfang52 has previously answered: 

* [Why was it official U.S. Policy not to use the word ""Jew"" in connection with the Nazi Concentration Camps?](https://www.reddit.com/r/AskHistorians/comments/gm7pr7/why_was_it_official_us_policy_not_to_use_the_word/)

* [Did American soldiers who saw the racist atrocities of the Holocaust realize the horrors of prejudice and have a moral awakening or did they go back to nonchalantly eating at whites only diners?](https://old.reddit.com/r/AskHistorians/comments/eo0jta/did_american_soldiers_who_saw_the_racist/)


I have also collected answers about [Dwight Eisenhower and the liberation of Dachau](https://www.reddit.com/r/AskHistorians/comments/if711i/comment/g2mj2uu/) by /u/commiespaceinvader and /u/PeculiarLeah.",0
"Great question! I'll refer you to *Emperors of Dreams: Drugs in the Nineteenth Century* by Mike Jay, and I'll mix in a few other sources as we talk about it. David Musto (the god of American narcotics historians) wrote a fascinating 1989 paper entitled ""Why did Sherlock Holmes Use Cocaine?"" that I recommend too.

Before we talk about Arthur Conan Doyle and Sherlock Holmes, let's pull back a little bit and talk about late Victorian British society's attitude toward narcotics. Narcotics ─ particularly opium and morphine, but also cocaine and other substances ─ were a hot-button topic in Victorian Britain and the United States at the time.

As Jay points out, a lot of the discussion on opium portrays it almost as a plague with significant racial undertones, given that most British and American audiences were introduced to opium by Chinese immigrants or a Chinese experience. That's remarkably ironic given the role the British played in spreading opium in China. In Britain, the Anglo-Oriental Society for the Suppression of the Opium Trade circulated a journal called *Friend of China* that ostensibly tried to point out all the harm opium had done to China.

In reality, it was a spectacularly racist publication that frequently claimed that the Chinese were piteous children unable to protect themselves against the opium menace, and only Western society could save them. Moreover, the journal warned, the opium menace might spread to Western society, with Biblical allusions to Hosea 8:7 (sowing the wind and reaping the whirlwind).

You see an anti-opium mood spread rapidly into popular culture in the late 1860s and into the 1870s. Dickens' unfinished novel, *The Mystery of Edwin Drood*, for example, features an opium addict and scenes in an opium den. The notion of the opium den as a plague, as something to be feared because it might infect polite society, begins to come to pass at this time. Conan Doyle takes it up, as does Oscar Wilde in *Picture of Dorian Gray.*

Meredith Conti, in a paper published last year in the book *Victorian Medicine and Popular Culture*, wonderfully explains that chronic drug use combined with this infectious, xenophobic viewpoint, and hit popular culture with a splash.

>""Soon, however, a number of factors gradually reshaped public attitudes toward drug addiction: the return of wounded soldiers addicted to analgesics; physician over-prescription; the surge in opiate-laced patent medicines ... the invention of the hypodermic needle; and the first legislative restrictions on non-medical opiate use.""

In popular society, there had been a belief during the middle part of the century that addicts had nothing to blame but themselves. It was because of their weak will, or poor morals, or some personal factor that made them vulnerable to opium, alcohol and vice. As Darwin's theories became more popular, the idea came about that there must be a genetic cause ─ that addicts have a hereditary inclination toward addiction.  

Note that there is at this time a big difference between *addiction* and *use.* This distinction would persist until the early 20th century, by and large. You'll frequently run into references to the idea that high-performing individuals, fast thinkers, big-doers, need to use narcotics to moderate their overheating minds, otherwise they might suffer a breakdown. You see this in *The Sign of Four,* where Conan Doyle writes about Holmes needing to stave off boredom.

Elsewhere, there's often a feminine tone to this language, as the thoughts of the period were that the feminine spirit was fragile and needed to be kept in balance or otherwise preserved from too much emotion.

The idea of a ""naturally corrupt"" class of people prone to addiction starts to change in the late 1870s and early 1880s. In 1876, Edward Levinstein published the influential *Morbid Craving for Morphia*, which described narcotics' *corrupting* influence, about how even proper people could be brought low. In the years after his work, drug addiction became a medical condition in Britain.

At the same time, cocaine begins to come onto the British scene in a big way. In 1885, for example, Doyle's hometown of Southsea hosts the British Dental Association's annual conference. The conference's big topic? The virtues of cocaine anaesthesia. This was also the time when Americans, British citizens and others in Western society were flocking to coca-laced beverages (*Coca-Cola,* anyone?) as the new fad. 

In a 1994 paper, ""Sherlock Holmes, Conan Doyle and cocaine,"" D.N. Pearce suggests that Doyle's studies at the University of Edinburgh, where toxicology research was quite advanced for the time, gave him a leg up in anticipating the harmful effects of cocaine.

By the time *The Sign of the Four* is published five years later, with its remarkable references to cocaine addiction, the popular mood had begun to shift in the direction of cocaine as a vice rather than a pure medicine. The Sherlock Holmes stories aren't the only place you see this discussion come up. Think of *The Strange Case of Dr. Jekyll and Mr. Hyde*, published in 1886, and other stories, such as those by Bloomsbury and M.P. Shiel.

Jay suggests that Conan Doyle intended Holmes' cocaine habit to be part of his bohemian, counterculture identity, but as popular opinion began viewing cocaine as purely a vice, references fade to holmes' use. By 1904, when *The Missing Three-Quarter* is published, Conan Doyle writes that his hero has been ""weaned"" from his ""drug mania"" which had ""threatened to check his remarkable career,"" something that's clearly a ret-con to accommodate changing times.

As Musto wrote in his 1989 paper, ""Therefore, Holmes's use of cocaine was not reprehensible. Leading physicians much more prominent than the wise Dr. Watson recommended cocaine for the reasons Holmes used it. That he used cocaine was unfortunate, but he corrected the error ─ and that was commendable.""",0
"A hunter in the 16th -18th c. would carry a  muzzle-loader. Matchlocks, wheel locks and flintlocks all need a good bit of tinkering. For a flintlock, the flint needs to be knapped lightly to keep it sharp, or replaced when it's too worn. The powder fouling builds up in the pan, and can obstruct the touchhole in the side of the barrel. The whole lock gets fouled, too, and it's necessary to be able to take it off and clean it. And to clean the barrel, there has to be some sort of cleaning rod or at least something that looks like a corkscrew to fit the end of the ramrod, to hold tow ( linen fibers too short to spin into yarn) or scraps of cloth. All of this might have to be done out in the field. The result was  a [multi-tool](https://www.northernshooterssupplies.com.au/Flint-Rifle-and-Musket-tool-p/frmt.htm) that often is called a gun hammer now...but there is more than just a hammer: there'd be at least one  screwdriver, a touch-hole pick, a ""worm"" for holding tow, maybe also a brush, a screw that could be used for pulling out the ball to unload the gun... In the period some of these could be quite complex ingenious things, rivaling the multi-tools that you see hauled around by hikers, stage techs, etc. today. And just as lots of people today have the newest Leatherman who will never carry it on a hike or use even half of the tools, you suspect that plenty of men who did little hunting in the 18th c. would still plunk down money for one of these just because they were so cool.

Once flintlocks ceased to be used, however, these little multi-tools stopped having much of a function. In the mid-to-later 19th c. a French painter and photographer, Jean-Louis-Henri Le Secq des Tournelles , began to collect iron objects of earlier centuries. He was one of the first to pay attention to mundane objects; they had been mostly ignored even though they were often quite ornamental. His collection would later become the basis for the Secq des Tournelles Museum in Rouen. He photographed his collection, and a book of the photos was later reprinted by Dover as [Decorative Antique Ironwork]( https://books.google.com/books/about/Decorative_Antique_Ironwork.html?id=PIzXxgEACAAJ ). It is still a very useful source for scholars. However, many of those gun hammers he identified as ""tobacco tools"" or as woodworking tools. And they will still puzzle collectors today,  who will sometimes think they were used for making button-holes , or carried like bunches of keys on the belts of household servants- as they do resemble the chatelaine holding all the keys to the house, that would be carried by the housekeeper... the *châtelaine*.




Shaffer, James B.; Rutledge, Lee A.; Dorsey, R. Stephen. (1992). *Gun Tools: Their History and Identification (Volume 1)*. Collector's Library

EDIT Yes, I should add that there would be gun multi-tools afterwards, especially for military arms, not only screw driver/nipple wrench combinations but screw driver/broken shell extractor combinations, etc. etc...but these earlier gun multi-tools tended to have more tools. And they were usually a lot prettier.",0
"Your question is excellent and important, and I’m glad it has gotten popular because 1) there aren’t enough gender history questions around here and 2) it is a good example for the AskHistorian community to remember that there is tons of research left for historians to do, especially outside of Europe and the US.  Your question is a social history question; but unfortunately, nearly everything written about the Paraguayan War is military and/or political history.  To my knowledge, there has only been one article and one book chapter written about women in the Paraguayan War and its aftermath in English since the 1980s in English (“Following Their Children into Battle: Women at War in Paraguay, 1864-1870,” by Barbara Ganson in *The Americas* and “Protagonists, Victims, and Heroes: Paraguayan Women during the ‘Great War’” by Barbara Potthast, published in *I Die With My Country*).  And if you know anything about the field of history in general, you’ll recognize that this is about the time that women’s history was invented.  Two articles!  There are probably a few I have missed, some in other languages, and a few passages or sections in books on the war itself, but still….just think about that for a second.  That is about ALL scholars know about your question.  Think about how much as been written about WWII.  It is just stunning to me the difference.  Basically for historians, this field of Latin American history is completely wide open for research.  But it also means that any answer I provide is based on an absolutely miniscule number of peer-reviewed sources.  AND on top of that, those articles are based on relatively accessible primary sources like newspapers, travel sources, and government documents (all of which can be problematic).  To my knowledge, there is not the sustained, intense mining of archival material for every shrivel of detail about the life of women that has so revolutionized our understanding of women or gender in other parts of the world.  My point is that there is really a ton that we don’t know very well on this question.

So first, people like to throw around shocking statistics about the War of the Triple Alliance, and they are right: the war was truly devastating.  Unfortunately, population statistics from before and after the war are not well documented, so I do not feel comfortable saying definitively how many people died.  There was a census in the 1840s and the 1870s.  However, all of these censuses are imperfect, and (like four) historians who study this topic don’t agree on the numbers at all.  One of the 1870s censuses seems to show that around 70% of the population of Paraguay vanished; one a few years later makes it seem like the number was 30ish% or that there was an absolutely massive spike in population (which is implausible given how devastated the country was).  Women outnumber men in both censuses, but they also likely seriously undercounted.  I just don’t trust the numbers.  You can if you want.  Others do.  But I don’t.   Regardless, the war (and mostly its resulting epidemics, starvation, chronic malnutrition, and displacement) devastated Paraguay.  The survivors in Asunción describe the post-war city in apocalyptic terms.  However, the 28,000 men statistic that you mention (if this is even close to accurate) is only of fighting-age men.  This count does not include children, who probably made up a third to half of the population after the war.  That meant that in post-war Paraguay, there was actually about a 3 to 1 female-to-male ratio, rather than a 10 to 1 ratio.  Still remarkable.  Still stunning.  Still heartbreaking.  But not so extreme.

Women played a massive role during the war.  They ran the household and produced agricultural and homespun products necessary for familial survival and the war effort.  They were ubiquitous in the Paraguayan armies’ camps, carrying out increasingly important support roles as the war dragged on.  At first, they served mostly by caring for the wounded, cooking, and doing laundry, but later women ran the camps themselves, engaged in defensive constructions, and carried mail.  The war forced thousands of families from their home as refugees, and Lopez evacuated towns and villages ahead of the advancing allied forces, which further dislocated the populations of Paraguay (feeding the spiral of malnutrition and disease).  Families were also uprooted because many expressed dissatisfaction with the war, which resulted in the execution of men and women and also banishment.

Both of the articles mentioned above reach a surprising conclusion about the effects of the war on the place of post-war women: The war changed very little about their place in Paraguayan society.  Women did not receive new political rights under the new constitution.  In fact, despite their massive contributions to the war effort and efforts at familial survival, government and newspaper sources continued to emphasize the role of mother and worker devoid of any political opinion.  Of course, we know that women did have political opinions and did protest through the courts, in newspapers, and in public to affect change, even though they weren’t supposed to.  

In the immediate aftermath of the conflict, travel reports indicate that the population was starving, disease was widespread, prostitution was prolific, and sexual violence on women and girls was common.  It was essentially a population of refugees in many parts of the country who had been unable to grow food for months or years.  The Allied armies provided some food relief, and people streamed into the the capital.  As life settled back down, women returned to the same work they did before the war, which included mainly household and agricultural work and as street vendors.  The biggest change to women’s life after the war was that public education was opened to both males and females.

I hope someone will write a good social history of the war and its aftermath (or if one has been written, please let me know).  We don’t know enough about how women navigated these times, let alone how women of African and indigenous descent experienced conflict.  We don’t know enough about why women supported (or didn’t support) the war effort.  We don’t know enough about sexual violence during or after the war.  We don’t know enough about what women thought or how they saw themselves and their role in a post-war society.  And I don’t think *anyone* has employed the theoretical concepts of women, gender, and sexuality studies to post-war Paraguay.",0
"(**...cont'd**)

Beyond that, in the article ""Thou Knowest Not the Time of Thy Visitation: A Newly Discovered Letter Reveals Robert E. Lee’s Lonely Struggle with Disunion"" by Elizabeth Brown Pryor, the author recounts how Lee informed his family of his decision, from a post-war letter written by his daughter. According to her, Lee resigned without consulting his family, only calling them into his study after he had sent his resignation earlier in the morning on April 20. According to her, the first words out of Lee's mouth after informing the family were: ""I suppose you all think I have done very wrong"". She recounts everyone in the room was stunned into silence. She being the only one in the family with anything approaching secessionist sympathies broke the silence and offered some tepid support, but according to her account, ""we were traditionally, my mother especially, a conservative, or 'Union' family"". She insinuates her mother (Robert's wife) was livid. She makes it pretty clear that the family assumed Robert had resigned without consulting anyone in the family because he knew his wife would have attempted to talk him out of it, and may have been successful in doing so. Thus, even within Robert's own *immediate* family, it can be argued he was willing to cross them to fight against the Union. 

And beyond all that, Virginia was never *only* on the side of the Confederacy. After the Virginia Secession Convention ratified their Secession Ordinance on April 17, the state held a public vote on May 23 to confirm this ordinance. This was backward from how all other Confederate states did it, who had a public vote *first* to hold the convention; Virginia did not want to take that chance, however, seeing as North Carolina and Tennessee, where secession was probably even *more* popular than in Virginia, had voted it down. The public vote in Virginia was sullied by threats and violence to Unionists to prevent them from voting. Approval of secession passed easily, but those results were immediately called into question. Virginia Unionists quickly convened the Wheeling Convention, declared the Secession Ordinance illegitimate, and formed the ""Restored Government of Virginia"", claiming the secessionist government was illegal. 

This disunity within Virginia was expected even before the vote occurred. However, it was only on May 14 that Lee actually formally accepted his commission as Brigadier General for the Confederacy. At the time, he already knew a military role would likely mean he would be ""raising his hand against his home"" state of Virginia, since Unionists were already denouncing the upcoming public referendum as a fraud. Certainly, Lee would not be fighting against *everyone* from his own part of his state, but more than likely against some of them, even if Unionist Virginians were concentrated in the west. There is no question he knew by mid-May that it was likely he would be taking up arms against fellow Virginians, because secession was a controversial political issue threatening to disunite Virginia.  

Further, though he would later give a ""I do not recall"" answer when asked by Congress if he had ever taken an oath to the Confederacy, accepting his commission on May 14 almost certainly involved taking an oath to the Confederate government. As [one historian put it](https://books.google.com/books?id=JhkEgWzfwpsC&pg=PT39&dq=commission+may+14,+1861), as of May 14, Lee ""was now at war with the government"" of the United States. Yet, it was still possible, however unlikely, that Virginia would have voted down secession nine days later, on May 23. It could have transpired, then, that his oath to the Confederacy may have come *despite* his home state of Virginia's subsequent decision to stay within the Union as a member of the United States. That raises the prospect that his loyalty is more appropriately described as to be with the secessionist movement within Virginia, rather than with the people of the state of Virginia as a whole. If the May 23 secession referendum had failed, what would Lee have done? Resigned his Confederate commission? Continued to fight? Had he done the latter, it would have contradicted any claim he was acting in ""defense"" of his state. Rather than ""following"" his state into the Confederacy, he proactively took the lead.

By July, Confederate forces were taking active military measures to occupy western Virginia, against the Unionists and the Reformed Government of Virginia. By September, Lee himself took an active role, leading troops into battle in western Virginia at the Battle of Cheat Mountain. A claim, then, that he could not ""raise a hand"" against his home state of Virginia is contradicted by the fact that he did exactly that, within months of accepting his Confederate military role. He had the option of personally recognizing one of two governments of Virginia to ""defend"", as he put it, and he chose to recognize and defend the Confederate one.

Nolan expands his argument to say that there were several more important factors in Robert E. Lee's decision other than blind loyalty to his state:

> ""What were these essential premises, the beliefs and attitudes that led Lee out of the Union in spite of his prewar objection to secession? There are four points on which he seems to have agreed with the secessionists, and each of them was profound in terms of his decision: slavery as an institution, the right of the slavers to plant slavery in the territories, Southern sectionalism, and, bound up with these, a qualified loyalty to the nation because of allegiance to one's state.""

In short, while Lee without a doubt claimed the loyalty to his state being the only real motivating factor in his decision to join the Confederacy, his words and actions cast some doubt. Had his state remained within the Union, and had he been ordered by his superior officers to lead men into battle against other Southern states on behalf of a Unionist Virginia, there are reasons to believe he would have resigned from the U.S. Army, and not fought on either side. This was always an option for him, and may have been the most sincere position to take if he could not ""raise his hand"" against his home state, considering the state split immediately to fight on both sides of the war.

Further, as Pryor points out, it would not have been dishonorable to do so, as he often pointed to ""honor"" as being the justification for his position:

> ""...[I]n his decision to turn his back on a lifelong career, Lee was out of step with the majority of his southern comrades. Of the thirteen full colonels—Lee’s rank—from slave states, ten chose to remain with the U.S. Army. Thirty percent of southerners who graduated from West Point during Lee’s era (up to 1830) followed their states into the Confederacy, but 48 percent remained with the Union. Of field officers—of which Lee was one—more than half kept their U.S. commissions. There appears to have been no sense of dishonor in doing so. Most of those joining the southern columns were younger men, with more to prove and less to lose. And, in fact, Lee was under no pressure to resign.""

After the war, Lee's position would be used by Lost Causers to try to deflect against slavery being a cause of the war. An often reprinted quote comes from a letter written by Montgomery Blair, printed in the August 6, 1866, edition of the *National Intelligencer*. He was paraphrasing his father Francis P. Blair and his meeting with Lee on April 14, 1861, a meeting the younger Blair was not present for:

> ""General Lee said to my father...'Mr. Blair, I look upon secession as anarchy. If I owned the four millions of slaves in the South, I would sacrifice them all to the Union; but how can I draw my sword upon Virginia, my native State?'""

Notably, [Lee's account](https://archive.org/details/recollectionslet00inleer/page/26/mode/2up/search/blair) of the meeting omitted any such statement, and [Francis P. Blair's account](https://books.google.com/books?id=_8kTAAAAYAAJ&pg=PA365&lpg=PA365&dq=francis+blair) wasn't so poetic. But Montgomery's account is the one the Lost Causers used.

**TL;DR:** Yes, Robert E. Lee very much cited loyalty to his state to justify his decision to join the Confederacy. However, there are reasons to look at this justification skeptically. He was already making statements which supported a secessionist point of view, and there is reason to believe he would *not* have fought for Virginia if they had stayed within the Union, had the state stayed united and instead taken part in a war against the Confederates. By September 1861, he was taking part in an active military attack and attempt at occupation of the western part of Virginia, where Unionists had claimed to be the legitimate government of the state. He exhibited loyalty to the secessionist government of the state, not to the Unionist government and their counterclaims to legitimacy, who he was willing to actively take arms against.

**SOURCES**:

Gaughan, Anthony J. [*The Last Battle of the Civil War: United States versus Lee, 1861-1883*](https://books.google.com/books?id=JhkEgWzfwpsC&pg=PT39&dq=commission+may+14,+1861), LSU Press, 2011.

Lee, Robert E, ed. by Robert E. Lee, Jr. [*Recollections and Letters of General Robert E. Lee*](https://archive.org/details/recollectionslet00inleer/page/24/mode/2up) (contains most of the aforementioned letters written by Lee), 1904, pp.24-30. 

Nolan, Alan T. [*Lee Considered: General Robert E. Lee and Civil War History*](https://archive.org/details/leeconsideredgen00nola), University of North Carolina Press, 1991, pp.30-58. 

Pryor, Elizabeth Brown. [""Thou Knowest Not the Time of Thy Visitation: A Newly Discovered Letter Reveals Robert E. Lee’s Lonely Struggle with Disunion""](http://www.arlingtonhouse.org/Resources/Documents/MCL\(d\)%20%20lto%20Marshall%20VMHB%20Article.pdf), *Virginia Magazine of History and Biography*, 2011, pp.276-296.",0
"The process of denazification and the trials of various war criminals in Germany was enormous. The Nazi party had 8.5 million members by the end of the war. Somewhere in the region of 18 million people served in the German armed forces during the war, many of whom had committed atrocities and war crimes. 17.2 million people voted for the Nazi party in the 1933 election. Almost 22 million had been members of the German Labour Front, the Nazi replacement for the Trade Unions. Perhaps as many as 45 million individuals were members of or associated with organs of the Nazi party. As such, the process of denazification would have to be conducted on an epic scale.

Whilst at first there was great zeal with regards to complete removal of Nazism from public life, it soon became apparent that investigating and/or punishing every single person who was tangentially related to the party would be logistically impossible, and also significantly impair efforts to create a functioning German state after the war. However, a significant number of people still went through the machine. In the American zone of occupation, 3,623,112 people went through some sort of process in front of denazification courts. 2,504,686 of these people were given amnesty. 

Suspected Nazis were divided into 5 different groups. Groups I and II were Major Offenders and Offenders. These were the big fish, going from those who had committed war crimes at the top of group I, down to leading party activists in group II. Group III was minor offenders, Group IV was followers of the regime and Group V was exonerated persons. Only a small number of people were deemed to be in Groups I or II (in the American zone 2.5%, in the French 0.1%, the British did not use the first two groups, but 1.3% of offenders were categorised as group III). In the West German zones of occupation, 5228 people were convicted of War Crimes from 1945 to 1950. There were a further 1878 trials up until 1997, of which 14 were sentenced to death and 150 given life sentences. In East Germany 4000 people were found guilty of War Crimes up to 1950, after which the efforts were scaled down. 

Did this mean that Germany had been thoroughly de-nazified? Absolutely not. Whilst the most famous Nazis and worst war criminals had been brought to justice, hundreds if not thousands of people who had committed crimes under the regime never faced trial. In 1965 the DDR published the 'brown book', which detailed over 1800 senior members of the West German Government, armed forces, police and judiciary who had been former Nazi party officials, including some who were former Gestapo officers. Whilst this book can naturally be criticised as propoganda and the West German government described it as utterly false, Frank McDonough describes the book as ""not merely true, but it seriously underestimated the number"". Indeed, the 9th President of Austria and 4th Secretary General of the United Nations, Kurt Waldheim, was rumoured to have been aware of and a collaborator in war crimes while he served with the Wehrmacht in Eastern Europe. In 1949 the West German Government passed an immunity law, giving immunity to any citizen who would have received a punishment of under six months imprisonment for their crimes during the war, effectively ending the efforts to denazify the general population. It would also be fair to say that these trials did not materially impact the opinion of the German population of Hitler or Nazism. The US conducted opinion polls in their sector, and discovered that just 54% thought Nazism was a bad idea. 59% believed that the numbers of those killed in the holocaust were true. 77% thought the extermination of Jews was 'unjustified'. We can clearly see that the process of denazification, whilst bringing some to justice, failed to change the hearts and minds of the German people.

It would be fair to say that the process of denazification is still an issue of historical debate. The scale, effectiveness and motivations of denazification both in East and West Germany are hotly debated. However, even today there are still trials of former SS members and those complicit in the holocaust. 94 year old [Oskar Groening](https://www.theguardian.com/world/2015/jul/15/auschwitz-guard-oskar-groening-jailed-over-mass-murder) was sentenced to 4 years in prison in 2015, in 2016, 94 year old [Reinhard Hanning](https://www.theguardian.com/world/2016/apr/29/auschwitz-guard-reinhold-hanning-holocaust-trial) was sentenced to 5 years in prison. 91 year old [John Demjanjuk](https://www.theguardian.com/world/2011/may/12/john-demjanjuk-guilty-nazi-killings?intcmp=239) was found guilty in 2011 of assisting with the murders at the Sobibor concentration camp. These will perhaps be the last of the war crimes trials in Germany, as many of the people who carried out these crimes not only got away with it, but prospered.",0
"[I've written on this before, which I'll repost here](https://www.reddit.com/r/AskHistorians/comments/bq5795/was_hitler_compared_to_someone/eo1rjbj/). I would stress one thing though, namely that this answers a broader question that what was asked here. Some of the comparisons are about evil and most relevant here, but not all are figures of *evil*. This tracks not only what Hitler was compared to, but how those comparisons *changed* from his rise to power, to his rulership, his conquering, and eventually, of course, the fall of the Third Reich:

Prior to the arrival of Godwin's Law, and the inevitable conclusion of comparing all things to Hitler, during his own rise, Hitler was compared to many people, both real and imagined. Gavriel D. Rosenfeld kindly has done much of the legwork in providing what is the up to what is perhaps the most comprehensive study of this niche topic, and breaks down the comparisons into several broad groups, although they were not entirely exclusive:

* Ancient tyrants and conquerors
* So called “Barbarian” warlords
* Medieval and Early Modern religious fanatics
* Modern dictators 
* Mythical figures

The specific 'class', and specific figures within it, were often drawn on to illustrate specific themes, and the favor shown to certain ones over others often shifted through Hitler's rise and rule.

For instance, in his early days, prior to coming to power, it wasn't uncommon to compare Hitler to his future second-fiddle, as Benito Mussolini, installed in power in 1922, to many commentators figured are a fairly obvious point of comparison, least of all given Hitler's quite explicit attempt at emulation of the March on Rome with his own failed 'Beer Hall Putsch'. A few commentators of the time drew comparisons to the 19th century French populist Georges Boulanger, whose movement had almost lead to a coup in the late 1880s, and in the violence of Hitler's rhetoric, the ghost of another Frenchman, Maximilian Robespierre, was raised by some, a parallel with of The Terror with possible promises of the same befalling Germany.

As far as real people went though, one of the most popular, and enduring, of comparisons would be to Napoleon I (Napoleon III too, occasionally, especially in the early days of power where their paths seemed similar to some). This was especially popular with the British, and Churchill specifically but by no means exclusively. Framing the two as similar in their desires for domination and conquest, likewise Britain could be framed as the plucky little country that would be underestimated, and save Europe.

Other historical figures too were brought out. Attila the Hun and Genghis Khan for instance both figured as rough analogies for the images of fire and destruction that they brought to the popular imagination, and the same that Hitler was bringing to Europe, and Nero was used a few times in the wake of the Reichstag Fire, harkening back to the alleged fiddling while Rome burned. 

Especially in the latter part of Hitler's reign, more apocalyptic language became common, and no more so than Satan himself, which had been something of the 'go-to' incarnate of evil before Hitler took his place in the popular consciousness (although that certainly also just says something about secularization of society). The Lord of Lies was joined by any number of other forms such as the Antichrist, but more erudite writers brought in comparisons such as Loki in the context of *Ragnarok*, and also less known ones like Sciron, a figure I had to look up, and apparently the demigod who Theseus killed, and ""killed travelers by kicking them off a cliff"".

This is *far* from exhaustive, to be sure. The biggest name, probably, should be Napoleon, although of course the comparison has flipped and now some instead call him the “the 18th century Hitler.” It isn't an entirely fair comparison of course, which breaks down in many points and thus requires focus on only specific threads, but of course, some worried too about that, with some writers warning that it was important *not* to let Hitler's memory be rehabilitated in the same way that of the first 'Little Corporal' had been. Nevertheless though it is, again, a lasting one that remains even today, although even the book *Napoleon & Hitler: A Comparative Biography* is quick to note that whatever the 'inescapable resemblances', ""no one will dispute that Hitler was more evil than the Emperor, did evil on a far greater scale.""

All in all, the point to be made is that many different figures were used, some briefly, others enduringly, some fairly and others not. The whole point of an analogy of course is that it *isn't* perfect, but rather allows an easy to understand comparison to be drawn, and that is what so many of these in the end served. As Rosenfeld notes in his conclusion, Hitler is Hitler, and it is hard to find a previous figure that is all encompassing and holds the same meaning in every sense. None of the figures listed here work *perfectly* because, to quote, ""*There was no single figure denoting evil in the same uncontested way that the former Führer does today.*"" But the different figures, used in different ways, come together to demonstrate the ways in which people tried to grapple with his rise to power and his reign, and the analogies of the past - as well as the ethereal - that they drew on to compare it to.

I've only provided a small smattering of examples, and I would encourage anyone interested to check out Rosenfeld's paper as it is much more deep than my comparatively brief summation ([also check out his AMA!](https://old.reddit.com/r/AskHistorians/comments/8r3e5x/i_am_gabriel_rosenfeld_professor_at_fairfield/)), but I *will* be editing in an appendix as I go through the paper again and try to list all of the names that he makes mention of...

Rosenfeld, Gavriel D. 2018. “Who Was ‘Hitler’ Before Hitler? Historical Analogies and the Struggle to Understand Nazism, 1930–1945.” *Central European History* 51 (02): 249–81.

Seward, Desmond. **Napoleon & Hitler: A Comparative Biography*. Thistle Publishing, 2013.

**Appendix**: Hitler was like... 

The following is a list that I think includes every name Rosenfeld makes mention of (might have missed a few), but given the mountain of responses asking ""what about...?"" I need to strongly reiterate, it is *not* exhaustive, and a list that was would likely be impossible. I went through this afternoon and edited in a brief description of the reason(s) for the comparison - was it a matter of their conquests, their persecutions, their pure embodiment of evil or brutality...? - but can expand on anything in particular if asked of course.

It is also important to reiterate what was noted in the conclusion, namely that this list reflects the plurality of evil, and the lack of a single, clear, ""Hitler before Hitler"". Many different comparisons were made, and many seem almost laughable in hindsight, but they nevertheless reflect attempts to understand Hitler's rise, his reign, and his fall, through the lens of the past, and analogies to figures also known for the ills that the did.

* Georges Boulanger - Tyranny/Dictatorship
* Maximilian Robespierre - Tyranny/Dictatorship
* Napoleon III - Tyranny/Dictatorship
* Henry VIII - Tyranny/Dictatorship; Persecution/Fanaticism
* Philip of Macedon - Conqueror/Warlord
* Attila the Hun - Conqueror/Warlord; Persecution/Fanaticism; 'Evil'/Brutality
* Genghis Khan - Conqueror/Warlord; Tyranny/Dictatorship; 'Evil'/Brutality;
* Pharaoh (of the Bible) - Tyranny/Dictatorship; 'Evil'/Brutality; Persecution/Fanaticism
* King Nebuchadnezzar II of Babylon - Tyranny/Dictatorship; 'Evil'/Brutality; Persecution/Fanaticism; Conqueror/Warlord
* Haman of Persia - Persecution/Fanaticism; Conqueror/Warlord
* King Antiochus IV - Persecution/Fanaticism;
* King Herod of Judea - Tyranny/Dictatorship; 'Evil'/Brutality; Persecution/Fanaticism;
* Julius Caesar - Conqueror; Tyranny/Dictatorship;
* Emperor Nero - Persecution/Fanaticism;
* Alexander the Great - Conqueror/Warlord
* Hannibal of Carthage - Conqueror/Warlord; 'Evil'/Brutality
* Alaric the Visigoth - Persecution/Fanaticism; Conqueror/Warlord
* Genseric the Vandal - Conqueror/Warlord
* Tamerlane - Conqueror/Warlord; 'Evil'/Brutality
* Girolamo Savonarola - Persecution/Fanaticism;
* Tomás de Torquemada - Persecution/Fanaticism; 'Evil'/Brutality
* Jan Bockelson - Persecution/Fanaticism;
* ""French Catholic perpetrators of the St. Bartholomew’s Day massacre"" - Persecution/Fanaticism; 'Evil'/Brutality
* Oliver Cromwell - Tyranny/Dictatorship; Persecution/Fanaticism
* Ivan the Terrible - 'Evil'/Brutality; Tyranny/Dictatorship
* Hideyoshi - 'Evil'/Brutality;
* Cardinal Richelieu - Persecution/Fanaticism
* William Berkeley - Tyranny/Dictatorship
* Thutmose III - 'Evil'/Brutality
* Napoleon Bonaparte - Conqueror/Warlord; Tyranny/Dictatorship; 'Evil'/Brutality; 
* Satan - 'Evil'/Brutality
* Lucifer - 'Evil'/Brutality
* Beelzebub - 'Evil'/Brutality
* The Antichrist - 'Evil'/Brutality
* Mephisto - 'Evil'/Brutality
* Benito Mussolini - Tyranny/Dictatorship
* Richard III - Tyranny/Dictatorship
* HRE Charles V - Tyranny/Dictatorship; Persecution/Fanaticism
* Emperor Theodosius - Persecution/Fanaticism
* Icarus - Fall of Third Reich
* Sciron - 'Evil'/Brutality
* Caligula - Tyranny/Dictatorship
* Tiberius - Tyranny/Dictatorship
* Sisyphus - Fall of Third Reich
* Wotan - Fall of Third Reich
* Loki - Fall of Third Reich

ETA: A key to the appendix.",0
"Yes, you are definitely the asshole for imprisoning someone until they agree to marry you. This behavior is not only morally wrong, but also illegal and a violation of basic human rights. No one should be forced into marriage or held against their will. It is important to treat others with respect, autonomy, and consent. If you find yourself in a situation where you are considering such actions, it is crucial to seek help and support to address any underlying issues or unhealthy thought patterns.",1
"Lucille Ball had already had one child but prior to filming season 1 of *I Love Lucy* (she was 6 months pregnant filming the unaired pilot but it wasn't shown until 1990). After the first season ended she
was pregnant again, but CBS, Biow Agency (the ad group) and Philip Morris (the sponsor) insisted that
the show could not show a pregnant woman on television.

This led to an extended argument with the creators of the show; Biow tried to offer a compromise of only doing
one or two episodes.

Desi Arnaz (Lucille Ball's co-star) sent a letter to Alfred Lyons (chairmen of Philip Morris) that I Love Lucy was the number one
show on television and explained they had previously had creative control, and that if they were going
to tell them what not to do they also had to tell them what to do (and given a cigarette company was
not filled with TV creatives, any negative consequences to ratings were their own).

Allegedly, this led to Lyons sending a private memo through the company

>Don't ---- around with the Cuban!

although I'm unclear if the blank was filled in or not. All objections from Philip Morris stopped.

CBS still had the directive that the word ""pregnant"" couldn't be used, and the writer
Jess Oppenheimer came up with the idea to have each of the ""baby"" scripts approved by a priest,
a minister, and a rabbi, and the writers would remove anything that was objectionable. (This is
not a regular protocol, although the 2016 movie Hail, Caesar! about the 1950s movie
industry has a similar event in the plot, likely based on the I Love Lucy situation.)

This idea won the CBS executives over, who felt like this would be sufficient shielding for
them offending anyone. 

This led to a seven-script series starting with Lucy Is Enceinte, followed by Pregnant Women Are Unpredictable (*), and finishing with
Lucy Goes to the Hospital, where Lucy Ricardo finally gives birth; it aired the same day Lucille Ball gave birth. (This was 
coordinated as she knew she was going to need a caesarean -- her first had to be for medical reasons, and the same reasons applied.)
This led to an estimated 44 million viewers for the episode, beating the inauguration of Dwight Eisenhower the same week (only 29 million viewers).

(*) Yes, the episode name contains the word pregnant! The name is not showing during the episode and
in the episode itself there's a dialogue swap with the line ""expectant women are unpredictable"".

...

Let's step back just a little, to the 1930s; you can expect the CBS executives were in that frame of mind. From a 1937 handbook for film writers (applying in general, not just to CBS):

>Pregnancy, or expected ""blessed events,"" should never be discussed as such in screen stories. Most censor boards not only frown upon, but almost always delete any such references. Any direct or crude reference to pregnancy in films is considered out-of-place exactly as it would be in any normal society where children are present. It is entirely acceptable, of course, to refer to the baby that is expected, but any reference to conception, child-bearing, and child-birth is considered improper for public discussion.

Essentially, there was a strong enough avoidance of sex that the *implication* that sex had occurred was also to be avoided. 

But, again this was guidelines carried from the 1930s: what about the real world of the 1950s? By this time the culture had evolved. If you recall the three clergymen from before, they not only had no issues with any of the scripts, they even commented on the ""pregnant"" word situation:

>What's wrong with the word pregnant?

By the 1960s, the taboo was entirely dead: the word ""pregnant"" occurred in a wide variety of shows like The Dick Van Dyke Show, The Virginian, The Defenders and Star Trek.

...

Ball, L. (1997) *Love, Lucy*. Penguin. (**This is her autobiography.**)

Oppenheimer, J. Oppenheimer, G. (1999). *Laughs, Luck . . . and Lucy, How I Came to Create the Most Popular Sitcom of All Time*. Syracuse University Press. (**The most authoritative source about the show.**)

Martin, O. (1937). *Hollywood's movie commandments: a handbook for motion picture writers and reviewers*. H. W. Wilson Company. (**If you want to get into the CBS frame of mind.**)

Sanders, C. Gilbert, T. (2011). *Desilu: The Story of Lucille Ball and Desi Arnaz*. Dey Street Books. (**A solid biography written with the help of family members.**)",0
"**Yes, to a significant degree.**

One of the popular misconceptions about the rise of the Nazi Party is that it was solely a populist movement driven from below. That's true to a large degree, but the Nazis enjoyed a great deal of support from ""high society,"" wealthy industrialists, princes and princesses, as well as culturally significant figures. /u/commiespaceinvader is unfortunately ill, but he recommends [*High Society in the Third Reich* by Fabrice d’Almeida](http://www.lrb.co.uk/v31/n07/christopher-clark/vases-tea-sets-cigars-his-own-watercolours) if you're interested in learning more. I'd also recommend Henry Ashby Turner's *German Big Business and the rise of Hitler*, even though Turner's thesis is that business *did not* in general fund Hitler's rise.

I personally don't have a copy of either, but I do have Ian Kershaw's *Hitler* as well as Richard Evans' three-volume *Reich* trilogy and Weber's *Hitler's First War* which is less helpful for this question. Evans' *The Coming of the Third Reich* has a significant amount of information about your question.

During the 1920s and early 1930s, many of Germany's big firms (I.G. Farben and Krupp were just two examples) became disillusioned with the Weimar system. As Evans writes, ""The influence it had enjoyed before 1914, still more during the war and the postwar era of inflation, now seemed to be drastically diminished. Moreover, its public standing, once so high, had suffered badly as a result of financial and other scandals that had surfaced during the inflation. People who lost their fortunes in dubious investments searched for someone to blame.""

In many cases, they began to blame big business and industry. As the parties of the left grew in influence, these large corporations sought ways to protect themselves. They turned to groups like the Nationalist Party (with whom they had a long association) and the Nazi Party, which in 1930 had undergone a split that purged the left-wing ""socialist"" portion of the National Socialists. 

In 1932, Hitler spoke to the Industry Club in Dusseldorf, appealing to the audience by denouncing Marxism and offering general industrial views. This 2.5-hour presentation didn't make much of an effect at the time, Evans writes, but it laid the groundwork for future contributions by big business. Fritz Thyssen, a wealthy industrialist, was already on board by this time, and as the Depression deepened, other industrialists began to climb onboard the bandwagon.

By the 1933 elections, Evans writes, ""Hitler was backed in his election campaign by a fresh, indeed unprecedented flow of funds from industry.""

The industrialists tended to disdain Hitler, but Hitler had allies who were more friendly, people like Goering, von Papen and Schacht. In February, Hitler delivered another address, this time to a private audience, and declared that democracy was incompatible with business interests, that Marxism had to be crushed, and that the forthcoming election was crucial to that effort. 

After that address, they paid up even though some wanted their contributions to be split between the Nazis and other conservative coalition partners. The Nazi Party wasn't alone in the 1933 election; it was part of a right-wing coalition government, and the industrialists believed that these other coalition partners would be able to moderate and control the Nazis to some degree.

With the industrialists' money, however, the Nazis were able to spread their message and champion the idea of Hitler rebuilding Germany during the Depression and restoring order in the streets from the Marxist menace. Hitler in return promised a huge program of road-building and infrastructure, coupled with tax breaks for auto manufacturers.

After the 1933 elections, leading businessmen and corporations founded the ""Adolf Hitler Donation of the German Economy,"" a program that steered 30 million Reichsmarks into Nazi coffers over the following year. This was intended to stave off the ""forced contributions"" and *de facto* extortion that the SA and local party members exerted from time to time. It didn't work very well, but Hitler helped the industrialists anyway with his program. As Evans writes, ""With the trade unions smashed, socialism off the agenda in any form, and new arms and munitions contracts already looming over the horizon, big business could feel satisfied that the concessions it had made to the new regime had largely been worth it.""

Overall, Germany's biggest industrialists and corporations supported the Nazi rise as part of their general support for conservative, right-wing government in Germany. They hoped that the other conservative parties would dilute and moderate the Nazis' influence, but the Nazis had their own plan, using this funding to push an aggressive and populist agenda that made them the leading figures in the coalition government and ultimately to one-party rule.",0
"This is kind of like the main question in that I'm going to slip all around it - basically, farming families did what they could to maintain something of gendered workdays, with men digging around and building things and women doing labor indoors, whether cooking/baking or making preserves/medicines or sewing/weaving and so on. Women *did* often manage the milking and other dairying, but that was still a relatively clean and sedentary job. So ""choice of tasks"" is a big part of how they kept their clothes clean and out of the way.

Apart from that, we don't really have a lot of direct information in the sources, and have to extrapolate based on a very few visual primary sources. (That being said, ""before pants were an option"" is an extremely broad time period, and if you were to ask about a specific era or century you could likely get more specific information.) It seems most likely that skirts would be made in a practical length and width to accommodate movement, and in a sturdy and hard-wearing fabric, like wool, which could be brushed off when it picked up dry dirt or hay. It was also understood that if you didn't have paid laborers to do the dirtiest tasks for you, or younger women staying with you to learn housekeeping, you would not be able to be spick-and-span.",0
"Awesome question. **An answer to your question lies at the crossroads of the rising middle class in 19th century Western society, Western fascination with the Orient, and noir detective novels.** I'll refer you to Stacey Pierson's *From Object to Concept: Global Consumption and the Transformation of Ming Porcelain*, which is spot-on what you're looking for, and some of the works of Maxine Berg, who edited *Goods from the East, 1600-1800* and has written extensively on trade between East Asia and western Europe.

From the 17th century onward, Chinese porcelain was exported in bulk to Western Europe, which saw it as a luxury commodity. The Dutch East India Company was a huge importer, and other European nations were eager to follow suit. Chinese porcelain was seen as a high-quality item when compared with contemporary European ceramics. Its delicacy, design and durability made porcelain a major consumer good in the Eurasian trade. Berg's [""In Pursuit of Luxury: Global History and British Consumer Goods in the 18th Century""](http://enseignement.typepad.fr/files/bergpastpresent2004-1.pdf) (*Past and Present*, 2004) describes this trade and goes on to explain that when it came to Asian manufactured goods (including porcelain), ""these goods were ... not the ancient or Persian luxuries of corruption and vice, the gold and rubies of the Indies. They were luxuries associated with a civilized way of life, appealing especially to the middling classes.""

European manufacturers were no slouches, of course. When they saw the prices porcelain was fetching, they were eager to duplicate this ""white gold."" Starting in the 18th century, they managed to do just that. Porcelain moved from a luxury product to a bourgeois one. In the 19th century, mass production allowed it to become a lower-class consumer good as well. At the same time, the men and women who worked in the factories creating European ceramics (and a bewildering variety of other products) had the money for hobbies and interests beyond the means of the lower class.

One important thing to know about the middle class, and particularly the British middle class in the middle of the 19th century is the uncertainty of it all. Men and women alike were striving to join the upper classes, but they were uncertain whether their industrial employment was acceptable in polite society. I've written before about the [rise of the beard in the 19th century West](https://www.reddit.com/r/AskHistorians/comments/6cgaa2/how_would_i_a_man_who_cant_seem_to_grow_facial/dhvkjhf/) as a reaction to the crisis of masculinity among the urbanizing middle class.

This uncertainty about middle class life and constant striving toward respectability meant there was a surge in affectation: Middle-class households attempted to imitate aspects of upper-class society as best as they could afford. One of these affectations was the collecting of ceramics. The Chinese porcelain imported in the 17th century was extraordinarily high quality and highly collectible. In 1889, [the *New York Times* quoted a *Saturday Review* piece](http://query.nytimes.com/mem/archive-free/pdf?res=9505E2DD1F3BE533A25756C2A9679C94609ED7CF) that explained, ""To a native collector an unquestionable piece of Sung Dynasty crackled porcelain is much what a Mazarin Bible is to a book collector among ourselves, and the best specimens of the products of the Ming Dynasty (1368-1644) are scarcely less valued. Vases within which are painted with lions rolling balls, or dragons with phoenixes, fetch prices which would astonish even the eager buyers at the salesrooms in King Street; and when genuine pieces of blue and white of this period are offered for sale they find a constantly rising market. It is recorded that about a hundred years ago a pair of cups of this last kind was valued by native collectors at £300.""

As Pierson writes on page 62 of her book, ""the notion of connoisseurship of such ceramics ... was not novel at this time at all, especially when connoisseurship as an activity was gaining in popularity and recognition through collector's clubs and related literature. ... Art dealers were also active in this arena with a number of shops in London featuring what were still often called 'oriental ceramics'. The art market was well developed and connoisseurship naturally accompanied this.""

Chinese porcelain, not just the high-quality material manufactured at Sevres or by Wedgwood, had a special cachet in this time period. During the 19th century, there was a surprising affinity for Chinese products and Western products that imitated Chinese styles in the technique known as *chinoiserie.* Authenticity was prized just as it is today, and the combination of the period's Sinophilia with the Euro-American penchant for antiquarianism meant older Chinese porcelain was valued. 

Ming porcelain itself was singled out for particular treatment starting in the 1870s. Charles Dickens' periodical, *All the Year Round*, mentions it specifically in its April 1875 issue, and the *Art Journal* references it as well that same year. Now, that's different from *the Ming vase*, which develops out of the general idea of Ming porcelain a few years later.

In 1885, *The Decorator and Furnisher*, in a series of essays about Boston artists' studios, discusses the way high-quality Japanese and Chinese porcelain is displayed: ""You notice there is nothing in the room of a very white tone that is exposed to the light. A bust of Venus, in plaster, is in the dark corner on the piano top, side of it rests a brass tray and nearby is a Ming vase catching the light in silvery spots.""

As Pierson points out, this description is unusual for the time because it doesn't identify the porcelain as a ""Chinese vase"" or ""Old China"" or ""Blue China,"" but literally as a *Ming vase.* This is the period when we start to see people come to think of *the* Ming vase as a particularly high quality piece of art.

Three years later, the *New York Times* publishes its first mention of *the* Ming vase in [a short story entitled (appropriately enough) *The Ming Vase*](http://query.nytimes.com/mem/archive-free/pdf?res=9F07EFDB173AEF33A2575AC1A9649C94699FD7CF), which awards that vase special, magical qualities appropriate for the Orientalist beliefs of the period in the West. 

In the late 19th century and early 20th century, ceramics collecting guides [take pains to point out the details of specific kinds of porcelain](http://query.nytimes.com/mem/archive-free/pdf?res=9F06E4D6103DEE32A25750C0A9639C946397D6CF), including Ming. Moreover, there's evidence that even though Ming is considered a quality item, it's not beyond the means of particularly focused middle-class buyers or of upper-middle-class buyers (at least in the early stages). In 1886, [there's an account of an 8-inch Ming vase being sold for £13 in London](http://query.nytimes.com/mem/archive-free/pdf?res=9401E5DB133FE533A25750C2A9649D94679FD7CF). 

As Pierson shows, Ming porcelain starts to show up in fiction at the end of the 19th century and the start of the 20th century. Edith Wharton's stories are a good example of this. In *The Fruit of the Tree* (published in 1907), there are shorthand references to ""Ming,"" referencing Ming porcelain and thus demonstrating its position as a high-class object. Fascinatingly, Pierson says that there's actually a divide between fiction and reality at this point. Qing porcelain, according to auction records, was fetching higher prices than Ming. 

Discussing [a new show at the Metropolitan Museum in 1916](http://query.nytimes.com/mem/archive-free/pdf?res=950DE4DE1439E233A25756C0A9659C946796D6CF), the *Times* wrote, ""A very considerable public is acquainted with the polished beauties of Chinese porcelain, but as yet comparatively few have had opportunity to become even superficially familiar with the noble line of Chinese ware antecedent to the true porcelain, although possessing certain of its characteristics. ... When the public learns the game it ardently pursues it, and one may predict in the near future a public hot on the trail of Sung and Tang.""

Instead, it was Ming porcelain that took the title and the public's interest, continuing to feature in popular fiction. In the 1920s, Pierson writes, there are repeated stories about unscrupulous art and antique dealers, mysteries involving murders or crimes targeting the Ming vase, and more. There's *Quinney's Adventures* in 1924, with a story involving a Ming vase fought over by two collectors. The winning collector lords his victory over the other, only to accidentally drop it and shatter it. For many of these stories, the Ming vase takes the role that the Maltese Falcon would hold in Dashell Hammett's eponymous work.

Now, what about the smashing of the Ming vase in comedy? Well, there's nothing particularly unique about the vase in that regard. It's simply a widely known object that's both fragile and valuable. The comedy comes from its accidental destruction. [It might as well be a crystal football](http://www.espn.com/college-football/story/_/id/7821303/alabama-crimson-tide-crystal-bcs-trophy-shattered-mishap). As Pierson writes, ""By 1968, it had become a widespread literary and colloquial stereotype for something rare, precious, priceless, elegant or sophisticated."" 

",0
"Hopefully my reply above answers some of this. You are correct that of NOI's ideas were seen as heretical by most other Muslims. There was particular controversy over the idea that Wallace Fard was God, and that NOI's leader Elijah Muhammad was a prophet.  A number of American Muslim groups in the 1960s publicly condemned NOI.

Still, it's important to note that Saudi Arabia did allow members of the Nation of Islam, including Elijah Muhammad and Malcolm X, to enter Mecca on pilgrimages, something forbidden to non-Muslims. Malcolm X's *Autobiography*  discusses the fact that his status as a Muslim caused some controversy when he tried to enter Mecca, but he was eventually allowed in (no doubt aided by the fact he was designated a state guest of Prince Faisal). This was an important form of international recognition for NOI's claim to be Muslims.

As I wrote above, the majority of the members of Nation of Islam did embrace Sunni Muslim practices and beliefs in the 1970s under the leadership of Warith Deen Muhammad. Warith Deen Muhammad then renamed the movement. The current Nation of Islam is a schismatic group which refused for accept Warith Dean Muhammad's reforms.",0
"I'm not sure what you mean by ""interacted with."" Do you mean, are there any records of people from sub-Saharan Africa in 14th century Scotland? If so, I should certainly hope not--because *people in 14th century Scotland had no idea what ""sub-Saharan Africa"" was.* To have documentation of their presence would pitch us into a very special episode of *Outlander* indeed.

The Latin medieval understanding of world geography, for whose presence we have ample evidence for among the late medieval Scottish literati (t.i. the people writing documents), traditionally divided the world into Europe, Asia, and Africa. But just like our modern seven continents are cultural constructs, so were theirs--and they weren't the same. 

For example: John of Fordun's *Chronica Gentis Scotorum*, from the 1380s, assimilates Egypt into Asia. It outlines Africa as including Libya, Upper Ethiopia, and Lower Ethiopia. Reasonable, and reasonably kind of sub-Saharan...except that John's Ethiopia is not our Ethiopia (or, probably more to the point, medieval Ethiopia):

> For Ethiopia is threefold: its western portion being mountainous, beginning at Mount Atlas; its middle portion sandy; and its eastern, a desert.

Africa, then, is specifically (our) North Africa:

> By the Inland Sea, on the northern coast, is the country of Zeugnis, where Carthage once stood, and this is Africa proper.

I'm citing the *Chronica Gentis Scotorum* here because it takes us to the requested 14th century. But it's important to point out that even 15th century sources--when there were Ethiopian communities established in Rome, Ethiopian Orthodox delegates to the Council of Florence, Aragonese and Ethiopian royals negotiating marriage alliances--don't differentiate a North and sub-Saharan Africa, and have very European-culture-oriented understandings of ""Africa.""

For example, Andrew of Wyndoun's *Original Chronicle of Scotland*, from the 1420s, has a bit better geographic sense than John's. He recognizes the land around the southern Nile as part of Ethiopia, and beyond Ethiopia is just ""wilderness."" He situates ""Africa"" as:

* Ethiopia, where the Queen of Sheba is from
* Libya, the Pentapolis (a Roman province)
* Carthage, where Dido reigned
* Hippo, whence Augustine
* Mauritania, where...nope, Andrew got nothing here.

So we really should not expect to see records of ""sub-Saharan Africans"" in 14th century Scotland.

But I assume the interest in this thread has nothing to do with sub-Saharan Africa and everything to do with black people.

Well, I'm afraid you're out of luck there, too. Because medieval European ideas of ""race"", skin color, and geographic origin were...not ours.

By the 15th century, recall, Europe had more than ample contact with Ethiopia--in Jerusalem, in Cairo, in Rome. Well, here's a [15th century Queen of Sheba](https://i.kinja-img.com/gawker-media/image/upload/s--2x1WUZQ1--/c_fit,f_auto,fl_progressive,q_80,w_320/njfe5glcf9sxv2koile4.png). Here's a [15th century Queen of Sheba](https://c8.alamy.com/comp/DE6T9K/king-solomon-receiving-the-queen-of-sheba-from-the-bible-historiale-DE6T9K.jpg). Here's a [15th century Queen of Sheba.](https://imgur.com/a/GaUpNdz) Here's one from [right around the turn of the 16th](https://collectionapi.metmuseum.org/api/collection/v1/iiif/465954/946134/main-image). Heck, here's one [from the 18th century.](https://imgur.com/a/9L1l5Eg).

Medieval Europeans' ideas about skin color and geography were...very loosely linked. Why should that surprise us? A person of color in medieval Spain or medieval France was...a Castilian or a French person, right?

So what we see in art *and* in writing is a blend of skin color:geography-based heritage and skin color:morality. (And let's be clear, we are not dealing with nuances of color here). In romance literature, 'Saracen' characters sometimes have their skin change from black to ""white as milk"" when they convert from Islam to Christianity (*Cursor mundi*, 14th century). In the most famous German take on Arthuriana, *Parzival*, Feirefiz has a Muslim parent and a Christian parent, and...is *spotted*.  The *Estoria de Espana* blasts the conquering Saracens:

> Their faces were black as pitch, the most handsome among them was black as a kettle, their eyes shone like candles

and the *Song of Roland* says about Saracen character Abisme that he is ""black as pitch,"" with the archbishop wishing, ""This Saracen seems quite heretical; it would be much better if I were to kill him.""

You'll notice a pattern here: dark skin, in medieval writing, is associated *much* more closely with Islam than with Africa, much less Ethiopia. By the 14th century, Latin Europeans looked to Ethiopia to *save them from Muslims.* (I've discussed the issue of stark white/black skin color and morality [in this earlier answer](https://www.reddit.com/r/AskHistorians/comments/42zw50/why_are_african_people_and_people_from_the/)).

Benjamin Braude has discussed how European scholars gradually linked skin color and geographic heritage over the course of the fifteenth century: via Spanish and Portuguese slavers in West Africa, via Latin European courts deciding that dark-skinned enslaved people made excellent *objects* to keep at court as a sign of their power and prestige. By the turn of the 16th century, these courts included one of particular interest to us. That is when we start to read about ""black Moors,"" ""black ladies,"" ""Blak Elene"" who also seems to be named ""Elen More,"" and so forth at the court of King James IV--of Scotland.

Which is exactly what we should expect, assuming no fluxes in the space-time continuum.",0
"Generally the former. While women in this time were raised to be inferior/subservient to men, I’d argue rape was slightly less common in those times as it is today; I base that mainly on the fact that rape - along with other felonies like murder, poaching, or (sigh) witchcraft - resulted in a very public punishment at its most lenient and death at its most strict ([source](https://books.google.com/books/about/Stories_of_True_Crime_in_Tudor_and_Stuar.html?id=MJSsBwAAQBAJ&source=kp_book_description)).That’s not to say it didn’t happen, only that those pregnant brides ended up with child through consent.

Your wording of “widespread pre-marital sex” doesn’t quite encompass the cultural and religious “loopholes” young couples were eager to exploit. Yes, these brides and grooms had likely coupled before reaching the altar. But that altar - and the subsequent ceremony - just made their marriage official in the eyes of the church and government. Those two entities were different from the eyes of God, who (according to canon law) saw a man and woman as married if, before copulating, they first *promised* they’d marry. Which is to say, couples quickly started having sex once marriage to each other seemed assured. The Church wasn’t too happy with this practice but there wasn’t much they could do beyond making it official. If you’ve got your whole life with this person ahead of you, after all, why wait? 

[Source](http://hfriedberg.web.wesleyan.edu/engl205/wshakespeare/policingsex.htm)",0
"Okay.  Well that didn't pan out.

I called UGA Anthropology department and spoke with them.  The guy basically said because it is on private property no one would touch it.  I didn't understand the logic but he said there are apparently legal issues that can arise from that (?).

He then suggested that I could dig it up myself and if there were bones then the archaeology department might look at them if I brought them up.  

Took me a moment to get my jaw off the ground to tell them - NO.  I'm not digging up a potential slave grave ""just to see"".  I was interested in having an expert confirm for me whether it is or not so I can preserve it correctly.  Crazy.

Side note: I have looked at some slave grave markings on Google images and I see some [like this one](https://bloximages.chicago2.vip.townnews.com/insidenova.com/content/tncms/assets/v3/editorial/1/44/14484ab3-6c83-5fbc-941f-da6d4f64c8df/561c8e4e7ee0a.image.jpg) that look almost identical",0
"I may actually have a relevant answer to this! For full disclosure, I'm not a historian and this is my first response to an AskHistorians thread, so if I mess anything up I hope my post is justly smote.

If I'm allowed to rephrase your question a bit to give it a broader context, it seems that what you're asking, more generally, is: **did soldiers in WWII, in the course of being soldiers, find themselves in conditions that led to painful blemishes or skin irritations?**

While this might not specifically mean acne as you're imagining it, the answer is, undoubtedly, yes! My grandfather, now deceased, served in the 6th Emergency Rescue Squadron, one of [seven squadrons of the USAAF](http://3rders.com/3rdERShist.htm) (the United States' air division before the creation of the USAF in 1947) designed for autonomous rescue during WWII. [Here's some brief verification of him to validate the source.](http://i.imgur.com/SRp2sg8.jpg) In looking at your question, I recalled a story he once told of a soldier laying on his stomach on a bed who was receiving injections to treat a pilonidal cyst, to which my grandfather asked ""what's that?"" and the soldier responded, ""it's a thing on your butt!"" Unfortunately, without any sort of documentation of this treatment or exchange, it's purely anecdotal for the purposes of your question.

However, I can do better!

As it turns out, in researching more specifics on what pilonidal cysts are, I discovered that it was actually such a common occurrence during World War II that it had a name: **Jeep Riders' Disease** (or just Jeep Disease). 

So before we get ahead of ourselves, what exactly is pilonidal disease? According to the [American Society of Colon and Rectal Surgeons](https://www.fascrs.org/patients/disease-condition/pilonidal-disease), pilonidal disease is a skin infection in the crease of the buttocks, close to the coccyx, which can become an inflamed, painful cyst and may also become infected. I don't think it's a stretch of the imagination to see how this could be bad for soldiers - discomfort and pain while moving or sitting would be present, and the treatment, which would likely involve [phenol injections](http://emedicine.medscape.com/article/192668-treatment?pa=FOK0p2BPH2xBy%2BPOS8DkPqlB0jg7Xdz8WxSQfLU%2BhMB3vecAyiYCl8NXlVpBhcdpLYSTPqBPITs28e%2FfjVymGRgdh0IYUyErgfy%2FYxjHzUE%3D#d9) or lancing and drainage, could leave victims bedridden for days. There's also considerations for the potential for infection and sepsis - soldiers are going to be moving through dirty, hot environments that are going to aggravate their condition by sweating and the inability to bathe regularly. Though the initial question seems to be one of beauty, I think it's imperative to consider the implications of skin conditions that won't just be unsightly, but taking soldiers away from active duty. 

Which brings us to the question of prevalence. Was this common? Most articles I've found, such as [this one](http://www.sciencedirect.com/science/article/pii/S0039610909001248) seem to lead back to the same place: A February, 1944 article published in the Southern Medical Journal called [Jeep Disease: Pilonidal Disease of Mechanized Warfare](https://sma.org/southern-medical-journal/article/jeep-disease-pilonidal-disease-of-mechanized-warfare/). Although I can't personally access the article, other articles citing it make the claim that about about 80,000 soldiers during WWII were hospitalized for pilonidal disease due to long and frequently-rough jeep rides which put pressure and irritation on the coccyx of soldiers, hence the name Jeep Riders' Disease. Indeed, a [study that occurred from 2005 to 2011](https://tinyurl.com/lo2gf89) found that, even today, pilonidal disease retains a significant association with military personnel. Even though my grandfather's story will never be the primary source that I wish it were, it seems like he may not have been too far off the mark about this condition affecting soldiers during WWII.

So, to get back to your original question: Was acne ever an issue in WWII? To the degree we're talking about facial blemishes or irritating marks, I can't speak. But if we ask more broadly about the kinds of conditions that soldiers worked and fought in, could we say that they contributed to health issues, especially of the skin, which could impact service? Definitely. In the case I've described here, perhaps the soldiers have a great complexion, but it's what you *don't see* that's the real problem. While I find the question of whether acne as you imagine it would be common a good one, I think there's definitely enough information to say that there were skin problems in World War II, caused by the conditions soldiers were in, that were not only unusual or annoying, but downright painful and dangerous.",0
"As often happens, many of the removed comments are simply ""what happened to the comments""? However, we've also removed a number of comments that reflect common misunderstandings around the genocide(s) of American Indians. 

This message is not intended to provide all the answers, but simply to address some of the basic facts, as well as genocide denialism in this regard, and provide a short list of introductory reading. Because this topic covers a large area of study, the actions of the United States will be highlighted. There is always more that can be said, but we hope this is a good starting point. 

##What is Genocide?

Since the conceptualization of the act of genocide, scholars have developed a variety of frameworks to evaluate instances that may be considered genocide. One of the more common frameworks is the definition and criteria implemented by the United Nations. The term ""genocide,"" as coined by Raphael Lemkin in 1943, [was defined by the U.N. in 1948](https://redd.it/6mg3j3). The use of this term was further elaborated by [the genocide convention.](https://treaties.un.org/doc/Publication/UNTS/Volume%2078/volume-78-I-1021-English.pdf)

Article II describes two elements of the crime of genocide:

1. The mental element, meaning the ""intent to destroy, in whole or in part, a national, ethnical, racial or religious group, as such"", and
2. The physical element which includes five acts described in sections a, b, c, d and e. A crime must include both elements to be called ""genocide.""

Article II: In the present convention, genocide means any of the following acts committed with intent to destroy, in whole or in part, a national, ethnical, racial, or religious group, as such:

* (a) Killing members of the group;
* (b) Causing serious bodily or mental harm to members of the group;
* (c) Deliberately inflicting on the group conditions of life calculated to bring about its physical destruction in whole or in part;
* (d) Imposing measures intended to prevent births within the group;
* (e) Forcibly transferring children of the group to another group.

##American Indian Genocides – Did they happen?

Since the arrival of Europeans to the Americas, typically signaled with the appearance of Columbus in 1492, Indigenous Peoples have experienced systematic oppression and extermination at the hands of colonial powers. These colonizing governments either organized or sponsored acts of genocide perpetrated by settlers, targeting Indigenous settlements for complete destruction; eliminating sources of food and access to life-sustaining resources; instituting child separation policies; and forcefully relocating Indigenous populations to often times inhospitable tracts of land, now known as “reservations.” All of these acts constitute what scholars now recognize as genocide. The horrendous acts that occurred in the Americas were even an example proposed by Lemkin himself, where it is noted from his writings:

>[Lemkin applied the term to a wide range of cases including many involving European colonial projects in Africa, New Zealand, Australia, and the Americas. A recent investigation of an unfinished manuscript for a global history of genocide Lemkin was writing in the late 1940s and early 1950s reveals an expansive view of what Lemkin termed a “Spanish colonial genocide.” He never began work on a projected chapter on “The Indians of North America,” though his notes indicate that he was researching Indian removal, treaties, the California gold rush, and the Plains wars.](https://oxfordre.com/americanhistory/view/10.1093/acrefore/9780199329175.001.0001/acrefore-9780199329175-e-3)

These actions took place over the entirety of the Americas, exacerbating the rapid depopulation of Indigenous Nations and communities. Exact figures of the population decline are inconclusive, giving us only estimates at best, with Pre-Columbian population numbers ranging anywhere from as low as 8 million to as high as ~100 million inhabitants across North, Central, and South America. What we do know is that in the United States, records indicate the American Indian population had dropped to approximately 250,000 by 1900. Despite any debate about population statistics, the historical records and narratives conclude that, at least according to the U.N. definition, genocide was committed.

##Mental Element: Establishing Intent

In order for genocide to be committed, there must be reasonable evidence to establish an intent to commit what constitutes genocide. Through both word and action, we can see that colonial powers, such as the United States, did intend at times to exterminate American Indian populations, often with public support. Government officials, journalists, scholars, and public figures echoed societal sentiments regarding their desire to destroy Indians, either in reference to specific groups or the whole race.

>”This unfortunate race, whom we had been taking so much pains to save and to civilize, have by their unexpected desertion and ferocious barbarities justified extermination and now await our decision on their fate.”

[--Thomas Jefferson, 1813]( https://www.loc.gov/resource/mtj1.047_0147_0150/?sp=3)

>""That a war of extermination will continue to be waged between the races until the Indian race becomes extinct must be expected.""

[--California Governor Peter Burnett, 1851](http://governors.library.ca.gov/addresses/s_01-Burnett2.html)

>"". . .these Indians will in the end be exterminated. They must soon be crushed - they will be exterminated before the onward march of the white man.""

[--U.S. Senator John Weller, 1852, page 17, citation 92](http://digitalcommons.law.utulsa.edu/cgi/viewcontent.cgi?article=2654&context=tlr)

##Physical Element: Acting with Purpose

**U.S. Army Policy of Killing Buffalo (Criterion C)**

[In this post,](https://redd.it/4j42ag) it is explained how it was the intention and policy of the U.S. Army to kill the buffalo of America off in an attempt to subdue, and even exterminate, the Plains Indians.

**Sterilization (Criterion D)**

The Indian Health Service (IHS) is a federally run service for American Indians and Alaska Natives. It is responsible for providing proper health care for American Indians as established via the treaties and trust relationship between tribes and the U.S. Government. However, on November 6, 1976, the Government Accountability Office (GAO) released the results of an investigation that concluded that [between 1973 and 1976, IHS performed 3,406 sterilizations on Native American women.](https://cbhd.org/content/forced-sterilization-native-americans-late-twentieth-century-physician-cooperation-national-) Per capita, this figure would be equivalent to sterilizing 452,000 non-Native American women. Many of these sterilizations were conducted without the consent of the women being sterilized or under coercion.

**Boarding Schools (Criterion E)**

The systematic removal of Indian children from their parents and placement into boarding schools was a policy implemented by the United States meant to [force American Indian children to assimilate into American culture,]( https://redd.it/8zgozt) thus “[killing] the Indian, [and saving] the man.” These schools were operated by various entities, including the federal government and church/missionary organizations. While constituting cultural genocide as well, American Indian children were beaten, neglected, and barred from practicing their cultures. Some children even died at these schools.

##But What About the Diseases?

In the United States, a subtle state of denial exists regarding portions of this country's history. One of the biggest issues concerning the colonization of the Americas is whether or not this genocide was committed by the incoming colonists. And while the finer points of this subject are still being discussed, few academics would deny that acts of genocide were committed. However, there are those who vehemently attempt to refute conclusions made by experts and assert that no genocide occurred. These [“methods of denialism”](https://redd.it/6kywre) are important to recognize to avoid being manipulated by those who would see the historical narratives change for the worse.

One of the primary methods of denial is the over severity of diseases introduced into the Americas after the arrival of the colonizers, effectively turning these diseases into ethopoeic scapegoats responsible for the deaths of Indigenous Peoples. While it is true that disease was a huge component of the depopulation of the Americas, often resulting in up to a 95% mortality rate for many communities and meaning *some* communities endured more deaths from disease, these effects were greatly exacerbated by actions of colonization.

##Further Reading

Though there is much information about this topic, this introductory list of books and resources provide ample evidence to attest the information presented here:

* [*Beyond Germs: Native Depopulation in North America* edited by Catherine Cameron, Paul Kelton, and Alan Swedlund](https://books.google.com/books/about/Beyond_Germs.html?id=yUw-rgEACAAJ)
* [*American Indian Holocaust and Survival: A Population History Since 1492* by Russell Thornton](https://books.google.com/books/about/American_Indian_Holocaust_and_Survival.html?id=9iQYSQ9y60MC)
* [*Murder State: California's Native American Genocide, 1846-1873* by Brendan Lindsay](https://books.google.com/books/about/Murder_State.html?id=TfiD-E7VBKYC)
* [*Blood and Soil: A World History of Genocide and Extermination from Sparta to Darfur* by Ben Kiernan](https://books.google.com/books/about/Blood_and_Soil.html?id=XR91bs70jukC)
* [*American Holocaust: The Conquest of the New World* by David Stannard](https://books.google.com/books/about/American_Holocaust.html?id=RzFsODcGjfcC)
* [*Myths of Conquest*](https://redd.it/2vf565) by /u/anthropology_nerd
* [AskHistorians FAQ](https://www.reddit.com/r/AskHistorians/wiki/nativeamerican#wiki_european_contact_and_conquest)",0
"The Victorian era is infamous, rightly or wrongly, for its repression of sexuality. But its temporal and philosophical heir definitely did repress the possibility of the homoromantic relationships between women and between men that had been normal, if not the norm, for centuries and centuries. This process was rooted in one of society's most fundamental adopted divisions, gender, so you can imagine that there are a whole lot of factors implicated in the shift that are all tangled around each other and mutually reinforcing. Some of the key ones include: industrialization and urbanization, women's colleges, class concerns, a *crisis in masculinity* (masculinity is always in crisis), and most importantly, the invention of ""sexology"" as a field of science at a time that science played a central role in cataloguing and normatively ordering society. 

Anthony Rotundo, primarily studying men, argues that ""romantic friendships"" in America start to become visible in the Revolutionary War era and flourish in the mid-19th century. The 18th century is kind of a black hole for me so I'll take his word for when the concept of romantic friendships was jump-started, but it was by no means new. In the Middle Ages, Christians and Muslims alike wrote poetry and composed letters depicting homoromantic and even homoerotic relationships. I'm going back this far not for the heck of it, but because medieval society helps clarify key qualities of male and female ""romantic friendships"" that contributed to their eventual demise: a societal value on men expressing emotion (knightly tears; religious devotions) and the very, very limited possibilities for unmarried women to rise above the poorest classes. Romantic friendships did not threaten men's sense of themselves as men, patriarchal control of women, or marriage.

Socio-economic changes in the late 19th and early 20th centuries knocked all of that askew.

The 1870s-1920s saw a massive influx of young women and men into U.S. cities. On one hand, this was an age-old process that, for centuries, was basically the season cities could exist (they were population sinks--on their own, city residents could not reproduce enough to replace themselves given mortality rates). On the other, the type of work they found and the pathways for success in that work were much more recent. The old system of apprenticeships and family connections for men, and almost exclusively domestic servant work for women, absolutely persisted but were swamped by the numbers of factory workers and non-domestic service workers. To support the population boom, cities constructed residential hotels/dormitories/apartments that were often designated single-sex.

That situation made both male and female romantic friendships a threat to the gendered prescriptions of society. For men, it diminished the utility of romantic friendships as potential economic and social connections, meaning they wouldn't be stepping stones towards supporting their eventual family. For women, it opened a much more achievable possibility of financial stability outside marriage. 

The blossoming of women's colleges at this time made that problem even clearer to the sexuality reformers and sexologists we'll meet in a little--because ""these women"" were most assuredly middle and upper-middle class. In short: the ideal marriage partners for men...in an environment where romantic friendships could permit them both prestigious social roles (scholars, administrators, politicians, professional artists, etc) and economic *success* without men. This was true, even long-term, for both students and teachers. About 10% of American women at the end of the 19th century never married; the figure was around 50% for graduates of women's colleges. So when men observed, as in this letter to the Yale student newspaper:

> There is a term in general use at Vassar, truly calculated to awaken within the *ima penetralia* of our souls all that love for the noble and the aesthetic of which our natures are capable, The term in question is ""smashing.""

> When a Vassar girl takes a shine to another, she straightway enters upon a regular course of bouquet sendings, interspersed with tinted notes, mysterious packages of ‘Ridley’s Mixed Candies,’ locks of hair perhaps, and many other tender tokens, until at last the object of her attentions is captured, the two women become inseparable, and the aggressor is considered by her circle of acquaintances as ""smashed.""

they might not have seen sexual competition, but the possibility of a lifestyle threat was lurking.

Men's romantic friendships were also under fire with respect to their emotionality. The gradual militarization of western culture over the 19th century (think the Salvation *Army* or the military trappings of the Boy Scouts) drove/was driven by a narrowing definition of masculinity on ""muscles""--vigor, strength, athleticism, the Teddy Roosevelt stereotype. Whereas emotions had once been the healthy counterpart, gradually the internal dimensions of character and a value on openness and gentleness became a liability. (Marriage was still okay, because the idealized marriage was the husband/father rising up to 'be a man' and take care of his family).

Steeped in all these burgeoning developments and their implications came the sexologists, with an agenda not just to categorize society but to evangelize their ""discoveries.""

A lot of us are at least in passing familiar with the ""homosexuality didn't exist as 'homosexuality', an identity, before 1900"" trope. This can be taken too far (and often is), but it is nevertheless true that the later decades of the 19th century and early 20th century saw professional, middle-class scientists coalescing ideas of same-sex sexual relations according to Science rather than morality. Instead of a wrong step by step choice, it was an abnormal physical, inherited trait.

This idea got mixed up in Progressive Era utopian visions of societal improvement that, among other things, tagged ""deviants"" and lower-class people as hindering forward progress--just as same-sex sex, now identified with the people who practiced it, prevented heterosexual, reproductive sex.

And scientists like Bernard Talmey exhibited one of my favorite characteristics of historical men writing about women: in his 1904 book on, well, women, he announced his deep concern that the American public ""does not even surmise of the existence"" of sex between women. It was a scientific version of what I see in my medieval (male) clerics skating gingerly around actually mentioning lesbian activity because they don't want to put the idea in women's minds.

But this view of American sexologists, lagging somewhat behind their European counterparts, was crucial to the decline of romantic friendships among men and women. First, because it started off with a condemnation of these friendships that took away from social order *regardless* of whether there was sexual activity involved. 

Second, because of the label first stacked onto the participants: inverts. That is, the inversion of proper sex/sexual order. Here we meet up with the rise of muscular masculinity *against* emotionality and gentleness, as well women's political activity and independent economic power *against* the norm of a separate women's/domestic sphere.

And so romantic friendships, instead of a natural part of growing up for men and women, became an aberration--not in the sense of ""rare"", but in the sense of ""wrong.""

...Unbeknownst to the sexologists, however, their codification of language and an identity for homosexual men and women gave people who did experience same-sex attraction a *mutual* self-understanding--a certain legitimacy. It's seen as the beginning of an LGBTQ+ movement (if not yet a civil rights one). So there is a lot to mourn about the loss of romantic friendships and what it signified. But this is one story about the past that also has a future.",0
"To cover a bit of the “obvious—” pickling has long been a tool for food preservation in the United States. Long before canning because a safe and reliable way to preserve foods, pickling (along with salting, drying, etc) was one of the most reliably safe ways to preserve fresh vegetables. While today pickled cucumbers reign supreme, most vegetables can be safely pickled and preserved several months for consumption. I wrote more about food preservation practices in the 19th century [here](https://www.reddit.com/r/AskHistorians/comments/gqe25i/what_did_food_security_storage_look_like_in_the/).

Since pickled vegetables were such a common preservation technique, they appear in American cookbooks onwards from the earliest “American” cookbooks we have from the 18th century. The Virginia housewife, an early 1824 example of a “southern” regional cookbook, has a whole chapter on pickling. Likewise, the 1904 cookbook the Blue Grass Cookbook contains a whole chapter on “pickles,” and includes recipes for a whole range of pickled vegetables. Pickled cucumbers are referred to as such, while, for example, a “chopped pickle’ is a range of vegetables that have been pickled.

While both of these texts include recipes for pickled cucumbers, the classic “Dill pickle” owes its heritage to eastern European jewish immigrants who popularized the classic Dill Pickle. Even more than in the US, pickled vegetables were a staple to the eastern European diet, and immigrants continued to make and privilege eastern European pickle flavors. Frequently referenced in oral histories and memories are “sours” and “half sours,” types of pickled cucumbers distinguished by fermentation time. Pickles were fairly popular street foods, and became a staple of Jewish Delis in the city.

There is no source that definitely tells us when pickled cucumbers just became pickles. Community cookbooks, in the early-to mid 20th century, like the 1921 Durham Cook Book, often continued to have a section called “pickles” that included all manner of pickled vegetables. Pickled cucumbers were, largely, still referred to as pickled cucumbers.  While it wasn't yet ubiquitous, around this time, the term “pickle” was becoming increasingly associated with pickled cucumbers in the public eye. An excerpt Alfred Kazin’s Walker in the City refers to pickled sours and half sours as Pickles. While this is a recollection of the early 20th century, it was published in 1951. I am pretty confident in my understanding of the research on food preservation, but I could not find a single source that says with any certainty why we call pickled cucumbers pickles. There's a PBS article that take on the question but does not answer it. There’s no real definitive advertising campaign by a pickle company, no seminal sources, just a shifting conception of pickles over the early 20th century that pretty solidly gave way to the term ‘pickle’ being colloquially used to refer to pickled cucumbers by the mid 20th century. There was a 1893 advertising campaign at the Chicago World's Fair where Heinz company had ""pickle pins,"" so while it as clearly legible I've also found sources that postdate it that continue to specify pickled cucumber.

I’d like to be clear that my “answer” is just based of my knowledge of 19th and 20th century cookbooks and food preservation practices, and isn’t something I or anyone else I personally have found has made in a peer reviewed publication. But as sour and half sour pickles, which historically referred to an array of pickled eaters European vegetables but in the US referred mostly to pickled cucumbers made by Jewish immigrant communities, became a popular street food and part of the New York culinary lexicon that was later emulated by processed food companies, “sour and half sour” and “cucumber” was dropped from the name because cucumbers were just the most popular option for a pickled vegetable.  Companies, like Heinz, used it to refer to pickled cucumbers, cookbook authors began just calling pickled cucumbers pickles by the 1920s and 30s.

I know this isn't a perfect answer but I hope it helps, and I welcome anyone to tell me what I'm missing!

Also, let it be known, I have found no AskHistorian question I've tried to answer more deceptively difficult to answer.",0
"I'm not a huge Dan Carlin fan and can't comment on his specific point here. However, the more general question of how we talk about rape, sex, and consent in the past is a *really* important and difficult one for historians in general. In my opinion (and in my writing), we need to honor historical women's/girls'--and sometimes adolescent boys'--own perspectives and subjective experiences, while also acknowledging the constraints and sometimes horrors of their socialization and circumstances directing choices/consent.

I don't want to get too far into the philosophical weeds or we'll be here all day, so I'll just focus on the immediate example here. All these political marriages start from the idea that the marriage is a transaction between the husband and the wife's family, or the husband's family and the wife's family--not the woman herself. To us today, there's an element of coercion and assertion of power that would make any kind of willing consent possible. 

But put yourself in the coat of a Korean (yes, really) princess who has spent your whole life learning that someday you'll marry a man of your parents' choice to do good things for yourself and your family. That doesn't mean it's a *good* norm to be socialized into (medieval men were socialized to believe it was A-OK to rape women and children while sacking cities), but it does mean we have to consider our historical subjects have their own perspectives. We do, after all, call them historical *subjects* and not *objects*.

The case of Chinggis's wife Ibaqa, who was what you might say his first wife by conquest, is a labyrinthine example of how difficult it can be to unwind questions about women's perspectives and consent. Ibaqa was a member of the Kereit royal family, probably the cousin or niece of the king through her father. The king sought to arrange a marriage alliance with Chinggis, was *probably* plotting a kind of Red Wedding scenario, Chinggis was alerted to the possibility and destroyed the Kereits instead. But! first, Ibaqa's father had gotten into a major dust-up with the Kereit king, and he and his family were taking refuge with another people, the Naimans. Her father also had a pre-existing relationship with Chinggis.

So then Chinggis conquered the *Naimans*, of course. And even though he was a conquest, Ibaqa's father drew on their earlier relationship as well as Chinggis's original desire to make an alliance with the Kereit royal family--he got *his* daughters, Ibaqa and Sorqoqtani, to substitute for the originally intended royal children.

And *then*, a few years later, Chinggis turned around and gave Ibaqa to one of his followers as a wife. Sources differ (and I actually mean, the primary sources differ, not in a Magic 8 ball sense), but they indicate it was to reward this particular follower but *possibly* also to punish Ibaqa's father for rebelling. She eventually followed her new husband to northern China, but freely traveled back to the khan's camp every year or so to party and visit her sister.

How on *Earth* do we untangle questions of choice and consent there? It's tempting, in the absence of other evidence, to assume that women's first loyalty was to the most immediate family--either marital family if they had one, or natal family if they were unmarried; and if married, a second allegiance still to their birth family, and then to their birth culture. In this case, even *that* assumption doesn't really help. Ibaqa's dad revolted from their nation which Chinggis conquered and was Chinggis' friend, but then also got conquered by Chinggis, but then maybe revolted against Chinggis who sent Ibaqa away, but would she be *relieved* to have a different husband or angry about it? And in all of this, how far can we go to talk about *willing* consent versus ""I don't have a choice so might as well choose to be okay with it* 'consent'?

In other words: in cases of 'conquest brides,' we are dealing with extra levels of coercion for the bride's *family*. Whether or not we're dealing with extra coercion for the bride herself--whether she would have perceived a difference between another type of marriage and this one, how family loyalty shaped her view, perhaps whether she thought she could use this new marriage to wield more power or to protect her people? In my opinion, it doesn't do justice to women as historical subjects to condemn every sexual act as rape without qualification, because that strips away any subjectivity or choice on their part. But at the same time, we should recognize the violent and toxic power dynamics involved--even for women who were socialized to make the best of a bad situation.

For a western European perspective, you might also be interested in this earlier answer of mine:

* [How common was sexual violence in medieval Europe?](https://www.reddit.com/r/AskHistorians/comments/9fzxny/how_common_was_sexual_violence_during_the_middle/)

which looks at Latin European ideas of what was and wasn't rape compared to modern ones.",0
"Not OP, but one really great book on the East German political system is *The People's State: East German Society from Hitler to Honecker* by Mary Fulbrook. It goes into more detail than any other English-language source I am aware of, and was written with the benefit of lots of official records declassified after reunification.",0
"Since this isn’t my area of expertise, I was interested to see if any other historians would weigh in on this before I posted my response.

Rebecca the raccoon, as you stated in your question, was intended to be eaten at Thanksgiving in 1926, but the Coolidge famously “[pardoned](https://www.history.com/news/the-thanksgiving-raccoon-that-became-a-presidential-pet)” the animal and adopted it as a pet.

Raccoon does not appear to have been commonly consumed by members of the “high society” of the United States, but it was not unknown as a source of protein, either. While it was an additional source of meat for rural and southern Americans, it did appear in some northern United States meat markets during the 19th century.  It appears to have [lost some popularity](https://www.google.com/books/edition/Twain_s_Feast/arRAgNAsJmsC?hl=en&gbpv=1&bsq=raccoon) due to the its perception as a [slave](https://www.google.com/books/edition/The_Big_Muddy/0oTU7Xz4AasC?hl=en&gbpv=1&bsq=raccoon) dish. A recipe for raccoon appeared as late as the 1931 edition of the *Joy of Cooking*, but appears to have been removed in subsequent years. [This source](https://www.outdoorlife.com/blogs/cast-iron-chef/how-cook-raccoon-recipe-brined-bandit-and-sweet-potato-hash/) has a picture of the recipe. I was unable to find a .pdf online of the entire book.

As for when did eating raccoon go out of fashion…it didn’t. It was likely always a niche protein source, and in the American south, it is still consumed, to a small extent. Gillett, a small town in Arkansas, hosts an annual “[Raccoon Supper](https://katv.com/news/local/an-arkansas-tradition-continues-with-gillett-coon-supper)” where hundreds of raccoons are prepared and eaten for charity.

Perhaps the most famous Americans known to have eaten raccoon are [Mark Twain](https://www.google.com/books/edition/A_Tramp_Abroad/4kQPAAAAYAAJ?hl=en&gbpv=1&bsq=raccoon) and former [President Bill Clinton](http://memory.loc.gov/diglib/legacies/loc.afc.afc-legacies.200002687/).

Interestingly enough, in the 1920s, raccoon fur coats were also popular and considered [stylish](https://www.google.com/books/edition/Franklin_Simon_Fashion_Catalog_for_1923/26T7RvSyQwkC?hl=en&gbpv=1&dq=raccoon+fur+fashion&pg=PA18&printsec=frontcover).

If you are interested in the evolution of American Cuisine in general, I would recommend Paul Freedman’s *American Cuisine: How It Got This Way*, which also includes a traditional recipe for raccoon.",0
"In the movie BlacKkKlansman, the portrayal of David Duke using the phrase ""America first"" and speaking about returning America to its greatness is rooted in historical accuracy. The KKK and white supremacist groups in the United States have often used such language to promote their ideology. The phrase ""America first"" was popularized by the America First Committee, an isolationist organization that advocated against American intervention in World War II.",1
"In most simple terms, it is an aesthetic choice that authors make, and you aren't entirely wrong in your observation. On the one hand it of course ought to be noted that it being a common convention, the usage simply self-perpetuates, with many authors likely not even thinking about *why* they chose to use *Luftwaffe* instead of German Air Force. Many, many decades of commonly refering to the *Wehrmacht* and *panzers* and *Kriegsmarine* kind of leads to a loss of any real thinking about the *why*. But still, I would stress that translation is inherently an editorializing act. The fact that the convention established itself *says something*, even if we don't think too much about it. The flipside of course is that because it is so common, choosing to translate to German Air Force, or German Navy, or just saying ""tank"" instead of ""panzer, stands out too and says something. And in fact it is something that some authors do more now, in no small part because of the issue you raise.

The best commentary on this trend comes from Richard J. Evans, who spent a little time in his Third Reich trilogy to explain why he chose *not* to follow this convention. Words such as *Führer* he renders merely as ""Leader"", and *Mein Kampf* shows up under the English title of ""My Struggle"". He is quite blunt in his reasons, which jive well with your own thoughts, as well as are ones I agree with (although I realize I unconsciously slip into the untranslated use frequently because, again, it is so common you just don't think about it), stating in the introduction to *Coming of the Third Reich* that ""*[r]etaining the German is a form of mystification, even romanticization, which ought to be avoided""*.

The exceptions he makes are very specific. He notes, generally, how the lack of specific English equivalents can impact translation, such as with the term *Volk*, where he notes:

>Some German words have no exact English equivalent, and I have chosen to be inconsistent in my translation, rendering national variously as 'national' or 'nationalist' (it has the flavour of both) and a similarly complex term, Volk, as 'people' or 'race, according to the context.

But in the case of *Reich* (and *Reichstag*), its ""particular, untranslateable resonances in German far beyond its English equivalent of ‘empire’"" made it impossible to translate without, as he noted, sounding ""artificial"". No one talks about ""The Third Empire"" or the ""Parliament Fire"". Similarly the term *Kaiser*, because, in his words, ""it, too, awakened specific and powerful historical memories."" But otherwise, he uses the English equivalents throughout the book. 

The romance that he notes, and you observed as well, is something which he aptly calls out, and it is impossible not to make connections in how we use those terms and 'otherize' the Nazi warmachine in a way that adds an unwarranted, and at times offensive, mystique around them. I'd go back to where I started though, ands again stress that translation isn't a neutral act. Even aside from the example of *Volk* highlighted, and how different translations need to be used at different times, it just, in a general sense, brings an approach that may be new and unfamiliar. Evans even notes that he expects his choices may be ""rather irritating"" for specialist readers, but (and maybe I read to much into it in thinking he is *throwing shade*) advises them to read ther German edition if this is the case for them. It being a general work, for English speakers, he is of the opinion (and rightly, in my own estimation), that his choices avoid the baggage that many bring in with those terms, and offers that new perspective in allowing ""readers to gain a feeling for what these things actually meant"".

So anyways, so sum this all up, there are different reasons we can say ""why"". There wasn't ever some convention of WWII historians where they agreed on what terms to use, and the ones that we do developed, and entrenched themselves, and become self-perpetating in their uncritical use and reuse, but they *do* carry with them baggage we can't ignore. They don't *explicitly* ""come from a place of fascination with the Nazis and their perceived military prowess"", as you put it, but they do play a *part* in it, less pure cause and effect though than intertwined dual-support. Many historians continue to use the terms untranslated, even if they perhaps recognize that to a degree, because the convention is *so* entrenched, and to many it would feel artificial to abandon at all, but others like Evans are more of the opinion that in recognizing that, we ought to be pushing to change the convention.

--------

ETA: One additional thing I would note. It is common to see talk of the *Wehrmacht* as the German Army, but that *actually* would be the *Heer*. The Wehrmacht was the armed forces as a whole. Something that I would note is that authors will often leave *Wehrmacht* untranslated, but even if they are using it properly, and then talk about the army separately, I can't think of any book which uses *Heer*. It usually is, basically, ""The Wehrmacht is made up of the *Army*, *Luftwaffe*, and *Kriegsmarine*"". This is its own interesting tangent. It speaks to two things, I believe. The first is that *Heer* just isn't an appealing word, and the second is that many people use *Wehrmacht* to mean German Army, incorrectly.",0
"The dragon banner, sometimes referred to as the ""Dragon of Wessex,"" was indeed a real flag associated with the medieval Plantagenet kings of England. It depicted a red dragon on a yellow background. The dragon flag was not used as a symbol to justify or endorse acts of rape and pillage. Instead, it was a royal standard that represented the authority and power of the English monarchy.",1
"All \_known\_ serial killers isn't remotely the same thing as ""all serial killers"". Given that the United States is and has been roughly %5 of the world's population for the 20th century, the %75 factoid can't possibly reflect the reality.  I'm not even sure where the %75 number comes from . . . whatever the case, it can't be right.

Have you ever heard of a serial killer from Burkina Faso, Peru, or Myanmar? From Zaire, Egypt or Mexico?  From India or China? They surely exist/have existed, and there are a few cases that get reported -- but who's finding them? Which journalists are writing them up?

In order to be known as a serial murderer, two conditions are necessary: first the murderer must be able to carry out his murder and \_not\_ be identified and apprehended. Second, he must later be identified and apprehended, if not already dead. So you have a paradoxical situation-- a serial killer can operate freely when the ability to detect him is low, but we'll only know of him when we can actually detect him. 

The US has a particularly active media and police interest in serial murder, producing a perception that dwarfs the reality, conflating murders with unknown perpetrators into serial murder.

>Whatever the actual increase in the prevalence of serial murder in  recent years, it is clear that fear associated with such crimes has grown.  Prompted by exaggerated media reports (e.g., Darrach and Norris  1984), the American public has been scared into believing that there is  an epidemic of serial murder in the United States, totaling as many as  5,000 victims annually.  
>  
>{snip}  
>  
>At some juncture, ""unknown motive"" was equated  and confused with ""no motive,"" leading to the erroneous inference  that serial murder claims 5,000 victims per year (see Fox and Levin  1985; Jenkins 1988, 1994). Even when the flawed reasoning was uncovered, there remained a tendency to inflate uncritically the extent of the  serial murder problem. When asked how many of the 5,000 homicides  with unknown motives could be the work of serial killers, Justice Department sources speculated it to be two-thirds of the 5,000, or approximately 3,500 (Starr 1984).

Sources:

[Multiple Homicide: Patterns of Serial and Mass Murder](https://www.jstor.org/stable/1147545)

&#x200B;

&#x200B;",0
"As the excellent comment by /u/Daeres notes, there were always unresolved tensions in the Greek conception of Olympos. Even in the Iliad, the earliest extant work of Greek literature, Olympos is described both as a physical mountain (with epithets like ""snowy"" and ""craggy"") and as a metonym for the heavens. At times, Homer's Olympus is clearly conceived as something more than a physical mountain, as when Zeus tells the other gods:

""If you tied a chain of gold to the sky, and all of you, gods and goddesses, took hold, you could not drag Zeus the High Counselor to earth with all your efforts. But if I determined to pull with a will, I could haul up land and sea, then loop the chain round a peak of Olympus, and leave them dangling in space. By that much am I greater than gods and men."" (*Iliad* 8.19-26)

This dual conception of Olympus as both physical peak and heavenly realm continues throughout Greek (and later Latin) literature. The discrepancies between these conceptions are clear in the mythological compendium known as Apollodorus' *Bibliotheca* (probably written in the second century CE). In a myth about twin giants who attempted to storm the homes of the gods, the author notes:

""When they were nine years old and measured eighteen feet across by fifty four feet tall, they decided to fight the gods. So they set Mount Ossa on top of Mount Olympus, and then placed Mount Pelion on top of Ossa, threatening by means of these mountains to climb up to the sky"" (1.53)

Here, at least, there is a clear distinction between Olympus and the home of the gods. The distinction in even clearer in Lucian's *Icaromennipus,* a rather strange second-century text about a man who decides to fly to the home of the gods. Mennipus (the protagonist) doesn't bother with Olympus; he sets sail directly into the sky, and figures that the gods leave very far off indeed. To quote his calculations:

""Let me see, now. First stage, Earth to Moon, 350 miles. Second stage, up to the Sun, 500 leagues. Then the third, to the actual Heaven and Zeus's citadel, might be put at a day's journey for an eagle in light marching order.""

So it was widely assumed, at least by educated men of the imperial era, that the gods were not confined to Olympus. So did they climb the physical mountain? They certainly got close. Although the Greeks and Romans were not usually recreational mountain climbers, a few were in the habit - the emperor Hadrian, for example, once climbed to the peak of Mt. Etna to watch the sunrise (SHA, *Hadrian* 13). And in the case of Olympus, there was actually a sanctuary of Zeus quite close to the top. From the third century BCE to the fifth century CE, offerings were made at an altar on Hagios Antonios, a peak about a mile from the main summit. 

We don't know whether anyone made the climb from the altar to the main peak, though it certainly would have been possible to do so. And we don't know whether the experience of climbing so close to the traditional home of the gods affected anyone's conception of Olympus. At least some, however, seem to have considered the lack of winds around Zeus' altar a sign that it was a sacred place:

""The things that are to be seen at Olympus show that Homer did not celebrate it rashly. First, it rises so high, with a preeminent peak, that the inhabitants call the top of it heaven. On the summit is an altar dedicated to Zeus. If burned offerings of entrails are brought to it, they are neither blown off by windy breath nor washed away by rain, but as the year rolls on, whatever is left there is discovered unchanged; what is consecrated to the god triumphs over time and the corruption of the air. Letters written in the ashes remain until the next year’s ceremony."" (Solinus 8.6)",0
"Thoroughness of financial accountability. To give another example: a key firefighting strategy, in the days before pressure hoses, was to chuck clay pots filled with water at burning buildings. (People were apparently required to keep them on hand for such a situation). They'd hit the roof or walls and presumably shatter, releasing the water. For one major 14C fire, the treasury records painstakingly index the compensation rendered to individual Sienese for the pots they used, apparently with descriptions of each pot to ensure precision.",0
"In their construction of the ‘Sword and Shield’ theory, French scholars rejected the use of German documents, considering them tainted by Nazi ideology and propaganda. Paxton, an American, was one of the first academics to consult German intelligence reports and treat them as actual historical sources (of course, as all historians should, he also viewed these sources critically – the problem with previous scholars was that even the mere consultation of German sources was dismissed). These intelligence reports recorded opinions from intercepted mail, statements from informers and overheard conversations. Paxton also had access to some reports from Vichy prefects (prefects were state representatives in French administrative regions). In these documents, the prefects reported extensively on public perception and opinion of the Vichy government. Paxton recognised that there was a significant problem with the use of official reports. Officials had the tendency to report what their superiors wanted to hear, and often confused silence with support. Despite this, intelligence and official reports were much more reliable than the Vichy or German press, and, unlike many memoirs and diaries, had the added benefit that they were not retouched post-war. From these, Paxton was able to draw an estimate of popular support for Vichy. His conclusion was this:

>**A crude graph of French public opinion from 1940 to 1944 would show nearly universal acceptance of Marshal Pétain in June 1940 and nearly universal acceptance of General de Gaulle in August 1944, with the two lines, one declining and the other rising, intersecting some time after the total occupation of the hitherto “free zone” of Vichy in November 1942.**

As seen, this was a broad estimate rather than any concrete number. Paxton was also at pains to point out favourable opinion had a wide spectrum. On one hand, one could support Vichy through a positive belief in the infallibility of Petain or a negative fear of revolution. On the other hand, those who complained about the regime but did nothing positive against it were also collaborating from a standpoint of functionality, for

>they provided the broad public climate of acceptance that lent legitimacy to a more active participation.

**For Paxton, his estimate was not one concerning popular support from a political standpoint, but popular support from a functional standpoint.**

French scholars were horrified by Paxton’s conclusions – not only had he overturned the established historiography, he was also a ‘junior American historian’ treading on the toes of esteemed colleagues. But by the 1990s, Paxton’s conception of widespread functional support for Vichy had become the new orthodoxy. There remains significant debate over the extent and timeline of popular support, as seen in the works of J.F. Sweets, Philippe Burrin, J.P. Azema and Richard Vinen, and historians now tend to see collaboration and resistance as a spectrum rather than a divide. Still, Paxton’s work remains a starting point for any new research on Vichy France. \[For those of you interested in the memory and historiography of Vichy France, Henry Rousso's *The Vichy Syndrome* is an amazing read into, well, the politics of academia.\]",0
"One of the points that Joe Miller makes in *Way of Death* is that for newly enslaved Africans bound for the Americas, the horrors of the trans-Atlantic journey had already begun in the African interior. Those designated for sale to Europeans endured a forced march to the coast that was (a) terrible in pretty much every way; Miller estimates that about a quarter of new slaves died *before they even reached the slave ship* (b) long. The second part is especially relevant here because the journey would have taken the slave caravan through quite a bit of inhabited territory, including stopping in the occasional village. There was ample opportunity for rumors and gossip to spread among the slavers, the slaves, and the village hosts. And, equally important, there was ample opportunity for the new slaves to escape. Not only does this point to another means of news transmission, it offers another example of African awareness of white slavers' involvement and agendas. In Angola, for example, the area of Kisamba actually because quite well known for local rulers being willing to shelter fugitive slaves to keep them out of Europeans' hands.

Unfortunately, most contemporary documentation of the experience of Atlantic-bound slaves in Africa--almost all of it from Central Africa--comes from Europeans. (West Africa is a little more complex situation in terms of long-distance trade, because the trans-Saharan trade to the Maghreb and the Near East was centuries old--and white Europeans had no qualms about dealing with Arab traders instead of white ship captains if it made the better profit. This was probably a fairly small fraction of European involvement in the African slave trade, but the trans-Saharan trade overall adds a layer of complexity). That said, there are a few reasons to take seriously some of the European reports about central African slaves' perceptions.

First, as slavers, the involved Europeans had at least some interest in slaves being able to work. A related concern that emerges on both sides of the Portuguese imperial Atlantic, is the web of behavioral/emotional/physical symptoms called *banzo* (sometimes *banzamento*), which seems to mean an enduring melancholia with a sense of longing and loss mixed in. A Portuguese petition from 1689 notes:

> Once they [new slaves] were branded with the royal stamp, they become absorbed by thoughts about their fate in Brazil and become *banzado*, which causes many of them to die. (trans. Ferreira)

Definitely as time went on, the fate of slaves across the Atlantic was deeply entrenched along the African slave coasts. The (Portuguese) governor of Luanda around 1800, Miguel Antonio de Mello, was keen on mustering ""Brazil!"" as a psychological weapon to enforce obedience still in Africa. The Luanda authorities even threatened local *slaveowners* with the forced deportation of their slaves if the owners ""let"" them misbehave in public.""There is no other punishment so deeply felt and feared,"" Mello noted, ""as being sent away to America."" He also made the connection between deportation to Brazil and the condition of *banzo* among slaves. 

As if slavery wasn't bad enough--as if disease and malnutrition and injury and abuse wasn't bad enough--as if deportation wasn't bad enough--it is apparent that up and down the central African coast, at least, different peoples nurtured variations on the same essential fear of the white slave traders. Specifically: cannibalism.

Different versions of this fear are recorded (by Europeans). The slaves passing through Luanda, it seems, were aware of one malicious rumor/belief that white people sought to cook/fry Africans and extract oil from their dead bodies. At Cassanje, further inland, the fear was straightforward murder and consumption. And Francisco Damiao Cosme, the leading physician in Angola in the late 18th century, recorded a range of gruesome terrors from the different populations of slaves he encountered. Some feared that white people wanted to turn the insides of deported slaves into cheese. Others believed their bones would be ground up and used as gunpowder.

So even though these are the records of Europeans and not the words of Africans, the underlying similarity yet topical differences points to the spread of rumors, enduring over a *long* frame of time. Accusations/fears of cannibalism against other groups of people, anthropologists recognize and historical research tends to confirm, are typically rooted in a view of that group as ""barbaric"" or uncivilized in some way. (Although this point is most commonly raised with respect to European views of various island peoples, I recently read a stomach-churning article on accounts of outsider cannibalism in medieval Islamic sources--including a picture-perfect occurrence of the ""shipwrecked sailors stranded on a desert island and taken before the cannibal king"" topos! [Reference here](https://www.reddit.com/r/AskHistorians/comments/5yzqeo/were_africans_generally_aware_of_where_slave/deutchv/).)

But in the context of sub-Saharan African religious beliefs, there seems to be something more at work. A frequent belief in African traditional religions, such as among the Wimbum people, is a close connection between witchcraft (here *tfu*) and actual or psychic (soul) cannibalism. This connection, I think, helps drive home just how *real* the psychological terror of ""America"" and the white traders could be among Africans who believed they were facing it, might face it, or--if they were the few and lucky--had narrowly escaped from it. 

Further reading:

* Joseph Miller, *Way of Death: Merchant Capitalism and the Angolan Slave Trade, 1730-1830*, ch. 11
* Roquinaldo Ferreira, *Cross-Cultural Exchange in the Atlantic World: Angola and Brazil in the Era of the Slave Trade,* ch. 4
* Kalle Kanonoja, ""Atlantic Slave Trade and Melancholia of Enslaved Africans: A View from the Southern Atlantic,"" is an unpublished paper, but it's [free on academia.edu](http://www.academia.edu/11500930/Atlantic_Slave_Trade_and_Melancholia_of_Enslaved_Africans_A_View_from_the_Southern_Atlantic) and analyzes *banzo* from a disability studies and medical history perspective--definitely worth a read",0
"In Anglo-Saxon England, the bounty for killing an outlaw was the same as for killing a wolf. By the late Middle Ages, this had evolved into the legal principle of *wolfesheed*: the outlaw *was* a wolf, able to be hunted and killed exactly like a wolf, legally and by any means. England wasn't the only place that so closely equated wolves with criminals: in the Norse sagas, murderers are *Ulfr* or wear wolf cloaks.

Although it is fairly common for cultures to equate criminals with dangerous animals, the link in medieval Europe between bandits and a type of animal particularly known for attacking travelers and traveling in packs is both strong and specific. Travel in the Middle Ages was dangerous business, and it wasn't just a case of humans versus animals and environment.

Overland medieval travelers faced several varieties of human threat. First--you might have noticed that university diplomas grant the holder *the rights and privileges* associated with the degree. Well, those rights and privileges [originated in the Middle Ages](https://www.reddit.com/r/AskHistorians/comments/459h5n/my_diploma_refers_to_the_rights_responsibilities/czwg7k3), and the very first one--from twelfth century Germany, even before universities existed as such--was safe conduct for students and teachers traveling between schools or between home and school. The retinue of a lord whose land they were passing through, or a band from a city, had a very very nonzero chance of kidnapping travelers for ransom--whether they could come up with the money on their own, or whether a messenger had to be sent back to their family. (The struggle to reign in robber knights and lords is an important part of the medieval political narrative--it was 
a slow and very hard-fought process). Timothy Reuter posits that noble/aristocratic robbers (including those more or less employed by them) were actually the primary danger in the Middle Ages versus ""career bandits.""

It wasn't just people and their ransom that roadside robbers would be after. When attacking merchants' caravans, wine was a popular theft item--also copper, iron, cloth, basically anything that could be sold. Remember, in many or most cases, we're dealing with people well integrated into the socio-economic fabric of medieval society. This would in many cases continue to apply to the next grouping I'll discuss.

There were also robbers and kidnappers from lower social classes who *acted like* they were legitimate groups or armies. In general, this type of banditry would rise out of a broader conflict or war, like a massive heterodox movement/suppression. The best example here is probably the *bratczycy* or ""brothers"" in fifteenth-century Hungary. They seem to have the veneer of--and claimed to be--Hussite armies but in reality were pretty much gangs of bandits. The wartime context of groups like these is key. Medieval banditry--murder, plunder, robbery, arson, rape--looked pretty much like what the average soldier did during war. More to the point, what the average soldier was expected to do and what was societally *accepted* as ""what soldiers did."" While making little difference to travelers, the attempt for a group to portray themselves as legitimate soldiers at war could matter for their own legal circumstances if caught.

And then, of course, there were indeed archetypal bandit gangs of vagabonds--maybe already outlaws where such a status existed. Feodor Glowaty's family/gang at the end of the 15C Poland lasted longer than most (about three years), so it's a good example of the sort of ""career bandit"" life as opposed to a one-time robbery or a landed robber-knight. Glowaty's group, which had about twenty members at its height, became most infamous for a massive robbery of the Rozgonyi estate, including making off with a large number of horses. They also engaged in attacks on merchant caravans and locals moving between village to village. And most famously, in 1493, they held an entire town for ransom as punishment for capturing and executing two of their members. Well, they tried to, at least. It's pretty clear the town didn't pay.

It's tempting to say that over the course of the Middle Ages, the proportion of aristocrat/servant-robbers declined and the proportion of vagabond and/or outlaw and/or desperate robbers increased. Civic criminal records show that many thieves operated on a bare survival level rather than bigger heists, and it seems reasonable that their counterparts outside the town walls might have been in similarly desperate circumstances. The agonizing long-term reigning in of wayward lords, on the other hand, operated at different speeds and scales in different parts of Europe. And as the case of soldiers/claimed soldiers demonstrates, it's not a matter of ""noble OR outlaw,"" but a sliding scale that likes to tip up and down at different times.

As far as protection, the most important thing to remember is that attack was by no means a guarantee. Travel increased exponentially over the course of the Middle Ages--both local and long distance. In other words--most of the time, it was successful, and that most of the time was enough to be worth the risk. That said, there were a couple of basic ways travelers could protect themselves. The first is probably the most obvious: travel in groups. When Ibn Battuta crossed North Africa on his way to Mecca, he was basically told, ""Nope, you're waiting until the caravan goes this year, full stop.""

It's also evident that travelers, perhaps especially merchant groups with valuable cargo, were often armed. Even at the sporadic times and places where weapon-carrying was regulated in medieval Europe, exceptions were frequently made for travelers.

Escorts are a trickier business to suss out of the sources. There are a few references to a ruler decreeing a soldier would accompany caravans between towns (especially in Italy), but in practice, this seems to have worked out symbolically far more than actually--almost a means of insurance, if cargo was stolen the merchant might be reimbursed for part of it. Italian bankers paid handsomely for military protection of their largest transports, especially if straight-up currency was involved.

Disguise was another popular method of attempted protection. By the late Middle Ages, pilgrims in medieval Europe had a distinctively coded style of dress. It might sound silly, but in fact, there are comparably few reports in sources of pilgrims being attacked--and a *whole lot* of complaining about not-pilgrims using pilgrim attire to conduct espionage, moneymaking business, or simply avoid payment of tolls at bridges and towns.

And, indeed, the final method of protection wasn't really protection at all. Jewish and Christian sources alike debate whether it is moral to just go ahead and buy back your stolen merchandise from the thieves.

Overall, highway banditry was indeed a problem in the Middle Ages, particularly in the more lucrative high-traffic areas between nearby towns or around a city. The most risk, indeed, was carried not by long-distance travelers but by everyday business. And while *homicides and robberies* flare up in legal sources, a lot of bandit activity would basically have consisted of bribes or ""paying for safe passage, wink wink."" Scholars have also suggested that roadside crime increased over the course of the Middle Ages--laws against banditry become more common proportionally; inns and hostels spring up on the roadside to accommodate/protect travelers overnight. But this only makes sense. More people traveling more often and with more money--and more to the point, more reasons to have money. ",0
"It's a misconception that most of the artworks hidden or stolen during WW2 are truly 'lost'. It's certainly the case that many paintings, sculptures, rare books and antiques were 'expropriated' (looted) by the Nazis, on a small scale from individual Jewish families, and on a large scale from the museum collections of occupied countries. However, the process of expropriation was usually well-documented, and the locations of looted works were, more often than not, easily traced.

&#x200B;

The [Art Loss Register](http://www.artloss.com/en), the international body which maintains a list of all stolen and missing artworks, estimates that about 200,000 works were stolen during WW2, but of those, around 170,000 have been recovered, with [30,000 still missing](http://world.time.com/2013/11/07/the-top-10-most-wanted-missing-art-works-from-world-war-ii/). Today, reputable museums will consult the register before acquiring something for their collection, if there is even a possibility that the piece might have been looted during WW2. (Source: me, I'm a curator).

&#x200B;

Perhaps unsurprisingly, those works which are still missing tend to be the ones which were stolen from private collectors, or art dealers; such as Raphael's *Portrait of a Young Man*, stolen from a Polish family, and a Klimt portrait originally in the possession of a Viennese Jewish collector. That said, most 'lost' works are lost only in the sense that they are no longer with their original owners, or the owners' descendants - the actual locations of the works are frequently common knowledge, as was the case with several pieces in the possession of [Horst Wächter](https://www.theguardian.com/artanddesign/2017/feb/26/nazi-art-stolen-poland-returned-horst-waechter), the son of Otto von Wächter, Nazi governor of occupied Poland. Similarly, Klimt's famous 'golden' portrait of Adele Bloch-Bauer, which was expropriated from her family after the *Anschluss*, was openly displayed at the Belvedere Museum in Vienna for decades, until Bloch-Bauer's surviving niece successfully mounted a legal challenge to have it returned to the family.

&#x200B;

Large museums had - and still have - systems in place for controlling their inventory, its condition and location. For example, during the German occupation of Paris, the *Jeu du Paume* courts near the Louvre were used as a kind of depot or staging post for looted artworks from French museums and private collections. Ostensibly working under the orders of the  *Einsatzstab Reichsleiter Rosenberg*, or ERR, the Nazi taskforce established to oversee the expropriation of cultural treasures, the French curator and art historian [Rose Valland](https://artresearch-service.com/rose-valland-art-spy-at-the-jeu-de-paume/) was responsible for taking an inventory of all the artworks which passed through the *Jeu de Paume*. Unknown to the ERR, Valland (a) spoke excellent German, and (b) was an agent of the Resistance. She made secret copies of the inventories, which would subsequently allow most of the works which passed through her hands to be recovered after the war. Furthermore, she passed information on to her Resistance contacts relating to the shipment of artworks by rail, so that they would not mistakenly be blown up during sabotage operations.

&#x200B;

That said, there are  - as OP asked - definitely still hidden caches of looted art to be discovered. It is highly likely that many missing paintings are locked away in Swiss bank vaults, which is thought to be the fate of the Raphael portrait mentioned above. In 2012, a collection of over 1400 artworks was uncovered in the Munich apartment of Cornelius Gurlitt, a collector and art historian who was being investigated on tax evasion charges. Gurlitt was the son and grandson of notable art historians, and his father, Hildebrand Gurlitt, had collaborated with the Nazis on the expropriation and movement of 'degenerate' modern art, much of which he appears to have 'purchased' for himself at a fraction of the works' real value. Thanks to his part-Jewish ancestry, the elder Gurlitt was not prosecuted after the fall of the Reich, and claimed that his art collection had been destroyed during a bombing raid... which was taken at face value, until it showed up in his son's apartment, almost 70 years later. It is possible that we may see another 'Gurlitt' in our lifetimes, although as the period moves out of living memory, art historians will be increasingly reliant on archival records, and luck, to make these discoveries.",0
"Absolutely. I would be surprised if anyone paying attention didn’t notice that. The nationalist interpretation was extremely common and arguably the most widely accepted, especially in the period we are talking about here. Even to this day the memory of the Hussite period draws heavily on this idea of a historic Czech nation, triumphant against all odds in a crusade ""against all"" ( *Proti všem* in Czech). The Hussite period had been widely considered the apex of Czech history by nearly all stripes of Czech nationalists going back to the 19th century Revivalists like František Palacký, but what that history meant exactly has been debated. It might be interesting to explore the specific narratives that Mussolini came across and how he arrived at his own interpretation of Jan Hus. 

There were some common threads of the various interpretations, mostly drawn from Palacký; he argued that the history of the Czech nation is one of two eternal battles: one between German and Slav, and one between Authoritarianism and Freedom/Truth. The German-Slav one is incredibly important but less subject to interpretation. Where it gets tricky is the Authoritarianism-Freedom/Truth divide. One point I should make is that this is not 'freedom' in the Western sense of a liberal democracy, but a freedom that is highly opposed to outside control and oriented toward 'living life in the truth'. Truth is an especially important word here because Jan Hus often spoke about living life devoted to the truth and that word comes up again and again throughout the period when Mussolini would have been paying attention to this topic. For the 19th century revivalists, Authoritarian is associated with German (Catholicism, the Holy Roman Empire, Habsburgs, Nazi Germany) and Slav with Freedom. This was also the interpretation favored by the the communists after WW2, in part because it made the most sense in the context of recent history.

At the end of WW1, the most widespread interpretation of Jan Hus would have been the one offered by Tomáš Garrigue Masaryk. Throughout the late 19th and early 20th century, Masaryk argued that the Czech nation's historic purpose is to work towards Truth, and this Freedom/Truth being sought was his brand of a religiously-couched triumphant Liberalism. In his 1895 book, *Česká otázka* (The Czech Question), Masaryk claims Jan Hus as a proto-Enlightenment figure whose ideas went out into the world and now, in the early 20th century, are returning to their birthplace in Bohemia. Masaryk was a relatively minor figure before the war, but his arguments made him **very** popular with U.S. President Woodrow Wilson, who had a similar worldview and would help support Masaryk's narrative and push for an independent Czechoslovakia after WWI.

While the academic discussion is relevant, it really doesn't prove how widespread this idea was. For that, we can look to less academic instances of this narrative from the late 1910s. For example, there were [recruiting posters](https://content.wdl.org/679/thumbnail/1430159784/616x510.jpg) for the Czechoslovak Legions that show WW1 soldiers fighting under Hussite flags and some variant of the Hussite battle hymn ""Ye Who Are Soldiers of God"" ( *ktož jsú boží bojovníci* ). When the newly independent 1. Czechoslovak Republic was created after the war, it's first president was Tomáš Masaryk and the national motto, chosen my Masaryk, became 'Truth Prevails' ( *Pravda vítězí* ); this phrase was derived from Jan Hus' own use of the word 'Truth' and Masaryk's interpretation of what that meant.

To be clear, Masaryk's interpretation was by no means unopposed. It was based on dodgy historical work done by the Revivalists and it is clearly a Mussolini-style use of Jan Hus as a vehicle for his own political views. There were also competing versions of this narrative that could have aligned more closely with Mussolini’s own ideas at the time. However, WW1 made the idea of an independent Czech nation a widespread aspiration for the first time and Masaryk's interpretation of what that nation should be was highly influential in discussions about Czech nationalism in the 1910s. I am fairly confident that anyone with a passing interest in Czech affairs around 1918-1919 would have come across this narrative that Jan Hus and the Hussites were the pinnacle of 'Czechness'.",0
"I have a (hopefully) up-to-snuff answer for this one.

A quick bit of background first: Ancient Greek names were commonly made up of two parts: a given name, and a patronym. In some cases a third part was added, denoting ethnicity or locality of birth. Given names were frequently dithematic (made up of two parts), for example ""Socrates"", from ""sos"" (σως), ""unwounded or safe"", and ""kratos"" (κρατος), ""power"". This is very common among Ancient Greek names. ""Philippos"", ""lover of horses"", etc. Patronyms were derived from the individual's father.

Names derived from gods and goddesses (""Theophoric"", or ""god-carrying"" names)were fairly common. For example, derivations of ""Dionysos"" were common among both men and women. So too were names like ""Apollodotos"", ""given by Apollo"" (think, ""god's gift"" sort of thing). Gods associated with the underworld and death were not used, or at least not frequently (no ""Hades"", ""Persephone"", etc.), likely because they were thought to be cursed or unlucky. 

Simple nouns were another popular source of names. These can run the range from basically descriptive to outright insulting-sounding. The word ""aischros"", ""ugly"", is the source of at least twenty known names, and is the basis for the name of the tragedian ""Aischylos"", better known in the Latin form as ""Aeschylus"". Parts of the body, traits, colors, etc. were all used in these simple names (""Kephalos"", from the word for ""head"", ""Euthyphro"" of Platonic dialogue fame, from ""εὐθῠ́ς"", meaning ""direct""). Animal-based names were also common (""Moschos"", derived from the word for ""heifer""). A personal favorite is Kopreus (""shitty""). The supposition is that a lot of these names are derived from affectionate nicknames for kids that continued on into adulthood and thus entered the historical record. 

The names you are asking about, though, are heroic names (those associated with great heroes). ""Hercules"" (the Latin form of the Greek ""Heracles"", from the name of the goddess Hera, who despised him because he was the product of a union between her husband Zeus and a mortal woman), Icarus, Odysseus, etc. were not popular in Classical Greece. The etymology of some of these names is disputed or unknown, and some of them (Odysseus, for example, the derivation of which is disputed, but associated in scholarship with words meaning ""to wail"", ""to be wrought against"", etc.) have a decidedly dark turn. Not to say they were never used, but they weren't common in the Classical or Hellenic periods. Why is unclear. Perhaps it was considered too great a burden to name a child after someone who was so revered. However, after the Roman conquest of Greece a taste for Heroic naming arose and continued until Christianity became mainstream. At that point biblical names began to be more popular. 

See:

Parker, 2000 (in the volume listed below): https://www.britac.ac.uk/pubs/proc/files/104p053.pdf

Hornblower and Matthews (eds.), 2000 (for a broader discussion of the topic): https://www.britac.ac.uk/pubs/proc/volumes/pba104.html

The absolute definitive work on this subject is the voluminous [""Lexicon of Greek Personal Names""](http://www.lgpn.ox.ac.uk/publications/index.html), started by the Oxford classisist P.M. Fraser and still undergoing revision. ",0
"Why was the growth so lackluster though? Well, at least as concerns what I've covered here, it is also worth noting that the aforementioned carrots weren't always effective. As before the war, illegal, underground abortions weren't uncommon, and divorce rates nevertheless rose through the decade after the Great Patriotic War despite the legal barriers and financial disincentives. And while the propaganda machine continued to trumpet motherhood as ""a 'sacred duty' to the state"", a common complaint, especially of single women who tried to balance a career alongside motherhood, was that the actual offerings by the state in support often fell very short of what was promised. Whatever the complaints though, the policies certainly seemed to have some effect:

>The 1944 legislation certainly resulted in an increase in the number of extra-marital children in the U.S.S.R. It is estimated that there were approximately five and a half million extra-marital children under eighteen years of age in the U.S.S.R. in 1957, and a peak of over six million in 1962, when there were approximately five million unmarried mothers. Part of this increase would, of course, be accounted for by the over-all increase in the population, especially in the non-Russian Republics.

Still though, abortion remained a problem, and it was practicality more than anything else  - such as the loosening of Stalinist era control policies - that saw it relegalized in 1956, for up to 12 weeks of pregnancy, as following legalization, the official line continued to harshly condemn what was characterized as an abrogation of a central civic responsibility for women. Statistics remained shrouded for decades more though, with none published again until the 1980s, so estimates for that period are very rough, but estimates certainly indicate more pregnancies ended in abortion than in a live birth, but at a declining rate:

>In the mid-1960s, of the 8 million abortions registered in the USSR, there were roughly 7 million 'complete' abortions induced in a medical establishment, that is, about 150 abortions for 100 live births. After 1965, there is a slow but steady fall. The abortion ratio was 148 in 1970, 138 in 1975, 130 in 1980 and the present level, in 1990, is 124.

Likewise, despite the attempts prevent it, divorce rates continued to rise and rise - doubling between 1960 and 1970, and commentary from that period points to women being the instigator in most cases ""suggest[ing] that Soviet marriages and families are unstable and emotionally unsatisfying, especially for women"". Abusiveness and boorishness of husbands drove most of this, alcoholism being cited in more than half of petitions for divorce. Rising employment opportunities and ability to provide for themselves and their children also likely helped contribute. In a nutshell, women felt more empowered to leave a bad marriage and more capable to support themselves once single. 

So to sum it up, the policies we see on this front teetered between ideology and practicality. Legalization of abortion originally came about due in large part to the understanding that it was necessary, and driving them underground simply hurts women who likely will seek them anyways, but also we can't discount the new found sense of civic freedom and equality for women that characterized the early days of the Soviet Union. As Soviet policies started to shift to a more 'traditional' view of family life and structure, and (supposedly) the circumstances for motherhood were improved, the necessity of legal abortion could be dispensed with, but in reality the supports were not as good, and women continued to desire control over their reproductive rights - leading to the continued use of abortions simply driven underground. Once again relegalized in the '50s, the Soviet government prefered to keep the policy low-key, and refused to allow information on its extent to be made public, which helps to demonstrate a continued ideological opposition, even if they realized that they had to make some concessions to the reality of the situation.

Works Cited:

* Bucher, Greta. 2000. Struggling to survive: Soviet women in the postwar years. Journal of Women's History 12, (1) (Spring): 137-159,
* Stone, O. M. ""The New Fundamental Principles of Soviet Family Law and Their Social Background."" The International and Comparative Law Quarterly 18, no. 2 (1969): 392-423.
* Ashwin, Sarah. ""Gender, State and Society in Soviet and Post-Soviet Russia"" New York: Routledge, 2000
* Avdeev, Alexandre, Alain Blum, and Irina Troitskaya. ""The History of Abortion Statistics in Russia and the USSR from 1900 to 1991."" Population: An English Selection 7 (1995): 39-66.
* Engel, Barbara Alpern, Anastasia Posadskaya-Vanderbeck, and Sona Stephan Hoisington. A Revolution of Their Own: Voices of Women in Soviet History. Boulder, CO: Westview Press, 1998.
* Goldman, Wendy Z. Women, the State, and Revolution: Soviet Family Policy and Social Life, 1917-1936. Cambridge: Cambridge University Press, 1993.
* Heitlinger, Alena. Women and State Socialism: Sex Inequality in the Soviet Union and Czechoslovakia. Montreal: McGill-Queen's University Press, 1979.
* Randall, Amy E. 2011. ""Abortion Will Deprive You of Happiness!"" Soviet Reproductive Politics in the Post-Stalin Era. Journal of Women's History 23, (3) (Fall): 13-38,204
* Hoffmann, David L. 2000. ""Mothers in the Motherland: Stalinist Pronatalism in its Pan-european Context."" Journal Of Social History 34, no. 1: 35
* Mazur, D. Peter. 1967. ""Reconstruction of Fertility Trends for the Female Population of the U.S.S.R."" Population Studies 21, no. 1: 33-52.",0
"The story of Richard the Lionheart's perception by the British public is a long and complicated one. He has always been controversial, and opinion has generally swung on a pendulum between harsh condemnation as a vagabond who didn't bother to rule England verses a lionised hero who led the Third Crusade and *nearly* won. So here's the basics of why. 

Richard's father, Henry II, had been a strong king but also a controversial one for his own long and complicated list of reasons - he had almost been overthrown in a rebellion led by his own children, including Richard. When Richard I was crowned there was hope that he would be a steady ruler and keep his slippery brother John under control. His coronation degenerated into an anti-semitic riot in the streets of London, and that rather set the tone. 

In his own day Richard was not particularly loved, other than when he was freed from captivity, which was met with widespread celebration. Numerous sources, including people who personally knew him, openly describe him as an asshole even by medieval standards. He was a serial rapist, his marriage was dysfunctional and so lacking in love that the pope publicly called him out on it, he had little regard for the lower classes of society, he killed prisoners, he (allegedly) had people assassinated including the king-elect of Jerusalem, and he was known to be difficult to work with (a shock given what I just told you, I know). We know him as 'the Lionheart' but during his lifetime he was also known as 'Oc-e-Non', which in Occitan means 'yes and no'; a reference to his terseness. He was cruel and difficult. 

The Third Crusade, for which he is best known, was a mixed affair for Richard's reputation. On the one hand, he had led the counter attack against Saladin, who was rightfully perceived as the greatest threat to Christian dominion over the Holy Land in living memory. On the other hand, he delayed going (for which he was attacked by the famous troubadour Bertran de Born) and... he lost. There's no getting around the fact that he did not fulfil his crusading vow by visiting Jerusalem. The decision to leave was his call, and almost his call alone - his men even mutinied against him and I can't emphasise how rare that was in the Middle Ages. Whilst he was away, king Philip II of France invaded Normandy and his brother John attempted to expand his personal domains in England and potentially seize the throne and, because Richard was thought to have had the king-elect of Jerusalem assassinated, he was imprisoned on his way home by the duke of Austria, who he'd been such an asshole to on the Third Crusade that most of the remaining German contingent departed because they couldn't stand him. His ransom nearly bankrupted England and he spent a year unable to fight back against John and Phillip so lost chunks of Normandy to the French that would never be securely recovered. He cocked up royally. 

Then a few years later he was shot in the collar and killed by a teenager because he was too busy taunting a guy defending a castle with a frying pan to spot the kid lining up a crossbow on him. 

From what I've just written, you might be wondering why anyone remembers him positively at all given that he messed up so much. 

He did have some things going for him. He dazzled contemporaries with his brilliance in war, thanks partly to Roman military doctrine laid out in Vegetius' *De Re Militari*, a late Roman treatise on warfare. Richard wasn't so much interested in battle tactics - his army was no Roman legion, not even close - but in training and logistics he took a lot on board from Vegetius. Crusades in particular had been blighted by poor logistical preparation but Richard had a solid grasp of what was needed. He prepared flat-pack trebuchets that he could ferry to the Holy Land, he conquered Cyprus in part to secure a supply of food and money to support the crusade, he successfully worked out how to counter Saladin's tactics, but he also viewed the crusade as a personal battle between himself and Saladin and lost sight of the mission. Commenting on Richard's qualities, the historian Stephen Runciman put it eloquently: ""he was a bad son, a bad husband, and a bad king, but a gallant and splendid soldier""

He was also good at delegating. Richard has been infamous in history for not speaking English and not taking an interest in the running of the kingdom. These were not criticisms levied at him in his lifetime (u/CoeurdeLionne goes into more detail below). Henry II had created a centralised government in Westminster that handled the daily affairs of state and Richard left them to it. They were nothing special, but it kept the kingdom in safe-ish hands. 

So he was a good military leader, and his lack of interest in politics turned out to be a good thing, but why the lionisation? 

Nostalgia is powerful, especially in a time of great difficulty. We think 'well the current time is bad, so the before time must have been at least a bit good', and this was essentially what happened to Richard's reputation. The problems of Richard's reign were pretty small compared to John's. John's reign was so bad we got the beginnings of constitutional monarchy (through Magna Carta) to restrain the worst of it. People looked back on Richard favourably as a result. John was intrusive in the affairs of state and routinely overruled the civil servants that had competently run the kingdom in Richard's absence. His disrespect for the barons led to civil wars, and the trouble they caused both to the aristocracy and the common folk. 

In the years after John's death, literature emerged praising those nostalgic days before John ruined everything. In late medieval stories like the early Robin Hood tales, John was invariably an antagonist. In any story set while Richard was away on crusade, Richard was cast as the just ruler undermined by his nefarious brother. Robin Hood is the most famous example but there were plenty such tales in circulation. As I've described, there's a lot of truth to that portrayal, and writers seized on it as way to further vilify John. 

Furthermore, Richard's battles against Saladin had all the elements of a naturally excellent story. It was two great leaders facing off against each other in a great struggle for the future of the Holy Land, and Richard was without a doubt its hero. In particular, the Battle of Jaffa showed him to be a mighty warrior; he led the vanguard personally and cut down dozens of Saladin's men,  wielded a crossbow with great accuracy, and then utterly crushed Saladin's army in it's last charge. It wasn't just his own court poets writing about him like this either,the Muslim sources also describe him as a terrifyingly good warrior who always led from the front. To round off the victory, Richard mounted a horse and rode down the front of Saladin's line taunting his men, *and nobody dared to attack him*. He was great, and he knew it. He had been occasionally called 'the Lionheart' before the Battle of Jaffa, but afterwards it defined his memory in popular culture. 

Against the backdrop of John's terrible reign, and the controversial reign of Henry III (John's son, who lost secure possession of all Normandy and was deemed so bad there was a revolution against him in 1258 that saw him entirely removed from government for a few years), Richard became a figure representative of the time before all the problems. People latched onto those people who defied the misery of John and Henry, Richard was one, another was William Marshall - eulogised as the greatest knight of all time. Compared to the difficulties of the 13th century, the reign of Richard looked positively peachy. And as crusades continued to fail badly, his limited success  became more praiseworthy as people realised what an achievement it was to even win a battle in the Holy Land. 

Richard's lionisation was not (imo) mainly down to his own merits as a ruler, because as king he got a lot wrong. But compared to the trash fire that was England in the 13th century, Richard looked great and was an emblem of the before times when England had a chivalric king who took on Saladin and achieved more than anyone else did. Poets of the later Middle Ages celebrated his military achievements and forgot his misdeeds, in part because John was worse in many respects, and because it made for a better story. After all, rapists and murderers don't make for sympathetic protagonists. 

Finally, I'd like to point out that Richard was not always lionised. There's a statue of him outside Parliament in London, awkwardly away from the road where nobody can really see it. It's there because MPs could not decide whether they wanted to venerate Richard or not, and ordered the statue before deciding where to put it because they loved the work of the artist (who had produced an earlier clay version). So they stuck it in a corner as a compromise. An often quoted passage on Richard comes from William Stubbs, who offers a damning picture of Richard that was pretty common for its day: 

> He was a bad king: his great exploits, his military skill, his splendour and extravagance, his poetical tastes, his adventurous spirit, do not serve to cloak his entire want of sympathy, or even consideration, for his people. He was no Englishman, but it does not follow that he gave to Normandy, Anjou, or Aquitaine the love or care that he denied to his kingdom. His ambition was that of a mere warrior: he would fight for anything whatever, but he would sell everything that was worth fighting for.

So he has not always been lionised, and u/CoeurdeLionne goes into more detail on this in his excellent answer below. I'd also recommend John Gillingham's biography of Richard for how his reputation has fluctuated.

Sources/Further Reading

Gillingham, John. *Richard I*. Yale University Press, 2002.

Markowski, Michael. ""Richard Lionheart: bad king, bad crusader?."" *Journal of medieval history* 23.4 (1997): 351-365.",0
"From the outset I will admit that I cannot answer the question of whether or not this quote is true. (EDIT: please see the brilliant comment above from u/Takeoffdpantsnjaket for this answer). I've seen it before and the fact it's just resurfacing now makes me question its accuracy. However, I am a drug policy and human rights researcher and part of my MA thesis was about the racist origins of drug prohibition in the United States. Perhaps this could give some context into why many people find it easy to accept this quote is real.

The criminalization of drug use in the US has its genesis in the late 19th century, when xenophobia against Chinese immigrants was rampant among white Americans on the West Coast. Even after the Chinese Exclusion Act of 1882 effectively shut down immigration, those who were already settled in the West Coast were viewed as threats by white communities. These fears were fueled in part by unproven rumors Chinese men were using opium to lure white women into sexual slavery.  These rumors fueled public calls for cities such as San Francisco to respond by criminalizing the smoking of opium. In 1909, US Congress would adopt the same policy on a federal scale after passing the Anti-Opium Act. Although opium was used for medical and sometimes even recreational purposes throughout American society (particularly among women), it was primarily consumed through injecting or drinking tinctures rather than smoking, which was a means of consumption more popular and thus associated with Chinese immigrants. By criminalizing only the smoking of opium, the law effectively targeted Chinese immigrants by giving law enforcement officials justification for arresting, detaining, and deporting members of their community.

A similar approach would be used against African Americans when the Harrison Narcotics Tax Act of 1914 prohibited cocaine use. Myths that cocaine was fueling violent behavior among Black people were fueled by sensationalists newspaper articles, such as a 1914 piece by *The New York Times* titled “Negro Cocaine Fiends are a New Southern Menace: Murder and Insanity Increasing Among Lower Class Blacks Because They Have Taken to Sniffing”. Headlines such as these coming from popular newspapers reinforced the negative stereotypes whites held about the Black community and resulted in demands for the federal government to do something. Thus, Congress once again made a decision to criminalize a drug on the basis of white xenophobia against a minority group.

The United States continued to use drug prohibition to enforce harsh laws on minority groups in 1930 when the newly formed United States Narcotics Bureau appointed Harry Anslinger as its first commissioner, who immediately started a media campaign seeking the prohibition of marijuana. Just as opium was associated with violent Chinese behavior, and cocaine was associated with violent African American behavior, marijuana at the time was associated with violent behavior among Mexican and Latin American immigrants. Increasingly high racial tensions in border towns led to sensationalist claims by law enforcement about how marijuana promotes lawless behavior among Mexicans. Anslinger, backed by law enforcement, sought to use these fears to argue in favor of marijuana prohibition. Enlisting the help of newspaper mogul and anti-Mexican advocate William Randolph Hearst, he started spreading false myths that marijuana promoted interracial marriage, caused white women to behave provocatively, and fueled Mexican violence. This led to another public outcry, and eventually Congress officially prohibited marijuana with the Marijuana Tax Act of 1937, once again legislating based on unsubstantiated claims stemming from racial xenophobia.

In 1968 the United States ratified the UN Single Convention on Narcotic Drugs, and followed up by passing the Controlled Substances Act of 1970, which established the drug scheduling system we still implement today. By the time Nixon officially declared the “War on Drugs” in 1971, drug prohibition was widely accepted in American society and its racist origins nearly forgotten in the public eye. Nixon was instrumental in turning the public rhetoric around drug laws away from controlling the behavior of minority groups towards protecting national security and the rule of law, famously calling drug abuse “public enemy number one.” This allowed millions of dollars to be allocated to law enforcement for the purpose of upholding laws rooted in racist fears without directly evoking the xenophobic rhetoric. White communities eagerly bought into the idea that militarized enforcement of drug criminalization was necessary, in part due to reports of US soldiers becoming addicted to drugs such as heroin in Vietnam. Additionally, rumors continued to persist about drug use leading to violent behavior among Black and other minority groups, especially throughout the Civil Rights Movement. The origins of drug prohibition were unimportant to the masses, but many suspect the Nixon Administration knew exactly what they were doing by passing policies which would strengthen law enforcement’s ability to legally harass Black and other minority communities.

I'll end there because I think there's plenty written on how the War on Drugs fuels systemic racism and militarized policing, and admittedly I have not done enough research on Nixon himself to provide sourced information about why he might consider Black Americans ""enemies"" as Ehrlichman described.

TL;DR: Drug prohibition in the US originated as a means to control and discriminate against minority communities by criminalizing certain behaviors over others. While it's difficult to say whether Ehrlichman's quote is accurate, the history and current awareness about the racial disparities fueled by the War on Drugs can make it easy for people who read the quote to accept it as accurate.

&#x200B;

Sources:

Redford, Audrey, and Benjamin Powell. ""Dynamics of Intervention in the War on Drugs: The Buildup to the Harrison Act of 1914."" *The Independent Review* 20, no. 4 (2016)

Block, Frederic. *Disrobed: An inside Look at the Life and Work of a Federal Trial Judge*. Eagan, MN: West, 2012.

Herer, Jack, Leslie Cabarga, and Todd McCormick. *Jack Herers The Emperor Wears No Clothes*. Austin, TX: Ah Ha Pub., 2010.

Niesen, Molly. ""Public Enemy Number One: The US Advertising Councils First Drug Abuse Prevention Campaign."" *Substance Use & Misuse* 46, no. 7 (May 06, 2011):",0
"Oh, sex, of course. All the sex. For fun and for profit.

The attitude of the public and the authorities wavered across time and space on the condemnation/permission spectrum. In Christian Europe, it could mean outright closure of all public baths on the assumption that they were all fronts for sex work. Or it could mean that one of the most famous stories about purgatory is about a dead bishop who haunts a bathhouse--a ghostly attendant handing out towels--because, as Bonaventure interprets it in the 13th century, people are punished ""where they have sinned.""

[Here's a good vision of a public bath](https://i.ibb.co/ScM0L7g/medieval-baths-master-burgundy.jpg) from a 15th century Dutch/Flemish artist (known as the Master of Anthony of Burgundy because he worked for...right). Note especially the leering ruler outside...",0
"""There...in the palace of greedy Pluto, the savage dog \[Cerberus\] frightens the shades; tossing back and forth his triple heads, with terrible baying he guards the realm. Around his head, foul with corruption, serpents lap; his shaggy mane bristles with vipers, and in his twisted tail a long snake hisses. His rage matches his shape. Soon as he feels the stir of feet he raises his head, rough with darting snakes, and with ears erect catches at the sound..."" (Seneca, *Hercules Furens*, 782f)

That is not a dog you want to mess with - nor, in fact, a dog at all. Cerberus was a monster who just happened to be dog-shaped. As such, he was never associated in art or literature with any particular ancient breed. He tended, however, to be represented more or less as a Molossian hound, the classical world's default guard dog.

Molossians were huge, deep-chested mastiffs (you can get [an idea of what they looked like from this](https://research.britishmuseum.org/research/collection_online/collection_object_details/collection_image_gallery.aspx?assetId=365531001&objectId=467443&partId=1) famous sculpture). Although their name referred to the tribe in northwestern Greece said to have first bred them, Molossians weren't anything like a unitary breed; the name was probably applied to any suitably large and ferocious dog.

Molossians were originally bred to hunt boar and guard flocks. In the Roman era, however - besides their occasional service pulling chariots in novelty races - they were famous as guard dogs. In Petronius' *Satyricon*, the vulgar freedman Trimalchio has an enormous Molossian guard dog named Scylax (""Puppy""), which terrifies the protagonist. Not all guard dogs were Molossians; the unfortunate guard dog [buried in Pompeii](https://commons.wikimedia.org/wiki/File:Dog_in_chains_cast_Pompeii_Museum_Boscoreale.jpg), for example, was smaller and leaner, as was the fierce critter represented in the famous [""Cave Canem"" (Beware of dog) mosaic](https://en.wikipedia.org/wiki/Beware_of_the_dog#/media/File:Dom_dramaturga.jpg) in the same city.

Several other ""breeds"" are well-attested. The so-called Indian hounds, valued by the wealthy for their hunting prowess, were thought to have tiger blood (this, it was thought, explained their rarity; the tigers supposedly ate the dogs as often as they mated with them). Laconian (Spartan) hounds (which probably [looked like this](https://www.pinterest.com.mx/pin/398005685791717803/) Roman pair) were famously swift, and were used to run down hares and deer until the mid-imperial era, when they were replaced by the Gallic vertragus, an ancestor of the modern greyhound.

The favorite pet dog, however, was the thoroughly ridiculous ""Maltese."" In the Roman world, this was the stereotypical pet of the rich (and especially rich old widows). The poet Martial wrote a famous poem about a Maltese named Issa (""Missy,"") and several wealthy owners immortalized their lapdogs with miniature tombs. There's an especially interesting example in Athens, complete  with a life-sized sculpture of a [rather overfed little dog](http://www.ekathimerini.com/231352/gallery/ekathimerini/in-images/roman-era-sarcophagus-with-dog-sculpture-goes-on-display-in-athens) (in this case, not a standard ""Maltese"") with a jeweled collar.

Since no variety of Greek or Roman dog was as unitary as a modern, pedigreed breed, it is hard to find a modern equivalent for any given type. Molossian hounds were probably the size of large modern mastiffs; my mental image of them resembles the modern Turkish Kangal (likely because so many of those have chased me over the years). Laconian hounds seem to have looked like whippets. ""Maltese"" lapdogs apparently resembled modern Pomeranians more than the modern Maltese dog.

Whether Maltese or Molossian, the Greeks and Romans associated none of these dogs with wolves, as I discuss in [this older answer](https://www.reddit.com/r/AskHistorians/comments/d2n3xn/did_the_ancient_greeks_understand_that_dogs_were/ezwblq0?utm_source=share&utm_medium=web2x). And Cerberus, with his terrible baying heads and venomous tail, was identified with no earthly creature; his pedigree, after all, was pure monster.",0
"Hello everyone, 

In this thread, there have been a large number of incorrect, speculative, or otherwise disallowed comments, including many asking about the deleted comments, which merely compounds the issue. As such, they were removed by the mod-team. Please, before you attempt answer the question, keep in mind [our rules](http://www.reddit.com/r/AskHistorians/wiki/rules) concerning in-depth and comprehensive responses. Answers that do not meet the standards we ask for will be removed. 

Now, I know that people get curious about what the removed comments say, and especially in a thread of this size, there is simply no way that we can provide removal notices to everyone as it would be incredibly cluttered. But here is a run down for those wondering:

* Of 243 comments, 33 of them (not counting this one) are Top Level responses, *32* of which have been removed.

* [**1**](https://www.reddit.com/r/AskHistorians/comments/4f91zc/jrr_tolkein_singlehandedly_created_our_modern/d26x8ya) response is a reasonably substantive answer, [from an accomplished author in the field, I would add, if you want something more in-depth.](https://www.goodreads.com/author/show/188824.Ronald_M_James)

* 13 responses were removed because they consisted of no more than 3 actual substantive sentences.

* 3 responses were removed for being entirely speculative.

* 5 responses were removed because they were asking why all the responses were removed.

* 7 responses were removed for being composed mostly or entirely of a link or quoted block of text without contextualization.

* *2* responses were removed because, while sourced, they were reliant on published material from the game Dungeons and Dragons as their source, which does not meet the standard or academic literature.

* 2 responses were removed for making jokes.

I digress though. I know people love to talk about META concerns, but if you're one of them, it is unfair to the OP to further derail this thread with off topic conversation, so if anyone has further questions or concerns, I would ask that they be directed to [modmail](http://www.reddit.com/message/compose?to=%2Fr%2FAskHistorians&subject=Question%20Regarding%20Rules), or a [META thread](http://www.reddit.com/r/AskHistorians/submit?selftext=true&title=[META]), where the conversation may continue. Thank you!",0
"The charge that Islam was an import to Africa did show up among Nation of Islam’s (NOI) critics. Kwame Ture (at the time known as Stokely Carmichael) of the Black Panthers charged that “Islam is not an African religion,” and argued Islamic expansion was linked to the slave trade. NOI members rebutted this in different ways. The most common way was to argue on the basis of NOI’s cosmology that Islam actually predated Muhammad and was connected with the creation of the Black people.

**Nation of Islam’s Creation Story**

The Nation of Islam’s (NOI) version of Islamic history differs quite dramatically from most historians' or popular understandings of Islamic history. NOI understood Islam to be the original religion of humanity from the beginning of time 66 trillion years ago and original humans to be Black. According to NOI’s understanding, Black people had brought Islam to the Middle East, not the other way around.

According to the teachings of Elijah Mohammad, the founder of NOI, the first people were Arabic speaking Black Muslims of the Tribe of Shabazz. Earth was originally a utopia governed by a council of 24 Black Muslim scientists. This paradise was lost when an evil scientist named Yacub created white people out of experiments on the Black race. White people were understood to be devils. Islam’s origins according to NOI were not actually in the Middle East or connected with Muhammad.

**Defending Islam as a Black religion**

In 1971 Edward L.X. Truitt spoke out against Carmichael’s claim that Islam was not an African religion by contending that “Islam is the religion of the Original people, who are Black, brown, red, and yellow descendants of the original Black Man, the maker and owner of planet earth.” Thus, Islam was actually the creation of Black people. Embracing Islam for Black people was seen as a restoration of a lost heritage.

Other NOI members argued that the any effort to portray Islam as foreign to Africa was a plot to undermine NOI and Elijah Muhammad. Diogenes X. Grassal, for example, denounced any contemporary focus on African culture as a plan by white people to undermine Black men. According to Grassal, Yoruba and Swahili were inferior languages to Arabic, and Islam was the source of all Black achievement in Egypt, Arabia, Asia, and West Africa. Charles 67X denounced Black nationalists like Amiri Baraka for contending Arab Muslims had harmed Africa. 67X maintained that Islam had brought advanced civilization to Africa. NOI members often took harshly critical views towards Africa, which they believed to be primitive without the teachings of Islam.

**NOI and Moorish Science**

I have previously written on [AskHistorians about the NOI’s roots in the earlier Moorish Science Temple](https://www.reddit.com/r/AskHistorians/comments/hnois4/is_there_any_continuity_between_west_african/), an Islamic movement which appeared in Chicago after World War I. Like the later NOI movement, Moorish Science, was not grounded in traditional Sunni or Shia understandings of Islam or deeply concerned with the historical particulars of Islam, Muhammad, or early Islamic history in the Middle East. Both movements instead claimed that Islam was racially connected with Black Americans.

**Summary**

NOI’s understanding of Islam’s origins, teachings, and spread, had little to do with secular or mainstream Islamic understandings of history. For NOI, Islam was not a Middle Eastern religion, it was a Black religion, which had been associated with Black people since human beings had existed. Debates about Islamic expansion and involvement in the slave trade did not matter to NOI’s sense of sacred history.

Sources consulted:

Curtis IV, Edward. *Black Muslim Religion in the Nation of Islam, 1960-1975* Chapel Hill, NC: University of North Carolina Press, 2006, particularly 89-91.

Gardell, Mattias. *In the Name of Elijah Muhammad: Louis Farrakhan and the Nation of Islam*. Durham, NC: Duke University Press, 1996.

GhaneaBassiri, Kambiz. *A History of Islam in America: From the New World to the New World Order*. New York: Cambridge University Press, 2010.

Marable, Manning. *Malcolm X: A Life of Reinvention*. New York: Viking, 2011.

Note: Fixed a few spelling and technical errors.",0
"Hamilton was from Nevis, a small anglophone island in the Caribbean, and not from Puerto Rico. He has no connection to Puerto Rico. 

I’m also not at all an expert on this, but to what extent can you consider Hamilton an immigrant when he migrated between British colonies? If I move from the US Virgin Islands to Samoa, am I an immigrant? These are very culturally distinct places, but they’re both American colonial holdings. The same would be true for Hamilton moving from the British colony of Nevis to the British colony of New York. ",0
"The early Mormon Church invested heavily in its proselytizing efforts and found its greatest success with missions to the United Kingdom. New converts were encouraged to take the arduous trip to America and across the plains to Utah as soon as they were able. Even now, descendants of immigrants from the British Isles form the largest source of the populations in Utah and Idaho .

As a consequence of these efforts, the British experience of Mormonism, which Conan Doyle might, in part, have been reacting to, was of young missionaries preaching to people who would subsequently disappear (to the U.S.) forever. Mormonism was commonly considered a cult and, of course, their practice of polygamy was well-known and widely denounced in Victorian England. Mormon missionaries were sometimes portrayed in the press and early films as Svengalis, who would mesmerize and brainwash their targets (typically young women).

Conan Doyle was known to be an avid reader and historians have suggested he based his opinions of Mormonism on several books published in England, including two by defectors from, and critics of the practice of polygamy: Fannie Stenhouse and Anna Eliza Young (one of the 55 wives of Brigham Young). These books describe a regime that was, in fact, seriously oppressive toward women. Other books commonly cited among his influences were authored by William Hickman, William Jarman and John Hyde.

William ""Wild Bill"" Hickman was a member of the Church, who claimed to have murdered several people at Brigham Young's direction. These included an extermination order against the Timpanogos tribe of Utah. He was later excommunicated when, according to his account, he refused to assassinate someone else at Young's order. Hickman then wrote a book admitting numerous murders, which was published under the title, *Brigham's Destroying Angel: being the life, confession, and startling disclosures of the notorious Bill Hickman, the Danite chief of Utah*.

Conan Doyle's first Holmes novel was clearly inspired, in part, by the Danites, a mysterious fraternal organization of Church members. The Danites were organized as a vigilante group to fight in the 1838 Mormon War, and were sometimes referred to as *Destroying Angels*. There is little evidence of the group's continued existence after 1838, but they became the subject of myth both in and outside the Church. Named for one of the twelve tribes of Israel, the Danites were supposed to a cabal of Mormon leaders, who acted as secret enforcers for the Church. Brigham Young repeatedly denied their existence, as he did in June 1857, when he said publicly: ""\[people claim that the Danites\] are in every town and city throughout the whole of the United States, and that their object is not known by the people. That they are all over the world; that there are thousands of them, and that the life of every officer that comes here is in the hands of the Danites. That even the President of the United States is not safe, for at one wink from Brigham the Danites will be upon him and kill him...It is all a pack of nonsense, the whole of it."" Perhaps not surprisingly, such denials were not universally successful in suppressing the rumors.

Conan Doyle was swept up in and convinced by some strange, fad beliefs, such as spiritualism and the existence of fairies. In any case, he wrote *A Study in Scarlet* in three weeks, while he was also a practicing physician, suggesting it was not a heavily researched work, but sprang from his impressions of the Church formed from rumor and whatever prior reading he had done.

Conan Doyle defended his work against a backlash from Mormons by saying the kind of events he described (murder, kidnapping, secret surveillance...) were, ""a matter of historical record,"" though he admitted the descriptions were, ""lurid."" His daughter would later say, ""You know, father would be the first to admit that his first Sherlock Holmes novel was full of errors about the Mormons.""

To conclude, and to answer your questions, I wouldn't say Conan Doyle had weird prejudices, but he was reflecting the unsorted mix of prejudice, myth and fact available to him at the time. Again, though clearly highly intelligent, he could be overly credulous at times. He was also a fiction writer, more concerned with delivering a sensational story than carefully recounting facts.",0
"Fair warning, I'm not an expert on the subject by any means, however I found this interesting book titled ""The Secret Service: The Hidden History of an Enigmatic Agency"" by Philip H. Melanson (available partially for free on [google books](https://books.google.com/books?id=KMysiHydkQ0C&lpg=PA24&ots=6xzyTPcCCQ&dq=William%20P.%20Hazen&pg=PP1#v=onepage&q&f=false)).

Also, however well known this is I'm going to say it anyway: the Secret Service was originally created to combat currency counterfeiting. It was an arm of the US Treasury until 2003 when it was transferred to the newly created Department of Homeland Security.

Apparently it was born from an unofficial spy network of the Union during the Civil War. Keep that in mind that it was a band of skilled investigators who offered their services to the government. After the war, they were set to the task of finding the source of fake money that accounted for almost 1/3 of all currency in circulation at the time. It was a huge issue.

In the case of Lincoln, the Service didn't officially exist yet. Lincoln was on the verge of authorizing their official creation by the Treasury when he was assassinated. Since counterfeiting was still a major problem, the agency was created by Johnson instead.

The first director's story is mildly interesting, but the second director, Hiram C. Whitley, established all kinds of procedures and norms for the agency and really transformed it into an elite investigative force. His hiring standards had a lot to do with that. They got so good at it that congress started giving them more and more mandates, namely ""detecting persons perpetrating frauds against the government."" That varied from veteren pension fraud to smuggling to illegal voting schemes. They even led a crusade against the KKK in the 1870s.

A scandal caused major issues for the agency, so their mission and influence was shrunk quite a bit by congress back down to currency crimes, but they eventually regained some respect by 1890. This was in the period when Garfield was assassinated (1881), so that's why the SS didn't get assigned to protection at that time.

In 1894 however, they were investigating a counterfeit ring when they caught wind the ring was making threats on President Cleveland's life. Of his own accord, the current director of the SS assigned an agent to guard the President. Even though it was outside their jurisdiction, the White House didn't oppose the measure, though it was kept secret from congress and the Treasury.

Eventually, they were found out and the SS director was demoted for what was seen as blatant misuse of budget. Their mission wasn't to protect the president, so using funds for that purpose was not taken kindly.

Then the Spanish-American War broke out and congress immediately backpedaled and wanted presidential guards. They gave the SS a temporary war-fund for that purpose in 1898 to guard President McKinley as well as a new mandate to work counterintelligence for the war.

When the war ended and the fund ran dry, the director basically ignored that they were supposed to stop and kept at it, once again outside agency authority. Despite this, McKinley was still assassinated.

After that, however, the SS wasn't stopped from guarding the president. Oh sure, various people tried, including congress officially forbidding them from doing it, but they still did anyway. Finally, in 1906 a bill was passed that actually gave them funds for the purpose.

The book puts forth that congress disliked the idea of a presidential guard because it reeked of monarchy and treating the president like a king. They were toying with the idea of a presidential guard following McKinley's death, and some pushed for that duty to be given to the US Army, but it was all shot down on the basis of not-being-a-monarchy.

And all the while the SS ate the budget cost and did it anyway. They basically got the duty because no one could physically stop them from doing it.",0
"It's an interesting question that almost doesn't seem to fit this subreddit. It's as much a question of linguistics and etymology as it is of history, since it makes some presumptions that relate to modern perception of surnames.

**What I want to do in this comment is to cover this kind of *non-historical issues* with the examples you cited — not the history of Russian surnames proper.**

It seems that definitions you cite often stretch the meanings quite a bit to fit the ""insulting"" angle. The article at the link seems to be more for fun (Russia Beyond is an entertainment blog made to ""represent"" Russia in the West and create a more playful and attractive image of its culture, created and run by a Russian state information agency). Many of the etymologies in it seem to be taken from this book, *Russian Surnames: A Popular Etymological Dictionary* ([online searchable version](https://gufo.me/dict/surnames_ru#)) by [Yuri Fedosyuk](https://ru.wikipedia.org/wiki/%D0%A4%D0%B5%D0%B4%D0%BE%D1%81%D1%8E%D0%BA,_%D0%AE%D1%80%D0%B8%D0%B9_%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80%D0%BE%D0%B2%D0%B8%D1%87). Like similar articles on entertainment websites, the article plays a bit loose with its facts, and simplifies things for effect; its theme is ""surprising truths about famous Russian names"", so it chooses good-sounding bits from the dictionary and leaves the rest out. 

I also think there are a few problems with the premise of ""so many insulting Russian names"". First of all, even if we suppose names you listed are actually insulting or negative, there are a vast number of other, neutral or positive Russian names/surnames (from the number of non-church names — not Greek- or Bible-derived). Further, regarding the later stigma or perception of these ""insulting"" names: A) many of these names do NOT read/sound as their purported origin meanings in modern language; B) some of these etymologies may be a stretch, as these things tend to be, and then further simplified by the article's author; and C) even clearly word-derived surnames (Durov, Kozlov) tend to take on their own existence, and people do not take them literally, completely separating the surname and the underlying word (also, unglamorous meanings are not exclusive to Russian names).

For example, TolstOy does not directly read as literally fat/thick in the modern language (and in Tolstoy's time's language as wel) — it's tOlstyi (even though the underlying meaning is easy to see, the stress and one vowel are different). His distant ancestor very probably has been nicknamed ""fat"" (although it would have been a compliment in that era), but in count Leo's time, Tolstoys was an ancient, prolific, and illustrious aristocratic family, so the name stood perfectly on its own.

Chekhov may be connected to sneezing, but it does not sound it. Fedosyuk's dictionary states that ""Chokh"" or ""Chekh"" is an ancient name derived from the verb ""chikhat'"", to sneeze (e. g. the cannon-maker Chokhov). But to a modern ear, the surname literally means ""of Czechs"", the descendant of a Czech. It wouldn't sound like the verb to sneeze to Anton Chekhov's contemporaries as well.

Similarly, Sholokhov means ""Sholokh's son"", because Sholokh also was an ancient name; with that name in turn derived from an old word, presumably for roughness. Same with Bulgakov < Bulgak. In any case, it's impossible today to perceive his surname as related to smallpox or to roughness (sherokhovatost', shershavost'). At most, the non-word ""sholokh"" sounds vaguely alike to ""shorokh"", a quiet rustle. Same with ""bulga"".

Nabokov may have been derived from an ancient nickname meaning ""lopsided"" or ""limping on one side"" (nabokiy), but it does not read so now. If you consciously decide to read it literally, it sounds similar to adverb ""na boku"", something lying on its side. In any case, this was again an old aristocratic surname which stood on its own for centuries. Nabokov himself said (tongue-in-cheek, precisely because this is impossible to verify) that the version his genealogy-loving uncle preferred was that the name derived from some minor Tatar prince named Nabok. 

Bunin is linked by the dictionary to two words: an old slang word ""bunya"", and (noted as more likely) to the antiquated/regional verb ""bunit'"", to roar, bellow. But this is, *again*, an old aristocratic line, whose official lore says that it derived from a Polish officer Bonikowski (transformed to Bunkevsky) who served prince Vasiliy II in the XV century. Whether this is true or not, ""Bunin"" or ""bunya"" do not sound like any meaningful words now.

An important exception is Gorky (his is very clearly an intact modern word meaning ""bitter""): it is a pseudonym, a pen name, chosen specifically to reflect the bitter truths and realistic images of real life that the writer strived to depict. 

But the more general question is whether unglamorous surnames are limited to Russian language. Surnames that reference (or *seem* to reference, such as Paine, which is unconnected to ""pain"") unglamorous concepts or objects are often heard in other language names as well; most of them don't read as direct descriptions of their bearers, or even can't be understood at all (being archaic or transformed). You don't think a man called Poore is poor, Petty is petty, or Black is swarthy. Similarly, you don't expect Hale to be especially healthy, and Osborne to be divine or bearlike. Whether surnames that resemble animals are negative or insulting is also an open question (is Horse or Hound negative? is Roach or Beetle? (even though not Roach nor Beetle are even derived from insects!..)).

Clearly meaningful names like Komarov (mosquito), Tarakanov (cockroach), Kozlov (goat), Durov (fool, *durak/dura*), and even Smertin (death) do not jar the listener and do not strongly invoke the word because they're surnames. E. g. there are at least three famous Durovs known to most Russians (a beloved Soviet actor, a tech mogul, and the founder of the famous puppet theatre), and no one ever thought to imply that they are stupid. Surnames can indeed sound mildly ironic (one of the most famous Russian tenors ever was named Kozlov), but otherwise not be perceived jarring at all. That is to say, to raise the problem of ""insulting"" names in Russian, they first have to be actually insulting.

Finally, regarding your question about warding the spirits: the Fedosyuk's dictionary in fact says that the first name Durak (lit. fool) which was widespread in XV-XVII centuries, was indeed one from a wide variety of ""warding"" names to protect from the thing named, or other dangers. It notes as an example a dyak (a high government official in pre-Peter Russia) signing all of his papers as Durak Mishurin.

**The actual history of surnames in Russia is outside my expertise**. Although your additional questions seem to be on point: peasants were indeed introduced very lately to surnames (instead of only patronyms) compared to nobility that had that distinction, and there was also a rich tradition of artificial surnames in church education.",0
"To my knowledge, no. He nominally opposed the resolution. However, he also did little to work *against* it. Some speculate that this is because of his history as a Nazi; others, that this history’s more gruesome details were being kept under wraps by the KGB in return for his acquiescence and/or silence with regards to Soviet proposals at the UN.

I’m not sure the truth of that matter, though I find the blackmail proposition to be a bit fantastical. Still, that particular question is not my area of expertise, so I cannot say for sure. But he certainly did not make public statements that I’ve managed to find supporting the resolution. When it passed, his statement was lukewarm, referencing divisions and the need for peace in the Middle East via a solution to the conflicts there. Certainly Israel did not seem to blame him for passage outright; in 1977, Waldheim met with newly elected Israeli Prime Minister Menachem Begin, and during the press conference after, Begin referenced bringing up the resolution but said he “understood” that Waldheim was in a difficult position with the majority in the General Assembly. That is not to say the two were friendly, but that Begin did not choose to blame Waldheim when given the chance, or pressure him to fight for the resolution’s repeal, and met with him cordially is a sign in and of itself.",0
"> When we discuss impeachment today it's often called a political process. Is this a relatively new view?

From the US Constitution, article 2, section 4:

> The President, Vice President and all civil Officers of the United States, shall be removed from Office on Impeachment for, and Conviction of, Treason, Bribery, or other high Crimes and Misdemeanors.

That leads to a lot of confusion because ""high Crimes and Misdemeanors"" is a term of art that doesn't only mean crime as in violation of a criminal statute. The first federal official to be impeached was judge who habitually showed up in court drunk. There's no doubt that the framers of the Constitution understood that term of art because it had been used in England since at least the 17th century. There's also various commentary from the time of the debate about ratifying the Constitution that shows people understood that term of art.",0
"Absolutely! And I know that I've actually answered this question before, but can't seem to find the link.

The easy answer is yes, they definitely did.  Alaskan Eskimos in fact remained aware of the other side of the Berring Strait - i.e. it was a body of water that they crossed at times, the same would be true of Inuit living at the Eastern end of the Arctic - Greenland was definitely a known place ""across"" the Atlantic from them.

I don't know much about Eastern North America, or much about South and Central American Indigenous histories (besides bits and pieces like Viracocha leaving and crossing the Pacific), but I do know that in the Pacific Northwest there are stories of crossing the Pacific, circling the Pacific, or of people coming from across the Pacific.

Part of many origin stories for different lineages begin with a journey to the lands of the copper, abalone, and so on. These aren't always in the same place, sometimes the copper is in the far north, other times it is across the sea, or even to the East.  In the Nuxalk story of the Salmon boy there is a good example of at least one of these stories.  The Salmon boy and his brother travel to the west, visiting in turn various lands of the fish people which are located out across the open sea, where the salmon people live, who wear salmon skin clothing (McIlwraith ""The Bella Coola Indians"" and oral stories).  This always makes me think of the Ainu, indigenous Japanese who did wear salmon-skin clothing, but the chances are that this story has more to do with the order of fish arriving in the spring.

Then there are other stories that are more historical. There's a story recorded by Cliff Kopas (also a story about the Bella Coola Valley) where it talks of canoes who arrive in the valley, with people in them who wear grass clothing.  They stay for several years, then return to where they came from.

I have several personal accounts of Nuxalk people who say that they have met Pacific Islanders who have the opposite story in their lineages, but have yet to find an independent source for it, but between this and the theories of Thor Heyerdahl (who spent time in Bella Coola during the war) regarding connections between the area and New Zealand, the nation here has developed quite a strong relationship with several communities in New Zealand, resulting in a number of marriages and large group trips.",0
"The scarring was apparently caused by very frequent sexual congress, and was confined to the pelvic region. You can read more about the skeletons here:

https://www.google.com/books/edition/The_Natural_History_of_Pompeii/3xfjyTqqR7IC?hl=en&gbpv=1&bsq=prostitute",0
"As someone who's written a dissertation and book on Nazi political mobilization, including their culture of speeches, marches, and rallies, I too am skeptical of the linked article. I'm not familiar with the author, but he seems to not be a historian and instead is writing from a sort of free speech absolutist position. Leaving aside the pros and cons of his efforts in that field, in this case it tends not to make for good historical writing. It's the type of history that goes about looting the past for incidents to support a prior ideological position, rather than approach a case in a genuine effort to understand its nuances.

Take for example one of his supposedly damning points, that ""Hitler himself was banned from speaking in several German states from 1925 until 1927."" That's a very limited period if you're trying to make the case that Weimar had overly strong speech restrictions. If anything it makes the opposite case: here we have a foreign citizen convicted of inciting rebellion against the legitimate government, and two years later he is allowed to tour the country rallying his followers to further actions. That is not a strong speech law. 

As anyone can see, Hitler went on to speak prolifically during the ensuing and far more critical period of 1928-1933. And if we look past Hitler, and to the local level, Nazi pubs continued to host speeches throughout the period, at least weekly at any given tavern, and often more than once per week. Meeting halls rented themselves out to larger Nazi rallies with regularity. Street marches in uniform were under blanket ban for a short time (in the aftermath of the Hitler Putsch), but then allowed again. At times, specific rallies or speeches might be banned, but this was common on both sides of the political extremes, and usually linked to specific incidents of violence.

The author also cites the fact ""in a two year period, they shut down 99 \[Nazi newspapers\] in Prussia alone"". While granting that it's possible, I find this hard to believe as well. I reviewed their cited source -- Oron James Hale's ""The Captive Press in the Third Reich,"" a fantastic work on the internal Party politics and administration of its print media -- and I have not found the 99 bans figure. I could have missed it. But the overall content of that work heavily implies that the statistic is either false or misleading. Hale has a very useful chart (p59 if you have access to the book) listing the number of Nazi daily newspapers over the years. It is far fewer than you'd think. Only one in 1926, four in 1928, then a comparatively large expansion in the next two years took place as the Party focused its efforts in print media, which brought the total to 36 by 1931. In 1933 (the year of the takeover), they still had only 86 daily newspapers. So I just don't see how it's possible to get this figure of 99 newspapers banned in alone Prussia, when they had only 86 nationally. Is he including weekly papers? Perhaps, but those are still not numerous enough to get you to 99 in Prussia itself. 

And also let's note that the Prussian state government was by far the most activist and anti-fascist. As the largest state and the one most solidly under SPD control, it was Weimar's bastion until a state-level coup turned it over to the Nazi-sympathetic conservatives. So it's no coincidence that Prussia appears in this story, as the most aggressive anti-Nazi state government. But it would be misleading to think that all German states issued similar efforts. They did not. 

Back to the statistic itself, perhaps he's talking about the number of bans issued, rather than the number of papers banned. This could be possible. State-level Weimar governments could and did issue bans on newspapers, but these tended to take place only after specific incidents connected to political violence. As you can imagine, the Nazis fell afoul of this frequently, as did the Communists as well. For instance, a massive and deadly riot in Hamburg's Sternschanze neighborhood in 1930 led to bans on political uniforms and some newspapers on both right and left. In 1931 in Hamburg, a Nazi policeman shot and killed a Jewish police officer, and the local Nazi paper's rhetoric was so bloodthirsty that the state banned it.... for eight days. (See Andrew Wackerfuss, ""Stormtrooper Families,"" for both these cases.) 

So as we can see here, bans were almost always for a limited period just to let tempers cool and political heat to die down, and then the papers eventually (sometimes quickly!) would resume. In the major cities, which is what I'm most familiar with in my research, you'd get a ban maybe once or twice a year for a period of a few weeks. 

So that's a long way of saying that I find claims about the strength of Weimar speech bans less than credible. With both rallies and print media, the Nazi Party was left generally unmolested by the Weimar government, or more specifically by the component state level governments that had authority in this area. As with the Communists, bans took place in response to specific incidents, but rarely as a blanket or preemptive measure. If that had been the case, you would have seen bans before Nazi rallies and major events, such as the period leading up to Altona's Bloody Sunday, when the Nazi media was free to rally its members to essentially invade a town and cause another massive and fatal riot. (After which the conservatives used the deadly results to motivate the above-mentioned Prussian Coup, perhaps the Republic's fatal blow.)

Now, did the Nazis claim that they were the unique victims of the speech police? Yes. Did they publish cartoons like the muzzled Hitler? Yes. But it's hard to disentangle these propaganda efforts from the general realm of grievance politics that fascists practice. Nazis were well versed in claiming victimhood while also being the aggressors. Whether people believed it came down to whether they aligned with the Party for other reasons.",0
"AskHistorians is an actively-moderated subreddit. Answers which do not meet our standards and comments that violate our guidelines (including ""where are all the comments?"" and ""All mods suck duck farts"" etc.) are removed by the moderators. They still show up in the comment total, but you can fix that with the AskHistorians Browser Extension.",0
"**Part 1**

So, to answer this as best as possible, there is a bit of context necessary, especially in that both the Allied leadership and the public in Allied countries had been aware of the Holocaust occurring while it was going on and had already started to collect evidence, mainly for the purpose of trials already during the war.

[In this answer](https://www.reddit.com/r/AskHistorians/comments/6696bw/new_understanding_of_allies_knowledge_of_the/) I go into detail concerning Allied knowledge of the Holocaust but in summary, it is imperative to know that Allied government had been aware of the Einsatzgruppen shooting as soon as it started via decrypted documents, had already seen other decrypted documents that went into detail concerning the large scale murder of Jews, had produced aerial reconnaissance photos of the process of cremation going on in Auschwitz and so on. Crucial documentation of what was occurring also came from the governments in exile in London, namely the Polish government with members of the Polish resistance going do far as to infiltrate Auschwitz and smuggle reports to London as well as from the Soviets who held the first war crimes trial in December 1943 and had liberated the first major concentration camp Majdanek in June 1944.

Similarly, the press did report extensively on these matters, just not in places of prominence. The New York Times e.g. [published an article in 2001](http://www.nytimes.com/2001/11/14/news/150th-anniversary-1851-2001-turning-away-from-the-holocaust.html) admitting to its own failure to report more prominently on the Holocaust. They wrote:

> Why, then, were the terrifying tales almost hidden in the back pages? Like most -- though not all -- American media, and most of official Washington, The Times drowned its reports about the fate of Jews in the flood of wartime news. Its neglect was far from unique and its reach was not then fully national, but as the premier American source of wartime news, it surely influenced the judgment of other news purveyors.
>
> While a few publications -- newspapers like The Post (then liberal) and PM in New York and magazines like The Nation and The New Republic -- showed more conspicuous concern, The Times's coverage generally took the view that the atrocities inflicted upon Europe's Jews, while horrific, were not significantly different from those visited upon tens of millions of other war victims, nor more noteworthy.
>
> (...)
>
> Only once did The Times devote its lead editorial to the subject. That was on Dec. 2, 1942, after the State Department had unofficially confirmed to leading rabbis that two million Jews had already been slain and that five million more were indeed ''in danger of extermination.'' Even that editorial, however, retreated quickly from any show of special concern. Insisting in its title that Jews were merely ''The First to Suffer,'' it said the same fate awaited ''people of other faiths and of many races,'' including ''our own 'mongrel' nation'' and even Hitler's allies in Japan if he were to win the war.

Following the less than enthusiastic coverage of this topic, on March 9,1943, screenwriter and Zionist Ben Hecht staged the play *We Will Never Die* in Madison Square Garden in front of 40.000 people in order to raise awareness of the plight of European Jews and then further traveled around the US with it, even winning over Frank Sinatra to participate.

In Britain too – though complicated by British media laws – the public was aware of what was going on if they chose to read the newspapers. The Daily Telegraph reported in 1942 about [traveling gas chambers](https://i.guim.co.uk/img/static/sys-images/Guardian/Pix/pictures/2015/1/27/1422356585641/ab29aa28-7ed8-4c38-a45d-dc0b13072e01-bestSizeAvailable.png?w=300&q=55&auto=format&usm=12&fit=max&s=422223adda84a80fa704ab4e1d4c2ff8), which given that the Einsatzgruppen did indeed use gas vans is surprisingly accurate. Simon Leader's 2004 [PhD Thesis on the British regional press and the Holocaust](https://core.ac.uk/download/pdf/42016442.pdf) (pdf warning) shows that

> [British] newspapers were fully aware of the Nazis’ intention to murder all Jews under their control by December 1942. They all reported the events that came to be understood as the Holocaust, (some in extraordinary detail) but the Manchester Guardian stood apart because of the consistency of its coverage.

The reason, why Allied governments did act they way they did or did not act at all on this knowledge is varied and complicated but it is a fact that both an interested public and the governments of various Allied countries were very, very aware of what was occurring.

Nonetheless, the discovery of the camps and seeing the consequences of Nazi policy in the form of starved and beaten humans was a shock to those who experienced it and it was really only then – when confronted with the bloody consequences – that the troops liberating those camps as well as leadership such as Eisenhower became fully aware of the utmost criminal nature of the Nazi state.

Efforts to document these crimes had already been in place when the Americans liberated the first camps they came across. [As I detail in this answer](https://www.reddit.com/r/AskHistorians/comments/74zl84/why_did_nazis_try_to_destroy_their_concentration/), the Allies had since 1941 expressed their goal of putting the persons responsible for war crimes on trial and had since 1943 with the foundation of the United Nations Commission for the Investigation of War Crimes created a framework for the collection and documentation of these crimes.

It was this agency and its affiliated agencies within the US Army in form of the US Army's War Crimes Branch that contributed a lot to the documentation of Nazi crimes that was presented to the public almost immediately after Eisenhower and other Allied leaders had expressed their desire to do so. The US Army had a very infrastructure for producing war reports during WWII (think Robert Capa's famous photographs of D-Day or movies shot by famous directors during the invasion of Italy) and this infrastructure together with the war crimes investigators started producing material for presentation to Allied and Germany publics alike almost immediately.

Reporters from various outlets were invited to visit liberated camps almost immediately to write about what they witnessed. Heavy hitters from the press produced reports and newsreels on what they saw there, e.g. [Edward R. Murrows report on the liberation of Buchenwald](https://www.youtube.com/watch?v=d3SCSouI8WE) that was available to the Allied public shortly after the liberation of the camp. Even the Soviets followed a similar model with famous Soviet writers such as Vissily Grossmann writing the text [The Hell called Treblinka](https://www.facinghistory.org/holocaust-human-behavior/hell-of-treblinka-vasily-grossman). Famous movie directors were engaged to shot documentaries about the Holocaust: Resnais [Night and Fog](https://www.youtube.com/watch?v=gAakK4X2_0o) was a movie that used the footage of liberation shot immediately after the war. Alfred Hitchcock [shot a Holocaust documentary](https://www.theguardian.com/film/2015/jan/09/holocaust-film-too-shocking-to-show-night-will-fall-alfred-hitchcock) that was never shown but still shows the efforts that went into this documentation.

Similarly, the Allied commanders in Germany also wanted to confront the German public who largely claimed to have known nothing about this with the crimes of their regime: Germans from surrounding villages and cities were forced to bury the dead in camps and were regularly forced to watch movies about the Holocaust and Nazi war crimes by Allied commanders.

The largest efforts at documentation however were certainly the Allied post-war trials like the IMT in Nürnberg, the subsequent NMT trials, the Buchenwald trial by the British, the Dachau Trials by the Americans and so on and so forth. Nürnberg wasn't just to put the Nazi leadership on trial but also a conscious effort at documenting what the Nazis had wrought. The trial itself was filmed all the way, the documents and transcripts were published, media attention was enormous and even the selection of NMT trials was designed to highlight how every aspect of the Nazis had been criminal: Hence a doctor's trial, a jurist trial, a general's trial and so on and so forth.

So efforts to document and make that documentation available to the public immediately were made extensively and succeeded heavily in getting the information out there.
",0
"Summing up Caesar's position throughout the last 2,000 years is somewhat difficult to do in simple terms, because his significance to Western culture varies from era to era.

To begin with the more immediate reception of Caesar, that is in the immediate aftermath of his death, while his assassins would style themselves as liberators and freeing Rome from tyranny (Cicero goes so far as to call Caesar a tyrant and parricide of the fatherland, *De Officiis* 3.82f.), the people of Rome grieved and honoured Caesar as a god. This is reflected particularly in Octavian adopting the name Gaius Julius Caesar **Divi Filius** after Caesar's death and ""ascension"", and while he would go on to change his name again after assuming power, he kept the *Divi Filius* aspect of his name throughout.

The peoples' veneration of Caesar was exploited quite cleverly by the Triumvirs in the wake of their victory against Brutus et al. at the Battle of Philippi (though whether this was opportunistic reverence or genuine veneration of Caesar is perhaps up for debate); the fifth month of the year, Quintilis, was renamed Iulius (July), and Caesar was officially venerated as the Divine Julius, with Octavian even founding a Temple of Caesar.

Octavian continued to use his adoption by Caesar, and vengeance for Caesar's assassination, as justification for the moves he made in his career. His own legions were comprised of Caesar's veterans, and he continued to exploit Caesar's civic reputation to bolster his own. Even complaints about Octavian largely focussed on his reliance upon Caesar's memory.

It's worth noting, though, that once Octavian became Augustus, Caesar's significance dwindled somewhat under the cult of Augustus (though this is not to say that Caesar became objectively insignificant); Augustus came to rely on his own reputation rather than that of Caesar's before him. Augustan literature in particular came to downplay Caesar in favour of revering Augustus himself; Horace, for example, drew direct association between Caesar's triumvirate with Pompey and Crassus, and the origins of civil war (*Carmina*, 2.1.3f.), while Virgil's *Aeneid* drew attention to Caesar as a bellicose figure (see Anchises beseeching Caesar to lay down his arms, *Aeneid* 6.834f.) while depicting Caesar's bitter rival Cato as a lawgiver (*Aeneid* 8.670).

Caesar continued to be revered throughout Imperial history, albeit at varying degrees. The month of July continued to be held in his name, of course, and the title *Caesar* was held by Roman Emperors throughout the period. It's perhaps worth noting, though, the significance awarded to Augustus' position over Caesar's that after the Empire's split the title of *Caesar* was given to sub-emperors, while the emperors themselves held the title *Augustus*.

Imperial depictions of Caesar held him as the cure to the late Republic's numerous problems; he broke the cycle of aristocrats and do-nothings by installing a new sort of power. In short, he was seen as a turning point. Imperial historians, such as Velleius Paterculus and Appian, awarded Caesar perhaps disproportionate attention against later rulers; Cassius Dio devoted a tenth of his eighty books on Roman history to Caesar's career, despite this comprising only a short period of his work's 1,000-year coverage.

By the time of the later Roman empire, however, Caesar's centrality to Roman history had diminished, and his achievements were used largely as a comparison to the superiority of writers' contemporary rulers; Caesar was no longer the turning point of Roman history, as Christianisation had seen Rome's adoption of the faith as the seminal moment of history.

However, Caesar himself was not forgotten even after the fall of Rome; his name continued as a designation of the highest power, reflected in the Holy Roman Imperial title of ""Kaiser"" (and, later, in the Slavic title ""Czar/Tsar"").

Caesar, as founder of the fourth great empire of Daniel's Biblical prophecy (Daniel 2:40-43), became seen as an instrument in God's divine plan, and was thus depicted as a model of chivalric virtue and the ideal king, and was even held as a comparison for contemporary rulers to aspire to. Medieval English and French writers in particular praised Caesar for his chivalry, while German writers, who saw the Holy Roman Empire as the natural successor to Rome, held Caesar as the founder of their own empire, and depicted him as an ideal ruler from as early as the 11th century (see, for example, Archbishop Anno II of Cologne's *Annolied*). It's worth noting as well that Caesar continued to be held as a somewhat multifaceted hero, though; while the embodiment of chivalric virtue, he was also victim to his own hubris.

Perhaps the most notable depiction of Caesar aligning with medieval ideas of the man is Dante's *Inferno*; as an unbaptised soul (obviously), Caesar was condemned to Limbo but was held as the one of the virtuous there (*Inferno* 4.123), while his murderers Brutus and Cassius are condemned to Hell to suffer alongside even the likes of Judas (*Inferno* 34.61-67).

The Renaissance brought a rejuvenation in Caesarian reverence, particularly after the ""discovery"" that it was in fact Caesar himself who authored his *Commentarii*, which was held during the Renaissance as a key text in Latin education. The *Commentarii*, seen as a unique glimpse into pre-Roman Gaul, even developed into a key work in the study of Gallic history. It became a work transposed even into contemporary times; poet Giannantonio de Pandoni composed a prose piece that drew on Caesar's *Commentarii* as inspiration for depicting his own contemporary war between Venice and Milan (albeit without drawing direct comparison). Even Pope Pius II, who himself authored an autobiographical *Commentarii*, appropriated the Caesarian model by portraying himself rather explicitly in the cast of Caesar, associating his own struggles against the enemies of the Papal States with Caesar's struggles in his civil war. Writers like Petrarch and Machiavelli continued to draw on Caesar as a figure of contrasting reputation; an agent of hostility to the virtues of the Republican, but simultaneously a praiseworthy genius and chivalric ideal.

Of course, his significance is also represented in Shakespeare's own plays about Caesar and the aftermath of his death; Shakespeare's depiction of Caesar suggests that the people were familiar, or at least aware, of Caesar as a historical figure. At the very least, they will have been aware of him after the fact.

I will bring my answer to a close here by bringing us to Caesarian reception during the Enlightenment. In this period of philosophical advancement, Caesar was regarded as one of the emblems of absolute monarchy, representing the martial prowess and cultured education that was considered the ideal for an 18th-century ruler. He was seen, as well, as a somewhat benevolent despot; using his absolute authority to impose social reform on a nation very much in need of it. Voltaire in particularly, while acknowledging Caesar's failures, held Caesar as the ideal philosopher-king.

The Revolutions of the 18th century were themselves no stranger to drawing upon Caesar as a significant figure; albeit not in the positive light he'd been awarded during earlier periods. As a symbol of absolute power, he was course seen as contrary to the ideals of the French and American revolutionaries, particularly as a warning for the dangers of a demagogue rising to power in the face of populism. Alexander Hamilton's *Federalist Papers* in particular used Caesar as a shorthand for autocratic power, and Hamilton also called Thomas Jefferson ""Caesar"" in warning of his potential.

Aside from the primary sources cited above, I believe some helpful sources should include;

• Baehr 1998, *Caesar and the Fading of the Roman World: A Study in Republicanism and Caesarism*

• O'Brien 2009, 'Arms and Letters: Julius Caesar, the *Commentaries* of Pope Pius II, and the Politicisation of Papal Imagery', *Renaissance Quarterly* 62, 1057-1097

• Ramage 1985, 'Augustus' Treatment of Julius Caesar', *Historia* 34, 223-245

• Wyke 2006, *Julius Caesar in Western Culture*

EDIT: formatting",0
"Just to address this from a different perspective, ie why is the Holocaust considered unique, I will link to an answer by  u/commiespaceinvader to the question [""When people discuss the Holocaust, why do they focus mainly on the killing of the 6 million Jews?""](
https://www.reddit.com/r/AskHistorians/comments/564gvm/comment/d8g92dz/).

Specifically:

>""In short the Western imagination of itself had experienced atrocities and horrors inflicted against political opponents, ""deviants"", and colonial subjects but it had never experienced that all it used to define itself as good and progressive – the modern state and its bureaucracy, industry, science, the police – was used to murder an entire group of European peoples.""

It's old but I will also link to a [comment](https://www.reddit.com/r/AskHistorians/comments/1nj5uj/was_the_holocaust_unique/ccja56t/) by u/400-rabbits on Stannard, which adds a bit of complexity - there are multiple historic Holocausts and they are all unique, if interrelated.

One thing I will add in addition to those answers is that when one is comparing the Holocaust to Indigenous peoples of the Americas, you're dealing with vastly different things in terms of time and scale - it's not just a numbers game. When we are talking about indigenous peoples of the Americas, we are discussing hundreds of different groups, who had very different experiences at the hands of different actors. Which is to say it makes it hard to talk about a singular ""genocide"" - the answer from u/EdHistory101 specifically focuses on the United States government in the 19th and 20th centuries, but that is part of a much larger history of conflict and dispossession of indigenous peoples lasting from the 1490s to [literally this very moment](https://www.survivalinternational.org/brazilgenocide), involving a vast array of actors. To be more comparable you'd probably need to compare the entire history of European anti-semitism from 1492 to the present. 

The Holocaust itself is historically a much more concentrated event, involving one government as a prime mover (the NSDAP regime in Germany), which intentionally targeted Jews in Europe for industrialized mass killing, most of which took place over a three year period during the war (ETA a 2019 study found 25% of Holocaust victims were actually murdered in a [three month period](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6314819/)). Not only was this genocide extremely intentional and organized and planned to an exceptional level of detail, but Germany made it an overriding policy objective, even in its relations with friendly/allied countries - there were even low-level discussions between German and Japanese officials about the possibility of murdering the 20,000 Jewish refugees in Shanghai (the Japanese refused). 

However, even with that said I should point out via this [thread](https://www.reddit.com/r/AskHistorians/comments/pyuvqy/how_true_is_the_claim_that_the_united_states_of/) that there is an ongoing historic debate at the moment as to how much white settlement of the Americas directly inspired Nazi policies and goals in Eastern Europe.",0
This may be what you are looking for https://www.reddit.com/r/MapPorn/comments/d55ci0/map_of_vienna_19131914_showing_where_hitler/?utm_medium=android_app&utm_source=share,0
"We can further examine how height plays a factor by looking into Tang Chinese statues. Corresponding with Gandhara, China prior to the fifth century seemed to have stupas as a focus for worship and gathering, rather than statues. This is still a form of idolization, as these structures may contain the remains of certain notable individuals.

Here is a relevant section from the Mahāparinibbāṇa Sutta:

> *Then the Venerable Ananda said: ""But how, Lord, should they act respecting the body of the Tathagata?""*
> 
> *""After the same manner, Ananda, as towards the body of a universal monarch.""*
> 
> *""But how, Lord, do they act respecting the body of a universal monarch?""*
> 
> *""The body of a universal monarch, Ananda, is first wrapped round with new linen, and then with teased cotton wool, and so it is done up to five hundred layers of linen and five hundred of cotton wool. When that is done, the body of the universal monarch is placed in an iron oil vessel, which is enclosed in another iron vessel, a funeral pyre is built of all kinds of perfumed woods, and so the body of the universal monarch is burned; and at a crossroads a stupa is raised for the universal monarch. So it is done, Ananda, with the body of a universal monarch. And even, Ananda, as with the body of a universal monarch, so should it be done with the body of the Tathagata; and at a crossroads also a stupa should be raised for the Tathagata. And whosoever shall bring to that place garlands or incense or sandalpaste, or pay reverence, and whose mind becomes calm there — it will be to his well being and happiness for a long time.*
> 
> *""There are four persons, Ananda, who are worthy of a stupa. Who are those four? A Tathagata, an Arahant, a Fully Enlightened One is worthy of a stupa; so also is a Paccekabuddha, and a disciple of a Tathagata, and a universal monarch.*
> 
> *""And why, Ananda, is a Tathagata, an Arahant, a Fully Enlightened One worthy of a stupa? Because, Ananda, at the thought: 'This is the stupa of that Blessed One, Arahant, Fully Enlightened One!' the hearts of many people will be calmed and made happy; and so calmed and with their minds established in faith therein, at the breaking up of the body, after death, they will be reborn in a realm of heavenly happiness. And so also at the thought: 'This is the stupa of that Paccekabuddha!' or 'This is the stupa of a disciple of that Tathagata, Arahant, Fully Enlightened One!' or 'This is the stupa of that righteous monarch who ruled according to Dhamma!' — the hearts of many people are calmed and made happy; and so calmed and with their minds established in faith therein, at the breaking up of the body, after death, they will be reborn in a realm of heavenly happiness. And it is because of this, Ananda, that these four persons are worthy of a stupa.""*

However, the political and social turmoil between the Han and Sui dynasties meant Buddhism needed to adapt to the needs of the people. As an alien religion, this was especially the case. As the dharma offers salvation, addresses the nature of suffering, and captures the imagination with various magical and fantastical elements in deities and mythical figures, this could be readily accepted by the common masses, especially the lower classes effected by the hardships of war and fragmentation. But stone domes would not do, people needed some greater way to contextualize and visualize their practice. By creating human images of the Buddha, in the familiar architecture of palace-like temple buildings, the Chinese would be able to better comprehend and accept Buddhism on a conceptual level. This is the shift towards the Buddha hall, the place where the statue resides in the temples, as the focal point of temples and monasteries. This mirrors the palace of a king or emperor, and in a sense, the Buddha takes the central position as an almost divine leader. Furthermore, monasteries, pagodas, and the statues that they housed would also be objects of charitable donation, in other words, a sign of piety and servitude. This also corresponds with the creation of statues in the Theravadin tradition, where commissioning the creation of a statue is once again a part of merit exchange. Materials may vary from the availability of natural resources, the cost of materials and wealth of the donors, and the skills of the craftsmen. 

Now when one steps foot into the temples of imperial China, say Tang or later, the height of the statues would be immediately noticeable. It is recorded that Yongning Monastery in Luoyang housed a 21.5m/70.5ft tall Buddha statue, the tallest in the city. Imagine the image of the Buddha, set in an imperial city, where worshippers would come from across the realm. They would see the splendid and grand statue, and with the context of the imperial capital, would almost instill a heavenly environment that matched with the emperor's authority.

However, the height of the statues also means the onlooker would see certain things within the space of the temple. Depending on the height of the statue, the space of the room, the angle the viewer takes, and the position they are in in such a place, the view of the body and head changes dramatically. One might see the topknot of the Buddha from far away, but up close, they are staring up his nose. This suggests the importance of the Buddha as a higher, revered figure, a teacher that's teachings are of such value and significance, that they are all greater than life, higher and grander and mightier than the everyday masses. With light coming into the building, it would also cast onto the chest and face of the Buddha, further illuminating the Buddha in an awe-inspiring way.

The likeliness of Buddha statues would be further emphasized with the paintings of the eyes, a monumentous occasion that signified the completion of a statue and bringing it to life. In a sense these statues were not just representations of a Buddha or Bodhisattva, but rather embodying and becoming the real thing. There are even certain tales which say statues have come to life. For example, the supposed first-ever statue of the Buddha was commissioned by King Pasenadi of Kosa, who had traveled to meet the Buddha in person, only to find that he was absent that day. Upon the Buddha's return, the king expressed his disappointment and asked for a statue to be made so he could gaze at the wise teacher whenever he pleased. With the Buddha's agreeal, a sculpture captured all thirty-two signs of the enlightened one. When the Buddha came to see the finished piece, the statue rose to greet him as a gesture of recognition. Regardless of all this, the main takeaway is that sight is of great importance, and helps to capture the life within the statue. This is the connection that must be made between viewer and statue, it's a human-human bond that transmits the dharma in a personal sense. The gaze of the Buddha furthermore offers a sense of serenity and calmness, and would help with the feelings generated in temple visits.

I am now going to move on to Japan. This edict by Emperor Shomu may show the motives behind the constructing of massive projects related to Buddhism, supported by the state. It will also show another concrete example in the form of the Daibutsu of Todai-ji, and its significance:

> *""With the great vow of a Bodhisattva, I solemnly pledge to erect a gilt bronze statue of the Buddha Vairocana. I will melt down all the bronze in the country for it, and cut down all the trees in the mountains to build a temple. I will announce this widely throughout the country to my fellow ""friends"" and practicers. In the end, we shall benefit and attain enlightenment.*

> *I, the emperor, am the one who possesses the wealth of the land; the one who possesses the power. With this wealth and power, the matter of building this sacred figure is easy. But to do it with one's whole heart is difficult. If it is only a cause of the vain spending of the people's labor, we cannot meet the Buddha's wish; rather, we commit a sin by abusing the Dharma. This I fear.*

> *One who joins the ""friends"" should worship the Buddha Vairocana three times a day, in all sincerity. ... If someone should desire to help build the figure by donating even a branch from a plant or a handful of earth, accept it unhesitatingly. The chiefs of provinces and counties should not, because of this project, harm the peasants or exact taxes or labor from them. Proclaim this widely and let my intention by known.""*

Todai-ji's Daibutsu depicts Vairocana, a celestial Buddha that sent the historical Sakyamuni Buddha onto earth to guide and teach people the dharma. Nara, where Todai-ji is located, would also hold special significance as the capital, and center of the world. However, the Buddha sitting in Todai-ji isn't the just the Buddha of Nara, nor of Japan, nor China or India or the world. Rather, it is the Buddha of *the whole universe*, beyond comprehension. Such a prospect can only naturally mean the size of the Vairocana would be as large as possible.


It seems that in the next several hundred years, large scale construction of Buddha statues would see issues. The costs of building and maintaining such large bronze statues meant only three would be made of the sort, the 752 Birushana Buddha at Todai-ji, the 1252 Amida at Kotokuin, and the 1660's Shaka at Kan’eiji. Interestingly, by the Meiji period, the buildings housing the Buddhas at Kotokuin and Kan'eiji were destroyed and the statues sat in the open. Apparently they were all popular among tourists, which we will discuss later on in the modern day.",0
"Note that Polybius wasn't present at the battle. Though it is often treated as a primary source, it properly is a secondary source. Polybius wrote his account some 60 years after the battle of Cannae, based on eyewitness accounts (the few remaining who were by then quite old), written accounts in the Senate archives and oral traditions.

There's no reason to believe there are major errors in the account (other than the question who was in command), but the account suffers from the same problems all secondary sources have: it is based on available, possibly flawed, primary sources (and thus does not represent things that may have been in unavailable sources) and it is coloured by its time (principally the Third Punic War) and its author (Polybius was a client of the Scipio family).",0
"Some questions on /r/AskHistorians are difficult to answer, because the answer is probably 'we don't know'. And while reaction videos may be popular on YouTube, and people asking questions here very commonly want to know how people reacted to this or that...writers in the past just don't focus that much on what other people thought of things. So the chances of this having an answer is not high. Of course, maybe if I looked at some obscure journal article written in German in 1946, I'd discover what Freud's mum thought of his theories. Maybe I'm missing something embarrassingly obvious (I hope not!). But we don't know is *probably* the answer here.

Considering the centrality of the mother-child relationship to Freud's theorising, there's quite a lot of discussion of their relationship when Sigmund was a child. But there's really very little research into Freud's relationship with his mother (called 'Amalia' on her gravestone, but mostly called 'Amalie' in the family) when they were adults. There's even fewer that I can currently access; the book I'd want to have on me to answer this question (*Freud's Women* by Lisa Appignanesi and John Forrester) is in my office, and I've been working from home recently (but a look at the snippets that are available on Google Books suggests there is not that much about his mother as an adult in the book). The other book that I don't have access to that might be useful is *Freud And His Mother* by Deborah Margolis (published in 1977). Neither of the two letters in the 1960 *Letters of Sigmund Freud* addressed to his mother mention his theorising, instead being about life events. Certainly the big biographies of Freud (e.g., Peter Gay's *Freud: A Life For Our Time*) don't mention what his mother thought about his theories when they were both middle-aged or older - he was in his forties when he published his first big work, the *Interpretation Of Dreams*, and she was in her sixties. Such biographies are much more interested in Sigismund Freud's relationship with his mother when he was a child (perhaps for obvious reasons).

Freud himself seemed to both idolise and fear his mother, judging by his (really quite brief) discussions of his mother in his voluminous writings. He apparently felt a sense of freedom when she passed away. He doesn't seem to have discussed what she thought of his theories.

Another view of Amalie Freud - probably the most detailed - comes from Judith Bernays Heller (the daughter of Freud's sister), who discusses her remembrances of the family in [an article published in *Commentary* in 1956](https://www.commentarymagazine.com/articles/judith-heller/freuds-mother-and-father-a-memoir/): 

> My grandmother...had a volatile temperament, would scold the maid as well as her daughters, and rush about the house.

and

> She was charming and smiling when strangers were about, but I, at least, always felt that with familiars she was a tyrant, and a selfish one. Quite definitely, she had a strong personality and knew what she wanted, and the best evidence of that is the way she held her two sons and five daughters together, in spite of all the divergences and differences in their interests and their temperaments. And she had a sense of humor, being able to laugh at, and at times even ridicule, herself. I remember her saying to me, when I was supposed to choose some present for myself from her cabinet, where she kept a few treasured antiques: “After all, the best antique in my house is myself, and me you cannot take.” 

Judith Bernays Heller doesn't, in her memoir of Freud's parents, mention what they thought of his theories. 

Freud was happy for his daughter Anna to become, basically, the most prominent keeper of his legacy after his death, so he wasn't opposed to women being practitioners of psychoanalysis, or having opinions about psychoanalysis. The impression you get from what is written of Freud's childhood is that his family largely kept to the tight traditional division of labour between the patriarch as the breadwinner (his father the successful wool merchant) and the matriarch as the head of the household within the household. Perhaps either she refrained from stating in public what she thought of the theory (if she thought about it at all - perhaps she had more important stuff to deal with, from her perspective), or nobody bothered to ask her (women's opinions were not always valued in the strongly patriarchal culture in that time and place - women did not get the vote in Austria until Amalie was eighty-four).

It's worth remembering, too, that Freud *didn't* think that male children wanted to sleep with their mothers, despite the pop culture impression of Freud. Instead, the Oedipal complex is much more metaphorical than literal - the child competes with the father for the mother's attention and love, and ultimately (given that Freud believed we are evolved beings whose behaviours ultimately derive from sex and survival, given the centrality of sex and survival to evolution) this competition is driven by biological drives which - later on, once puberty has occurred - also play a role in driving all that sexy sex stuff. So chances are that if he discussed his theories with her, he didn't present them in the salacious pop culture form of it, but rather as a relatively dry and technical theory of what causes mental illness.

...or maybe what she thought about her son's theorising *is* out there somewhere in one of the archives of Freud's papers and letters, and has simply never been translated into English (remembering that Freud grew up speaking German in Vienna)...or perhaps I'm missing something. It's hard to prove that negative - all I can say is that it does not appear to be widely known information available in every biography of Freud. In any case, Amalie Freud's opinion of stuff like the Oedipus Complex is, for better or worse, not seen as an important factor in Freud's biography.",0
"Hi u/Spectre-Cat!

So there is a lot of orientalism informing the assumptions that underpin the question. The dance is not necessarily seductive, and Islamic countries have not been monolithic or uniform over history. There are so many amazing books on this - and please someone feel free to link a good biblio, but I really like Zachary Lockman's *Contending Visions of the Middle East: The History and Politics of Orientalism*. The whole basis for the question would benefit tremendously from some re-thinking on that front.

My focus was on mid 20th c. belly dance. I'll talk primarily about Egypt. In belly dance's arguably contemporary heartland, Egypt, raqs sharqi can be found easily all over the place today, and it was this way for sure since the golden age of cinema there. Living rooms, nightclubs, and top hotels --- It is definitely not always seduction. 

Contemporary Western assumptions about how provocative raqs sharqi is in any given context are heavily influenced by hardcore ingrained orientalism.  Najwa Adra explores how in the Arab world belly dance is not considered a dance of seduction in the majority of social contexts . You can check out her chapter “Belly Dance an Urban Folk Genre,” in *Belly Dance, Orientalism Transnationalism & Harem Fantasy*, ed. Anthony Shay and Barbara Sellers-Young. You can also see Kathleen Fraser's work, *A Trade Like Any Other*, that talks a lot about how the dance form in Egypt has only been considered seduction work in specific professional contexts (this was an ethnography from the 90s).  If a general audience in Egypt is going to see a performance of raqs beledi or raqs sharqi as seductive or as fun and folkloric is highly dependent on who is doing the dancing, when, where, and with who around.  If you are paid and a pro, it is more likely to be seduction. If you are dancing with your pals at home, then not so much. For more, check out Anne Vermeyden's ‘The Reda Folkloric Dance Troupe and Egyptian State Support During the Nasser Period’. *Dance Research Journal* 49, no. 3 (December 2017): 24–37. Vermeyden talks about this exact issue --- the whole positionality thing is part of why the Reda Troupe's dance is considered folklore, but the dance of Fifi Abdu is considered beledi or sharqi. 

Just another thing to note---the movements/stylizations that came to be termed raqs sharqi by a contemporary individual really came into existence during the 20th century. You can check out Kathleen Fraser's  *Before They Were Belly Dancers: European Accounts of Female Entertainers in Egypt, 1760-1870*  for academic work on what I would argue is the closest studied dance ""lineage"" to modern raqs sharqi. Any dance form that was pre-Islamic would have a completely different context/meaning than the forms that were performed in the 19th c., and Fraser argues (I would too) there are different from the forms popular in Egypt in the 20th or 21st c.

An answer to your question would be if professional (ie paid dancers who make a living entertaining) belly dance's evolution over time began in the pre-islamic period and shifted and morphed until today, it would not have been considered provocative in the way it is now. Also, law has not been conservative for centuries, and women have been dancing publicly in Egypt, for example, for sure since the 18th c, and I'm sure earlier. There were periods of persecution against pro dancers (see Fraser's awesome book), but this was interspersed with great popularity.  

Actually the debate in the scholarship is more about what effect colonialism had on criminalizing women's public performance/dance, but that's a tale for a different day XD.",0
"[2/3]

***Part 3: Making the New Soviet Man***

Those tropes were employed with particular frequency by Lazar Kaganovich, often called Stalin's right-hand man, who was First Secretary of the Moscow Party Committee from 1930 to 1934, where he oversaw the most important phase of Metro construction, and for which service the Metro was named in his honor when it opened in 1935. Kaganovich loved to speak about how the Moscow Metro was going to make life easier and more comfortable for everyone who came into contact with it, and about how it was more beautiful than anything in the West, but limiting his rhetoric to those two practical points is a little bit reductivist. It goes much, much deeper than that.

Let's take an example. Perhaps more than anything else, the Soviet leadership, and Kaganovich above all, loved to talk about ""brightness"". Brightness stood for both better living quality, joy, happiness and all that; and beauty, superiority to the ugly stations of the West. Andrew Jenks suggests that all of this essentially amounted to a number-measuring contest: ""Moscow metro ceilings would ascend to 5.6 meters, compared to 2.7 meters in New York [...] Soviet lighting would outshine London's, 50 lux to 24 lux."" But, taken out of context, those facts can be misleading. Soviet Metro stations needed to be brighter because — this is the key — brightness was intended to create an altered, transcendental space. It was intended to transport Muscovites, literally speaking, yes, but also figuratively, to another realm of existence.

The beauty of the Moscow Metro is an attempt to give Soviet citizens a glimpse of the future of socialism — not the present, the future. That is (part of) why Stalin and Kaganovich were willing to prioritize it over fixing the urgent housing problem. The lack of housing space arguably affected more people more deeply, with many old bourgeois apartments subdivided between several working families, and having to share a kitchen and toilet made life very unpleasant for Muscovites indeed. However, simply building more housing cheaply and quickly was not good enough for a mature, developed, sophisticated socialist society. If they were going to do it, they had to do it right, or else it wasn't worth doing. The Metro was a glimpse of what that perfect world would be, and it was accessible to everybody.

(Note: they built a bunch of cheap, simple housing too, but they pretended they weren't doing so, and then moved people out of it as soon as they could. The barracks that were built to house new migrant laborers had often even worse conditions than communal apartments, with no room dividers or internal plumbing at all, and for hundreds of people. These barracks were built for all sorts of industrial laborers, but also laborers working on, ironically, the Moscow Metro.)

Part of this mature socialist society that Stalin wanted to represent in the Metro is the perfection of man. My academic advisor might even say that that was the core of Stalinism: a belief in the perfectibility and moldability of man. (I'm probably misquoting him — hi, Matvei Filippych!) Entering the Moscow Metro is supposed to be a transcendental experience in the way I discussed above, where you see the end-point of history manifested in the station architecture, but it's transcendental in another sense too. Even though the body physically descends, the spirit and the mind are raised up to the highest heights of achievement. That is another aspect of the concept of brightness here, a pun which works both in Russian and English: *enlightenment*. Metro stations were designed to enlighten the people passing through them.

One architect put it this way: within the brief period a Metro rider spends in one station, ""the architecture, emblems, and entire artistic image should actively act upon [affect, influence] him.""^1 Beauty was put on display to educate the citizens in proper aesthetic taste, to uplift their spirit, to show them the heights they could achieve under Stalin's guidance. This is also reflected in the way the dangerous, difficult, and often badly-mismanaged construction of the first line was spun in propaganda. It was a grinding slog, sure, but the Metro builders had defeated the odds, defeated nature itself. ""Technically,"" Sergei Kirov had said of some other construction project, ""it may be impossible, but Bolshevik-ly, we'll do it anyways.""^2 The point is, under socialism, anything could be done, in extremely short time frames, and done aesthetically pleasingly and well, to top it all off. (But the story of that mindset running into the realities of Metro construction is another question that I would love to answer some other time.) Man stood triumphant over all, and beauty showed the extent of his mastery.
___

^1 Kolpinskiy, in *Dni i gody metrostroya*, edited by Reznichenko. Translated in Jenks, ""Metro on the Mount."" But I don't like his exact translation, so I modified it a little.

^2 Paraphrased in Wolf, *Russia's Revolutionary Underground*. The word I translate as ""Bolshevik-ly"", *po-bolshevistski*, could also mean ""in a Bolshevik manner"", referring to the construction methods. He translates it that second way, but I don't exactly like that. The point isn't the difference in construction methods, it's the difference in *states of reality*.",0
"Mutual sinkings were rare in any war that I am aware of. One situation that is similar to the topic of your question came during the Battle of the Heligoland Bight, during WWI. The battle was a British attempt to strike German patrols in the Heligoland Bight using light forces (destroyers and cruisers) supported by battlecruisers under Beatty. The early stages of the battle saw British destroyers and cruisers tangling with a smaller force of German destroyers. During this engagement, the German torpedo boat (small destroyer) *V-187* was hit heavily, and began to sink, assisted by scuttling charges laid by her crew. A flotilla of British destroyers approached the stricken ship, and *Defender* lowered her boats to take off the crew. However, just eight minutes after this had begun, the appearance German light cruiser *Stettin* forced the British ships to withdraw. *Defender*' was unable to recover two of her boats, abandoning their crews to what seemed like certain captivity. *Stettin* declined to pick up the boats, first steering to engage the British destroyers, and then withdrawing to repair damage sustained in the engagement with them. The crew of *Defender*'s boats took this abandonment in their stride, bringing a considerable number of German survivors aboard. A total of 28 Germans (18 wounded, ten unwounded) were taken aboard the boats, which were crewed by just ten Britons between the two. The German survivors were well treated, with the wounded being looked after well.  Fortunately for both the British and Germans alike, they were being observed by the British submarine *E-4*, commanded by Lieutenant Ernest Leir. Once the coast was clear, Leir took *E-4* to the surface, and approached the boats. Leir took the British seamen aboard the submarine, along with three injured Germans. He left the rest of the wounded in the boats, along with the unwounded men, giving them food, water, and a compass with which they could steer their way back to German-held territory. This incident was typical of behaviour between and towards men in the water. Sailors always saw the sea as being as big an enemy as whoever it was they were fighting. A survivor could no longer hurt you; instead, the impulse was to help them - at least in part because all involved recognised that it would not take much for them to be in the same situation. ",0
"> In his introduction to Funeral in Berlin, Len Deighton mentions that East and West Berlin shared their water, sewage, gas and electricity networks. It makes sense that you can't just suddenly split these networks (especially water and sewage) into two, but I'm interested to know if this continued until unification and how this worked in practice. How did charging work?

He appears to be mistaken. The separation of East and West Berlin's underground utility networks (electricity, water, town gas/natural gas, and sewage) began during the period of the Berlin Blockade (June 1948-May 1949) amid heightened tensions between the United States and the Soviet Union. This process was mostly completed by the early 1950s, save for a minority of the sewage network as, presumably, separating, blocking, or re-routing a certain number of the large underground sewage tunnels that happened to pass in between the two halves of the city was logistically difficult. Although West Berlin invested in its own sewage treatment plants, most of the sewage that West Berlin produced was actually disposed of by sewage farms located in the German Democratic Republic (East Germany) just outside the boundaries of the city, and East German authorities never disrupted this arrangement. During the Berlin Blockade, however, sewage was disposed of by dumping it into the Spree and Havel Rivers for want of sufficient electricity to run the pumps to the sewage farms.

After the 1971 Berlin Agreement, which reduced tensions, East Berlin and East Germany formally agreed to process a certain proportion of solid waste originating within West Berlin; before, this waste had been disposed of in the city itself by incineration.

During the Berlin Blockade, the separation of utilities was not yet completed, and electricity to West Berlin was cut off by Soviet authorities as the generating plants were still in their zone. Fuel such as coal, in addition to food and other supplies, had to be flown in to keep West Berlin's lights on as all road and rail links had been severed. After the end of the blockade in May 1949, East German power stations began to supply West Berlin with electricity again, but authorities in West Berlin had committed by that point to becoming independent from the East in this regard. A new power station, funded with Marshall Plan aid, was opened in December 1949. On 21 September 1950, the last power lines to East Berlin were turned off, and the West Berlin power station assumed the entire electrical load for the half-city.

The city of Berlin took over the task of municipal water supply from individual consumers' wells in the 1870s after the unification of the German states, and shifted drawing water from a point on the Spree River at Stralau to plants above and below the city. Berlin is located in an *urstromtal*, or glacial valley, and West Berlin chiefly relied on groundwater processed through a water treatment plant. The rapid consumption of this water, as it was estimated that only ten to fifteen percent of runoff actually re-entered the water table, began to become a problem in the 1960s. Efforts to restore the level of groundwater through natural sources, such as the utilization of groundwater that infiltrated into the area of West Berlin from outside the boundaries of the city, and the conservation of water, were deemed successful by the 1980s.

By the 1950s, West Berlin was independent in the production of town gas (before the widespread usage of natural gas, gases derived from coal or oil were commonly used for for domestic heating), but in 1985 secured a spur of the Soviet natural gas line that supplied East Germany.

The land border between East and West Berlin was essentially sealed in May 1952, leading to a transformation of the public transit networks in each part. The remaining sewer tunnels between East and West Berlin proved to be an attractive route for escapees, many assisted by a secret organization of West Berlin university students known as the Girrmann Group. It is estimated that as many as 300 people used the sewers to escape East Berlin between 1961 and 1989, but by the early 1960s with the Berlin Wall being constructed, the prospect of escaping through the sewers became ""increasingly dangerous;"" the sewer entrances were patrolled by the East German secret police (*Stasi*). Telephone communication for ordinary citizens was cut between the two Berlins, and was not completely re-established until the mid-1970s.

> Even then the two Berlins were not totally isolated from one another, but the extent of remaining contacts was very limited: postal service, a teletype connection between the police forces, telephone connections between the transit systems and the fire departments. Western subway lines that traveled under Eastern territory without stopping, a train system in the West operated by the East, Eastern water serving a few corners of West Berlin, and sewage flowing freely under the Wall wherever gravity so dictated. The result was a need for a few discreet technical discussions and an occasional skirmish over fees or sovereignty. The thaw in East-West relations in 1970 led to a resumption of telephone service, a few more opportunities for people to cross the wall, and negotiations to share supplies of electricity, gas, and garbage.

**Sources:**

Bainbridge, John. ""Die Mauer: The Early Days of the Berlin Wall."" *The New Yorker*, October 27, 1962.

Elkins, Dorothy, Thomas H. Elkins, and Burkhard Hofmeister. *Berlin: The Spatial Structure of a Divided City*. London: Methuen Publishing, 1988.

Ladd, Brian. *The Ghosts of Berlin: Confronting German History in the Urban Landscape*. Chicago: University of Chicago Press, 2008.

Merritt, Richard L. ""Infrastructural Changes in Berlin. *Annals of the Association of American Geographers* 63, No. 1 (March 1973): 58-70.

Robinson, G. W. S. ""West Berlin: The Geography of an Exclave."" *Geographical Review* 43, No. 4 (October 1953): 540-557.

Smith, Jean Edward. *The Defense of Berlin*. Baltimore: Johns Hopkins University Press, 1963.

Taylor, Frederick. *The Berlin Wall: A World Divided, 1961-1989*. London: Harper Perennial, 2006.

Turner, Henry A. *The Two Germanies Since 1945: East and West*. New Haven: Yale University Press, 1987.",0
"First of all, thank you for the great answer above. Really engaging and thorough! A friend has been bugging me to weigh in on this topic, since he knows I study masculinity and emotion in literary and cultural history. I’ve never posted on this subreddit (or any subreddit) before though, so apologies if I’m doing it wrong. I agree with the historical account given above, which also cites a few works I am familiar with and a few more I have tagged for further reading. All that I would like to add, if I can, is that I think a crucial figure in this conversation is a literary and cultural icon known as the “Man of Feeling.” This figure is a recurring archetype in the highly sentimental novels of mid-1700s (alluded to a bit above), including Sarah Fielding’s The Adventures of David Simple [1744] (which may have first introduced the archetype), Laurence Sterne’s A Sentimental Journey [1768], and Henry Mackenzie’s The Man of Feeling [1771]. When I try to explain this recurring character type to students, I usually describe him as like a comic book superhero in that he usually shows up in fictional texts that read like a series of serialized adventures (rather than aspiring to the unity of plot or character development that mark later novels), BUT with the notable exception that the “superpower” of men of feeling is an ability to spontaneously shed copious amounts of tears. From roughly the 1740s to the 1770s popular novels were written that included variations on this literary figure, and they were quite popular and often critically-acclaimed. People celebrated and avidly consumed narratives about emotionally-sensitive men who embraced the pleasures of sympathy and moral virtue, while rejecting the different variations of rationalism that undergirded the enlightenment and neoclassicism. When people read these novels at the time, they often would openly weep, as a public display of their private sensitivity, and many men self-identified with characters like Harley from The Man of Feeling and Parson Yorick from A Sentimental Journey. 

A few things to note: the fashion for emotional sensitivity (sometimes called a “cult” or “culture of sensibility”) seems tied to new concepts of the relationship between public and private at the time. Classical understandings of these divisions aligned men with the public realm (government, military, public offices and public authority) and women with private domesticity, but as Britain and Europe underwent a commercial revolution (which predates and contributes to the Industrial Revolution), the usual boundaries between public and private were beginning to break down, and private citizens (middle class men) and women were starting to have more influence in politics and public culture (attending and writing plays, circulating with the aristocracy in coffeehouses, contributing to literature, intellectual debates). As well, private feelings, experiences and interiority were starting to be more highly regarded as important to one’s identity. The man of feeling was innovative and interesting in part because he reveled in his own private feelings and emotions rather than aspiring to traditional public accomplishments (military and sexual conquest, etc.). 

Second, it does seem like things seem to change around the 1790s, possibly due to a combination of the influence of the French Revolution, which polarized political debate, and made everyone suspicious of the authenticity of emotional expression as a tool for ideological manipulation by radicals and conservatives alike, and perhaps the Industrial Revolution. Sentimental men still appear in the fiction of this decade, but they are often less trustworthy or hypocritical. After the 1790s, the figure of the “Man of Feeling” becomes either an antagonist, or an unnatural deviation from proper gender boundaries, which are increasingly viewed as a threat to social order. By the early 1800s, readers looking back at the sentimental novels I mention above found them laughable, over-the-top and basically unreadable. Which is where they still stand today. Even literary scholars who read these texts now acknowledge that we live in a distinctly “post-sentimental age” as Maureen Harkin has called it, and so these texts still seem cheesy to us. 

Male tears remained acceptable in certain contexts after the fall of the man of feeling, and male Romantic poets who come later certainly celebrate feeling, though some critics argue that Romantic writers celebrate feeling as a springboard to celebrating individualism, while sentimental fiction tends more focused on emotion as sympathetic exchanges between people. So the “man of feeling” is more of a social and urban creature than the individualistic Romantic ideal wandering in nature that emerges after the 1790s. 

At any rate, people are often interested to hear that there was a period of time of a few decades (1740s to 1770s) where fiction devoted to men who cry (a lot!) was not only acceptable, but, in fact, tremendously popular and widely celebrated. It appears to show the malleability and variation in different historical and cultural concepts of gender. Hope this adds something to this really interesting conversation! 

Some sources:
Ellis, Markman. The Politics of Sensibility: Race, Gender, and Commerce in the Sentimental Novel. Cambridge, UK: Cambridge University Press, 1996.

Ellison, Julie K. Cato's Tears and the Making of Anglo-American Emotion. Chicago: University of Chicago Press, 1999.

Goring, Paul. The Rhetoric of Sensibility in Eighteenth-Century Culture. Cambridge, 
UK: Cambridge University Press, 2005.

Greene, Donald. “Latitudinarianism and Sensibility: The Genealogy of the ‘Man of 
Feeling’ Reconsidered” Modern Philology 75.2 (November 1977): 159-183.

Gross, Daniel M. The Secret History of Emotion: From Aristotle’s Rhetoric to Modern 
Brain Science. Chicago, IL: University of Chicago Press, 2006.

Benedict, Barbara M. Framing Feeling: Sentiment and Style in English Prose Fiction, 1745-1800. New York: AMS Press, 1994.

Barker-Benfield, G. J. The Culture of Sensibility: Sex and Society in Eighteenth-Century Britain. Chicago, IL: University of Chicago Press, 1992.

Wetmore, Alex. Men of Feeling in Eighteenth-Century Literature: Touching Fiction. Basingstoke, 
UK: Palgrave Macmillan, 2013.

Edit: Wow - thanks for the reddit gold on my first post! Can't wait to go and figure out what all the fuss is about!",0
"Can I also put forth a follow-up question / comment?

The question is, in your opinion, are there **professional reasons** to utilize some, or most, of these terms? By professional, I mean important for military historians and military science. I agree with all your points (desire to differentiate, alluring mystery, political whitewashing etc.), and let's assume it's some additional ""percentage"" of reasons why this terminology came about.

So do you think there are additional aspects to this terminology, related to A) peculiarity of Nazi war machine, and B) to innovations it had brought about? How important or needed they were in your opinion, compared to the reasons stated above?

For one, we know that all Hitler's organisations, party and military, were exceptionally convoluted. I always saw the need to use the original German words to not get lost in the myriad unique terms, names, and bureaucratic concepts that the Reich proliferated. Again this is not the main reason but I can see how it could factor in. Their passionate compulsion for word-like acronyms may also contributed.

Secondly, even if stripped of extreme romantisation and lionizing that the Nazi war machine was subjected to after the war, it was without doubt strikingly innovative. Sometimes unnecessarily so, as pop historians like to point out. The common perception seems to be that this relentless innovation lost them the war, but defined a good bit of military theory and R&D of all the winner countries for decades afterwards, from infantry tactics to jets to spaceships. This might explain some loanwords required to describe this explosion of technical and tactical developments. Although it still doesn't explain calling tanks Panzers =) even if troops called them that during the war, which is another good question and a rabbit hole...

*(Regarding the first point above, think about Soviet bureaucratic terms. It's pretty much impossible and unhelpful to translate words like ""gorkom"" and ""ispolkom"", since difference between them is by itself a very convoluted concept that calls for a special term for each. As I understand, they're used like this in English, right? In this sense, Third Reich may simply have the benefit of being scrutinized so heavily that its ""strange words"" became almost common knowledge.)*

On a parallel note, you said that there was no motive to separate Nazism and its concepts from ""normal"" word in this terminology, and the desire to build a separate mythology divorced from Nazism. But what about Soviet literature? Soviet and Russian history and technical books universally use the same traditional terms for ""Nazi stuff"". Most frequent terms even lost capitalization and became grammatically generalized (вермахт, гестапо). I personally see this as the clear desire to differentiate this hated (but meticulously studied) enemy from anything and everything else. Maybe it played the role in the desire to ""close the book"" on that chapter for other countries as well? Which, half-unwittingly, also worked towards creating the mystic aura around the stuff?",0
"The Amish did not choose to ""stay"" in any given time. Instead, when a new technology comes along they ask questions about how it will affect their community, tradition of self sufficiency, and values. 

For example, you might be surprised to learn many Amish communities have a telephone or two, but they do not belong to anyone. Instead, they'll be in a booth shared by many families, and used by necessity only. If they had phones in their homes, they might be inclined to call their neighbor up the road to check how the new baby is doing, instead of visit in person, or spend their time socializing on the phone instead of spending time with their family.

The story is similar for the technological conveniences present in many Amish communities. Diesel generators, solar panels, lead-acid batteries, and pneumatic tools are all regularly found, but they pay close attention to how they use them and their affects. You might have electricity (off grid, naturally) in your store to power cash registers and lights, but still use oil lamps in the home. At work, it enables your livelihood, but at home it's an unnecessary distraction.

For a much more eloquent explanation, see here: https://groups.etown.edu/amishstudies/cultural-practices/technology/

For further reading I highly recommend _The Amish_ by  Donald B. Kraybill, Karen M. Johnson-Weiner, and Steven M. Nolt (cited in the above). It's a fascinating read, it debunks most of the myths around the Amish way of life and does a good job of explaining their processes and traditions.

This is my first post on /r/AskHistorians after many years of reading. This is a topic that fascinates me but I have no formal education in Amish Studies. I hope it's up to snuff, mods please let me know if it's not!",0
"I study early islamic history and history of islamic theology so I can take a crack at this. 

First, I would like to challenge the question’s own wording: no major “sects” actually hold a stance against women education. Rather, individual actors within different sects use generalized rulings and claim the case of women’s education falls under the general umbrella of said ruling. An important first concept to unravel here is the idea of Ejma’a, I.e. Consensus. Most major islamic sects, including Sunni and Shia, believe that if the majority of reputable scholars agreed on a ruling within a given timeframe then this does imply that it is in fact what Allah wants. The actual definition of Ejma’a is a lot more strict and lengthy than this of course, and naturally there can be disagreements about whether a specific matter have reached consensus or not. However, for the interest of brevity, I will not expand on those.

In Sunni school of thought, a clear promise of ""[Allah] protecting the message"" in the Quran is the basis for equating Ejma'a with Allah's will. The rationale goes that if all reputable scholars were to agree on a wrong interpretation of the Quran then the message certainly will be lost. Given that Allah promised to protect the message, wrong Ejma'a isn't a possibility. Other sects have their own interpretation to the source of thr validity of Ejma'a, with the most notable being the Shia's Ma'soomoon.  

Essentially, most islamic sects agree on the legislative power granted by Ejma’a even though some disagreements between sects exist as to why exactly does consensus equate authenticity of claim. This is why Sunni’s give themselves the title of Ahl AlSunnah wa AlJamma’a, People of the Sunnah and Ejma’a. This title is supposed to hint at the two main sources which can be used to infer islamic rulings: either a direct ruling on an identical matter discussed in AlSunnah (i.e. the life and teaching of Prophet Mohammed) or by inference. In inference, a scholar attempts to project a modern issue into a comparable one in AlSunnah, where a direct ruling does not exist. Rulings on camels and carts are sometimes extrapolated on all methods of transportation, for instance. When sufficiently enough reputable scholars agree on the projection, the ruling reaches an Ejma’a status. 

As far as my knowledge goes, no direct quote from Sunnah nor an Ejma’a exists to discourage women’s education in any of the major islamic sects. In fact, many texts do encourage pursuing knowledge and education regardless of sex. For instance, the Hadith “whoever is pursuing a path of knowledge is pursuing the path to heaven” is generally understood to be unisex. As you stated, the wives and daughters of the prophet were major scholars; AlBukharri and Muslim had multiple female teachers etc. In fact, most of the surviving literature on any voices speaking against educating women in the medieval islamic period reached us through texts directly written to refute them. Famously AlJahith dedicated entire sections of AlBayan Wa AlTibyan to mock those views as he did not view them worthy of a proper refutal. 

Where then, do some individuals drive the power to make educating women haram? A second concept is needed in here: the islamic doctrine of giving means the same halal-haram categorization as their end. If a halal act inevitably leads to a haram one, or is made in pursuit of a  haram act, then the halal act is ruled as haram. Once again, most islamic sects agree on this general rule itself, but scholars even within one sect disagree as to when it is applicable and when is it not. A famous application of this rule all sects agree is a wrong application is forbidding farming grapes in fear that it might enable someone to make wine out of it. Again, this is an example that is used to teach a _wrong_ application of the rule. The commonly used correct application however is forbidding insulting unislamic idols. The reasoning here is that inevitably insulting idols of practitioners of other religions leads to hatred, wars, and deaths. This general rule is known as the rule of Sadd Althara’e.

Where some applications of rule of Sadd Althara’e are agreed upon one way or the other, it historically has been the easiest rule to abuse. One can see both the importance of such a rule, but also how easy it is to abuse to fit anyone’s agendas in forbidding an act as long as they can conjure up a hypothetical scenario in which said act inevitably leads to a haram act.

This all goes to show that the restriction of women’s education in question is not tied to a specific sect but rather to individuals choosing to enforce the rule of Sadd Althara’e under their geographical range of influence. This precisely is why you find two countries of similar islamic sects having different rules, and why the Taliban can feel justified switching back and forth between allowing and disallowing education for women. Proponents of prohibition would readily admit that no islamic text directly forbids education for women, but they will strongly argue the existence of inevitable haram consequences. Justifications vary way too wildly to count, and the reader is encouraged to play their own mental gymnastics to connect a haram act to education. 

According to Dr. Ahmed AlBassam, the history of western colonization or forced influence to the islamic world cannot be ignored in this regard. While education in general long existed in the region, the modern structure of K12-BS-MS-PhD as well as the structure of each given subject is still viewed as a wildly western import in the region. The history of colonization leads some to view this import with a lot of skepticism and as another mean of western cultural domination in the region. This skepticism leads to viewing western education as more of a necessary evil contact with which to be reduced if possible. It is no coincidence, AlBassam says, that countries more hostile towards western powers end up viewing education with a demonic look as apposed to countries with friendlier relations.

The influence of culture-based patriarchal power structures too cannot be ignored, as it determines which subgroup of society get the duty/privilege to perform that “necessary evil” when western-structured education is viewed in that manner. I will attempt to return later to elaborate on the last two points and answer questions.",0
"Man, those answers have a troubling level of variety. Basically they all say the CIA did shit with the Contras which is well documented and then everyone bring's up Webb's book Dark Alliance and the articles he wrote for the San Jose Mercury, but even the refutations of his work (which was *very* compromised) bounce around a bit. But then it kind of falls apart as far as dates etc. It looks like there were a few drug smugglers who may have gotten some special treatment from the CIA, which they do to protect sources etc. However, a lot of those answers are a poster referncing one source. ",0
"/u/trampolinebears focused on your second set of questions, about the authorship of the gospels. I'm going to address your first set, about the origins of the belief in Jesus' resurrection itself.

Before saying anything else, though, if you're looking for a great book-length study of this issue, you absolutely can't do better than Dale Allison's *The Resurrection of Jesus: Apologetics, Polemics, History*, published last year. Allison, one of the most respected scholars of early Christianity, by both Christians and non-Christians alike, has done absolutely superb work on this issue, and synthesizes the data and arguments in a singular and engaging way. 

Onto your set of questions.

First off, your intuitions about the unlikelihood of Jesus' followers ""colluding"" to knowingly fabricate this idea of his resurrection, despite not actually believing this themselves, are correct. Offhand, I can't think of a single scholar today, religious or secular, who considers it seriously. You also correctly surmise that many reject this idea in light of the early Jesus-followers' apparent willingness to die in their proclamation; but this isn't the only counter-argument, or even the main one. 

It's hard to explain this succinctly, but a lot of it has to do with the Jesus having already created a distinct and firmly established movement. Even before Jesus' death, his followers seem to have placed their utmost trust in him, as an expression of their own individual hopes in his teachings and in the future reality that he promised. They seemed to have abandoned their livelihood to follow him; and statements and teachings ascribed both to Jesus himself and to his followers in fact explicitly discuss this idea of unconditional support (Mark 8:35; Matthew 26:33; John 13:37).

What I'm trying to emphasize here is that they had first convinced *themselves* that Jesus was a singular figure, perhaps divine (cf. Ehrman 2014; Grindheim 2011), and who was to inaugurate the end of history itself (Allison 1998). Conversely, when we think of actual swindlers who try to invent a religion or cult de novo, we tend to think of those who have no real preexisting emotional and ideological stake in the belief itself. Instead, they invent the thing for largely utilitarian reasons, with a kind of sadistic desire to exploit the gullible or a fairly transparent profit motive. So the disciples' belief in Jesus' resurrection has a certain **continuity** with other preexisting strongly-held beliefs about him that also came about for reasons unrelated to trying to convince some external population of their reality or swindle them. Rather, it seems to be have emerged as an internal matter and belief of those within the Jesus movement itself, not influenced by external considerations.

But there are also a few caveats to all this; and discussing these opens up a few other important issues on the broader subject.

First, though, one of the reasons the work of Dale Allison (the scholar whose book I mentioned at the beginning) is so important is because he's one of the few scholars of early Christianity who's really devoted extended attention to examining the idea of the resurrection and its origins in a wider crosscultural and sociopsychological context. That is, he's tried to make sense of the idea and its emergence in light of *other* attested visionary experiences, and altered states of consciousness. (The terminology differs — ""hallucination,"" religious experience, etc. — but the ideas are similar.)

And this connects precisely with the disciples' preexisting conviction in Jesus' teachings and mission, which I already discussed. The idea here, then, is that **the notion of Jesus' persistence after death may have been influenced or facilitated by their conviction that Jesus could not have truly *failed* to fulfill his personal mission and the promises to bring about a new eschatological reality, without some decisive act on his behalf by God — especially if Jesus were a sort of preternatural, divine figure in his own right.** So scholars who look at this aspect suggest that this unconditional conviction could have had a profound psychological and indeed perceptual effect for them in relation to Jesus' death, further influenced by their spiritual beliefs about the dead and their fate in general.

Dale Allison gets even more specific here; and in the course of examining this idea in its broader crosscultural context, he looks at these apparent resurrection experiences in conjunction with those who've had spiritual experiences or perceived postmortem ""encounters"" with loved ones while grieving for them in the wake of their deaths, e.g. in dreams or dreamlike states. This has been a fairly popular little sub-area of interest for those who've considered the resurrection experiences as the result of altered perception/an altered state of consciousness; and you can also find other specialty articles on this, too. For example, the abstract for Stephen Smith's recent article ""[The Post-Resurrection Appearances of Jesus as Bereavement Experiences: An Engagement with Gerald O’Collins](https://journals.sagepub.com/doi/10.1177/0021140020906915)"" reads in part

>In recent years, debate has resumed regarding whether or not the post-resurrection appearances may have been hallucinations or delusions on the part of the disciples. A sub-category of this debate is the question of bereavement hallucinations. Was the disciples’ bereaved state in the wake of the crucifixion responsible for their ‘seeing’ Jesus?

_____

This comment is already getting a bit long, but I had mentioned some caveats to my earlier paragraphs. It's unfortunate that the New Testament gospels and other closely related documents are pretty much the only relevant surviving literary documents we have on this, since it's so hard to be able to cull ""objective"" historical data from these, in light of their ideological and apologetic aims, and their indulgence in what we can call fictionalization.

And this has direct bearing on pretty much all major things relevant to the origins of the disciples' resurrection belief and experiences. Here are a few of them, in no particular order: 

**1**) Although it's one thing for a single individual to have a hallucinatory experience, it's often quite another when we're talking about *shared* visionary experiences and encounters. However, the latter is exactly what the gospels suggest in terms of Jesus appearing and interacting with the disciples as a group; though again, Dale Allison covers this exhaustively. 

**2**) Did the historical Jesus himself proclaim that he'd die and be raised from the dead in three days (whether or not this ended up actually coming to fruition)? Certainly the Jesus of the gospels is *ascribed* this prediction (Mark 8:31; 9:31). Scholars debate the authenticity of this: see for example [this article](https://brill.com/view/journals/jshj/8/1/article-p47_3.xml?language=en). However, if this were indeed the case that he predicted this, here we'd have something that would have significantly influenced the disciples' expectations and perhaps experiences in the wake of his death. 

**3**) Though where might Jesus have gotten the idea of his resurrection after three days in the first place? Did it have any meaningful historical precedent, etc.? This is actually a pretty huge topic. Just to briefly point out one relevant Biblical text, though: Mark 6:16 suggests the unusual belief, ascribed to an unnamed large population, that Jesus himself was something like the reincarnation of John the Baptist, who had been beheaded by Herod. The crowd here wondered if ""John the Baptist has been raised from the dead,"" and if in fact this was how ""these miraculous powers are at work in [Jesus]"" in the first place! (For my own part, I've actually argued at length that the historical Jesus indeed proclaimed some major supernatural event having to do with the miraculous destruction and rebuilding/raising of the Jerusalem temple in ""three days,"" which may have relevance to the ""raised after three days"" *death* aspect.)

**4**) Was the catalyst — or at least *a* major catalyst — for the belief in Jesus' resurrection the mundane fact of a sort of misidentification of Jesus' tomb? That is, if some of his followers went to visit his tomb in the days after his death, as the gospels suggest, is it possible that they simply visited the wrong tomb — one that was unoccupied, and which then somehow led them to believe that his body had been assumed up into heaven? As you might imagine by now, Dale Allison discusses this in detail, but doesn't give it much credence,

**5**) The gospel of Matthew uniquely expresses this idea that Jesus' Jewish opponents actually expected the disciples to ""stage"" the resurrection, to make it conform to Jesus' alleged prediction while he was alive. Matthew 27:62-64 reads

>62 The next day, that is, after the day of Preparation, the chief priests and the Pharisees gathered before Pilate 63 and said, “Sir, we remember what that impostor said while he was still alive, ‘After three days I will rise again.’ 64 Therefore command the tomb to be made secure until the third day; **otherwise his disciples may go and steal him away, and tell the people, ‘He has been raised from the dead,’** and the last deception would be worse than the first.”

However, the historicity of this (along with the subsequent narrative of the placement of the guard to try to ensure that the disciples *didn't* do so) is more or less unanimously rejected. Because this is growing long, I'm going to be lazy and just link my comments [here](https://np.reddit.com/r/DebateAChristian/comments/9q4h7e/defending_the_stolen_body_hypothesis/e8776xn/), which address scholarly debate over this.",0
"(1 / 4)

You've asked a lot of questions, and the answers could (and have) filled books. I'll try to address them all, though given the scope, I've left out some details.

> *Specifically, were there Southerners on it?*

..

> *I’m assuming some of the Supreme Court justices were Southerners.*

**THERE WERE EIGHT JUSTICES, INCLUDING FOUR FROM SLAVE STATES, AND ONE EMPTY SEAT**

At the time of Lincoln's election, there were eight living Supreme Court justices. One, Peter Vivian Daniel, a Democrat from Virginia, had died on May 31, 1860, during that year's Presidential campaign. President James Buchanan did not nominate a replacement before Election Day. His party was deeply divided as it was, with a Democratic National Convention less than a month before Daniel's death failing to nominate a presidential candidate. This resulted in a second convention already scheduled just a couple of weeks after Daniel died. And ultimately, that convention also failed, the Democrats could not unify, and they nominated two different presidential candidates at competing conventions. 

Rather than risking a campaign issue that might aid the Democrats' opponents and further aggravate the Democrats' divide, Buchanan decided not to nominate anybody until after the election. Of course, immediately after the election, the Secession Crisis erupted, making it even more difficult for Buchanan to actually get anybody through, and the Supreme Court nomination took a back seat. Buchanan finally did make a nomination less than a month before leaving office. On February 5, 1861, he nominated Jeremiah Black but the nomination failed by a vote of 25-26 on February 21. (Black was Buchanan's Attorney General, though stepping in as Secretary of State in the administration's final weeks when SoS Lewis Cass resigned in December 1860 over Buchanan's handling of the Fort Sumter issue. Unlike Cass, Black was very much a ""Doughface"".)

**THERE WERE SEVEN DEMOCRATS AND ONE CONSERVATIVE REPUBLICAN ON THE BENCH**

Of the eight living justices, the court was decidedly conservative. All eight justices had been nominated by Democratic presidents. Only one non-Democrat was on the court - John McLean, the Whig-turned-conservative Republican, though even he had been nominated to the court by Andrew Jackson. 

The other seven were all conservative Democrats, though their brands of conservatism followed the geographic North/South line. Nathan Clifford, Robert Cooper Grier, and Samuel Nelson were Northern Democrats whose pre-war rulings were very much in keeping with what might be expected of Northern Democrats. John Catron, James Moore Wayne, John Archibald Campbell, and Chief Justice Roger B. Taney were all Southern Democrats whose rulings reflected that partisanship - though Taney was actually from a slaveholding family from the non-seceding slave state of Maryland.

The other three - Catron, Wayne, and Campbell - were all from states that seceded (Tennessee, Georgia, and Alabama, respectively). Of them, only Campbell actually resigned, and he waited over three months after Alabama seceded to actually hand in his resignation (after Fort Sumter).

> *Were they viewed skeptically by their colleagues if they stayed?*

**THE JUSTICES TRIED TO KEEP THE COURT INTACT**

No, in fact, there was an effort by the Northern Democrats to try to ensure that the three secession-state Democrats remained on the court. McLean, too, was somewhat involved in making sure the court stayed united, but it was really Samuel Nelson of New York who took the lead. Rather than antagonism, there was something of a camaraderie among them, that they didn't want to make the situation worse, and ""break up the court"". They were also all quite old, with six of them having been serving on the court between 15 and 30 years already. Though they had disagreements about law sometimes, socially, they all got along. (A few years earlier, Benjamin Curtis resigned after the *Dred Scott* decision, after having a falling out with Taney, but that's another story). 

There were a couple of news reports in April 1861 that Campbell, Wayne, and Taney were going to resign en masse and issue a statement, first reported in the *Richmond Examiner* and *Philadedlphia Inquirer*, but these reports were pretty quickly discredited as inaccurate. Of the eight justices, the only one who really considered resigning is the one who did, and that was Campbell. Campbell would even write later on about telling Wayne of his decision, and while Wayne didn't try to stop him, he disagreed with him, and Campbell defended Wayne's own decision to stay on the court.

Outside their colleagues, there was some criticism by the press. When Campbell took his time in leaving Washington after resigning his seat, the *New York Post* called him a ""convenient spy for the Montgomery mutineers"". On the flipside, when Wayne remained on the court after the war started, the New York newspapers all praised his decision, while Atlanta's *Southern Confederacy* newspaper wrote: ""Georgia does not claim him, and he is no more of us"".

> *Did they remain loyal to the union or leave to join the Confederacy?*

..

> *Do we know about their thoughts and their decisions?*

Yes, and I'm taking ""decisions"" here to mean their decision to stay on or leave the court. If you're asking about their history of Supreme Court case decisions, that's another matter, but to put it briefly, all except McLean were conservative Democrats, and McLean was a rather conservative Republican - in comparison to a moderate like Abraham Lincoln and a progressive like Sen. Benjamin Wade of Ohio. Though to the South, McLean was way too radical. 

Again, there were three justices from seceding states on the court during the Secession Crisis - Catron of Tennessee, Wayne of Georgia, and Campbell of Alabama. And again, of the three, the only one who seriously considered resigning was the one who did, Campbell.

Nobody on the court issued a press release or anything giving a direct reason for deciding to stay or resign. But we can glean from their actions and writings what they believed.

**JOHN CATRON**

At the time of the Civil War, Supreme Court justices actually had two jobs. In February and August, they sat in session with the Supreme Court. At other parts of the year, each sat in session as one of the Circuit Court justices. Even now, each SCOTUS judge is assigned to overseeing one of the nine circuits, but in those days, the SCOTUS judge doubled as an actual Circuit Judge whenever their assigned circuit was in session.

On July 10, 1861, during John Catron's duties as a circuit court judge, he issued what is known as ""The Charge of Judge Catron to the Grand Jury at St. Louis"". These were jury instructions in which he essentially told a grand jury to treat acts committed by Confederates against the U.S. government as treason against the Constitution:

> ""...[T]he crime [of treason] is committed whenever war against the United States by those owing allegiance thereto, is raised, created, made or carried on; or when, daring a war, they adhere to the enemy, giving him aid and comfort. The terms 'levying war' embrace not merely the act of formal or declared war, but any combination, in military array, forcibly to prevent or oppose, generally, the execution of a provision either of the United States Constitution, or a United States statute: or forcibly to subvert the United States Government or any department thereof; or by force to procure the repeal or alteration of the Constitution or laws; if such combination be accompanied or followed by an act of forcible opposition in pursuance of the treasonable design.""

If that's too obscure, the July 21, 1861, edition of the *Daily Nashville Patriot* printed Catron's Charge at length, and prefaced it with:

> ""We lay before our readers this morning that portion of the charge of Judge Catron to the Grand Jury at St. Louis on the 10th [of July 1861], which relates to the crime of treason. According to the definition of the learned Judge the volunteers of the State of Tennessee are traitors in arms against their Government, and liable to be punished as such. Judge Catron is a citizen of Tennessee, and the Confederate States, yet he clings to his position under the Federal Government, and gives it aid and comfort by holding its courts, and holding over the heads of our patriotic volunteers, struggling in the cause of liberty and independence, the penalties of treason.""
 
Just three months after Fort Sumter, Catron was telling a jury in that hotbed of conflict, Missouri, to consider Confederates as traitors if they were to try to resist the enforcement of U.S. Constitutional law.",0
"(1/2)

Feliz domingo para toda la juventud!

Welcome to a new episode of **So You Think You Can Settle La Patagonia?**

Buckle up. Patagonia has been settled on and off for at least the past 9 to 11 thousand years. In the following [link](https://imgur.com/a/8BuYoMZ), you’ll find a graphic presenting a series of archaeological locations that have been studied in recent decades, and which are the subject of an interesting paper on archeological analyses of the population densities and migration patterns of the area, both in Argentina and Chile, of the native nations and tribes that inhabited the area in what’s typically known as the Pleistocene-Holocene transition era, some ten thousand years ago. Very, very broadly speaking, this study, called *Poblamiento, movilidad y territorios entre las sociedades cazadoras-recolectoras de Patagonia* (2004) finds archaeological evidence suggesting that, over the centuries in this period, the climatological, geological and volcanological conditions were so rapidly changing that they forced different populations to switch between either new or previously inhabited lands, due to extreme modifications to their environment in humidity, hydric efficiency, availability, and even conditions of saturation, changes in plant development, volcanic eruptions, you name it. But I’m not an archaeologist, I much prefer books to bones, though no offense is meant, no archaeological sites were harmed in the making of this answer. 

But before I go on, I’d just like to clarify that, at least in the Argentine side, and I know this is also true for most of the Chilean Patagonia, the whole grasslands and lush forests just ain’t it. Not even close. The Argentine Patagonia is over 1 million square kilometres in extension, nearly half of the entire surface of the country. And of all those Km2, the vast majority of it is desertic. Is it extremely cold? That depends on what individual perceptions, doesn’t it? I love the cold, some people can’t stand it. I faint when it’s too hot, and many others thrive in the warm summer sun or whatever it is people do, I’m staying inside because to me, outside is scorched earth right now. But I digress. Even if there *are* beautiful forests, even if there are enough rains to get by in the regions closest to the Andes or the Ocean, the rest of it, what lies in the middle, is a gigantic desert, most of it privately owned. Don’t worry, I’ll come back to that

So let’s talk a bit about more recent events shall we? Let’s talk about, let’s see what’s on the aquatermain bingo for this week. Tango? Nope. Military coups? Nope. I know! Genocide. More specifically, the genocide of my ancestral tribes, the Aonikenk and Gününa Küne. These two tribes, cousins, some legends even tell that the Gününa Küne were Aonikenks who just broke off, lived together across the “grasslands, lush forests and rain”. Lol no. But they did populate the area we now call La Patagonia. They had a lot of trouble dealing with their neighbors the Mapuches, who constantly crossed the Andes to raid their populations and enslave their people. See? This goes to all my fans who love to tell me that I never show native populations in a bad light. But the real problem came from, as it usually does for us natives, white people. Fast forward to 1867, when, under the presidency of Bartolomé Mitre, one of the first constitutional presidents of Argentina, Congress passed Law 215 of Land Occupation. Among its first articles, the Law reads “Forces of the Army of the Republic the banks of the river Neuquén, from its origin in the Andes to its confluence with the Río Negro in the Atlantic Ocean” (Article 1°), “The nomadic tribes existing in national territories within these areas, will be provided with anything necessary for their subsistence” (Article 2°), “If all or *some* tribes were to resist the peaceful subjugation to the national authority, a general military expedition will be organized against them, until they have been subjugated and thrown South of the rivers Negro and Neuquén.” (Article 4°). This lovely law had to be put on hold, by its final article no less, because the newly formed Argentine government was in the middle of genociding other people, the Paraguayans, with the help of the Uruguayan and Brazilian governments. Fast forward again a few years, to the presidency of Nicolás Avellaneda. I’ve spoken briefly about him before [here](https://www.reddit.com/r/AskHistorians/comments/gkotwi/in_the_1890s_argentina_was_the_richest_country_in/fqtp6kx/?utm_source=reddit&utm_medium=web2x&context=3).

>In 1874, President Nicolás Avellaneda was sworn in. He had intended, following in the footsteps of his predecessor Domingo Faustino Sarmiento, to induce an influx of European immigrants that could work the land. In 1876, he introduced to Congress the Law of Immigration and Colonization N°817, which sought to promote Argentina as a growing economy, making it attractive for immigrants, who would be granted land for farming and cattle raising, while also authorizing the creation of exploratory expeditions into the “uninhabited” areas south of the border.  
>  
>Heavily influenced by the Eurocentric beliefs of the civilizing mission and the American manifest destiny, the oligarchy used several native malones, raiding parties the natives did to steal cattle, as the perfect excuse to exterminate the natives in what is now called the Conquest of the Desert, a series of military campaigns deep into native territories led first by Adolfo Alsina, then Minister of War, and second by general Julio Argentino Roca.

The Conquest of the Desert started in 1878, following the congressional approval of Law 947, which was instituted as a follow-up to Law 215, and it intended to push back against several large scale raiding parties led by the Mapuches and the Aonikenk, who were usually called Patagones at the time, the inhabitants of Patagonia. Law 947 gave the presidency one million six hundred thousand pesos fuertes, the currency at the time, which, if my calculations are correct, and they might absolutely not be, equals about 200 million current day USD. This money, according to the first article of the law, was to be spent in “subjugating or evicting the barbaric indians” who lived within the borders that had been established by the previous law. Once the frontiers had been expanding, the newly annexed territories were to be sold to landowners to reimburse the State for the expenditure incurred in the military campaigns. Let’s also keep in mind that, as I said in the answer I linked to earlier, in order to be able to keep up with the international demand of meat and agricultural products that the newly imposed agro-export economic model was creating, Argentina required more and more territories to be converted into farmlands.

And so, the Conquest of the Desert started. The most violent part of it was carried out under the leadership of Roca, who would summarily execute several captured natives per group, making examples out of them. By the time the Conquest was done, millions of hectares had been annexed to the Argentine territories, and according to the report produced by a Scientific Commission that accompanied the army, which is a staggering 610 document thoroughly documenting what was done and seen, states that “pasa de 14,000 el número de muertos y prisioneros que ha reportado la campaña”, the reported number of dead and prisoners exceeds 14,000. The awful truth we have to contend with, is that we simply don’t know. The Scientific Commission was there to document flora and fauna, not natives. We have some clues as to the number of captives that were taken back to Buenos Aires, some of them walking up to a thousand kilometres. They’re estimated to have been three thousand, separated from each other to avoid them from reproducing. You know, eugenics. But I digress again, we want to talk about Patagonia.

In the decade that followed the Conquest, several laws were passed by Congress allowing the State to grant free land to those who would be willing to populate the Patagonia, creating colonies of immigrants, and ensuring the enlargement of already existing colonies, like Gaiman and Rawson, colonies of Welsh immigrants founded a few decades prior to the Conquest; and the newly formed colony of Trelew, created in 1886, in Chubut province. The cities of Cipolletti, Viedma and San Carlos de Bariloche in Río Negro, the last one infamous for having been used as the location for a scene in one of the [X-Men movies](https://www.youtube.com/watch?v=FlCTVzqFVRM) (for reference, the actual city of Villa Gesell is located halfway across the country, right next to the Atlantic, very much *not* in the Andes), San Martín de los Andes in Neuquén, Río Gallegos and Caleta Olivia in Santa Cruz, all were either created or significantly expanded after the Conquest. But all of them remained small settlements, mostly designed to either create or maintain a sovereign presence in territories that could otherwise be easily taken either by Chile or by European nations.",0
"> I’m surprised that the profession-based names we have today (eg smith, baker, thatcher, cook, etc) all use the same Modern English terminology and indeed spelling that we would use today. I wouldn’t have expected this if the surnames became fixed as names distinct from professions during the Middle English period.

They don't. In addition to all the names that were loaned from other languages (e.g. ""Schmidt""), this seems to be selection bias, that people only recognize the names that are still familiar, modern English language words. In actual fact, there are many names derived from any common occupation, but not all of them are obvious anymore because the English language has changed, and words have fallen out of fashion.

One of the books I cited was the *Oxford Dictionary of Family Names in Britain and Ireland*, which catalogs the suspected etymology of all surnames found there before 1900. [This previous answer of mine](https://www.reddit.com/r/AskHistorians/comments/e3vlnm/comment/f96355w) about surnames meaning ""doctor"" or other health care professional should essentially address your question. Many other names dating from Middle English, often loaned from other languages, mean ""doctor"", not just the surname ""Doctor"" itself. But it's not obvious to most people, because those other surnames have not been used to describe a health care professional in a very long time, often centuries.

Similarly, to use one of the specific names you brought up, a very brief perusal of the *Oxford Dictionary* reveals that the surname Baxter comes from the Middle English for ""baker"". Other variants on the name include Bakker and Backer. The dictionary also lists Backhouse for someone who worked in a bake-house, or bakery, and that surname is listed with many variants: Bakehouse, Bacchus, Bachus, Bakus, Baccus, Bacus, and Baccas. The book also believes that some ""Becker""s might have been bakers as well, by that alternate spelling, including German immigrants who may originally have been ""Bäcker"" (which also means baker). There are more baker-derived surnames in the book, so that's just a start.


> When did the actual spellings become fixed? Such that Miller, Milner and Myller began to denote different families?

I'm not sure you can exactly say that surnames ever become truly fixed with spellings. But insofar as they have been, it's been relatively recently - the 1700s and, in many cases, the 1800s. From a genealogical perspective, surviving records are littered with a single person using a variety of spellings of their last name throughout their lives, especially it their literacy was limited. Only the highly educated really used fixed spellings until the last couple hundred years. 

And even that's not a given, especially the further back you go. For instance, William Shakespeare's name in his own hand is thought to have survived [on six different legal documents](https://en.wikipedia.org/wiki/Spelling_of_Shakespeare%27s_name#:~:text=Willm), where it is rendered in five different ways. (I normally wouldn't link to Wikipedia in this sub, but in this case, it has photo reproductions of all six of Shakespeare's known signatures.) Yet, Shakespeare was obviously a very literate man and could have settled on a single spelling of his name if he wished. But apparently, he didn't.

As another example, one of the most well-studied families of colonial New York is the Van Voorhees family, all descendants of the 17th century immigrant Steven Coerts van Voorhees. The Van Voorhees Family Assocation lists among the family progenitor's 
descendants people who [use/used a variety of different spellings](http://www.vanvoorhees.org/docs/9thgen-Idx/V.php#V) both with and without the preceding ""Van"" preposition: Voorhees, Voorhis, Voorhes, Voorhies, Voorhis, and many more. 

The discipline of genealogy is littered with this kind of thing. In fact, in each entry of the aforementioned *Oxford Dictionary*, the surnames are listed along with their known ""Variants"". To use another of the surnames you asked about, the variants in the entry for ""Cook"" are: Cooke, Coke, Cock, Cooker, and Cox.

Different branches of just about any family, as their respective progenitors became literate, settle on different spellings, whatever looked ""correct"" to them. A lot of times, the name was orally communicated and spelled any which way by record-keepers, and it was only generations later where the family was literate enough to settle on the ""right"" way. But with, say, ten different third-cousins living in different areas, those ten third-cousins may have decided on ten different spellings as being ""correct"".

Sometimes it's a result of conscious family feuds to separate one brother or relative from another, and establish their ""own"" family. A lot of times a person decided to change the spelling down the line, because of a simple matter of preference, or because the spelling no longer looked ""right"" according to their local accent of English, or because the locals kept pronouncing their name incorrectly, because the spelling didn't ""match"". Any reason you can think of, has probably been used to justify a spelling change at one time or another.

One surname that comes to mind is the colonial New York Dutch family the ""Swartwout"" family. There are many people that still use that earliest spelling, but it's not quite pronounced how an English speaker may suspect. Traditionally, it's pronounced ""Swart-out"". So, down the line, some family branches adopted the spelling ""Swartout"". Others use ""Swarthout"". Still, one branch of the family caved to the public pressure of how people *assumed* the name was supposed to be pronounced, and now spell it ""Swartwood"". 

This is just to exemplify how just one ancestor spelling a name differently has repercussions for centuries to come. Illiteracy, accents, spelling standardization through dictionaries, and simple preferences have resulted in virtually every surname that's been around long enough to have a variety of spellings. These can often have obscure origins on their surface, until you dig a bit deeper into the documented history of the name through the ages.",0
"**Were the most common hours in 1980 really from 9 am to 5 pm?**

9-to-5 was a catchphrase by that time. It had been a catchphrase for a very long time. It did not even represent an ""average"" job when the phrase was first coined.

Furthermore, the 9-to-5 phrase was introduced back when a six day workweek was common. So it wasn't even representing a 40-hour workweek, but a 48-hour one.

...

Let's start by jumping back a bit to the 1890s.

The average work-day for men was 10.2 hours a day and women 9.2 hours a day, out of a six-day workweek.

For men, the bottom decile worked an average of 10 hours a day while only the top decile made it to 8 hours a day. The top decile men got to start at 8 rather than 7 am, and took lunch for an hour rather than a half hour. Even the 70th-80th decile had an average of 10 hours a day.

In 1892, Massachusetts passed a law *limiting* the hours-per-week for women to 58 hours. So while the concept of an 8-hour workday was already around (most famously in 1886 where hundreds of thousands of workers went on strike to demand an 8-hour workday, and many got it) it was often not a thing in practice.

...

The earliest reference I've found regarding 9-to-5 used as the phrase is from 1918:

[1918](https://www.google.com/books/edition/Trans_communicator/swI1AQAAIAAJ?hl=en&gbpv=1&dq=9+to+5+job&pg=PA910&printsec=frontcover)

>Coe, when he returns from his vacation, will find he has graduated from the night force to a 9 to 5 job with the day regulars.

Let's follow up a bit through the years:

[1926, interviews with women who write](https://www.google.com/books/edition/Earning_a_Living_by_the_Pen/hK0OAAAAMAAJ?hl=en&gbpv=0)

>You can't make a 9 to 5 job out of it and really get anywhere. And it's not a snap job. But the work is absolutely fascinating...

(For reference, the above quote is roughly when work started to change from six to five days a week, but it wasn't instant or universal.)

[1931](https://www.google.com/books/edition/Advertising_and_Selling/UzPmAAAAMAAJ?hl=en&gbpv=0)

>You will be helping to lick ""the winter of our discontent"" -- and maybe fitting your wife for a 9-to-5 job should she ever need it.

[1948](https://www.google.com/books/edition/Phi_Delta_Delta/_iYOAQAAMAAJ?hl=en&gbpv=0)

>It is far from a 9-to-5 job, but it is a real challenge, and as law students are truly wonderful people...

[1949](https://www.google.com/books/edition/Program/V1AFAAAAMAAJ?hl=en&gbpv=0)

>For five years he got up early enough to practice a couple of hours before the 9 to 5 job, practiced in the evenings, studied until 2 or 3 in the morning.

[1953](https://www.google.com/books/edition/Overseas_Information_Programs_of_the_Uni/ftxJAQAAIAAJ?hl=en&gbpv=0)

>We know that we have a great responsibility when speaking to people who risk freedom and even life when they listen to us. We know ours can never be a 9-to-5 job from which one goes home and forgets.

[1956](https://www.google.com/books/edition/Sunset/b7f67REBtx4C?hl=en&gbpv=0)

>His is no 9-to-5 job. Too much is at stake; the lifetime dreams of those he serves.

[1957](https://www.google.com/books/edition/The_Hospital/z3QXAQAAIAAJ?hl=en&gbpv=0)

>Nursing does not appear to be all that was promised in the classroom. They feel inefficient, and long for a nice tidy '9 to 5 job' in an office or elsewhere.

Observations:

- There was a vague sense of 9-to-5 as a ""women's work"" stable job -- finding your wife a 9-to-5 -- but it wasn't universal; the phrase was more nebulous and could include both men and women.

- 9 to 5 was ""low responsibility, low stakes""; you could ""go home and forget"".

- 9 to 5 was ""nice and tidy"".

My general point is that the exact hours of 9-to-5 were picked up as a catchphase *very early*.

The first quote is notable because it involves railroad workers. Railroads are where much of the early 8-hour day push happened. The Adamson Act of 1916 -- only two years earlier -- established an 8-hour workday for a certain subset of railway workers, and other workers soon demanded the same. Here's another quote from the same section:

>Bros. Martin, Lee, Umbaugh, Howard, and Lynch are still on the 12-hour grind. However, relief has been promised ... When these brothers first made heir request for an 8-hour day, before joining the O.R.T., it was almost ignored. Since their committee strolled down to see the boss man a new man is now posting on the job, with the promise to line up more extra men as soon as they can be secured, thus giving the boys their ""eight-hour day"".

(Also, notably, early railroads are the *only* instance I have been able to find where 9 am to 5 pm are regularly the actual exact hours; it is possible they were even the origin of the phrase, but there isn't enough evidence to tell.)

9-to-5 certainly did not describe a typical job. It described, in some sense, an *ideal* job.

I can find no point in the history of work where it was ""the most common"". For example, in 1937, in the District of Columbia, 2,892 women who worked in department stores were surveyed about their working hours. Only 7 worked 40 hours exactly and only 15 worked 48 hours exactly. Overwhelmingly, the most common number of hours was exactly 45 (9 hours a day for 5 days a week).

I unfortunately haven't been able to find any survey of an exact start time for 1980, a survey from 1991 gave 8 to 5 as the most common hours, and the general data on hours indicate very little change between the two. So, in summary: there is no missing hour: as a catchphrase that dated back more than hundred years; even though such hours have existed in the past and even still exist today, the phrase ""9-to-5 job"" hasn't meant the actual hours of 9 am to 5 pm for a very long time.

...

Costa, D. L. (2000). The Wage and the Length of the Work Day: From the 1890s to 1991. Journal of Labor Economics, 18(1), 156-181.

DeVault, I. A. (1991). ""Give the boys a trade"": Gender and job choice in the 1890s [Electronic version]. Work engendered: Toward a new history of American labor (pp. 191-215). Ithaca, NY: Cornell University Press. 

Southerland, A. T., Best, E. L. (1937). Women's Hours and Wages in the District of Columbia in 1937. United States: U.S. Government Printing Office.",0
"> So, stunningly biased?

You know what, I'm going to take a moment to highlight this, even if nobody sees it this far down the comment chain.

Firstly. History is created by humans, written by humans, written for humans, studied by humans, interpreted by humans, read by humans.

The inherent problem here is that the human is a stupid, selfish, blinkered creature with entirely too many prejudices, preconceptions, and biases, and a very sharply limited point of view. In other words, since we're all human here, it's not about 'biased' or 'unbiased' - **everyone and everything is.** It's built into history. The historical method does not focus on 'unbiased' history **because there is no such thing**. Instead, the question is, *how* is this person biased, and *how* does it affect how they view the events at play?

And here we come to the reason why u/Snapshot52, who is Nimíipuu, took the lead in drafting the American Indian Genocide Macro. A lot of people are very fortunate in that they can simply study history as some detached topic, a quick jaunt into the past to experience what it was like and then out again. It's just that thing you do.

But for *a hell of a lot of people*? It's not 'just' history. It seeps into everything around you, and even everything in you.

I'm not Native American like Snapshot52 is, but I am Filipino. My country and its peoples were subjected to colonialism of a slightly different type than the Native Americans, but there are enough similar scars. As a Filipino, I carry the scars of colonialism *every goddamn second of my existence*. My nationality is something that we reclaimed from our colonizers, right before we were subjected to a second round. That second round forced on my country the language I use now. It is because of colonialism that I bear my names, which my ancestors would not recognise. It is because of colonialism that I speak a language my ancestors would not understand. It is because of colonialism that I work in an industry that exploits the drive to be 'more white', because that is the lash that the Filipino appearance labours under. ""Your born colour is bad. You must seek to be like the conqueror. If you look like the conqueror, you will be more beautiful.""

Oh, biased, you say? But is this not *our* history? Our scars as a colonised people? What, am I to step aside and let someone else speak for the Philippines? Then who? The colonizers who **invaded** us, **stole** our lands, **murdered** our gods and our cultures, and took from us not just our grace, our dignity, our own **freedom in our own land**, but also our names, our memories of who we were before?

*Those* people? So you are not content with murdering us, but you must also deny us our turn to speak, when it is **our story** that we are telling? You're really going to stand there and tell the Natives that they're 'too biased'...because they're Native? Then white people must recuse themselves from all matters relating to European or European-American history. Are they not themselves 'stunningly biased'?

You really want to continue the oppression that colonised peoples have faced for centuries?",0
"> Google's ngram viewer for English-language books (confined to English, since Victorian England was mentioned in the question) shows peaks of references to ""Egypt"" or ""Egyptian"" in 1690, again in 1738, and 1803. (source: Google ngram viewer)

You can't rely on ngram for these early years, because there is so little in the way of raw data.  The ""peak"" in 1690, for example, consists [overwhelmingly of Christian religious material](https://www.google.com/search?q=egypt&lr=lang_en&tbs=lr:lang_1en,cdr:1,cd_min:1685,cd_max:1695&tbm=bks&ei=l-2_W5eQO9u_0PEP-qeF2AI&start=0&sa=N&biw=1563&bih=795&dpr=1.82), not what we would think of as historical books about ancient Egypt.",0
">""Bambi,"" she whispered, and every now and then she raised her head, listened to the sounds of the forest, and sniffed the wind. Then she kissed her child again and was relieved and happy.

>-- from the original _Bambi: A Life in the Woods_

OK. I know what you want to hear about. You want to hear about what children of the 1940s thought when the hunter shot Bambi's mother.

We'll get to that. But some context will help understand the adult reaction, so let's talk a little about the original Austrian author, Felix Salten, who wrote both the original book as well as the sequel, _Bambi's Children_.

In 1921, the year before the book came out, he was mostly known as a journalist, writing a column in a Zionist paper with concerns about anti-Semitism and being critical of those who would hide their Jewish heritage. (He did have one famous book, but it was published anonymously: the pornographic _Josefine Mutzenbacher_ from 1906, of a sex worker who ""experiences everything a woman can experience"" and ""regret[s] none of it"".) He was also, importantly, a hunter; he had, by his estimate over his life, killed 200 roebucks, and his long forays with nature were what inspired his work:

>Bambi would never have come into being, if I had never aimed my bullet at the head of a roebuck or an elk.

He was adamant that ""Hunters can be compassionate"", and that the act of eating and hunting an animal was akin to an act of veneration, part of the same circle-of-life as other animals. The original novel was far heavier on the circle-of-life aspect, with predators acting like predators would. From Chapter Two:

>A long pause ensued. They walked on quietly again until Bambi finally asked with a certain unease: ""Are we also going to kill a mouse like that one day?""

>""No,"" his mother replied.

>""Never?"" asked Bambi.

>""Never ever,"" came the answer.

>""Why not?"" Bambi asked, very relieved.

>""Because we never kill anybody,"" his mother stated bluntly.

Death is subsumed into a larger pattern. Hunters, the world of people, were essentially deities, referred to as ""He"", but the most important death is near the end, at the corpse of a hunter:

>“Do you see, Bambi,” the old stag went on, “do you see how He’s lying there dead, like one of us? Listen, Bambi. He isn’t all-powerful as they say. Everything that lives and grows doesn’t come from Him. He isn’t above us. He’s just the same as we are. He has the same fears, the same needs, and suffers in the same way. He can be killed like us, and then He lies helpless on the ground like all the rest of us, as you see him now.”

The sequel, _Bambi's Children_ (1939), is even grislier (at least in the original German, the English translation reduces the violence and also removes the section on moose mating season) but importantly, it should noted a hunter character is in that novel as well, intended to be a ""humane"" hunter, and is modeled after Felix Salten himself. He didn't really consider his writings to be ""for children"", and wrote to his American publisher that

>At this time I beg you most urgently, quite apart from softenings, not to advertise my work as a children’s book or to launch it otherwise in such a way.

To summarize: the original was meant to bring soul and perspective to wildlife, but from someone used to hunting, who didn't step back from depicting animals killing each other, nor being killed by hunters, and the work -- despite becoming a children's classic -- was never really aimed in that sense. (It was, in the late 20s, a Book-of-the-Month club selection along with works like Bernard Shaw's _The Intelligent Woman's Guide to Socialism and Capitalism_ and _The Omnibus of Crime_ as edited by Dorothy Sayers.)

You might also surmised -- based on the date and the author's Jewishness -- _Bambi's Children_ was written when he himself was in peril, having fled to Switzerland from the Nazis. This led to one of the adult reactions, noting similarities between plights of animals and of Jews, with an American critic calling the fox character in _Bambi_ the ""Hitler of the Forest"". (The actual Nazis had taken notice and also considered it an allegory, banning the original novel in 1936.)

It is perhaps understandable there might be some political reading, given the release date of the movie: 1942.

...

_Bambi_ was a difficult film. It was embarked on the same year as the release of _Snow White and the Seven Dwarves_ (1937) yet came after _Pinocchio_, _Fantasia_, and _Dumbo_. (The rights had been in obtained in 1933 when Salten sold them outright, making $1000 flat. He made no royalties.) Disney went all-in on a desire for realism, and artists spent months in a park sketching from life; as opposed to a cartoonistic squishing and stretching of such mainstays as Mickey Mouse, Disney wanted his movie to look like a real forest.

The script also reflected the desire for a more natural story. Quoting Disney himself:

>We were striving for fewer words, because we wanted the action and the music to carry it.

This ended up being successful enough it was criticized; the New York Times called out the clash between cartoon and naturalism, writing that the movie ""throws into relief the failure of pen and brush to catch the fluent movement of real photography.""

Ticket sales weren't stellar, but WWII essentially was thrashing all the studios; it was also radically different from Disney's prior movies, with no magic whatsoever. (As Disney later said, ""when we released that picture and there was a war on, and nobody cared much about the love life of a deer"".)

Hunters were the most upset. Any kind of message of hunter-as-caretaker (which is admittedly more of an element in the never-animated sequel, although Dell Comics did a licensed Disney adaptation in 1943) was absent. Before the movie even came out, the editor of _Outdoor Life_ (Raymond J. Brown) had seen a preview and sent a telegram informing Disney that shooting deer in the spring was illegal, and objected to the depiction of hunters as ""vicious destroyers of game"". He tried to get a foreword put into the film but was unsuccessful, and tried upon the film's release to get hunters to rally.

He was perhaps not being absurd, as there was some public outcry against hunting at the time and it even had effect on policy; the year after the film's release, Aldo Leopold (author of _A Sand County Almanac_) pushed for an antlerless deer season and was shot down due to the public outcry: in other words, people didn't want hunters to shoot Bambi's mother.

...

Get on with it, you cry: were the children of the 40s traumatized?

I would say most definitely yes. The original scene, incidentally, was worse. Donnie Dunagan, the voice of Bambi, had seen the production version which did _not_ have the death offscreen; there was a bullet hole and you could see the mother's face as she was dying. Walt said (as reported by Dunagan):

>Take that out. Just suggest that the mother was shot.

Dunagan also reports, upon showing the final movie, ""mothers put their hand over the children’s eyes"".

The most vivid report I've found from not long after the movie's release (1949) is from Mr James Kenyon at the floor of Parliament; this was in reference to juvenile policies in general, and he was telling an anecdote:

>A few weeks ago I took my two daughters, who are children, to the pictures to see a film called ""Bambi"" ... It was a children's matinee and the cinema was packed with children. Much is said about juvenile delinquency, but I was cheered when I saw the reactions of the children to the film: the happy, joyous, and lovable scenes in that film called forth their admiration and joy. When it came to the hunting scene, when the dogs were there and the animal was shot, **the shock and horror that went through the cinema could be felt**. I realised then ... that the heart of our children, the children of the nation, is sound. They will stand for the things that are good, true, kind, clean and wholesome while they will revolt against that which is cruel, merciless and unclean.

Stephen King famously called _Bambi_ the first horror movie he ever saw.

...

Lutts, R. H. (1992). The trouble with Bambi: Walt Disney's Bambi and the American vision of nature. *Forest and Conservation History*, 36(4), 160-171.

Reitter, P. (2015). *Bambi's Jewish Roots and Other Essays on German-Jewish Culture*. United Kingdom: Bloomsbury Academic.

Tobias, R. B. (2011). *Film and the American Moral Vision of Nature: Theodore Roosevelt to Walt Disney*. United States: Michigan State University Press.

Whitley, D. (2016). *The idea of nature in Disney animation: From Snow White to WALL-E*. Routledge.",0
"We really have no way of knowing, because the people compiling lists of sex workers historically have generally been the people who thought that single women who lived alone were probably prostitutes. I would point to Amy Froide's *Never Married: Singlewomen in Early Modern England*:

> Urban authorities, such as those in Manchester, assumed that sexual immorality would be the by-product of any never-married woman living on her own. Officials elided singlewomen who lived in their own lodgings with prostitutes who rented lodgings from which they plied their trade. The line between a singlewoman who worked and lived on her own and a prostitute became a thin one. This seems to have been purposeful, since it allowed urban authorities to control any independent woman under the guise of moral policing. These urban ordinances caused the morality of all never-married women to be called into question and created a precedent for legislating and controlling singlewomen’s sexuality.

It's not about statistical probability. It's about patriarchal assumptions.",0
"Finally a question I feel I can actually answer, since I recently completed a research paper on the topic of Mossadegh's coup. I relied heavily on press and magazine sources to try an capture American popular perception of Mossadegh and related how this perception was informed by Cold War attitudes.

As a previous reply noted, the Mossadegh coup and it's lead up is far too complicated to adequately explain in a comment. I can give you an idea of how Americans understood Mossadegh and by extension how they understood the coup.

From the beginning of the nationalization crisis in 1951 to the coup in 1953, the situation in Iran was generally framed as heavily volatile, with many articles stressing the danger of communist subversion by the Tudeh party. Mossadegh, who the leader of the National Front party which included nationalist liberals *and* leftists, was often portrayed as toying with forces he could not control. Mossadegh's tendency to weep and faint became a focal point for derision in American press as ""evidence"" of his weakness and effeminacy. One article in the *Houston Post* labeled him a ""Fainting Francis"" or ""Weeping Willie"".^(1) Another *Time* piece asks ""What ails Mossadegh"" and answers that his "" “tantrum-my temperament” and “excitability"" were to blame.^(2) Perhaps the most potent image of Mossadegh in American minds came from *Time*'s 1951 Man of the Year piece. The article was hardly an honor for him, as it highlights his  “tantrums”, calls him “peculiar”, and compared him to a “willful little boy.”  The article paints a picture of an infantile, neurotic, and thoroughly foreign leader who has the entire world worriedly watching his shifting and unpredictable will.^(3) This sort of understanding was the standard in the press throughout the nationalization crisis.

American perception of Mossadegh as effeminate, indecisive, and naïve would have been benign without the additional hyperawareness of so-called anarchy on the streets of Tehran. Iranian politics was certainly characterized by mob violence, riots, and assassinations brought on by political turmoil, unemployment, and occasionally foreign (sometimes US sponsored) subversion. In the McCarthyite era, American reaction to exposure to images of turmoil were almost anaphylactic in nature. Newspapers and magazines built on this fear and created a feeling that Iran was close communist takeover. Americans, who at home were chasing communist ghosts, believed that Iran, along with much of the world, appeared to be in the process of burning. This conviction in conjunction their disbelief in the capability of fainting Mossadegh to handle the threat led the US to justify toppling Mossadegh.

Newspapers were more blatant in their depiction of Iranian chaos than they were of Mossadegh’s weakness. Just glancing at headlines written by one *New York Times* contributor Michael Clark, a person got a sense of disorder: “Iran Kept in Turmoil by Oil and Communism”^(4), “Terrorism Called Silent Ally in Triumph of Mossadegh”^(5), and “Mossadegh, Home Again, Faces Growing Crisis” to name a few.^(6) A common theme was to conflate nationalism with turmoil. One *New York Times* article contended that, like Arab nationalism, Iranian nationalism was conceptually unlike western nationalism or patriotism. Instead, the author espoused that Iranian nationalism was fanatical, religiously motivated, and prone to violence. Mossadegh, meanwhile, was painted as stirring forces over which he had no control. This article fits within the larger trend of growing American antipathy toward third world nationalism as a dangerous and chaotic force. Mossadegh was typed as unwittingly playing with a dangerous fire which he would not be able to extinguish. 

 Magazines also contributed to the sense of chaos in Iran. In the wake of Stalin’s death on March 5, 1953, *Life* published an article headed by “False God Dies, Crisis is Born.” The subsequent pages featured pictures of contenders for Soviet leadership, leftist agitation across the world, and communists executing landlords in China. Iranian political violence is included with photographs labeled with “confusion brings conflict” and “blood to Iran.” American fears would have been undoubtedly been aroused by the photographs. The subsequent article, “The Anglo-American Job” argues that Washington and London had to work together with “energy and political resourcefulness” to solve worldwide instability and prevent collectivist domination.^(7) To any reader the situation did not require a nuanced understanding; the world was in danger and action had to be taken. The positioning of the Iranian pictures next to those of communist subversion around the world forced a connection between Tudeh-inspired violence and the monolithic danger of world communism. *Life* magazine’s call for cooperation between the two powers, directly following images Mossadegh and riots in Tehran, was almost prophetic considering the MI6 and the CIA were sowing the seeds of the coup at the time of publishing. 

Evaluation of the press’ characterization reveals that a dominant narrative that Mossadegh was an incompetent leader and that Iran was in chaos established itself in the first year of the crisis. Alternate, less alarming, and more neutral descriptions existed, but they were subsumed in the months following Mossadegh’s visit so that by 1952 Americans thought of him as a threat to security.

**To more directly answer your question**, I would contend that the average well-inforemed American would not have pieced together that the US facilitated the coup or have noticed the involvement of oil companies. Certainly they would have understood that oil men were heavily involved as it was no secret that George McGhee, a former Texas oilman turned diplomat, attempted to facilitate a deal between the British and Iranians. However, in light of the press' focus on communism, questions about Mossadegh's character, and American Cold War attitudes, I doubt they would have saw an economic motive behind his overthrow. Even among scholars now it is debated whether anticommunism or oil forced US intervention (I land on the anticommunism side).   


Works Cited    

1.  Paul Gallico, “Several Good Reasons for Being Thankful”, *Houston Post*, (November 18, 1951) 

2.  “What Ails Mossadeq?” *Time* 58, (October 29, 1951), 43.  

3.  1951 Man of the Year: Mohammed Mossadegh,” *Time* 59, (January 7, 1952), 18-21

4.  Michael Clark, “Iran Kept in Turmoil by Oil and Communism”, *New York Times*, (April 29, 1951)

5.  Michael Clark, “Terrorism Called Silent Ally in Triumph of Mossadegh”, *New York Times*, (November 29, 1951)

6.  Michael Clark, “Mossadegh, Home Again, Faces Growing Crisis”, *New York Times*, (November 25, 1951) 

7.  Edward Crankshaw, “False God Dies, Crisis is Born,” *Life* 34, (March 16, 1953), 20-33 and Walter Lippman, “The Anglo-American Job”, 34 

Secondary Source

**B**alaghi, Shiva. “Silenced Histories and Sanitized Autobiographies: The 1953 CIA Coup in Iran.” *Biography* 36, no. 1 (2013): 71–96. https://doi.org/10.1353/bio.2013.0009.

Jacobs, Matthew F.. *Imagining the Middle East: the Building of an American Foreign Policy, 1918-1967*. Chapel Hill: University of North Carolina Press, 2011.

Abrahamian, Ervand. *The Coup 1953, the CIA, and the Roots of Modern U.S.-Iranian* 

*Relations.* New York: New Press, 2013.

Ansari, Ali M. *The Politics of Nationalism in Modern Iran.* Cambridge: Cambridge University 

Press, 2012.

Collier, David R. *Democracy and the Nature of American Influence in Iran*, 1941-1979. 

Syracuse: Syracuse University Press, 2017.

Elm, Mostafa. *Oil, Power, and Principle : Iran’s Oil Nationalization and Its.* First edition. 

Syracuse, N.Y: Syracuse University Press, 1992.

Heiss, Mary Ann. *Empire and Nationhood : the United States, Great Britain, and Iranian Oil,* 

*1950-1954.* New York: Columbia University Press, 1997.

Little, Douglas. *American Orientalism : the United States and the Middle East Since 1945.* 

Chapel Hill: University of North Carolina Press, 2002.

Gasiorowski, Mark J. “U.S. Perceptions of the Communist Threat in Iran During the Mossadegh 

Era.” *Journal of Cold War Studies* 21, no. 3 (2019): 185–221. 

https://muse.jhu.edu/article/731287.",0
"***...cont'd***

In Philadelphia, a Quaker woman named Elizabeth Drinker kept a diary for decades, and was among the people who criticized the public revelry that sometimes went on. In her Christmas Day 1793 diary entry, she wrote:

> Christmas, so called, kept by some pious, well-minded people religiously, by some others as a time of frolicking.

In New York in 1789, Abigail Adams, then the Second Lady of the United States, [wrote a letter](https://founders.archives.gov/documents/Adams/04-09-02-0001) to a friend, commenting on how different the New York holiday season was compared to the reserved celebrations in her native New England. Evidently, it was the first time she encountered the now omnipresent baked good known as a  ""cookie"":

> ...[I]n this state, & particularly in this city is celebrated with every mark of pleasure and satisfaction. The shops and publick offices are shut. There is not any market upon this day, but every person laying aside Buisness devote[s] this day to the social purpose of visiting & receiving visits. 
>
> The churches are open & divine Service performed begining the year in a very proper manner by giving Thanks to the great Governour of the universe for past mercies, & imploring his future Benidictions there is a kind of cake in fashion upon this day call’d New years *cooky*. This & cherry Bounce as it is calld is the old Dutch custom of treating their Friends upon the return of every New Year. The common people who are very ready to abuse Liberty, on this day are apt to take rather too freely of the good things of this Life...

Also in New York that Christmas were the President and First Lady, George and Martha Washington. Being from Virginia rather than New England, George Washington didn't express any surprise at the New York celebrations in his diary. He just briefly mentioned that Martha held an open house:

> Went to St. Pauls Chapel in the forenoon.
>
> The Visitors to Mrs. Washington this afternoon were not numerous but respectable.

While there are a couple accounts of Americans from New York/New Jersey/Pennsylvania from this era who spent a Christmas in the South and observed the celebrations there were muted in comparison to what they were used to, the Washingtons wouldn't have been as caught off-guard as Abigail Adams was. As one indication, in a December 26, 1786, letter, Washington wrote to a friend who couldn't make it to Mount Vernon for Christmas that year, that his absence ""has deprived us of your aid in the Attack of Christmas Pyes"". He tells the friend that ""one [pie] yesterday on which all the company, (and pretty numerous it was) were hardly able to make an impression"". From Washington's account, among others, the Southern tradition of Christmas as a domestic feasting day wasn't too far off from how it was celebrated in New York City and Philadelphia, albeit, with so much of the South being rural, there was little opportunity for any kind of public, drunken disturbances of neighbors like there were in those Northern cities.

So, to answer your question, this is what was going on in New York at Christmas in the late 1700s and early 1800s. It was celebrated as a domestic day of feasting with friends and family, with some visiting with friends, neighbors, and other associates, where drinks would often flow freely. There was very little emphasis on Christmas Day as being the ""children's day"". That change mostly occurred in the 1820s and after. However, I think Nissenbaum may be slightly overstating the case when he insinuates that commercial gift-giving wasn't really present at all before 1805. He profiles John Pintard at length in his book, who was a founder of the New-York Historical Society and did a great deal to promote St. Nicholas as a patron saint of the city. Nissenbaum argues that it was the efforts of Pintard, Washington Irving, Clement Clarke Moore, and their circle of high society friends who made Christmas as the children's day an ""invented tradition"".

However, he doesn't mention that Pintard himself didn't seem to think he invented anything, but instead, thought he was trying to preserve a pre-Revolutionary War tradition associated with New York in general, and the Dutch, in particular. He wrote a letter to his adult daughter on January 21, 1819, which suggests he had known the tradition of the gift-delivering Santa Claus from his New York City youth in the 1750s and 60s:

> ""In old times St. Class used to cross the Atlantic & brought immense supplies of cookies &c. from Amsterdam, ship loads for every house & family was visited, not only in this city [i.e., New York City], but in Albany & all the intermediate towns.""

So, when Washington Irving wrote in the 1812 second edition of his book *Knickerbocker's History of New York*, that St. Nicholas dropped gifts down New York chimneys, it's unlikely he was inventing the tradition, either. Nissenbaum is right, however, that both men certainly did promote a sentimental, domestic version of the holiday, and were against the public displays of drunkenness and brawling. Irving most thoroughly championed his sentimental view of Christmas in his 1822 novella ""Bracebridge Hall"", contained within the collection *The Sketch Book of Geoffrey Crayon* (the same collection which included ""Rip Van Winkle"" and ""The Legend of Sleepy Hollow""). 

The following year, the poem ""Twas the Night Before Christmas"" added several new elements to the Santa Claus legend, which did much to aid the perception of Christmas as the ""children's day"", since he's making his visit on Christmas Eve, rather than St. Nicholas Day or New Year's. Though, even by then, the New York City newspapers were already filled with seasonal advertisements for Christmas and New Year's gifts, so the poem didn't invent the tradition, but it certainly did much to popularize it.

And contrary to the text in the Wikipedia article, there were some observations that the holiday got worse, not better, as it was commercialized and the modern Santa Claus emerged. For example, in 1844, a woman named Laurie Todd wrote a reminiscence of the Christmas of 1794 in New York City, when she was still a child. She remembered going to the Methodist Chapel on John Street with a friend in the morning. She says the debauchery of ""shotgun weddings"" had only been replaced by bachelors visiting brothels with underage prostitutes, and the domesticity had given way to a worship of money and material possessions:

> ""On that day the stores and work-shops were nearly all shut up, a few belonging to the Friends [Quakers] in Pearl-street excepted. Then men had time to worship God; now they have only time to worship Mammon—that golden calf in Wall-street. Then we had only two banks, and not one broker; now we have thirty banks and ten times ten score of brokers...Then people were happy; now they live in splendid misery...
>
> ""Then if we took a notion to get married, we finished our day's work at 7 PM as usual, got supper at 8, put on our Sunday coat...and at 9 we walked to Rev. Dr. John Rogers in Pine-street, or Rev. Bishop Provost in Vesey-street....Now the bachelor of thirty-five takes his bird of fifteen to the public table of Madame B____'s boarding-house, or the promiscuous group in Howard's Hotel, where she suffers from the stare of some impudent, brainless blockhead...and this is the refinement of the nineteenth century.""

As to your question about Santa's Dutch origins, hopefully, this [previous answer of mine](https://www.reddit.com/r/AskHistorians/comments/k7v066/when_did_santa_claus_become_part_of_mainstream/gexauok/) may answer it. Suffice it to say that there was a general stereotype of Dutch New Yorkers as country bumpkins, who smoked too much, drank too much, dressed in an old-fashioned way, and often sported facial hair especially in wintertime. The description of St. Nicholas in ""Twas the Night Before Christmas"" is a play on all this. This wouldn't have been too lost on the parents of the 1820s reading the poem to their children (the names of the reindeer ""Donder and Blitzen"" is a dead giveaway, even if the spelling isn't quite right), though it's subtle enough that the characterization is not too well-known anymore. Especially considering that Santa's image has changed quite a bit from that original poem, the image of him as a parody of a country New York Dutchman of the early 1800s is only preserved in some elements of his modern form.",0
Thank you! I’m so used to the modern world that I forgot Japanese troops in the pacific couldn’t easily record Navajo radio transmissions and transfer the recordings to Japan to crack. I suppose transcribing Navajo was also tough since Navajo is so different from English and Japanese.,0
"Well, I think I can answer the first half of your question, though I don't know enough to answer the second half beyond the vague familiarity with questions about iconography and iconoclasm that comes with a Catholic upbringing and one year at a Jesuit university, so I think I'll leave that aside and stick to the Islamic half of your question.

Ok, with that being said, let's break this down into constituent parts.

Firstly, I want to answer something that you didn't ask, but is good for background:

1. Why was Muhammad named ""Muhammad"", what does that name mean and how do Arabic names work?

We might actually go back even further and ask an interesting side question of why Muhammad was named Muhammad. Etymologically in Arabic the name ""Muhammad"" means sort of 'one who is praised' or 'one who is praiseworthy'. In pre-Islamic Arabia it seems to have been a highly unusual name for males. My understanding is that all of the surviving examples of other men with the name Muhammad in pre-Islamic Arabia were also holy men. The female version ""Muhammada"" seems to have been much more common. This has even produced some (albeit fringe) speculation that Muhammad might have been an appellation rather than a name as such. I don't buy that, nor is it necessary to.

Etymologically it's also worth noting that as Arabic is a semitic language it works on tri-letter roots, in this case H-M-D, and therefore the name Muhammad is etymologically related and similar in meaning to names like Ahmed, Hamid and Mahmoud.

Also, ""Muhammad"" would not have been Muhammad's only personal name. Arabic names can also include what is known as a Kunya. Most commonly this takes the form of ""Father of X"" with X being your oldest male child. But it can also be an epithet. The first caliph Abu Bakr's name literally means ""Father of Camel Foal,"" but it just means that he spent a lot of time riding camels and had a family that was engaged in herding. Muhammad's kunya was Abu al-Qassim, the father of Qassim, who was his son who died in early childhood.

Arabic names also typically include a patronymic or nasab (ibn Fulan...son of so-and-so). This can string along for as long as is necessary back to whichever ancestor is most relevant for tribal or other purposes. It can also skip generations to get to a more uniquely identifying individual. I'm less familiar with pre-Islamic naming practices but it's also worth mentioning laqabs, which today are often translated as simply surnames but in a pre-modern context could include nickname-like titles, particular for important individuals and rulers, as well as nisbas, which today are likewise often a last name but could also refer to a tribe or place of origin. 

I'll come back to this later (and I'm sure one of our native Arabic speakers here will correct me if I've mis-identified part of this) but to take the example of a well-known medieval religious scholar you might have a name like: **Aḥmad** (Ism, or name) **bin Muḥammad** (nasab) **bin Ḥanbal** (nasab) **Abū ʿAbd Allāh** (kunya, i.e. Ahmad's son is named Abdullah) **al-Shaybānī** (tribal nisba, indicating descent from the [Banu Shayban](https://en.wikipedia.org/wiki/Banu_Shayban)).

--------

Phew, ok, with that background moving into your questions. I'll start with the easier one first:

2 . Why do Muslims name their male children Muhammad?

There's potentially a very long explanation of getting into how exactly these practices developed and whether this was universally true at all times and particularly in the earliest years of Islam, but to cut a long story short, the practice of Islam is based on carrying out God's commandments. Etymologically ""Islam"" means submission to God or to God's will as delivered through the message of his Prophet, the seal of the prophets (i.e. the last and most perfect prophet), Muhammad.

In Islam, the Qur'an is the perfect record of God's speech and revelation to mankind. But very early, in all likelihood during Muhammad's lifetime, it became assumed or presumed that in the absence of a Qur'anic message or commandment, it is best to follow the example of the Prophet in all things, down to the most minute details. Together the collected records of this side of Muhammad's personal life is known as the ""sunna"" and is recorded in the Hadith, and gives its name to ""Sunni"" Islam. NB that Shia Muslims also follow the prophetic example but also believe in following the examples of Ali and his descendants (who are also Muhammad's descendants, as he married Muhammad's daughter, and was incidentally his ~~nephew~~ [edit: h/t /u/asoomdeys ] cousin to begin with).

In any event, included among these hadith are statements by Muhammad suggesting that Muslims name themselves and their children after himself: https://sunnah.com/search/?q=use+my+name

""Name yourself...?"" you may be asking. Well, one of the traditions in Islam is to take on an Islamic/Arabic name when you convert. It's not 100% guaranteed universal, but was very common.

Also note that Muhammad forbade the use of his Kunya, Abu al-Qassim. Like all religious strictures of course, this one has been broken historically: https://en.wikipedia.org/wiki/Abu_al-Qasim

3 . Why is it that Muslims are opposed to the depiction of Muhammad?

This is slightly more complicated, at least historically or when discussing historical practices. The contemporary sort of Salafist or ultra-orthodox answer is very simple, which is that images of human beings generally are potentially idolatrous. In practice this can be somewhat bi-polar: the Arab Persian Gulf states plaster the faces of their rulers on practically every building, but only put gazelles and coffee-pots on their coinage.

In any event, the concern with images of Muhammad in particular is multifaceted. There is concern that the image of Muhammad is perhaps uniquely susceptible to idolatry and he must therefore, in particular not be depicted. Insofar as it is taken as being commanded against in the religion, depictions of Muhammad are therefore also ipso facto blasphemy. And lastly there is concern that in the case of cartoons or depictions of Muhammad drawn by non-Muslims that these are an assault on the honor of Muhammad (who, as we've discussed, is basically regarded as a perfect human being) and on the religion of Islam itself. It's therefore an incredibly emotive issue.

The more complicated answer would be how that came to be and whether it was always so as /u/FlamingBlankets and /u/Newtothisredditbiz allude to. This is a historiographically complex and deeply debated question, which to my knowledge largely focuses on two distinct periods in Islamic history.

The first is earliest Islamic period. Representational art was not really a feature of pre-Islamic Arabic culture. After the conquest period Muslims apparently adopted representational art quite readily. So you have the frescoes in the [Umayyad desert palaces](https://www.google.ae/search?q=frescoes+umayyad&safe=active&source=lnms&tbm=isch&sa=X&ved=0ahUKEwiQsqSEw8HQAhUKrY8KHXcKCd4Q_AUICCgB&biw=1680&bih=871) or the so-called ""standing caliph"" coins: https://www.google.ae/search?q=standing+caliph+coins&safe=active&source=lnms&tbm=isch&sa=X&ved=0ahUKEwil7bCcw8HQAhUKKY8KHQ0EAWYQ_AUICCgB&biw=1680&bih=871

There are lots of questions about this. Were these simply accommodation to pre-existing Greco-Roman and Zoroastrian culture? After all, these coins were clearly modeled on existing Byzantine coinage. Or did something about the depiction of humans in Islamic ideology change fundamentally under Abd al-Malik in the late 7th/early 8th century such that he adopted (or perhaps invented?) the more distinctly ""Islamic"" coinage style: https://attwiw.files.wordpress.com/2014/01/abd-al-malik-coin.jpg (And, indeed, did so in art and architecture as well, as in the construction of the Dome of the Rock.)

To bring this back to Muhammad, in particular, this is complicated by theories that the ""standing caliph"" in those coins might actually be Muhammad himself. This is unresolved, but there are plausible reasons for thinking that it might be.

I'm rapidly running out of space here, but suffice it to say by the end of the reign of Abd al-Malik, Islamic artistic and religious norm was distinctly an-iconic, and there are some really interesting essays and discussions about the interplay and dialogue between Islamic and Byzantine iconoclasm from this period.

The second period, which I'm far less familiar with, relates to the ""Persian Miniature"" tradition (which was also practiced by the Ottomans and Mughals.) I'm far less familiar with the historiography or debates around these images of Muhammad or why they subsequently went out of fashion and so I'll withhold comment.

-------------

Interesting addenda:

One of the features of the above that I find to be interesting is that there are actually far more Muhammad's in the world than many people may realize. Because it is traditional to name sons Muhammad, that obviously isn't an especially useful name to have for identification purposes-- like being named John Smith only much more so. The result in countries like Egypt is that it's very common for men to be named Muhammad but then go by a second name. So to give some well known examples: Anwar Sadat was actually Muhammad Anwar Sadat. Hosni Mubarak is actually Muhammad Hosni Mubarak. The ""Yasser"" in Yasser Arafat is actually an adopted name but he was actually Muhammad Arafat. Etc.

**Sourcewise**

I'm running out of characters and am at work but can post some citations and additional reading later tonight.",0
"**1. Think about women.**

Okay, yes, reddit as a whole is a deeply sexist place and any woman on this site has to deal with that. But despite strict, strict rules against bigotry and unwavering dedication to enforcement, our last few censuses of readers and flairs have placed the number of people self-identifying as women at around 15% of the sub.

Fifteen percent. That's *atrocious.* Somewhere around 45-50% of new history PhDs every year are women; 45% of high school social studies teachers are women.

From my informal observations of the flair community, our problem isn't necessarily attracting women in the first place--it's *keeping* them. And I get it. I really do. Because you don't see the real problem with AH being a user-driven history sub until you've monitored it for awhile looking for questions to answer.

The questions in this sub almost invariably adopt a male perspective Two of our absolute most-asked questions are:

* ""Did medieval babies all have fetal alcohol syndrome from their mothers drinking during pregnancy?""
* ""Did ancient and medieval soldiers have PTSD?""

Women as baby incubators damaging their children! Soldiers with PTSD from looting cities, raping women, and selling women and children into sexual slavery!

I could keep cataloguing these questions on and on.  /u/mimicofmodes is a fashion history flair, and will tell you that 90% of clothing-related questions we get involve men and neckties...

Meanwhile, I've answered questions like: 

* [""Wouldn't a visit to a brothel in pre-antibiotic days almost guarantee transmission of an STD? Maybe not with a single visit, but say after several? How is that sustainable?""](https://www.reddit.com/r/AskHistorians/comments/9gacbj/wouldnt_a_visit_to_a_brothel_in_preantibiotic/e635rz6/)
* [""In medieval Europe, was prostitution available to most men or only the emergent middle class? How widespread was prostitution?""](https://www.reddit.com/r/AskHistorians/comments/445918/in_medieval_europe_was_prostitution_available_to/cznmj63/) 

These both take an extremely woman-centric topic--in the Middle Ages as well as today--and turn it into questions about *men*. Questions that don't just neglect, but actively erase the experiences of women.

I'm not saying ""ask women's history questions."" **I'm saying, ""when you ask questions, realize that women existed and try to think about history from their perspectives, not just men's perspectives of them.""**

~~

**2. Think about women of color.**

For absolute heaven's fecking sake: *Stop asking questions about enslaver men raping enslaved women.* Women of color around the world have SO MANY STORIES. Heck, enslaved women of color in antebellum America have so many stories. Yes, a sickening amount of them involve being raped. But notice: ""she was raped"" still puts the focus on *her* and her experience. The questions we get are, ""How would the enslaver treat any potential children?"" and along those lines.

I could easily be making this point about men of color and about peoples of color more generally. But I think it's important to highlight just how NOTHING AskHistorians has in terms of content about women of color. 

~~

And that's what it is: a thousand paper cuts every week; not one gushing wound. (That's for the mod team to absorb the shock of, remove on reflex, and ban their ass with *glee*). It's relentless. 

And it's a great way to signal to WOC, white women, and (although I didn't discuss them here) MOC and NB people of all races that they are not welcome here--not part of history at all.

**Because remember: women don't just answer questions about women's history--in fact, most women are NOT women's historians.** When you lose the work of women historians, you're losing *history.* Period.

Yes, I realize the number of *repeat* questions we get about ancient PTSD and FAS mean that newcomers to the sub are asking them, and are not going to be reading this post. It's an entire way of thinking that we need to change. 

So maybe, when talking casually about history on other subs or in your life, empathize with women of all races, with POC of all genders. That doesn't mean think or talk exclusively about them, or even at all. Just realize that they have existed in history as people with thoughts, beliefs, motivations, and actions.

**And when you ask questions, think about *all* the people who are involved in the situation you're asking about. Think about them as people.**",0
"Jesus christ, those quotes in the intent section are god awful. Mods thank you for putting this together. It legitimately changed my opinion about a large swath of history.",0
"TL;DR Check out the books, *Been in the Storm So Long*, *Trouble in Mind*, *Slavery By Another Name*, *The New Jim Crow*, and *How Free is Free*.


One of the best books I've read on the subject is Leon Litwack's immense, *Been in the Storm So Long: The Aftermath of Slavery* (won a Pulitzer and National Book Award). It's nearly 700 pages, but covers a small time period, only about ten years. It's incredibly comprehensive and details every facet of life following Emancipation. It also deftly demonstrates the highs of Reconstruction quickly followed by the undoing of most progress.

Another one of his books, *Trouble in Mind: Black Southerners in the Age of Jim Crow*, may be more accessible, and I'd recommend starting with it first, since it's more of a survey that covers Reconstruction through to the Civil Rights Movement.

While *Trouble in Mind* is a survey of what life was like in many different areas, Douglas Blackmon's, *Slavery by Another Name: The Re-Enslavement of Black Americans from the Civil War to World War II* (also won a Pulitzer), focuses on how state governments and corporations exploited the 13th Amendment: ""Neither slavery nor involuntary servitude, **except as a punishment for crime**"". 

Following Reconstruction, corporations and business owners would literally pay local governments for prisoners to be used as slave labor on farms, in lumber, in coal mines, &c.. Local police would put out ambiguous arrest warrants, 'Strong Black Man Wanted,' arrest anyone that fit the description, and judges would quickly convict them and turn them over to business owners and labor camps. Or police would arrest men for ridiculous laws, like walking down the street alone at night, and they'd be sent to labor camps too. (A great accompanying text would be Michelle Alexander's, *The New Jim Crow: Mass Incarceration in the Age of Colorblindness*, which essentially picks up where Blackmon left off.) PBS made a documentary out of *Slavery By Another Name* (eponymously titled). https://youtu.be/UcCxsLDma2o

Litwack also published a very short book, nonetheless very moving, engrossing, heartbreaking, titled, *How Free is Free?*. I'd recommend it to everyone, even people who've never read or bought history books. You could finish it in an afternoon or two, but if you're new to the history, its brutality, its injustice, it will wake you up and hopefully get you interested in reading more.

If anyone has any other book suggestions, please let us know!",0
"I want to add a bit, but it’s literary analysis instead of historical, so I don’t think it’s really appropriate as a top level response. 

Specifically, I’d like to talk to the OP’s question about the lack of an over-arching plot. 

There is a long history of stories told by vignette that are loosely tied together by theme or character. This probably originated with oral and mythological traditions telling stories about heroes and gods through a series of discrete events rather than a big myth arc. Robin Hood and King Arthur are great examples of this, but figures like Hercules or Jack (of beanstalk/candlestick/Horner) fit too. 

It was the serialization model of story telling that really turned this vignette style into a real genre - [*Picaresque*](https://www.britannica.com/art/picaresque-novel#:~:text=The%20picaresque%20novel%20originated%20in,under%20a%20mask%20of%20hypocrisy.).  It originated in Spain, and it mostly differed by focusing on rogues - trickster characters who survived and succeeded through deception and cleverness (as compared to chivalric heroes who won by righteousness and the strength of their sword). 

The other major element of the genre was the way the trickster nature of the protagonist allowed the story to move through different settings - palaces and towns, forests and farms, outlaw caves and army forts. 

Of course, like any genre, subversions started almost as soon as it was codified. *[Don Quixote* is considered a classic example of a picaresque novel](https://blog.bookstellyouwhy.com/picaresque-authors-from-cervantes-to-bellow), despite the fact that the titular hero isn’t a trickster or rogue at all, and is instead basically a madman trying to live by chivalric ideals. But his madness allows the story the same flexibility that wits allowed others, and it was able to do much the same despite not operating around a true picaro. 

This tradition of storytelling shows up in film too, and although movies generally stitch their plots a bit tighter than an 19th century serialized novel, you can see the bones of the old plot formats. P[robably the most well known movie to use clearly recognizable picaresque stylings is *Forest Gump.](https://prezi.com/gh_8giploffe/a-comparison-of-don-quixote-to-forrest-gump/#:~:text=The%20Picaresque%20Tradition%20in%20Don,a%20journey%20of%20self%2Ddiscovery.)* Episodic story about a guy who’s to common to be ordinary, and his outsider qualities allow the story to flow through all sorts of settings. Other movies that I’d argue function similarly include *Crocodile Dundee,* *Bill and Ted’s Excellent Adventure,* *Harold and Kumar go to Whitecastle* (In fact if I was still in school I could probably write a few papers about how the raunchy sex comedies of the last few decades are picaresque), and *American Beauty*

I’d argue that Bambi was an attempt to follow picaresque conventions. Bambi wasn’t a rogue, but his childishness allowed him to move through and explore the setting in the same way that Gump’s childishness let him move through American history. 

So, to sum up I guess I’m just trying to point out that the vignette structure wasn’t that unusual or even unexpected to film audiences.",0
"You can listen to the BBC Radio 3 discussion where McKellen publically came out [here](http://www.bbc.co.uk/archive/gay_rights/12012.shtml), in conversation with the (very conservative) editor of the *Telegraph*, Peregrine Worsthorne. The first discussion of McKellen's coming out I can find is [a profile of McKellen in the British newspaper *The Independent* from February 20th, 1988](http://www.mckellen.com/writings/activism/880220independent.htm), which is preserved on Ian McKellen's webpage as his writing (which it clearly is not). McKellen's coming out is portrayed as political activism in opposition to 'Clause 28', mooted legislation forbidding local governments to promote the idea that homosexual relationships might be normal. McKellen became involved in activism against Clause 28 in early January. Maybe one could say that the future Gandalf was trying to tell parliamentarians: you shall not pass...this bill. (Despite McKellen's activism, it became law in May)

According to the summary of McKellen's activism about Clause 28 in the *Independent* article:

>On Sunday, 24 January, he walked on stage and on to television screens at the Laurence Olivier Awards. ""The greatest surprise"", says one of the invited audience, ""was the roar of approval that greeted him when he mentioned Clause 28"". The next day he shepherded a host of stars, and one baby, on stage at the Playhouse, before journalists and Lords bussed down the Embankment for the occasion. The radio and television blitz followed, and by the end of the week homosexuality and censorship were issues of national interest in a way unknown since the late Sixties.  

>To achieve this, McKellen played a very private card. One friend describes it: ""Coming out is the essence of gay liberation, and Ian understands that. There are only two or three other working actors who have done it, because it is dangerous. But he knew it was time to stand up and be counted."" Though he described himself as gay in a World Service programme on 19 January, the declaration was made to most of Britain on Radio 3's Third Ear, on 27 January.

>The occasion was a spirited argument with Peregrine Worsthorne. It provoked a remark about ""disgusting homosexual practices"" from the editor of the Sunday Telegraph, and a query laden with innocence from McKellen: did Mr Worsthorne mean the Garrick when he spoke about gay clubs? The following Sunday's leader did the Stop Clause 28 campaign no harm at all.

Unfortunately, the archives of the London *Telegraph* do not appear to go back more than 20 years, and Peregrine Worsthorne's response to McKellen's coming out in his paper is not accessible online as far as I can tell.

Additionally, McKellen wrote a piece for [*Capital Gay* magazine summarising his rather eventful 1988](http://www.mckellen.com/activism/activism_coming_out.htm). In it, he essentially claims that his homosexuality was not particularly a surprise to anyone in his life, given the theatrical circles he moved in:

>That I had actually come out probably surprised me more than my being gay can have shocked any listener who knew my work. Indeed, some of them have written to say that they'd known I was gay for years and couldn't care less. When I told my step-mother, soon after, she said the same. My friends had always known; so had my fellow actors, because backstage there are few secrets. But I'd always avoided saying I was gay to the media - even to the gay press - insisting that my private life was my own. My complacency had involved nothing more embarrassing than the occasional white lie about not being married - I was never, though, one to talk about looking for the right girl and relishing parenthood.

In terms of reaction in the US, the first mention I can find on Factiva is a reference in the *Boston Globe* to McKellen, in April 1988, winning the Norton Award for the best achievements in Boston-area theatre, for his one-man show, 'Ian McKellen Acting Shakespeare'. Giving a speech, accepting the award, according to a *Boston Globe* story by Jeff McLaughlin:

>With regard to the denouement of McKellen's remarks -- in which he recounted his decision to publicly acknowledge his homosexuality and work for tolerance and rights for gay people in Britain -- not even Elliot Norton knew what was coming. Norton said, in a Wednesday note, that McKellen had asked for at least 20 minutes to speak, but did not discuss his topic beforehand. ""But he had planned it,"" Norton wrote, ""to the point of writing out his whole talk and, in the half hour or so before the ceremony, studying part of it before a dressing-room mirror.""

>McKellen's speech may well approach historic status, and it certainly will become part of Boston theater folklore, since no one had thought ahead of time to set up a tape recorder.

One journalist's reference to McKellen from the point of view of someone not particularly inclined towards progressive views came from a review of the TV documentary Open Space on BBC2; Martin Cropper in *The Times* said of *Open Space* that:

>Open Space (BBC2) illustrated the condition [of homosexuality] with some predictable examples: the 'spanking judge', for instance, who resigned his recordership after exposure in the tabloids; the 'out' lesbian who enlisted in the Wrens specifically to meet like-minded adults, without realizing that the armed forces are the only sphere where such activity has ever been illegal. The most intelligent comment was Ian McKellen's plea for respectable homosexuals to help erode prejudice by declaring themselves publically - a move which would, however, posit a different society.

>As one has come to expect from access-television, the historical perspective was on the flimsy side. In this country at least, the 'homophobic' backlash started years before Aids had been heard of, for the obvious reason that some 'straights' had had it to here with the hectoring aggression of homosexual propaganda.

Another reference to Ian McKellen discussing his homosexuality comes from an article in the *Sunday Times* about the filming of a ITV Phil Donahue special (i.e., with the American talk show host filming a British version of the show): 

>On Wednesday the subject was clause 28. On the panel were two straights, Sir Rhodes Boyson and the Rev David Rushworth-Smith, and two gay actors, Ian McKellen and Michael Cashman, who plays Colin in EastEnders. Noisy claques in the audience booed Boyson and the vicar, and cheered the actors.

>Donahue had been told that the reserved British would never speak out; luckily many present were Americans who can utter words like ``sexuality'' without a blush.

>The debate goes like this. A fat woman is horrified to be surrounded by these homosexuals; a pigtailed young man says he's heterosexual but live and let live; a lesbian mother with two teenage children is rounded on by a man in favour of ""the normal family unit''; Donahue asks who is to say what is normal? And so on. When Boyson says, ""God created man and woman'' and McKellen rejoins, ""and together they created a homosexual'' such a sequitur is rare.

>A grey-haired English woman declares with dignity, ""I am the mother of a gay son'', but we hear no more from her: it is time for a commercial break. After it, Donahue has found another old lady. She says: ""People who are born like that, it's like being a cripple isn't it? I mean they can't help it] Well, it's sad they're born, isn't it?''

>Nobody in the audience knows whether to laugh or cry.

Ian McKellen was not the first theatre actor to come out as gay, as Simon Callow had come out in a book he wrote in 1984. In early June 1988, Callow mentions McKellen in a Diary piece written in *The Times*:

>The latest such event has been inspired by that fumbling piece of legislation, Clause 28 of the Local Government Bill. To raise money for the possible victims of the clause, an extraordinary group of artists Simon Rattle, Dame Judi Dench, Mike Gambon and many others across the spectrum of the performing arts are appearing at the Piccadilly theatre this Sunday. The event has been largely inspired and organized by Ian McKellen. It was his cheeky notion to call the show Promoting Homosexuality (echoing the ambiguous phrasing of the clause itself) but the intention behind the show is not polemical; it simply represents the anxiety of a large number of performers about a piece of law that affects us all.

Elsewhere, most of the references to Ian McKellen in Factiva from 1988 refer to his career continuing as per usual, which makes me suspect that nobody into theatre in England (or Boston) in the 1980s was enormously surprised, or cared particularly that a leading actor was gay; he starred as (the heterosexual) Profumo in the 1989 film *Scandal*. Ultimately, McKellen was knighted by the Queen in 1991; coming out in 1988 didn't stop him from symbolically being admitted to the establishment.

Nonetheless, anti-homosexual prejudice in the wake of the AIDS epidemic had a support base within the Conservative Party, not least with Conservative leader Margaret Thatcher, who in a 1987 conference speech said that ""Children who need to be taught to respect traditional moral values are being taught that they have an inalienable right to be gay...all of those children are being cheated of a sound start in life. Yes, cheated!"" Clause 28 remained on the books until the Labor government voted it out in March 2003. 

McKellen relates in the *Capital Gay* piece discussed Clause 28 with Tories at Westminister: 

>At Westminster, I met our lawmakers — and remembered their words: The Minister — ""I don't understand why young homosexuals need their own clubs — why can't they mix with everybody else of their own age?""  The Whip in the Commons — ""I'm sorry about Section 28 — it's just a bit of red meat thrown to our right-wing wolves.""  The Whip in the Lords — ""I'm sorry about Section 28 — but you appreciate my job is just to get our chaps to vote the right way."" 

",0
"Yay, Pillars of the Earth!

Agnes and Tom in the forest is a little different situation from a peasant family in a community, whether they own the house or are (the medieval equivalent of) subletting a room, but you probably guessed that part.

So among other things, a woman in medieval Europe most likely had a roof over their heads and something at least resembling a bed beneath her when she gave birth. But more to your question: the people assisting her were probably almost entirely women.

Midwives, for example, were mostly still women in the Middle Ages. Of course, healers' specialties were overlapping, so it's not like a barber would never serve as a midwife, and there's one awesome case where a woman noted in tax records as a midwife is the sort of 'official' medical practitioner at a *men's* monastery. (So either she had a broader skillset, or there were some very unmonastic things up in that monastery.)

But it does seem like midwives were mostly women. And formal physicians would only be called in for the most dire emergencies--basically, when it was all but certain that the woman giving birth was going to die, and they wanted a last-ditch, desperate effort to save the baby long enough to baptize her or him. (Midwives were, however, recognized by the Church as having the power to perform baptism if it was the absolute only way for the baby to be baptized before she or he died.)

Second, the other people in the room would probably also be female. In fact, from looking at upper-class Italy, Christine Klapisch-Zuber argued for a pattern of men *trying* to assert influence and even presence in the room where a woman gave birth/laid-in afterwards. (To which point: men tried to edge in on the midwife profession/midwives' traditional responsibilities towards the end of the Middle Ages into the early modern era.) But women seem to have persevered and kept childbirth a strong female and community experience. 

The major thing missing from PotE relates to my main problem with the book (which, let it be said, is my favorite fiction book): *religion*. Not that everyone in the Middle Ages was super-religious and praying all the time (BUT PHILIP IS A MONK. Yes, he's the prior and had other responsibilities, too. But BENEDICTINE MONKS SPENT MOST OF THEIR TIME *PRAYING AS A GROUP.*) 

But there's plenty of evidence for women keeping religious charms--objects, scraps of parchment/paper with ""magic letters"" (as we'd say) on them, or saying specific prayers--in the birthing room with them. St. Margaret of Antioch was the patron saint of women giving birth, so in the later Middle Ages, objects associated with her, or letters from her name, were popular. 

Tom's family in PotE isn't particularly religious (which does make the book more palatable to people today, I get it; from a modern standpoint, medieval Christianity could get weird AF), and the mid-12th century is a little early for the overwhelming popularity St. Margaret veneration. But there should at least be *some* kind of reference to a protective prayer or object or what we would see as a spell/charm.

I'm not sure how I feel in terms of historical accuracy with Tom and his family leaving Jonathan behind. On one hand, within medieval Christianity at the time, anyone not baptized was bound for hell. On the other, at least in 15th century Germany, there were definitely religious instruction books whose authors did backflips trying to justify non-baptized babies' entrance into heaven. It's kind of heartbreaking to read, actually.

Those religious, sort of ""self-help"" books--which were enormously popular among lay readers--also give us a couple of scraps of info into the lives of pregnant women that are harder to see in medical texts (which overwhelmingly focus on the infant). Dietrich Koelde in 15th century Germany writes:

> Furthermore, [pregnant women] should not run or ride or dance or do heavy work

The reference to dancing is kind of hilarious because a lot of didactic writers (and city sumptuary codes) frowned on public dancing, but I digress. Information like this couldn't apply very well to women who had to work daily, but the general principle of not being able to do the heaviest labor/heavy labor not being the healthiest idea was well integrated into later medieval culture.

But surely men would have some responsibilities in taking care of their pregnant wives? Let's listen to Koelde again:

> Furthermore, the woman should be meek and she should not anger her husband, because much evil and harm results from this.

Oh no, I'm sorry, it's not that men shouldn't abuse their pregnant wives. It's that pregnant women shouldn't make their husbands abuse them.

Obviously, these are all generalized and prescriptive situations. In practice, women were individuals and, if they had the ability, could and did change things up as they wanted. Queen Eleanor [not of Aquitaine] and King Henry III of England insisted that he be able to stay in France with her until she had given birth. Matilda of Saxony, on the other hand, insisted on traveling to the current residence of her father, King Henry II (the famous one), when she was nearing time to give birth to two of her children. Women who had the chance to decide what would help keep them comfortable, did so.

[SPOILERS FOLLOW]

One final, brighter note. The mortality rate for a woman giving birth in the Middle Ages was somewhere between 1.5% to 2.5%, to the best of our abilities to figure--and lower in rural parishes than urban ones. So there's at least one part of PotE that isn't *wrong*, but it's also not very *typical* for medieval Europe. But wow, would the other outcome change the rest of the story!",0
"> offering to head a conference of the six slave and free states closest to the border, but this too was rejected

This conference *did* happen. It was the Peace Conference of 1861, held starting on February 4 at the Willard Hotel in Washington, D.C., and adjourning on February 27. Tyler was elected at the conference to preside over it, although Tyler's open-mindedness as willing to be a force for compromise at the conference was questionable. He wrote a lengthy ([very lengthy](https://virginiachronicle.com/?a=d&d=RE18610118&e=-------en-20--1--txt-txIN-%22john+tyler%22+%22ex%252Dpresident%22+%22compact%22-------) - see columns 4-8 on page 1) letter to the *Richmond Enquirer* on January 18, 1861, that proposed the conference, but only after about four columns of why he thought secession was justifiable, and why the U.S. Constitution, in his view, operated under the ""states' rights"" ""compact theory"". He even wonders aloud why ""there should exist so great an instability of public opinion in regard to the origin and character of the government"" under the Constitution, that he thinks allows for any state to secede at their pleasure.

Anyway, the conference happened, and not much came of it. It ended with a 9-8 vote (each state represented getting two votes) in favor of extending the Missouri Compromise line all the way to the Pacific Ocean. This was a plan recommended way back in 1846 by Democratic Sen. William W. Wick of Indiana during debates on the Wilmot Proviso, when both North and South had decided back then that it wasn't enough of a concession from the other side, and failed in Congress. 

What Tyler's home state of Virginia was advocating for at the *close* of these ""negotiations"" at the Peace Conference were as follows:

* Extend the Missouri Compromise line to the Pacific Ocean.

* A Constitutional Amendment that says the federal government can never abolish slavery in Washington D.C. or in the slave states.

* Congress cannot interfere with the slave trade between the slave states, and travel/passage of enslaved people accompanied by slaveholders must be allowed through the *free* states.

* When an escaped person cannot be recaptured, then the free state where they reside must pay monetary restitution to the original slaveholder.

* The Three-Fifths Clause in the Constitution cannot be repealed except by unanimous consent of all the states.

* ""People of Negro blood to be ineligible for the franchise, whether federal, state, or municipal.""

So much for those ""states' rights"" of the Northern states! The Northern border states had proposals of their own, but all anybody could agree to was Virginia's first provision. And even that, barely. And these were the *border* states, not the more extreme states in the Deep South and Northeast.

Tyler evidently soured on the whole ordeal quite quickly. He actually left to attend the first day of Virginia's Secession Convention in Richmond on February 13, but then returned to see the Peace Conference out. [The day after the Peace Conference ended](https://archive.org/details/letterstimesoftyv2tyle/page/616/mode/2up):

> ""On the 28th of February, after the adjournment of the Peace Convention, Mr. Tyler stood on the steps of the Exchange Hotel, in Richmond, and told the people what he had done to preserve the Union; he informed them that he was now confident no arrangement could be made, that every hour's delay was perillous, and that nothing remained but to act promptly and boldly in the exercise of State sovereignty.""

He took his seat for the duration of Virginia's Secession Convention, and voted for secession on April 4, in a vote that failed 90-45. On April 17, after the Battle of Fort Sumter, the secession convention held another vote. Tyler again voted in favor, and that one passed 88-55.

He did talk a good game about being a compromiser, but he, like many people at the time, treated ""compromise"" as ""I can totally get the other side to concede to all our demands by giving them one or two table scraps"".

**SOURCES**: 

[*The Peace Convention of February, 1861*](https://www.jstor.org/stable/25080547) by Samuel Eliot Morison

[*The Letters and Times of the Tylers*](https://archive.org/details/letterstimesoftyv2tyle/page/616/mode/2up) by Lyon Gardiner Tyler

FWIW, to OP's original question, I think it's worth noting that aside from President, there were a few other high-level Cabinet officers around the same time as John Quincy Adams who took seats in Congress. Most notably, Adams' Secretary of State Henry Clay in 1831, and his Vice-President John C. Calhoun in 1832 took seats in the Senate, both within 18 months of when Adams started his term in the House. While both had gaps in their tenures, all three men served in Congress until the late 1840s or early 1850s, all dying in office, with Clay the last to pass away, in 1852.

Before them, there was also Timothy Pickering, who had served from 1795-1800 as Secretary of State under George Washington and then John Adams. He went on to serve two terms in the U.S. House, from 1813-17. And while not elected, President James Madison did accept an invitation to serve as a delegate to the Virginia Constitutional Convention in 1829, where the state adopted a new constitution. He wrote some preliminary thoughts on the matter in [a January 1829 letter](https://founders.archives.gov/documents/Madison/99-02-02-1663), writing that enslaved people should not be given rights to representation under the new constitution ""as people"", and he doubted they'd even get rights as ""privileged property"". 

So there was *some* precedence for what Adams did, of high-level officers of the executive branch re-entering public life in a politically partisan capacity, but Adams was certainly the most visible at the presidential level at the time he ran for Congress in 1830.",0
"It's a pity you didn't allow art, since there's hundreds upon hundreds of art 
pieces made by people from this period, including [animals now only found much farther south](http://www.iro.umontreal.ca/~vaucher/History/Evolution/Climate/img/Hippo_Lybia.jpg).

But: stories it is. What we can
do is talk about something that happened as a result of the African Humid Period ending.

Just to be clear, the change between wet to dry in the
subtropics was because of the precession of Earth's orbit. The
Earth ""wobbles"" on its axis so that where the Earth points
moves in a circular fashion every 26,000 years. The particular
orientation led to greater monsoons and the Green Sahara in question, but it was
doomed to end. Because it is based on the Earth's direction the change
was gradual by latitude, so monsoon levels started dropping from north
to south.

Additionally, climate change was clearly abrupt in some local sites, although the exact nature of this is still a subject of research.

However, it isn't as if the moment the Green Sahara is declared ended in a particular place meant the climate just
ossified in place. For Ancient Egypt and the Nile in particular we have historical
knowledge about the level of discharge fairly accurately (try [this figure](https://www.researchgate.net/profile/Leszek-Marks/publication/259563288/figure/fig1/AS:297223272321024@1447874958305/Palaeoclimate-changes-in-the-Nile-catchment-against-Egyptian-chronology-the-Nile.png))
and you can see a drop from the ""end"" of the Green Sahara period all
the way to the Old Kingdom.

The Nile keeps dropping during the Old Kingdom period -- that's
when the Pyramids were built -- until finally hitting rock bottom roughly
around 4200 BP (before present, so roughly 2200 BCE). This also is known as the Old Kingdom Drought, or more universally, the 4.2 ka event.

Now, there's some controversy here. I'm not going to argue about the global universality of the event, but I will say the science (using isotope ratios) is fairly firm about there being a Nile drought. And we definitely have narratives about that.

There's some Middle Kingdom texts of interest in this; Ankhtifi, one of the nomarchs or ""Great Chiefs"" of the Middle Kingdom has 
[a text](https://www.osirisnet.net/tombes/moalla/ankhtifi/photo/ankhtifi_cm_164.jpg) about
a period where ""All of Upper Egypt was dying of hunger, to such a degree that everyone had come to eating his children"" -- and he goes
on to explain how his nome (region) nobody went without hunger, and he was able to loan grain to others, even though people were
like (locusts?) starving going north and south looking for food.

This feels like self-promotion and is also slightly out of the time span. There's also the well known _Admonitions of Ipuwer_ or _Ipuwer's Annals_ (depending which translation you use) written 800 years after the fact which discuss a collapse of the Egyptian monarchy with riots and looting...

>See now, things are done that never were before,

>The king has been robbed/deposed by beggars/rabble.

>See now, men rebel against the Uraeus Serpent,

>The disintegration of the country through fighting:

>See now, fire has leaped high,

>Its flame will attack the land's foes!

>Men stir up strife unopposed.

>See, the land is tied up in gangs,

>The coward is emboldened to seize his goods.

...with dipping hard into sorrow.

>If only this were the end of man, no more conceiving, no births

>But since giving birth is desired, grief has come and misery is everywhere.

>So it is and will not pass, while these gods are in their midst

While science backs up a drought, archaeology does not necessarily back up events so extreme, but again, if we're simply on the lookout for folklore on what things were like at the disaster of the Nile, maybe it doesn't matter if it was real.

Perhaps the most interesting text -- as it claims a ruler from the actual Old Kingdom -- is one that, oddly, was made much, much, later. It in fact dates to the Ptolemaic Kingdom (332 to 31 BCE) but the text, known as the Famine Stela, claims to be in regards to pharaoh Djoser, 3rd Dynasty, he
of the famous [step pyramid](https://upload.wikimedia.org/wikipedia/commons/9/93/Zoser_Pyramid_%282347235367%29.jpg), a stepping-stone (ahem) of sorts to the Great Pyramid.

>I was in mourning on my throne,

>Those of the palace were in grief,

>My heart was in great affliction,

>Because Hapy had failed to come in time

>In a period of seven years.

>Grain was scant,

>Kernels were dried up,

>Scarce was every kind of food.

>Every man robbed his twin,

>Those who entered did not go.

>Children cried,

>Youngsters fell,

>The hearts of the old were grieving;

>Legs drawn up, they hugged the ground,

>Their arms clasped about them.

>Courtiers were needy,

>Temples were shut,

>Shrines covered with dust,

>Everyone was in distress.

In short: there was a famine of great length, and Djoser inquires with priests where the god of the Nile is born. He is
told of a sacred spring at a temple of Khnum. The high priest Imhotep goes to the temple and falls into a dream:

>I am Khnum, your maker!

>My arms are around you,

>To steady your body,

>To safeguard your limbs.

>I bestow on you stones upon stones,

>That were not found before,

>Of which no work was made,

>For building temples,

>Rebuilding ruins,

>Inlaying statues' eyes.

The story ends with Khnum's temple being rebuilt, causing the drought to end and the Nile to flow mighty once more.

It was written more than 2,000 years after the events (and ""seven years"" is suspiciously similar to the analysis of Pharaoh's dream in the Bible by Joseph). It is still faintly possible it was copied off an earlier preserved text; again, even if entirely written after the fact, it suggests a strong memory of the Nile being depleted at the end of the Green Sahara, long after the events occurred.

...


De Menocal, P. B. (2015). End of the African humid period. *Nature Geoscience*, 8(2), 86-87.

Middleton, G. D. (2017). *Understanding collapse: Ancient history and modern myths*. Cambridge University Press.

Nicoll, K., & Zerboni, A. (2020). Is the past key to the present? Observations of cultural continuity and resilience reconstructed from geoarchaeological records. *Quaternary International*, 545, 119-127.

Sołtysiak, A., & Fernandes, R. (2021). Much ado about nothing: assessing the impact of the 4.2 kya event on human subsistence patterns in northern Mesopotamia using stable isotope analysis. *Antiquity*, 95(383), 1145-1160.",0
"So I have no memory of seeing this movie, though my wife says we saw it in the theaters, but in any case: it's not likely Jack would have been to Singapore, because during the early 1700s, ish, when the movie is set, Singapore wasn't really a thing. It was the property of the ~~Sultan of Malacca~~ Sultanate of Johor, and it wasn't until 1819 with the arrival of the British East India company that the islands started to grow in prosperity. 

That said, though, there's no reason not to think Jack may have been to East Asia in general, or visited/traded with the Dutch settlements in Indonesia. I believe that the character was involved with John Company in some way before becoming a pirate, and the company had trade interests in East Asia during that time period. Many sailors of the time would have taken posts in merchant ships regardless of nationality, so it wouldn't be out of the range of possibility for Jack to have sailed there in a Dutch hull. Or he could have served on a voyage of exploration -- the English mariner and sometime privateer William Dampier made several voyages of exploration to East Asia and Australia, starting originally as a privateer in Virginia then in the Caribbean, around the time of _POTC_. 

**Edited to add:** A lot of the removed comments (which I can see because I'm a moderator) are really confusing the distinction between a pirate and a privateer. A privateer was a ship captain or master who had a letter of marque from a government, allowing them as a private citizen to legally prey on enemy shipping without being subject to the issuing government's laws regarding piracy. (The more rare letter of reprisal was issued to a captain whose goods may have been stolen by a representative of a foreign nation, allowing them to do the same in reprisal.) The US Constitution allows Congress to issue letters of marque and reprisal, though the Paris Declaration of 1856 legally renounced privateering; the US is not a signatory to that treaty, but hasn't commissioned any privateers since 1815 anyhow. 

Unsurprisingly, may nations regarded foreign privateers as pirates in any case, but there is a distinction in law. Captain William Kidd, for example, sailed under protection of a letter of marque, though he exceeded its authority and was convicted of murder and five counts of piracy in what was essentially a political trial. 

**Edit the Second:** So I probably shouldn't try to answer a popular thread like this while also trying to be at the park with my 5-year-old, because I thought I wrote this a bit more clearly. The example above of Dampier is what I had in mind as a direct analogy to Captain Jack: William Dampier was a privateer captain and later commissioned officer born in Somerset in 1651, who joined a privateer crew in the Caribbean in 1679; he was part of raids on Spanish possessions on the west coast of New Spain (now Mexico), then was part of an expedition that crossed the Pacific to the East Indies and returned eventually to England, having circumnavigated the globe. 

I'm sourcing this mainly from *The Social History of English Seamen, 1485-1649*, edited by Cheryl A. Fury, and *Royal Tars: The Lower Deck of the Royal Navy, 875-1850* by Brian Lavery. Sorry it's vague, but there's not a lot to be said about the accuracy of what is after all a fictional (and over-the-top) series of pirate movies that are based on a theme park ride.",0
"We actually don't know as much as we'd like about what N.A.M. Rodger referred to as ""the female half"" of the Royal Navy as we'd like -- there were some women aboard ship, and there were obviously many, ahem, casual carnal relations between men in port and sex workers, but we just don't know a ton about how many men were married, how many of those married men may have stepped out on their wives when they were at sea, and so forth. 

The reason for that is pretty simple -- unlike our current militaries, which enlist men and women for a fixed number of years, other than some standing petty officers (the bosun, carpenter and gunner) any man who enlisted for a voyage enlisted for the duration of the voyage, and was paid off when the voyage ended. (By voyage, I don't necessarily mean a ""there and back again"" thing, but the length of a unit of service of the ship, which could range from a season to several years -- this would be at the discretion of the Admiralty.) So for our purposes that means a couple of things: first, for enlisted men, there was nothing like a universal RN roster that would track them and have overall statistics about them; and second, what details we have of pay are episodic. In Rodger's work cited above, he estimates that maybe 20% of men were married, but that's a very rough estimate. 

But let's assume our hypothetical sailor does have a wife, and he's shipped off for a three-year voyage to the East Indies and back. How does she cash his pay while he's away? 

She doesn't. 

The reason for this is that sailors during the period I study would generally only be paid at the end of a voyage. If a ship were well-found in prize money, captains or pursers could distribute some cash at port stops, or pursers might have a supply of ready money to advance to seamen who had a need to spend it, but all that would be taken out of a seaman's wages that were generally just paid as a lump sum at the end of a commission. Given the hazards of maritime life, this made some economic sense, at least from the Admiralty's perspective -- no point in paying wages at home to the spouse of someone who might have been long dead by the time word reaches back to Britain. But the system put economic strain on the Treasury in times when many ships might be paying off at once, and during the Dutch wars of the 1650s/60s the Admiralty found a simple way to save money was to simply keep ships in commission, rather than paying them off in the fall as was usual. This expedient helped the Navy stay thrifty in terms of its spending, but created massive economic strain for the seamen and families involved in the Navy -- many of whom had also been impressed in this time period. On top of this, when men were paid, it was often in the form of tickets that would have to be cashed at the Admiralty in London, leading many men to have to sell them at sharp discounts to speculators rather than attempt to travel there themselves. 

Rates of pay often stayed stagnant for quite some time -- one of the reasons for the great mutinies at Spithead and the Nore in 1797 was that seamen's wages had not been raised since 1700. Though inflation was rather low during the 18th century in Britain, prices had risen enough that discontent spilled over into a large portion of the fleet refusing to sail at a crisis in the struggle with France. Able seamen were paid £1 4s per lunar month (one pound, four shillings -- there were 20 shillings in a pound and 12 pence in a shilling) from 1700 to 1797, when their wages were raised to £1 9s 6d (one pound, nine shillings, six pence). 

One interesting feature of the Navy was that it calculated wages on the theoretical basis of 13 lunar months plus one day per year (two days in leap years), though in practice the 13 months tended to be budgeted for and the day/s simply ignored. 

Hopefully this answered your question. I have lots of other answers on my profile page: 

https://www.reddit.com/r/AskHistorians/wiki/profiles/jschooltiger",0
"> I was out hunting together with two Yukaghirs, an elderly and a younger hunter, and they had succeeded in killing a brown bear. While the elderly hunter was poking out its eyes with his knife and croaking like a raven as custom prescribes, the younger one, who was standing a few meters away, shouted to the bear: “Grandfather, don’t be fooled, it is a man, Vasili Afanasivich, who killed you and is now blinding you!” At first the elderly hunter doing the butchering stood stock-still as if he were in shock, but then he looked at his younger partner and they both began laughing ecstatically as if the whole ritual were a big joke. Then the elderly hunter said to the younger one, “Stop fooling around and go make a platform for the grandfather’s bones.” However, he sounded by no means disturbed. Quite the opposite, in fact: he was still laughing while giving the order. The only really disturbed person was me, who saw the episode as posing a serious threat to my entire research agenda, which was to take animism seriously. The hunter’s joke suggested that underlying the Yukaghir animistic cosmology was a force of laughter, of ironic distance, of making fun of the spirits. How could I take the spirits seriously as an anthropologist when the Yukaghirs themselves did not?

> I experienced several incidents of this kind which, I must now admit, I left out of my books on Yukaghir animism, as they posed a real danger to my theoretical agenda of taking indigenous animism seriously. One time, for example, an old hunting leader was making an offering to his helping-spirit, which is customary before an upcoming hunt. However, while throwing tobacco, tea, and vodka into the fire, he shouted, “Give me prey, you bitch!” Everyone present doubled up with laugher. Similarly, a group of hunters once took a small plastic doll, bought in the local village shop, and started feeding it fat and blood. While bowing their heads before the doll, which to everyone’s mind was obviously a false idol with no spiritual dispositions whatsoever, they exclaimed sarcastically, “Khoziain [Russian “spirit-master”] needs feeding.” Direct questioning about such apparent breaches of etiquette often proved fruitless. One hunter simply replied, “We are just having fun,” while another came up with a slightly more elaborate answer, “We make jokes about Khoziain because we are his friends. Without laughter, there will be no luck. Laughing is compulsory to the game of hunting.” 

This incredible quote comes from an article by Danish anthropologist Rane Willerslev, so these were somewhat recent events; but I think the underlying idea is not a modern invention. If the spirit world helps us and exists all around us, then why exactly should we choose any one particular object to represent that spirit? And if we choose an object in jest, it doesn't dissipate the potency of the ritual because *we are his friends*. The spirits may laugh at us when we fail, why can't we laugh back at them? 

There are many societies who (at least historically) had periods of ceremony in which masqueraders would run around the village ""being clowns,"" as in doing tricks or pranks for a laugh. Sometimes they'd go further and really pick on certain people, and these were usually the ones who were rude or took themselves too seriously during normal times. The spirit world made itself physical in the form of a masquerader who, for the sake of laughter, helps remind individuals to laugh at themselves. Regarding ""sacred clowns"" in North American indigenous communities Peggy Beck & Anna Walters summarize this idea:

> ...we heard a number of individuals say that to learn you should not “ask why.” By asking “Why” you limit your chances of experiencing sacred knowledge. Another reason people say you should not “ask why,” is that the subject being asked may be too dangerous. Without proper instruction beforehand the person asking “why” might be harmed. In Native American communities the Clowns are the ones that “ask why.” They are often the only ones that may “ask why” in reference to dangerous objects, or “ask why” of those people who are specialists in advanced sacred knowledge. They ask in their backwards language, through their satire, and their fooling around, the questions we would like to ask. They say the things we might be afraid to say to those we might be afraid to speak to. Even though they may not or cannot conceptualize their knowledge, the answers to our questions - the truths, the philosophy, and the wisdom - comes through to us.

In the Roman world this experience was found in Saturnalia - when the social world was reversed. For a day or a short while, a slave in the household would act as *pater familias*, and was served by the master's family or simply served first. In this brief period in December slaves could vent their grievances and act on whatever they had been holding back. A frightening thought for a master, and so masters should never forget this and act accordingly on every other day of the year. But of course, this brief reversal would end; and so slaves too should not forget that any grievances aired would have to be bottled back up - it worked both ways. This tradition continued into the Christian world in a new form: ""Bishop for a Day,"" in which an altar boy was given this honor. And more generally the riotous rule-breaking festival was continued in the form of *Carnival*. A festival in which everyone dressed in masquerade, and under such armor you could even mock the clergy publicly. 

Saturnalia also included giving little gifts and sometimes these were serious (money, statuettes, books) but other times they were jokes - gag gifts. Of course, a gift intended as a joke is not really a *gift*; but that interpretation is too simplistic, because then the gift is the experience of laughter itself. This tradition continues unabated, gag gifts are still given at Christmas. And their nature as a *joke* does not diminish their value as an expression of one's love and friendship for another. In fact, the ability to give this type of gift may even hinge on such factors - would you give a gag gift to a social superior in a formal setting? 

But back to your question about jokes, they are so difficult to detect in historical texts. The late 16th century Italian gnostic and heretic philosopher Domenico Scandella said to an inquisitor, *""You might as well go and confess to a tree than to priests and monks.""* And at first glance, this appears to be an insult and a joke; in line with his other comments against monks and priests who think they're better than everyone else. But, other peasants reported him saying things like *""Everything that we see is god, and we are gods...The sky, earth, sea, air, abyss, and hell, all is god.""* While these comments are not directly from him, it does cohere with his other holistic sentiments; so knowing this we can look at his insult again in a new light. Perhaps he meant it as a joke, but perhaps he was quite serious...confessing to a tree was as valuable as confessing to a human since what difference was there really? 

From Rane Willerslev's work we only get brief glimpses at people joking with the spirits. In the ancient world we normally see people laughing at deity statues when they're coming from condescension, as Jewish Yahwhist prophets mock other Jews for creating false icons. Christians continued this trope, mocking pagans and their ""false idols"" which can't actually do anything for their worshipers. At the beginning of Aristophanes' play *Frogs*, we see a comic servant character Xanthias arguing with his master none other than the god Dionysus. Sometimes Xanthias is saying a joke at the god's expense, and sometimes the god is saying one at his. But to add a meta twist, the audience would've been laughing with/at the god all the while being seated with a statue of that god - as a statue was brought into the theater during the City Dionysia festival which Aristophanes wrote these comedies for. Afaik situations like this is the closest we can get in ancient texts if we're looking for examples of people ""laughing at the gods."" 

I originally used Rane's quote in an answer about [How did European preconceptions distort the study of Native American mythology?](https://www.reddit.com/r/AskHistorians/comments/cgqtn8/was_the_study_of_native_american_mythology_shaped/eur7qcc/) and I've written about Domenico Scandella in an answer about [How were 16th century ""atheists"" treated by society?](https://www.reddit.com/r/AskHistorians/comments/hq5fvb/what_would_be_the_consequences_of_being_openly/fxytojj/). If you'd like to read about indigenous North American ""clowns"" there's this great article [Sacred Clowns and Fools, by Beck & Walters](https://web.archive.org/web/20121025125710/http://cometogetherarticles.yolasite.com/sacred-clowns-and-fools.php), and if you'd like to read some about Saturnalia there's a great article [Encyclopedia Romana: Saturnalia, by James Grout](https://penelope.uchicago.edu/~grout/encyclopaedia_romana/calendar/saturnalia.html) and for more details there's [Celebrating the Saturnalia: Religious Rituals and Roman Domestic Life, by Fanny Dolansky](https://www.google.com/search?sxsrf=ALeKk03EmuOpRjLb8O367MxBj-7UCs-fLw%3A1609278182029&ei=5qLrX6CpAYms5wKi-ajACg&q=celebrating+the+saturnalia%3A+religious+ritual+and+roman+domestic+life&oq=celebrating+the+saturnalia%3A+religious+ritual+and+roman+domestic+life&gs_lcp=CgZwc3ktYWIQAzoECAAQRzoJCAAQyQMQFhAeOggIIRAWEB0QHjoFCCEQoAE6BQghEKsCOgcIIRAKEKABUIBeWP99YMV-aABwAngAgAGYAYgByx2SAQUzMC4xMZgBAKABAaoBB2d3cy13aXrIAQjAAQE&sclient=psy-ab&ved=0ahUKEwigtImzlPTtAhUJ1lkKHaI8CqgQ4dUDCA0&uact=5) and the second link at canvas.brown.edu downloads a pdf of it.",0
"Ok, trying this again with a little more detail because I think my first answer was removed. 

The 21st Amendment, passed in December 1933, repealed the 18th Amendment, which mandated nationwide prohibition. By February 1934, all pending cases dealing with prohibition violations were ordered to be wiped from Federal Court dockets. There were roughly 9000 such cases at the time. The Supreme Court ruled that the repeal of the 18th Amendment meant courts could not sentence or inflict penalties based on that amendment in pending cases (*U.S. v. Chambers,* 1934). 

No general rule was established regarding pardons or commutations of sentences for those who broke dry laws before the repeal of Prohibition, however. U.S. Attorney General Homer Cummings recommended leniency for casual offenders, but not those who made careers out of illegal liquor practices. 

Part of this lack of forgiveness for larger-scale liquor violations (involving bootleggers, rum runners and the like) stems from *revenue* violations. The 21st amendment repealed the 18th, but it did not override specific tax and permit violations incurred in the process of illegally distributing liquor. In 1934, for example, the Justice and Treasury departments expressed a desire to ""vigorously to prosecute all forms of liquor-tax evasion and other frauds upon the revenue which have resulted in a tremendous loss to the Government.""

At a state level, there are examples of larger-scale pardons granted for dry law violators. A New York Times article from Feb. 26, 1933 describes Indiana Governor Paul V. McNutt's decision to repeal the state-level prohibition enforcement law before Prohibition was ended at a federal level. McNutt announced that he planned to pardon or parole roughly 400 people who were serving sentences for liquor charges at the time. ""If these men were kept in prison after the liquor law is repealed, they would be political prisoners,"" McNutt said. He only granted amnesty to people charged with transporting, possessing, or selling liquor. He did not include people who were incarcerated for public intoxication or driving under the influence -- these charges would remain illegal even after prohibition ended.

Similarly, in 1932 California Governor James Rolph announced plans to pardon the roughly 1000 liquor law offenders held in California prisons following the state's decision to repeal the California liquor prohibition law. 

In summary, pardons depended on the specific nature of the crime, especially when it came to prosecution at a Federal level. There are certainly instances of retroactive pardons, some on a large scale. 

&#x200B;

Sources: 

""INDIANA WILL FREE LIQUOR PRISONERS."" *New York Times,* Feb 26, 1933

""Annual Report of the Attorney General of the United States for the fiscal year 1934."" Department of Justice, Jan 5, 1935. 

""GOV ROLPH TO FREE DRY ACT VIOLATORS."" *The Baltimore Sun*, Nov 5, 1932. 

""BIG OFFENDERS NOT TO GAIN BY DRY LAW EDICT."" *The Baltimore Sun,* Feb 6, 1934.",0
"The claim made in the TED-Ed video you mentioned is not entirely accurate. While it is true that some businessmen and intellectuals initially supported Hitler, it is an oversimplification to claim that they did so solely because they believed his extreme rhetoric was only for show. During the early years of Hitler's rise to power in the 1930s, some businessmen and intellectuals in Germany did see potential benefits in aligning themselves with the Nazi regime.",1
"**18th Century Degenerates**

/u/onthefailboat has given an excellent answer, and I want to reinforce the fact of our modern notions of race being fixed and intrinsic as something that developed in the centuries after (and in response to) contact with the Americas. It's not until the 18th Century that we see the solidification of the notions that would inform ""scientific"" racism and our own modern ideas of race, which is one that is primarily based on inborn qualities distributed on a geographic basis. 

The Comte de Buffon, an important transitional figure, still believed that the physical markers we consider so important for race (like skin color and appearance) could change over a single person's life, depending on their behavior and diet. Yet, he also believed that humans were ultimately subject to their environment, and that the environment of the Americas could cause the degradation of any people living there. Part of this is because he thought the Americas were *literally* a new world, having arisen from the sea after Eurasia and Africa, and thus being a land of swamps and wetlands whose noxious miasma would degrade the constitution of any living creature subjected to those conditions. Buffon, like so many who cast aspersions on the Americas, never visited any part of the continents. His assertions of miasmic conditions giving rise to nothing but stunted and weak creatures was famously refuted by Thomas Jefferson sending him the stuffed body of a moose. 

Cornelius de Pauw, coming a generation after Buffon and drawing heavily on his work, also relied on a theory of environmental degradation, and specifically the environment of the Americas as producing weak and malformed creatures. He took a more extreme view than Buffon though, for where Buffon claimed that climate led to the genitals of Native American men being small and poorly functioning, de Pauw hypothesized that all the men were functionally impotent, requiring the bites of insects to swell their penises for sex. The effect of the humid climate on Native American women, however, left them with such well lubricated genitals that babies slid from them easily and with barely a notice from them. The intellect of these people was also decried and de Pauw is infamous for outright dismissing *any* accomplishments of Native Americans and calling conquistador accounts lies, saying the depiction of Tenochtitlan as a grand metropolis was mere fantasy and that the palaces the Spanish wrote about were nothing more than huts. Keen's *The Aztec Image in Western Thought*, records a representative summation of de Pauw's view of the people of the Americas:

> Is it not astounding to find half the world occupied by men without beards, without intelligence, tainted by venereal disease, and so debased that they are incapable of being trained -- a defect that goes hand in hand with stupidity? The inclination the Americans have always had for the savage life proves that they hate the laws of society and the restraints of education, which, by dominating the most immoderate passions, are the only means that can raise man above the animal.

De Pauw, like Buffon, never actually visited the Americas. The popularity of de Pauw's extreme views, however, actually prompted Buffon to moderate his views and even write in defense of the achievements of Americans. The position of de Pauw, that Native Americans were fundamentally inferior and without achievement, would become the consensus among European naturalists. Cañizares-Esquerra's *How to Write the History of the New World* does an excellent job of tracing how the very same native works and oral accounts which the early Spanish based their works upon had, by the 18th Century, become evidence of the inferiority of Americans. He notes that a review of Clavijero's *La Historia Antigua de Mexico*, a book generally portraying the Aztecs as a civilized people (also, of note, the root of using ""Aztecs"" as opposed to ""Mexica""), completely dismissed the work. The sources for the text were nothing more than ""pictures either painted or wrought with party-coloured feathers"" and that it was:

> stuffed with impossible facts, absurd exaggerations, and such a barbarous jargon of uncouth names, as to to be within one degree of absolute unintelligibility.""

**Conquistadors and Virtuous Pagans**

Notice, however, the de Pauw and his contemporaries placed on decrying the first-hand accounts of the conquistadors. The enmity to these accounts was not a mistake; they specifically contradicted his views. The Spanish who arrived in Mesoamerica in the early 16th century were functioning on a more medieval, even romantic, view of race which had significant differences from our modern view. Martinez's *Genealogical Fictions* makes the case that, while we cannot completely divorce the historical concept of race from our current interpretation, connecting the two views must be done cautiously and with acknowledgement that the 16th century worldview was one ""strongly connected to lineage and intersection with religion."" Thus, the early Spanish might have seen themselves as superior, but they bolstered that assumption with assertions of their Christian faith and genealogical accomplishments. There was plenty of room, therefore, to see the Mesoamericans as inferiors to be instructed in the ways of the Church, while also allowing for praise of their accomplishments and specifically of certain pagan rulers.

Cortés, for example, in his First Letter back to Charles V, does not disparage the intellect or accomplishments of the Americans, but instead spends a great deal praising their art and architecture before moving on to decry their pagan customs. He finishes this section saying:

> your Majesties may reap great merit and reward from [God] in sending the Gospel to these barbarian people who thus by your Majesties' hands will be received into the true faith; for from what we know of them we believe that by the aid of the interpreters who should plainly declare to them the truths of the Holy Faith and the error in which they are, many, perhaps all of them, would very quickly depart from the their evil ways and would come to true knowledge, for they live more equally and reasonably than any other tribes which we have hitherto come across.

Though Cortés, in his letters, is famously kissing the ass of the Spanish King in an attempt to justify his expedition and not be arrested, we can still see the fundamental disconnect between the Spanish and the Mesoamericans was, in his eyes, less a racial distinction and more a religious one. These were, in his thoughts, a people ready to receive the word of Christ and enter into the overarching brotherhood of Christendom. The civilizations he was encountering were inferior to the Spanish only inasmuch as they deviated from good Christian morals, yet were otherwise held in high esteem. Writing of the city of Tlaxcala in his Second Letter, he says:

> The city is indeed so great and marvellous that though I abstain from describing many things about it, yet the little that I shall recount is, I think, almost incredible. It is much larger than Granada and much better fortified. Its houses are as fine and its inhabitants far more numerous than those of Granada when that city was captured. Its provisions and food are likewise very superior... There are gold, silver and precious stones, and jewellers' shops selling other ornaments made of feathers, as well arranged as in any market in the world. There is earthenware of many kinds and excellent quality, as fine as any in Spain. Wood, charcoal, medicinal and sweet smelling herbs are sold in large quantities. There are booths for washing your hair and barbers to shave you: there are also public baths. Finally, good order and an efficient police system are maintained among them, and they behave as people of sense and reason: the foremost city of Africa cannot rival them.

The Spanish, in other words, saw themselves as interacting with a society on parity with their own (though sadly pagan). Likewise, they saw the leaders of those societies through the lens of nobility. Xicotencatl and Maxixicatl, leading figures among the Tlaxcalans, were written about as wise and capable leaders, who provided good counsel and who would be ""good and faithful friends to the death."" Even Motecuhzoma, so often portrayed as weak and vacillating in later accounts, is painted as a wise and capable ruler of a magnificent land, who Cortés personally admired and liked. 

",0
"Oh Burr's life is very interesting after this.  In the immediately aftermath, he acted perfectly normal for the rest of the day even hosting a relative from Connecticut who was visiting later that day. Never once did he even mention the duel that took place that morning.  This actually had repercussion for Burr, because after his cousin left his house (just a few hours after the duel), his cousin got into a shouting match with a friend who had told the cousin that Hamilton was dead at the hands of Burr, which the cousin pushed back saying it was impossible - they had just met and Burr was acting perfectly fine.

There are many anecdotes like this, some likely true and others likely not, but the sentiment is what's important to note: New Yorkers, and soon the general public would view Burr as a cold person who showed no emotion about killing Hamilton.  Some of this was likely fabricated from Burr's political foes, but one thing is true: Burr never spoke of the duel again in public. 

> Critics accused Burr of a premeditated plot to kill Hamilton, and overwrought citizens threatened to burn down his house. James Parton observed, “It was from that hour that Burr became a name of horror. The letters, for a person ignorant of the former history, were entirely damning to the memory of the challenger. They present Burr in the light of a revengeful demon, burning for an innocent victim’s blood... Burr’s reputation perished along with Hamilton, exactly as Hamilton had anticipated.(1)

Hamilton's enemies, including Jefferson and the Democratic Republicans began to canonize Hamilton's legacy as they condemned Burr in public. This made Burr furious, knowing that these enemies were now only changing their tune because Hamilton was being viewed as a martyr. This was exacerbated by the fact that less than a month after the duel, the New York coroner labeled Hamilton's duel as murder, and an arrest warrant was made for Burr before New York's governor intervened and dismissed it, since arrests should never be made over duels, but Burr went into hiding anyway. Burr spent the majority of the next seven years away from New York, worried about both vigilante justice or formal charges of murder. During time time abroad, he tested many different ventures but found himself in repeated legal trouble with charges, as serious as treason, being brought against him but eventually was dismissed.

After fleeing to Europe, he returned in 1812 and returned to a more quiet life, including practicing law which he was able to do without as much notoriety that had followed him since the fuel.



1) Chernow, Ron. Alexander Hamilton. Penguin Publishing Group. pp. 716-717",0
"The short answer - it would have been pretty obvious to the intelligent observer very quickly. 

Long Answer

Part 1: **When Did People Realize They Were Not In A Republic**

So Augustus defeated Marcus Antonius (Marc Antony) in 31BC, and at that point the military hegemony of Octavian would have been obvious, however Octavian understood from history that military supremacy in Rome did not guarantee sustained power. Further, giving one absolute power such as making one self dictator for life such as Gaius Julius Caesar puts a target on one's back. 

 Octavian's power was based on three pillars - auctoritas, tribunician power, and the ""maius imperium."" Auctoritas is the Latin word that would be a combination of authority, prestige, and respect. Auctoritas was an idea taken very seriously in Rome, and the authority commanded by Octavian was extreme.  When Caesar died in 44BC, Octavian was only 19 and by 43BC was enshrined via plebeian law as one of the triumvirs in the Second Triumvirate. He was one of the most powerful people in a Republic where traditionally 10 years before he would normally have been even allowed into the Senate. The astounding amount of successes Octavian achieved all before Senatorial age would have been very impressive, and the military weight behind Octavian after the defeat of Marcus Antonius would have been terrifying. All combining to give him a large amount of auctoritas. 

The second pillar was Tribunicia potestas (tribunician power) and constitutional tweaking. Rather than declare himself as having ultimate authority in the manner of Julius Caesar, Octavian instead gave himself piece by piece crucial positions that already existed within the traditional concepts of the Roman constitution. Foremost of these, he gave himself ""tribunician power"" that is the power of the tribune of the plebeians. The Tribune was a crucial role, holding a veto on legislation, sacrosanct physical inviolability (you can't beat up the tribune or kill him, however that didn't stop people in the past with tribunes who pushed the system too far such as Tiberias Gracchus being murdered), and the claim of being able to represent ""the people."" This was so crucial a power to Octavian, that moving forward emperors' reigns were marked as having started from when they ""assumed tribunician power. There were additional constitutional changes, such as creating a load of new administrative positions called the ordo equitum (knights' order would be a loose translation). These were a series of key positions that Octavian was allowed to appoint. Elections would still fulfill the traditional cursus honorum (e.g. consul, aedile, praetor etc) from the senatorial class, thus Octavian could claim he was not changing the constitution or taking on the roles of these positions. However a) he massively undermined these positions by creating new, arguably more important positions such as the city prefect, imperial governors, grain officers, and b) he made sure that people he wanted would get elected to these traditional people. But crucially there was at least a facade of traditional constitutionality. Further, Octavian raised many new people into the Senate, giving him the upper hand in elections and a large group of senators loyal and in debt to him for their position. Finally whenever Octavian wanted something, he made sure to have the Senate vote on it for him. In the Res Gestae Divi Augustus (Life and Deeds of the Divine Augustus), Octavian is described as being *given* power by the Senate rather than seizing power for himself. Even the name ""Augustus"" was a title bestowed, officially, by the Senate on Octavian, though realistically they would give him nearly whatever title he wanted (so long as it wasn't Romulus). 

**I can now call Octavian Augustus**

Third pillar - maius imperium - the supreme military command of the Empire. This one is obvious, Augustus had the love and adoration of the legions and this gave him a lot of power. Importantly, Augustus made sure that the legions would remain under his control when he was not campaigning. Canny politician, he did this in a way that would again placate the Senate. Once Marcus Antonius was defeated, Augustus ""returned"" Senatorial control over many provinces in the Republic. He made this out to be a gracious act, restoring traditional Republican control. However Augustus said that he needed to maintain control over the border provinces and certain provinces that had been part of the rebellion of Brutus and Antonius/Cleopatra's rebellion. What was important about all those provinces? Border provinces and provinces recently pacified would be the ones in which almost all of the Republics legions would be operating. Thus, Augustus got the PR victory of restoring control of provinces to the Senate that had for years been effectively out of their control, but at the same time ensuring that he had *total military hegemony* through legal means. 

So now we know *how* Augustus ruled, we can see that it was extremely obvious that the Republic had changed, however people still referred to it as a Republic, and it functioned in familiar ways. 

Part 2: **Would it be different for patricians vs equities vs plebeians?**

This is very broad, but I'll try break it down. The Senators and patricians would most definitely have been aware. As would the equites/publicans (that is money lenders, bankers, businessmen). Plebeians is a very general term though. Plebs could be everything from a wealthy banker owning large tracts of lands, to a mud shoveller at a brick maker's in a small town. In this regard I am going to defer to Jo-Ann Shelton from ""As the Romans Did"" - 

>The opposition to the principate (Augustus' imperial rule) came mainly from the senatorail class in Italy, and it was effectively silenced by Augustus (as I the reddit poster discussed above). We have virtually no information about how the lower classes in Italy and in the provinces felt about the changes made by Augustus but it is unlikely that many people felt anger or expressed opposition. The lower classes, particularly outside of Rome, probably felt far removed from what must have seemed to them a power struggle between the aristocratic senators and the aristocratic Augustus. Moreover, Augustus' rise to power had brought peace and stability to Roman society. 

So yes, according to Shelton at least, it would matter depending on your economic status and educational background. In terms of dissent and its consequences, I am too tired and hungover to elaborate more on that right now, but shortly put - Augustus had fought years and years of wars to silence his opponents. He was not one to tolerate political enemies. 

**Sources:**
Frank Abbott *Society and Politics in Ancient Rome*

Dio Cassius *Roman History*

Jo-Ann Shelton *As the Romans Did*

Tacitus *Annals*

Lecture Notes - University of Toronto. 

Loose Reference to Res Gestae Divi Augustus. ",0
"Oh snap, I'm late to the party on this one. Let's talk about some McCaesar's, shall we? 

First, let's discuss the basic components of a cheeseburger and what those entail. Some of the modern stuff ([like heavily sugared buns](https://yourquestions.mcdonalds.ca/answer/how-much-sugar-in-a-bun/)), we'll obviously pass over, but we *will* be talking about what our ancient cheeseburger will taste like - and how it'd probably be pretty damn delicious, but with a bit of Roman flair. 

So most basic of the basic ingredients:

* buns  
* cheese 
* beef patty (possibly two if you're like me and eat more than your body weight in a day I mean what)  
* salt and pepper  

However, if you just make a burger with those things, it's going to be a boring burger. What makes the burger delicious isn't just its beef and cheese, but its toppings. Today, those include ketchup, mustard, mayo, and, oftentimes, some sort of mystery sauce. The Romans didn't have access to tomatoes, so we'll go ahead and skip that one, but not only can we get everything else, I think we can replace the ketchup and still have things taste just fine. Some other toppings'n'stuff are tomato (again), lettuce, onion, butter, and garlic (powder, used for seasoning). 

But we don't want any old McDonald's burger - [we want a *real* burger.](https://www.youtube.com/watch?v=iM_KMYulI_s). So can we make Gordon Ramsay happy with readily available ingredients in Rome? I'm gonna go ahead and say absolutely. 

First off, I'm gonna go ahead and start with a quote from Cato the Elder who, among being a cantankerous old coot who liked hating on Carthage, [wrote an entire book about how great cabbage is](http://penelope.uchicago.edu/Thayer/E/Roman/Texts/Cato/De_Agricultura/K*.html), as well as giving us this delightful quote:

> “Of this last kind of comparisons is that quoted from the elder Cato, who, when asked what was the most profitable thing to be done on an estate, replied, “To feed cattle well.” “What second best?” “To feed cattle moderately well.” “What third best?” “To feed cattle, though but poorly.” “What fourth best?” “To plough the land.” And when he who had made these inquiries asked, “What is to be said of making profit by usury?” Cato replied, “What is to be said of making profit by murder?” 

(Cicero, *De Officiis*)

That is to say, the Romans loved beef and recognized the amount of money that could be made in the proper care and raising of cattle. While grain is what everyone talks about being mass produced for the Roman people, it was a poor man's food (as shown by the constant grain subsidies that were in place to feed the people of Rome, not to mention the actually massive shipments coming in from across the Mediterannean). But the beef was not only high quality ([I'll refer you to the non-cabbage related portions of Cato](http://penelope.uchicago.edu/Thayer/E/Roman/Texts/Cato/De_Agricultura/home.html), written in the 2nd c. BCE), but it was certainly not a rare commodity. That being said, I'm not sure that the beef was particularly cheap - it's tough to nail down prices (and I'll see if I can do some side research into estimates for you, but since we don't even know for sure how much wheat cost....I'm digressing), but meat was meat, and the majority of Romans (the poor) did not have enough loose change to commit to it. That being said, other (more contemporary) authours talk about cows nonstop, and if you'd like to know more about cattle breeds, what they might have looked like, and what they were good for, I'd be happy to provide sources. For now, know that there were many, some were renowned for their meat, and some for their cheese.

And oh my, did the Romans love their cheeses. Cows are versatile creatures, and they offer a variety of substances that were useful for this endeavour - meat, cheese, and butter. Now, the Romans weren't themselves heavy users of butter (Apicius avoids it in all of his recipes, and the Romans themselves seem to have seen it as a weird German thing), preferring olive oil or other fats, but it was certainly theoretically available. If you can make a good cheese, you can make butter. So we can certainly check off the butter, the cheese (probably way better cheese than you'd get with most burgers, honestly), and the beef quite easily.

Salt is one of those things that everyone likes to misunderstand - there's a trope that the word *salary* came from the Roman soldiers being paid in salt, which has no real basis - but hey, the Romans themselves were unsure about where their word *salarium* came from. Either way, the Romans had salt, and they greedily held control over their salt supplies. One of those mines was at one of Rome's major ports in Ostia, giving easy access to the resource. 

Pepper, on the other hand, is not a naturally occurring European resource. However, by 1 CE (thanks for that date, gives me the excuse to rant about Roman trade networks), trade with India was *booming*. After the subjugation of Egypt in 31 BCE, the Romans subsumed the Ptolemaic trade routes through the province, driving the previous levels of trade to a fever pitch, a trade explosion which continued for over 200 years. Where trade had previously been slow, with as few as twenty ships making the trip, the Romans used their military as a workforce to create the necessary infrastructure for intensifying trade with the East; over a hundred ships were soon making the annual journey to India.^1 Where enterprising merchants had previously only been able to travel at night, heavily stocked with water and in constant fear of banditry, the Romans built and fortified roads, water stations, and the trading cities themselves. Shipyards were built in an attempt to support the failed invasion of Arabia in 26 CE, which were easily converted to civilian use afterwards.^2 This large-scale rise in infrastructure created a fertile environment for a wave of consumerism to sweep the Roman world, with demand for Eastern luxuries and spices increasing dramatically among those with disposable income. Those imports ranged from places as far apart as Madagascar and Vietnam. The primary partner of Rome in this sea trade, however, was India. Trade ports ranged across the subcontinent, each one offering a different selection of trade goods, the most common of which was pepper.^3 Other imports included varied types of luxury wood, precious stones, frankincense and myrrh, and textiles such as cotton and silk. Indian imports quickly became central to Roman life, with recipes and medicines commonly using exotic spices, such as malabatrum or the especially unhelpfully described *“ispicam Indicam.”*

I've provided a few sources at the bottom for further reading on this, since there's honestly a vast wealth of information discussing Roman trade and how quickly and deeply it was tied to Rome - it quickly became *the* big business in the Roman empire, with individual shiploads being valued in the millions of *sestertii*. MacLaughlin's book is an excellent one, and Lytle is a magnificent researcher when it comes to near eastern trade (and ancient fishermen). But I digress.

By this time, pepper would have been readily available in multiple types, and wouldn't be too terrifically difficult to come by. Garlic, too, is a native herb to Europe, Asia, and beyond, and would not have been rare or even worth special commentary for its difficulty to find. Fear not, your beef patty will be perfectly seasoned, assuming that you're a decent chef and/or have access to YouTube so that you can do a quick search on Gordon Ramsay's burger. The onion, too, was incredibly popular in all sorts of things, from cooking to medicine, and your request to put onions on this concoction would have been met with approval from whomever you were commissioning. That and the fact that it wasn't hard to come by, so it would have been reasonably cheap. Overall, by the way, this burger probably would cost you a pretty penny. Probably not those 30 pieces of silver, but certainly something that you would see at an Epicurean feast. 


",0
"Goddamn. This has to be the most random-ass hilarious question I might be somewhat capable of answering.

**As per usual, disclaimer:** I'm not an expert on poultry science. Culinary history is a hobby field for me, and I've occasionally looked into the surge of backyard chicken-keeping in the U.S. and Canada because apparently I have nothing better to do. Heritage breeds are very popular for small-time poultry keepers, and people are generally quite interested in the history behind these breeds. This may not be more than a decent guess, and even that might be kind.

**You're absolutely right that modern chickens have been bred to be more productive than their ancient forebears.** The modern laying chicken according to PSU's College of Agricultural Sciences is a White Leghorn for white eggs or a sex-linked hybrid of New Hampshire Red/Barred Plymouth Rock descent for brown eggs. (And by sex-linked hybrid, we mean a chick that can be sexed the day of its birth; the unfortunate reality is that cockerel chicks are of little value to the industry and are typically killed.) These birds exist solely to produce eggs, whether for consumption or breeding, and a hen in the prime of her life (20 to 85 weeks) can crank out 300+ per year.

300 eggs would have been an insane number for a bird in ... uh, whenever *Beauty and the Beast* is set. We'll get to that in a moment. Suffice it to say that the average small farmer in times past generally couldn't afford, or didn't have the space, to breed both meat and laying chickens. They were also reluctant to slaughter a bird that was still productive even if that productivity was declining, which is why the majority of chicken recipes from before the 20th century assume the use of an older bird. It is not a mistake that just about every poultry-keeping culture you can name had an abundance of recipes for old birds (e.g., the incredible array of chicken soups from around the world) and relatively few for younger ones.

**In terms of actually answering your question, I think the logical progression is:**

 - When is *Beauty and the Beast* set?
 - Where is *Beauty and the Beast* set?
 - What breeds of chicken were common in that era and place?
 - What are these breeds' productivity?

**So when is *Beauty and the Beast* actually set?** I googled this because I wasn't sure and just wanted a frigging date. Disney, *you're killing me*. The original story comes from a French fairy tale either written or transcribed by Gabrielle-Susanne Barbot de Villeneuve in 1740. However, if we assume the matches that Lumiere uses in the animated 1991 version date the film, then it can't have happened before 1805 when the match was invented (conveniently enough, in Paris). But then there's the Eiffel Tower that the magic forks assemble during ""Be Our Guest,"" and construction on the Eiffel Tower was between 1887 and 1889. I haven't seen the live-action version yet and I don't know the degree to which it differs.

Luke Evans (who plays Gaston in the live-action adaptation) said his characterization of Gaston assumes he was around 16 in 1740. Evans is 37 now, so let's argue that Gaston is somewhere between his late 20s and late 30s in the film, which dates it to 1750-1763. The fashion design seems to put the film squarely in 18th-century France. Other materials distributed by Disney vaguely reference ""Bourbon France,"" which means sometime between 1589 (Henry IV) and 1792 (French Revolution and boom goes the monarchy) and I assume not the brief-lived Bourbon restoration (1814 or 1815 depending on how you choose to define ""restored"" until 1830).

Mid-18th century it is. My head hurts and we are moving the fuck on.

**Where is *Beauty and the Beast* set?** For the 1991 film, Disney's art department used both the Loire Valley (central France) and the towns of Riquewihr and Ribeauvillé as inspirations for the setting. This is a bit problematic as both towns are several hundred miles to the east of the Loire Valley, but whatever. Unfortunately, I'm not sure if anyone on the live-action film has commented on the specific setting. We'll say somewhere in north-northeastern rural France where: a). people are forced into choirs at birth, and: b). can harmonize in adulthood at will.

**What breeds of chicken were common in that era and place?** This is very tough. France has produced an incredible variety of livestock, and unfortunately much of the historical record concerning it has yet to be translated. We'll do the best we can with the information available in English. There are a few we can eliminate right off the bat (e.g., the Estaires didn't appear in Europe until the late 19th century, the Coucou de Rennes is from the wrong region, etc.), but quite a few remain. However, there are at least three ancient French breeds that comfortably predate the mid-18th century and would have been common in northern/northeastern France:

 - **The La Flèche:** A good all-purpose bird that was (and remains) popular with small farmers for both meat and eggs. The modern La Flèche generally lays around 3 eggs/week between March/April and October.
 - **The Houdan:** A more ornamental breed that originates from Yvelines (west of Paris) and was a reliable egg producer but so-so as a meat bird (large but slow-growing). Like the La Flèche, it would generally produce around 3 eggs/week from spring to mid-autumn.
 - **The Bresse Gauloise:** This is now most famous as a meat bird and is the breed that, when slaughtered, becomes the *poulet de Bresse,* the darling of the professional chef's table. You can put this on a menu at any foodie destination and charge whatever you want. Again, another 3 egg/week layer.

There are definitely more, but these are the only ones I can absolutely confirm after running through my library and cross-checking online.

**Do we have records of these breeds' productivity?** If we do, I'm not sure that records of their mid-18th century production exist in English, but let's take the figure of 3 eggs per week between March and October. Gaston's already in trouble. The modern backyard/hobby farmer you cite who's getting 4 eggs/week per bird is nearly always raising chickens as a sort of combination pet and egg producer. The growth of backyard chicken-keeping has prejudiced the market in favor of breeds that fit this profile, e.g., the Buff Orpington, the Rhode Island Red, and Ameraucana. For that matter, even 4 eggs/week is a bit generous for these birds; not a lot of people have the stomach to slaughter their chickens as their productivity declines, so 4 eggs/week assumes a young bird that's eating well and is at her most productive. There *is* a French breed, the Marans, that can match this output, but it's not in contention for feeding Gaston here as it was developed in the 20th century. 

Again, without hard records on exactly how productive the La Flèche, Houdan, and Bresse Gauloise were during the mid-18th century, I can't say with 100% certainty that 3 eggs/week is what you'd get. I will say based on the scanty information I've seen on hen productivity before the industrialization of agriculture is that it's a very plausible number.

**All right. Gaston is probably -- or at least *plausibly* -- stuck with birds that were laying 3 eggs/week.** If that's the case and he's eating 420 eggs per week, then he will need a flock of 140 chickens -- or realistically a bit more to account for randomness and the occasional loss of a bird to predators or theft -- to satisfy his protein intake. Additionally, this also assumes that the birds in question are eating well, and that will depend on the farmer's income (can he afford to supplement their foraging?) and their environment (are they well-supplied with insects and plants, or are there too many birds for the farmer's land to support?).

Also, he's going to have a hard time between October and March, because all of these breeds would have been subject to the normal seasonal cycle, and egg output would have declined or stopped entirely. Due to this, he would have needed substantially more than 140 chickens so he could pool eggs ahead of time to get himself through the winter (and eggs, if not stripped of their protective membrane, will last for a few months).

SOURCES:

 - *British Food: An Extraordinary Thousand Years of History* by Colin Spencer (covers a lot of the livestock/agricultural exchanges between the U.K. and France)
 - *Backgarden Chickens and Other Poultry* by John and Cara Harrison
 - *Backyard Farming on an Acre* by Angela England
 - Entirely too much time spent on various school of agricultural science sites

**TL:DR: 140+ chickens. I'm off to go make an omelet.**",0
"The rise of the Nazis in Weimar Germany is a complex and multifaceted issue that cannot be attributed solely to the effectiveness or ineffectiveness of hate speech laws. While it is true that Weimar Germany had laws in place to combat hate speech and suppress Nazi propaganda, their enforcement was inconsistent and often undermined by political and social factors. Firstly, it is important to note that the economic and social conditions in post-World War I Germany laid the groundwork for the rise of extremist ideologies.",1
"Sir Ian McKellen's coming out as gay on BBC Radio on February 27, 1988, was indeed a significant moment in LGBTQ+ history. However, it's important to note that attitudes towards homosexuality varied in the UK and the US at the time, and reactions were not uniform across both countries. In the UK, McKellen's announcement was met with a mixed response. While many people expressed support and admiration for his courage, there were also elements of homophobia and prejudice present.",1
"\>  If it is a metaphorical story, what is the metaphor? 

[Here's a link to the entire story](https://www.biblegateway.com/passage/?search=Judges+19&version=ESV). Again, the context is important.

The entire book of judges is a series of stories that follow a similar pattern.  The israelites are settled in a dangerous land, but must remain faithful to god. They fall away from god and are punished and bad things happen to them.   Then god brings a leader to save them, they are saved, peace is regained and comes back to god.

The entire book is a collection of stories that have their written origins during 6\-8th centuries BC, but again are older versions.    The stories, even in their english translations, carry hallmarks of this origin.   ""In those days, there was no king in Israel"" which sets the stage for what is described.

Judges 19 is the introduction to the story of Battle of Gibeah.

A levite, that is a member of the tribe of Levi, the priestly class, was traveling through land ""owned"" by the tribe of Benjamin. He took a concubine, but something happened \(either she was unfaithful, or was angry or ran in fear because her husband was angry\) and she ran to the home of her father.    The levite went to visit her there, apparently to convince her to rejoin him, and while there, the people of city wanted to rape him, but they raped his concubine instead.

As the story goes, the levite then cuts the concubine into twelve pieces and sends a piece of the concubine to each of the 12 tribes with an explanation of how terrible the people of the tribe of Benjamin are, and how they have fallen away from god.

The people of the other twelve tribes of Israel, of course, rise up in anger and send an army to attack and destroy the men of the tribe of Benjamin who had done this terrible awful thing.  The army demands the tribe surrender the guilty parties and war ensues when they refuse.   The war results in heavy casualties  and the substantial destruction of the tribe of Benjamin. \(the killing of all the men, women and children except for 600 men who are given new women to re\-form the tribe).

The story ends again with the same refrain. ""In those days there was no king in Israel. Everyone did what was right in his own eyes."" 

[Rabbinical interpretation](https://jwa.org/encyclopedia/article/concubine-of-levite-midrash-and-aggadah) is that the moral of the story is that marital peace is a requirement for a functioning society.   

On a larger level, the story has political and religious elements.  Telling the Israelites that they must remain faithful to god or bad things will happen, and blaming the then\-leadership of the Israelite people for not encouraging the people to remain faithful.   Some scholars believe the story is political spin, spread and written as a post\-hoc justification of atrocities committed by the tribe of Judah against the tribe of Benjamin in a civil war. 

Its worth pointing out that this is one of those stories that, if you read it literally, is absolutely horrifying and any purpose for telling it is unclear.  But in context as an oral history describing how a nasty civil war started,  the motives of the players might start to become more knowable.",0
"The romanticism of the East Coast/West Coast feud makes lots of people want to point at Biggie as responsible, but most of the evidence seems to point to the man 2Pac and Suge jumped in the hotel lobby, Orlando ""Baby Lane"" Anderson was responsible. His uncle, Keffe D, said as much in an [interview with police] (http://www.laweekly.com/news/the-keffe-d-tapes-10-highlights-of-confession-from-gangster-who-says-sean-combs-hired-him-to-kill-tupac-2395421). 

According to [LAbyrinth](https://play.google.com/store/books/details?id=CQlPvwmA4l8C&source=productsearch&utm_source=HA_Desktop_US&utm_medium=SEM&utm_campaign=PLA&pcampaignid=MKTAD0930BO1&gl=US&gclid=CIHH7bbN_c4CFaRrMgodizYE_g&gclsrc=ds), Anderson was allegedly a member of the South Side Crips who had had issues with Suge Knight's Bloods-affiliated record label dating back to an incident at an LA mall in which Death Row associate Travon Lane had his DR medallion stolen in a Footlocker. Speculation claims that Puffy paid for this, as he wanted to use the chain in a Bad Boy video mocking DR, but if there's evidence that it was more than speculation, I've never seen it. 

Additionally, it's fairly well documented that in the weeks following 2Pac's death there was a large scale gang war in LA, seeking retaliation for his death. MC Eiht, a longtime Compton rapper, has [gone on record] (http://www.ballerstatus.com/2016/02/03/mc-eiht-says-tupacs-gang-affiliations-led-to-his-death/) stating he believes it was 2Pac's gang affiliation (through Suge Knight) that led to his murder. 

If you comb through his Death Row releases, they're full of references to Bloods, down to his repeated use of ""M.O.B.,"" a common Blood saying (Suge is also a known member of the Mob Pirus) and shout-outs to known Pirus ([Neckbone, Tray,Buntry](http://www.latimes.com/local/la-me-philipssuge15nov1502-story.html)).

That said, LAPD Detective Greg Kading wrote in his book [Murder Rap](https://www.amazon.com/Murder-Rap-Untold-Investigations-Detective/dp/0983955484) that he believes Puffy commissioned Keffe D for the hit. It is also alleged that Biggie [paid $1 million and provided the gun] (http://www.mtv.com/news/1457346/biggie-paid-gang-to-kill-tupac-report-says/) that took out his rival. 

There is further speculation that 2Pac was planning on leaving Death Row, which led to Suge Knight to order his death, but remember Knight was in the car when 2Pac was shot. 

At the end of the day, no one really knows. Everyone involved is either dead or not talking.    ",0
"Though one should keep in mind that this was a complicated situation for them. They were also justifying their own failure in moral terms — ""ah, we didn't succeed, but it's a good thing we didn't, and indeed, maybe we didn't even WANT to succeed!"" It's not entirely truthful. 

This is what scholars call the origins of the German _Lesart_ (""story"") about the atomic bomb, which misleadingly implied that they had morally avoided making it. In truth there is no evidence that this failure was intentional. They failed because their government did not commit to building it (because they thought it was too difficult to complete in wartime), and got some technical things wrong (which of course they were loathe to admit). Any morality was imposed after the fact, while they were in captivity, feeling sorry for themselves, looking towards how they were going to justify the idea that they were working on a nuclear program for Hitler. Twisting it into ""actually _we_ were the moral ones"" was an approach that several (notably von Weizsäcker, and Heisenberg to an extent) of them took, and they amplified this message in the postwar.

See Mark Walker, _Nazi Science: Myth, Truth, and the German Atomic Bomb_ among many other good books written about this subject by historians. For making sense of the Farm Hall transcripts, Jeremy Bernstein's _Hitler's Uranium Club_, an annotated version of them, is essential.",0
"The text *De re militari* of the mysterious 4th century writer Vegetius enjoyed wild popularity throughout the Middle Ages, first in Latin, then in the vernaculars as well. Vegetius both talks quite a bit about the ways a commander ought to use spies, and accepts their ubiquity on all sides--like a coach giving code-names to plays, a captain ought to regularly change the names of various tactics to confuse the spies that are everywhere.

This is a problem for studying espionage in the Latin Middle Ages, because it's impossible to tell whether battle commanders actually modeled their strategies after Vegetius' *advice*, or whether chroniclers modeled their reports after Vegetius' *text.*

So a lot of our less-troped information about espionage practices from medieval western Europe comes from the Hundred Years' War and the endemic fighting in fifteenth-century Low Countries. It's a little problematic to extrapolate backwards from the specific concepts in some cases. For example, the French-English coastal raiding of the late 14th century gave rise to criminal charges against English men for providing information on local defense to French military leaders. Fishermen might also be pressed into temporary intelligence service to report on enemy naval activity. 
In 1475, meanwhile, the military companies Strasbourg had sent to Burgundy got a *wicked* scolding from their city. It turned out the mercenaries had all dictated letters to their wives back home to the company scribe. A city councilor who got his hands on one of them was furious about the amount of information about troop strength and movements it contained, because who else might have seen that letter or a copy? Both of these are fairly exceptional and especially the latter, very *late* medieval cases. You can't have a spy in a town tavern overhear the target of an attack, the captains who would be participating, and the strength of their companies (like the brother of Sancerre's garrison commander reported to him in 1364) if there's no town to have a tavern.

And in other cases, what get called spying might not seem like spying to us at all. Thomas Walsingham's *Chronica maiora*, which covers some of the 100 Years' War period, says that Scotland learned of the 1381 Peasants' Revolt from ""spies,"" something we might be more likely to call news or gossip. But there is definitely a picture of both formal and informal networks of information gathering in the late Middle Ages.

That's not to say there was no spying earlier! Vegetius *was* enormously popular, even back to the Carolingian era. And one of the deep frustrations for historians working with medieval letters is (unlike the poor Straßburgers, apparently) that the written component of a classical medieval letter was only part of the message, and maybe even an irrelevant part except for its material quality. The *messenger* carried the true message in his memory, where it could not be seized and read. (Torturing people for information is...questionable, for the high Middle Ages). And when the Normans conquered Sicily in the late 11th century, they relied on the spy network cultivated by their local (Muslim!) ally Ibn al-Thumna.

But that might tell you something: the best medieval espionage stories come out of the early Islamic world.

A favorite tactic of the Mamluks as they piece by piece removed the last shards of Latin presence from Palestine, for example, was to make any ambassadors seeking a truce or other agreement travel exclusively by water. They didn't want any overland observation of Mamluk forces or defenses going on.

Suspicion of spying ran strong, indeed. In 12th century Cairo, the powerful (and Christian!) financial administrator Ibn Dukhan was summarily removed from power and ultimately executed for collaborating with the Franks. Court scholar Zayn al-Din had asserted that Ibn Dukhan was (a) writing letters to Frankish (European) leaders revealing details about the Fatimid administration (b) meeting with Frankish messengers in his own home to convey information orally (c) finding ways to sneak them into important gatherings to spy on their own.

But so did the cultivation of spy networks. In the early 11th century, Fatimid caliph al-Hakim bi-Amr Allah had pioneered the cultivation of *woman* spies, including delegating his half-sister to find out 'the real word on the street' for news of both local and far away developments. Even better is late 11C vizier al-Afdal Shahanshah, or rather--*his mother*. She reportedly acted in character as the mother of disgruntled soldiers, drawing out traitors by convincing them she was on their side.

And best of all, of course, were the Seljuq Turks--yes, that empire who shows up in Western history at the Battle of Manzikert and, well, sure, probably existed before and after it but...? Well, it turns out the Seljuqs both understood the value of information for successful governing and had the internal structure and power to organize its gathering officially in, apparently, all districts of their territory, not just centers of royal power. Seljuq spies against insurrection and infiltrators were so successful and ambitious that the government had to *regulate their strategies*:

> he must not climb up enclosures or walls in the pursuit of his office, or lift veils,
or break into closed doors, or give unworthy people power...thereby making public what God has commanded to be kept veiled and secret.

So--oh, yes, there were spies in the Middle Ages.",0
"The potential energy output of nuclear fission was well understood prior to hiroshima and Nagisaki. Moreover theoretical concepts like criticality, supercriticality and nuclear chain reactions all appeared in publically published papers prior to the outbreak of war. Further, by the late 30s several different institutes were working on creating a self sustaining nuclear fission reaction.


Had it not been for the outbreak of war it is likely that a civilian scientific reactor would have popped up in the 40s. However outbreak of war meant that research very quickly went very secret, as at least the majority of scientists working in the field were aware of the potentially catastrophic power of the atom. The most famous demonstration of this is the Einstien-Szilard letter to Roosevelt that warned of the potential power of an ""atomic weapon"". 

Thus every major power had at the very least a cadre of academics who understood that it should be possible to create a supercriticalty reaction that would unleash a truly terrifying amount of energy. In the US we were so certain that the cannon type uranium fueled bomb would work that we didn't even test it (there was also the issue of each cannon type requiring several months national production of u-235, more on that later). The implosion type Plutonium bomb was tested at Trinity, but that was more a question of if the conventional explosive shell could be detonated precisely enough to induce supercriticality in the Pu core.

The manhattan project was never strictly a question of determining if a nuclear explosion was possible (everyone was pretty certain it was), it was a question of a)if you could build a device to deliver that reaction precisely where and when you want it, the bomb (though obviously precision is a relative term when what you are delivering is an 8 kiloton explosion) and b)producing enough sufficiently pure uranium or plutonium to produce your supercriticality. Fuel production actually took up the majority of the Manhattan  Project's budget and efforts


When the first fireball bloomed over Japan the reactions from the major powers was thus not ""what was that?"" But rather ""dear god the Americans actually cracked it"". Everyone knew it was theoretically possible, but not how close it was. 

Perhaps more directly pertinent to your question, the concept of an ""atomic weapon"" actually appears in the public conciousness as early as 1914, in H.G. Wells ""The World Set Free"" a book about a world war ended with atomic weapons that operated by accelerating radioactive decay in a substance, expelling vast amounts of energy in a brief period of radioactive fire, with the side effect of contaminating the land it was used on for many years. While obviously Wells work was innacurate in the method, it was prophetic in depicting the effects of nuclear weapons, and implanted the idea of the atomic bomb into the public conciousness. Leo Szilard, father of fission, was even known to have read the book. I myself wonder if nightmares of Wells atomic fire danced through his head the night he first saw those blips of neutrons dancing on the oscilloscope.

Edit:the US also built a working uranium reactor in 42, so they were 100% certain that criticality was possible before they ever built a bomb.",0
"Great answer! But Odysseus DID NOT kill and eat the sacred cattle of Helios the sun god.  His men did, despite being ordered not to, because they choose to chance fate instead of starving while Odysseus was out scouting. All of them perish as a result, and the hero has to continue his Odyssey alone. ",0
"The sorts of things that happened to 14 year old girls who disappeared in Chicago in 1898 were not that different from the sorts of things that can happen today. They could run away from home, with or without someone else; suffer an accident; be kidnapped for ransom; or be attacked, abducted, robbed, raped, or murdered. Elsie Stahl was never, apparently, found, so it's very hard to know what the range of possibilities in her case actually were, but a survey of the contemporary Chicago press makes it clear the police considered most or all of these circumstances at one time or another, and that the case remained news, on a small scale, for almost a year.

To begin with the coverage itself, Stahl was last seen on Thursday 3 November, 1898, when she left her – apparently pretty affluent – family home at 104 Cleveland Street, Chicago \[*Chicago Tribune*, 8 November 1898\]; she told her family that she was heading to the home of her music teacher, a Miss L. Reubhausen, which was only five blocks away on Eugenie Street \[*Chicago Daily News* 18 November 1898\]. There had been some sort of family row that day or the night before, and several papers reported that she had been ""scolded"" shortly before her disappearance \[*Daily News*, 16 + 18 November 1898; *Inter-Ocean,* 30 July 1899\].

The first reports I've found do suggest there was initially some reason to suppose Elsie had chosen to disappear. She had been staying out each evening between the hours of 7 and 9 or 10pm, claiming she was practising the piano at her teacher's house; this was apparently untrue, and it's unclear what she was doing at those times, though one of her friends reported she had been seen on Clybourn Avenue – one of the city biggest and most bustling thoroughfares, and hence the sort of place two people might arrange to meet. Miss Reubhausen, the teacher, was tracked down by the press and reported that, while Elsie (""a swell little girl... \[with\] an air of dignity and breeding"")  had apparently been taking money from her parents regularly for her music lessons, she had not actually seen her since late July. This led to suggestions that the girl had been hoarding money to run away, that her disappearance had been triggered by the fear her deceptions were about to be found out, and that she had perhaps been ""persuaded by some friend who had an influence over her and was now hiding her."" \[All *Daily News*, 18 November 1898\]

It seems that Elsie's mother was initially prepared to concede that something of this sort had happened. She almost immediately hired a private Pinkerton detective, who discovered (or claimed to have discovered) something or other that Mrs. Stahl chose not to reveal to the press, but which persuaded her ""that the money she paid... was not wasted."" \[*Daily News*, 18 November 1898\]  Her father, Gottlieb, on the other hand – he was the owner of a furniture store on Division Street \[*Daily News*, 16 November 1898\] – always strongly denied that his daughter had run away from home, pointing out that Elsie had left both her meagre savings and a gold watch behind in the house when she left it \[*Dziennik Chicagoski*, a daily paper published in Chicago in Polish, 17 May 1899\].

The idea that Elsie had run off, and would eventually return, must have begun to seem less persuasive as the days and weeks passed, and a number of other reports suggested the possibility of abduction, but police ""scouted"" (doubted) the suggestion that ""a strange woman"" had been seen ""hanging around the Stahl house"" at the time of the disappearance \[*Tribune*, 8 November 1898\], and don't seem to have followed up on a report made, months later, by a reprographic company salesman by the name of Allen O'Brien to the effect that he had seen a girl closely resembling her at Chicago's Union station accompanied by an elderly woman \[*Daily News*, 16 May 1899\]. This last report may have been a hoax, as, almost certainly, was the strange tale that appeared in the papers six months after the disappearance to the effect that three schoolboys playing truant from their school had found a message in a bottle on the shore of Lake Michigan which read: ""I am a prisoner at the foot of Randolph Street pier. For God's sake notify my parents. Elsie Stahl."" This last lead was one that the police did – unsuccessfully – pursue. \[*Inter-Ocean*, 16 May 1899\]. They also shared information about the missing girl with the authorities in nearby towns \[*Daily News*, 18 November 1898\].

The theory the police seem to have given most credence to was the idea that Stahl's ""intimate friend"", a girl named Kittie Boyer, knew something about the disappearance. Detectives informed the press that Boyer had been ""plentifully supplied with money lately"", and although, interviewed by journalists, the Boyer family disclaimed all knowledge of the disappearance \[*Daily News*, 16 November 1898\], the police plainly found it suspicious that the girl failed to visit the Stahl family after Elsie vanished \[*Tribune*, 17 November 1898\].

It doesn't seem all that plausible that the authorities would have described the theft of cash sufficient to pay for a few piano lessons (these were apparently charged at the rate of $2.25 per month \[*Daily News*, 18 November 1898\]), which is all it seems Elsie might have had on her at the time, as a ""plentiful supply,"" even for a teenage girl, so I'm not convinced this last report necessarily implies that Boyer was suspected of attacking her friend herself. Rather, it seems to me to possibly hint at one of the potential outcomes that you mention in your query – that Stahl had been abducted and forced into the sex trade. Certainly some of the Chicago newspapers seem to have imagined this was a potential motive; why else note that Elsie was ""as well developed as a girl of 15 or 16""? \[*Daily News,* 17 November 1898\]  Moral panics concerning the so-called ""white slave trade"" were fairly common at the time, and certainly did occur in Chicago; Karen Abbott's *Sin in the Second City*, a book about the city's famous, and upmarket, Everleigh Club brothel, covers a number of such rumours dating back to 1887, when a police raid on a Michigan lumber camp uncovered nine prostitutes, one of whom secured an acquittal when her case came to court by successfully pleading she had been forced into sexual slavery.

Abbott very much downplays the likelihood that such coercion was actually commonplace, noting that the proprietors of the Everleigh Club, for instance, were adamant that they forbade drink, drugs and violence on their premises, and made regular medical check-ups available to the women who who worked there, with the result that ""there was actually a waiting list, spanning the continental United States, eager to join the house."" This may well paint a far too rosy picture of the contemporary Chicago sex trade as a whole, but Abbott unpicks a number of contemporary rumour-panics to show that the term ""white slavery"" was mostly applied, by evangelical Christian organisations, to sex workers who were, in fact, consenting adults. Certainly a missionary named Charlton Edholm, who worked for the Woman's Christian Temperance Union, claimed in 1899 that ""there is a slave trade in this country, and it is not black folks at this time, but little white girls —thirteen, fourteen, sixteen, and seventeen years of age—and they are snatched out of our arms, and from our Sabbath schools and from our Communion tables."" So the idea that teenage girls were being abducted and sold into ""white slavery"" was definitely current at the time of Stahl's disappearance. This may explain why the police were so suspicious of Kittie Boyer and her apparently recent and unexplained affluence, but it's reasonable to conclude by saying that historians of the Chicago of this period are pretty sceptical of such claims, and that Stahl's disappearance remains, very sadly, an almost complete mystery.",0
"There is no simple answer to this, no one single factor in American history and culture which can account for such broad trends. The roots of them go back to the cultures of the original colonies, as maintained, modified, and overthrown by every subsequent generation and every wave of immigrants. But I can talk about a small, representative slice of Americana, and how censorship was applied there.

Pulp magazines emerged at the turn of the century from the dime novels and nickel weekly magazines. Their contents reflected the tastes of the day, and ran the gamut from the sedate to the risque, from the action-filled western and crime stories to the intellectual and philosophical. The early pulps catered to a broad audience of all ages and walks of life; anyone that could afford the dime or quarter was welcome to buy them at any newsstand.

There were limits on the content; mostly imposed by editors' individual tastes. While an early pulp might contain an artistic nude or a vicious villain, risque jokes for the college set, none of the publishers wished to run afoul of charges for obscenity (which, being broadly defined in the 1920s, could apply for gore as well as sex) that might get them arrested - or worse, lose them mailing and distribution privileges.

Such concerns were real: the New York Society for the Suppression of Vice was an active force in censorship in the 1920s, 30s, and 40s, all the way until 1950. They targeted works of literature such as James Joyce's *Ulysses* (1922), pornographic Tijuana bibles, works on birth control, collections of ribald jokes, pulp magazines, and anything else they felt within their remit. Pulp magazines like *Weird Tales*, which sometimes included nude women on the covers, faced internal conflict from readers for and against the practice; H. P. Lovecraft once famously opined:

> I have no objection to the nude in art—in fact, the human figure is as worthy a type of subject matter as any other object of beauty in the visible world. But I don’t see what the hell Mrs. Brundage’s undressed ladies have to do with weird fiction!

— H. P. Lovecraft to Willis Conover, *Selected Letters* 5.304

Lovecraft was also less than thrilled with the violence in his friend Robert E. Howard's stories of Conan the Cimmerian, noting:

> Robert E. Howard’s omnipresent gore-spattering is surely getting monotonous, but I fear it will prove a hard fault to eradicate. 

—H. P. Lovecraft to Clark Ashton Smith, 11 Sep 1931, *Dawnward Spire, Lonely Hill* 322

Lovecraft of course made no effort to eradicate either the nudes or the violence from *Weird Tales*; to editor Farnsworth Wright, both practices had their sales value and their audience. The pulps could safely advertise for mail-order works on birth control or the mysteries of sex, often under the thin disguise of works on anthropology and ethnography; the Society for the Suppression of Vice's unwillingness to go after academic texts provided a cover for publishers to produce ""dry"" treatises like *Voodoo-Eros: Ethnological Studies in the Sex-Life of the African Aborigines* (1933), and works on flagellation and physical punishment which the Society did not recognize as ""sexual"" proliferated; Robert E. Howard himself owned a copy of *Flagellation and the Flagellants: A History of the Rod in all Countries from the Earliest Period to the Present Time* (1910) and several other works—not necessarily because of prurient interest, but as raw material for his stories.

> The rhythm of the swaying bodies grew faster and into the space between the people and the monolith sprang a naked young woman, her eyes blazing, her long black hair flying loose. Spinning dizzily on her toes, she whirled across the open space and fell prostrate before the Stone, where she lay motionless. The next instant a fantastic figure followed her--a man from whose waist hung a goatskin, and whose features were entirely hidden by a sort of mask made from a huge wolf's head, so that he looked like a monstrous, nightmare being, horribly compounded of elements both human and bestial. In his hand he held a bunch of long fir switches bound together at the larger ends, and the moonlight glinted on a chain of heavy gold looped about his neck. A smaller chain depending from it suggested a pendant of some sort, but this was missing.
> 
> The people tossed their arms violently and seemed to redouble their shouts as this grotesque creature loped across the open space with many a fantastic leap and caper. Coming to the woman who lay before the monolith, he began to lash her with the switches he bore, and she leaped up and spun into the wild mazes of the most incredible dance I have ever seen. And her tormentor danced with her, keeping the wild rhythm, matching her every whirl and bound, while incessantly raining cruel blows on her naked body. And at every blow he shouted a single word, over and over, and all the people shouted it back. I could see the working of their lips, and now the faint far-off murmur of their voices merged and blended into one distant shout, repeated over and over with slobbering ecstasy. But what the one word was, I could not make out.

—Robert E. Howard, ""The Black Stone"", *Weird Tales* (Nov 1931)

Howard included such scenes in his stories, just as many other authors did, because including a nude woman increased the chances of getting a coveted cover illustration—which often brought a boost in pay as well as prestige. Making the nudity a scene of flagellation was a way to help get it past the censors, since the editor could point to the exact scene being illustrated, demonstrating it's ""literary"" value.

Other publications were not so discerning. As the pulp marketplace grew, it proliferated and diversified. Pulps were in constant competition to find new niches, and quickly specialized. Two in particular stand out in the late 1930s: the Spicy pulps, which focused on sex, and the Shudder pulps or weird terror pulps, which focused on grue.

Despite the name, the Spicy pulps sold the sizzle but not the steak; with their blatant focus on sex, they were obvious targets for censorship, and developed strict editorial guidelines about what to write and not write; illustrations and stories were routinely censored to be risque or sensual without crossing the line - often stipulating that a woman could not get entirely naked, or that a nude corpse was acceptable but not a living woman. Robert E. Howard wrote of writing for the spicies:

> A nice balance must be maintained — the stuff must be hot enough to make the readers bat their eyes, but not too hot to get the censors on them. They have some definite taboos. No degeneracy, for instance. No sadism or masochism. Though extremely fond of almost-nude ladies, they prefer her to retain some garment ordinarily — like a coyly revealing chemise. However this taboo isn’t iron-clad, for I’ve violated it in nearly every story I’ve sold them. I’ve found a good formula is to strip the heroine gradually — she loses part of her clothes in one episode, some more in the next, and so on until the climax finds her in a state of tantalizing innocence. Certain words are taboo, also, though up to a certain point considerable frankness in discussing the female anatomy is allowed.

—Robert E. Howard to Novalyne Price, 14 Feb 1936, *Collected Letters* 3.19

Shudder pulps represented the other end of the spectrum: sadism was the rule. These were the pulps that often featured bound women on the cover, sometimes being tortured in inventive ways, buried or burned or swallowed alive, and the contents reached a peak of grue that the more mainstream pulps wouldn't touch. The sadism angle sometimes saved them from censorship, but New York mayor Fiorrello LaGuardia was still moved to ban the sale of pulps with nude covers in the city in the 1930s.

Pulps were the direct precursors to comic books, often sharing the same writers, artists, editors, publishers, and distributors. The group behind the *Spicy* pulps ran comic strips in their magazines, and eventually became DC comics. Martin Goodman published horror pulps like *Uncanny Tales* and *Marvel Tales* before switching to Marvel Comics.

The early comics, like the pulps, were varied and self-policing. By the end of World War II, as paper restrictions were relaxed, there were crime comics and romance comics, science fiction comics and westerns, horror comics and sex comics - although the more explicit of the latter were still sold under the counter, often produced crudely by small groups rather than major syndicates like DC. Most companies had their own internal codes and guidelines; Sheldon Mayer at DC provided one list of rules for writers and artists in the 1940s:

1) Never show anybody stabbed or shot.

2) Show no torture scenes.

3) Never show a hypodermic needle.

4) Don't chop the limbs off anybody.

5) Never show a coffin, especially with the body in it.

—Mike Benton, *The Illustrated History of Horror Comics* 52

Not every publisher kept to such strict guidelines, and crime and horror comics in particular would often be particularly gruesome, reflecting the standards of the shudder pulps that they ultimately emerged from. Here, though, something different happened: somebody thought of the children.",0
"Student in Middle Eastern Studies here, 

As /u/sunagainstgold mentioned below this is an extremely complex topic, but I'll try to explain it in the best way that I can. I'll answer the first question since I'm not sure what you mean with the second one. 

***How was Muhammad raised?*** 

The main sources for the understanding of the life of Muhammad can be found in the Quran and later sources. This already creates two problems; 

* First, there is a very big debate as to the final recession of the Quran. The general assumption is that it was Uthman, the 3rd Caliph of the Rashidun Caliphate, who ordered that the Quran be collected. The word collected is important, because the Quran is the literal word of God through his messenger Muhammad. If you look at the reign of Uthman (644-656 C.E) you'll see that it was after the death of the Prophet. Added to that there is a chance that the Quran may have been written in order to justify certain policies of the Caliph during that time, we know this because pieces of the Quran were still being found after Uthman ordered the collection of it, which means he might have left some things out voluntarily. Other historians agree that is was Abd al-Malik ibn Marwan (646-705 C.E.), the fifth Umayyad caliph who was responsible for the final collection of the Quran. This is also plausible since there is reason to believe that Uthman didn't have the means to achieve such a feat in his time. Plus, the collection of the word of God fits into Abd al-Malik's centralization and unification policies. So even if the Quran can help us understand how the Prophet lived, it was certainly not collected or redacted during his time. Added to that, some passages might have been written to justify certain decisions or events during the reign of the Caliphs. 

* The second problems comes with the various later sources (Greek, Hebrew, Armenian and Islamic); they were written after the death of Muhammad. But despite the existence of various sources of different origins, it is virtually impossible to write a somewhat reliable biography of Muhammad. Harald Motzki explains it in *The Biography of Muhammad: The Issue of Sources* as follow:
>  "" On the one hand, it is not possible to write a historical biography of the Prophet without being accused of using the sources uncritically, while on the other hand, when using the sources critically, it is simply not possible to write such a biography.” 

It is therefore best to regard the story of the life of Muhammad as a work of creative storytellers or myth makers seeking both to understand and to explain The Prophet —and thus themselves—in light of narratives that
were part of a shared or common Near Eastern religious, literary, and cultural
heritage (Aaron Hughes *Muslim Identities*)

What we do know comes from one of the earliest sources from seventh century Armenian bishop Sebeos,because it seems to confirm what will later be written in Muslim sources. We know that Muhammad existed, that he was a merchant and raised as such to become one and that his eventual preachings were centred around the figure of Abraham. 

According to the Islamic tradition, Muhammad was born in 570 C.E. in Mecca. He came from the Banu Hashim, a family within the larger Quraysh tribe. His family was quite important, if not the most powerful within the Quraysh. His parents and grandfatehr died when he was relatively young and so he came under the care of his uncle, Abu Talib. As a teenager, he accompanied his uncle on trading expeditions to Syria which gave him experience in commercial trade. Again, the sources tell us that a Christian monk named Bahira foresaw Muhammad's destiny to become a Prophet, the last Prophet to be precise. 

Muhammad later married an older woman and widow by the name of Khadija. When he was about forty he went to the Hira cave (probably thanks to a pre-Islamic custom), where he saw the angel Jibril (Gabriel). This is where, according to Islamic tradition, Muhammad's prophetic journey started. 

As you can see, answering the first question can prove quite tricky because of the two issues above. I hope however, you understand what I meant in the development of my answer.  As for the second question, I wasn't sure I understood it correctly so I preferred not to answer. I feared it might not have been the answer you were looking for. 

If you need more information about the things I said, please, feel free to tell me. 

If you want more information on this topic; I highly recommend

* *The Biography of Muhammad: The Issue of Sources* By Harald Motzki (Leiden, Brill, 2000)
* *Muslim Identities* By Aaron Hughes (New-York, Columbia University Press, 2013)
* *The Prophet and the Age of the Caliphates* By Hugh Kennedy (London, Routledge, 2016, third edition) 


If you want more information on the Caliphates and the Muslim conquests, Hugh Kennedy is the best I can recommend. Aaron Hughes' book is an excellent way to get to know the Muslim world and it's evolution through the years. From it's origins to 9/11.


",0
"Hi there! 

You've probably reading this page hoping for an answer to this really interesting question! If that's you, do check out this reply from 3 months ago to a similar question by our Top Quality Contributor /u/kieslowskifan: [Why did George V oppose asylum for Tsar Nicholas II? Did he regret it after the Tsar's murder?](https://www.reddit.com/r/AskHistorians/comments/6hf14o/why_did_george_v_oppose_asylum_for_tsar_nicholas/diyls2m/)

And if you're wondering why there's so many removed comments: the reason for that is that a) /r/AskHistorians has high standards, and b) the subreddit is not a place for free discussion, even below the top-level. If your post starts with things like ""I don't have a specific answer for you, but..."", chances are we will probably remove your comment. If it's only a paragraph long, we will probably remove your comment. If your post is complaining about there being no answer yet or wondering what happened to the removed comments, we will definitely remove your comment. For better or worse, Reddit is set up to show you the amount of comments, including removed comments: as a moderator who can see the removed comments, you're mostly missing posts like 'This is the meanest, most stuck up sub ever', 'Why is every answer removed?' and 'Eagerly awaiting response', along with people replying with about three sentences.

Our readers want accurate, comprehensive, in-depth historical answers based on good historical practice and high-quality sources (it's amazing how many downvotes and reports an obvious shitpost can attract on a popular thread on /r/AskHistorians within minutes, thanks to our readers!) On /r/AskHistorians, we want people answering questions to be able to explain not just what the basic facts are, [but why we know that these basic facts are right, and to put those basic facts into context](https://www.reddit.com/r/AskHistorians/wiki/rules#wiki_write_an_in-depth_answer). This is why we encourage [the use of primary and secondary sources in answering questions](https://www.reddit.com/r/AskHistorians/wiki/rules#wiki_sources), rather than tertiary sources like Wikipedia, podcasts and textbooks.

In other words, on /r/AskHistorians, we'd rather have no answer than bad attempts at answers. By deleting short, quick, bad answers, the well-researched in-depth answers (that take people time to research and write) are more likely to be seen. In general, on Reddit, [the early answers get the bulk of the karma](https://www.reddit.com/r/dataisbeautiful/comments/64y44g/the_mostupvoted_comments_in_reddit_threads_arent/#) regardless of quality. On /r/AskHistorians we want to make sure that the things that people likely actually read - the top post on a thread - are accurate. We want that top post to have the depth that a good understanding of history requires (people are complex, and so history is complex). Thus we remove answers that are not up to standard in order to reward the posters who - in their own free time, let's not forget - go to the effort of doing the research and writing up a quality response (which often takes *hours*). 

As you can see from [this post on the statistics of AskHistorians](https://www.reddit.com/r/AskHistorians/comments/6a5duv/a_statistical_analysis_of_10000_raskhistorians/) by /u/georgy_k_zhukov, 90%+ of popular threads like this will get an answer. That answer usually appears, on average, about 9 hours into the life of the thread. So while this thread does not yet have a suitable answer in it, if you are patient, something awesome will hopefully be here if you come back before too long. And if you want to read some awesome answers while you're waiting, might I suggest [the answers we link to on our Twitter](http://twitter.com/askhistorians), or [our weekly Sunday Digest feature](http://www.reddit.com/r/AskHistorians/search?q=title%3A%22Sunday+Digest%22&restrict_sr=on&sort=new&t=all) that links to overlooked posts that never got to /r/all but are often incredible?

Alternatively, if you want to discuss history or ask historical questions without these constraints, /r/history or /r/AskHistory might be a more appropriate subreddit for you than /r/AskHistorians. 
",0
"The short answer to this is: *not really*. 

First it might be worth pointing out that the censorship of the female form is not something that is necessarily the norm across human history. We have found plenty of fully nude depictions of women from antiquity for example. One particularly noteworthy artifact is a roughly 4,000 year old vase depicting the [Mesopotamian goddess Isthar](https://collections.louvre.fr/en/ark:/53355/cl010141456), clearly not censoring anything. We also have examples from the Etruscans, such as this [engraved bronze mirror](https://i.imgur.com/OTU1yxi.png) from circa 300 B.C. depicting female nudity entirely. 

However, as we move through time towards the ancient Greeks this changes. In the archaic period, depictions of female nudity are exceptionally rare. As we move away from the Archaic period into the Classical period we start to see some of the earliest depictions of female nudity, one of which being the now very famous [Aphrodite of Knidos](https://en.wikipedia.org/wiki/Aphrodite_of_Knidos). But, as you can see, it includes that ""barbie-like"" anatomy you mentioned. 

Within ancient Greek society, generally speaking, female nudity was considered very differently from male nudity. The majority of art depicting female nudity at all will be about goddesses, not about Greek women. For Greek women, nudity inherently related to ideas of vulnerability and as such they were mostly excluded. 

Even as we move the Hellenistic period, where nude depictions of women are plentiful, we do not find anything akin to male depictions with finely detailed genitalia, or anything similar to the ancient Indian depictions of female goddesses which did not usually censor female genitialia. 

I'm not aware of any explicit accounts explaining the decision to effectively censor the vulva, however, it is generally believed that within ancient Greek society it was simply considered too immodest or too (subconsciously) sexually aggressive to depict the female genitalia in detail. There is much more information about this in an essay by Larissa Bonfante^1 in which she compares ancient Greek art to that of their neighbors. Another worthwhile read is [Undressing the Female Nude: The Paradox of Morality in Ancient Greek Sculpture](https://www.academia.edu/27559215/Undressing_the_Female_Nude_The_Paradox_of_Morality_in_Ancient_Greek_Sculpture) by Lydia Schriemer. It provides a really thorough and through-provoking discussion about this subject.

Nevertheless, the fact remains that as far as I am aware, there are no depictions of female nudity as described by you (although some scholars do argue that the Aphrodite of Knidos would have had painted on pubic hair) from Ancient Greece. As for the Romans, because of their infatuation with Greek culture, they also copied many of their artistic practices. The result being that we don't (or exceedingly rarely) find any depictions of the female genitals in ancient Roman art either. 

> 1. Larissa Bonfante, Notes in the History of Art, Vol. 12, No. 2, ESSAYS ON NUDITY IN ANTIQUITY
IN MEMORY OF OTTO BRENDEL (Winter 1993), pp. 47-55",0
"But it's also important to recognize the past as having been fluid and multivocal, just like the present. There wasn't a single party-line British perspective about Ireland. A whole century earlier than ""the great famine"", Jonathan Swift mocked paternalistic attitudes towards the Irish in A Modest Proposal. During the famine, there were newspaper articles in Britain which blamed the government and landowners for the problem. And even the workhouse laws implicitly held the landowners responsible, by requiring them to pay for relief to tenants on their property. People were dying in the workhouses. Within Ireland, this wasn't a secret. So did the British know? And if they didn't know, was it because they preferred it that way? One thing to consider is that there was a lot of absentee administration by the British, meaning that many of the people making decisions weren't actually located on the ground. But I also think it's too convenient to say that nobody knew how bad the situation had gotten. Ultimately, it's a complex whole, with no single 'correct' representation of British perspectives. But we do tend to focus heavily on those Parliamentary debates, because they represent the views of the parties in power. And the actual history was guided by these parties.

You might notice that I'm hemming and hawing a bit. More key is that you might notice that I'm not taking particular pains to explain the specifics of Parliament's perspective, or the laws that Parliament passed. The reason is that it's ancillary to the point I'm building towards.

See, thing is, thus far we've been dancing around the real issue. Let's set aside for a moment whether or not the British acted with intent to kill many Irish people, and instead move onto the second question. How do we define a genocide? And does intent actually matter? Now conventionally, the term genocide does in fact require the systematic and intentional killing of a particular ethnic group. But let's work on breaking that idea down, so that we can take a look at what it really means.

I'll use my own cultural background as a Bengali for context. In Bengal, one of the dominant theological traditions is Shaktism, in which Kali is often symbolically centralized. Kali iconography is complicated, but overall the concept of Kali is intertwined with the fleeting, roiling nature of the material world, and its habit of ceaseless change. Kali's two most common representations are Kali Ma (or Kali as mother) and Mahakali (or Kali as Time).

Why does this matter? Well, if you take a look at any Kali iconography, you'll notice that she appears very demonic. In actuality, this is a reference to Kali's association with the material world, with her fearsome appearance intended to correlate to the idea of love as the ultimate overcoming of one's fears. Her appearance may also constitute a reappropriation of demonic imagery, as demons often operate as stand-ins for tribal and low-caste people in Puranic literature, and the Shakta tradition has roots in the union of the anti-caste Bhakti and Tantra traditions. Either way, Kali iconography may seem violent to those deprived of context, but in actuality there's no reason to associate Shaktism or Kali with violence. But the thing is, this didn't matter. Most westerners didn't have this context, and to their eyes, Kali looked evocative of Satanic imagery. They therefore ascribed gruesome rituals to ""Kali cults"", viewing them as dharmic equivalents to western Satanism.

In the 1800s, there was a problem of highway crime throughout India, or at least the British believed this to be so. The cause, and whether it even existed in a notable fashion, is up for debate. According to the British, these highway crimes (situated across an entire continent!) were all the work of a single organized group called the Thuggees. For comparison, that would be like ascribing all gang violence in North America to the Sicilian Mafia. What's more, the British also asserted that the Thuggees weren't actually common criminals, but a cult of religious fanatics practicing blood rituals in devotion to Kali. This is, uh, hilariously nonsensical in the actual context of the historical theology and Kali symbolism. But it didn't stop the British from conducting a sweeping campaign to pacify the highways, in which many alleged thuggees were given trials with no proper procedure and summarily convicted. The British then forced through a law called the Criminal Tribes Act, which stated that certain communities (such as Tantric tradition of Kali-devotees) are inherently criminal and that simply being born to such a community is a prosecutable offense. The purpose of this policy was explicitly the eradication of communities deemed undesirable. The policy was also explicitly justified by connecting the 'criminal tribes' to the (western-invented) Kali-devoted Thuggee cults. I here quote James Stevens, who headed the legal department of the British Indian government and oversaw the law's drafting, in his explanation of caste and how it defines a so-called ""criminal tribe"".

>It means a tribe whose ancestors were criminal from time immemorial, who are themselves destined by the usage of caste to commit crimes and whose descendants will be offenders against law, until the whole tribe is exterminated or accounted for in the manner of Thugs. When a man tells you that he is an offender against law he has been so from the beginning and will be so to the end. Reform is impossible, for it is his trade, his caste, I may almost say his religion is to commit crime.

Using these measures, communities were either forcibly suppressed, or put into camps and schools to stamp out their indigenous culture in favor of Victorian sensibilities. This destruction of culture was primarily inflicted upon lower-caste people, and was rooted in narratives invented wholly by the British about Kali representing a Satanic belief system built around blood sacrifice, supposedly associated with the Thuggees. Remember again that Kali was originally associated with Tantra and Bhakti, and that Kali iconography was originally connected to the emancipation of lower caste people. To this day, western neopagans and satanists continue to appropriate Kali imagery based on a perceived quality of witchcraft to the Kali tradition (often upheld as appreciation and 'defense' of the Kali tradition). What's more, the critically-acclaimed and high-grossing Indiana Jones franchise drew directly from British colonial sources to affirm the false assertion of a Thuggee cult practicing blood magic, which given the actions justified by that misinformation would be roughly the equivalent to adapting The Protocols of the Elders of Zion (a famed antisemetic text).

Okay, so why did I just go on a long diatribe about Kali traditions and western imperialism? Is it just because I wanted to unsubtly shoehorn in a passive-aggressive note on how astounding it is that we're even remotely okay with the Indiana Jones thing? I mean, it's not *not* because I wanted to do that. But I actually do have a real, relevant point to bringing this all up. Which is the following:

Um ... uhhhh ... was this genocide? I mean, it has to be, right? And yeah. I totally think it is. The people targeted were deprived of culture, security, health, and life. They were targeted for reasons of ethnic and cultural intolerance. We have the framer of the law saying that it was implemented with the express intention of exterminating certain groups of people, on the basis that such groups are inherently undesirable. Seems like an open-and-shut case. This is genocide, plain and simple. And, as previously established, the Irish famine wasn't genocide, because unlike the case with the Criminal Tribes Act and Thuggee culls, there's no proof of intent to eliminate the Irish as a people.

Problem is, apart from that one technicality, the two situations are extremely comparable. Uncannily so. In both cases, the British thought that they were improving the state of affairs in the respective colonies. That might be hard to imagine with the Criminal Tribes Act, but the British upheld that they were actually protecting other Indian communities from these less civilized criminal elements. In both cases, there was a trigger problem which was real, or at least in the case of Indian highway crime, very likely real. In both cases, the answer came in the form of residential systems which were used to stamp out indigenous culture or lives. And in both cases, they arrived at their particular solution specifically because of their lack of understanding about the cultures in question.

[Continued in Reply](https://www.reddit.com/r/AskHistorians/comments/pqjz96/the_irish_potato_famine_18451852_while_often/hddivfz/?utm_source=reddit&utm_medium=web2x&context=3)",0
"In the U.S. and U.K. at least, ""Back In the U.S.S.R."" was widely understood upon its release to be a satire, a deliberate pastiche of Chuck Berry and Beach Boys style late 1950s/early 60s Americana ironically looking fondly at the Soviet Commies.

This is found in contemporary reviews such as in [*Rolling Stone*](https://www.rollingstone.com/music/music-news/review-the-beatles-white-album-186863/), [*American Eagle*](https://idnc.library.illinois.edu/cgi-bin/illinois?a=d&d=AUE19681217.2.21&e=-------en-20--1--img-txIN-%252522back+in+the+u.s.s.r.%252522--------), *Record Mirror*, and the [*New York Times*](https://i.imgur.com/HnAqBgX.jpg). As an example, from the 1968 *Rolling Stone* review:

> ""...the Beatles can safely afford to be eclectic, deliberately borrowing and accepting any outside influence or idea or emotion, because their own musical ability and personal/spiritual/artistic identity is so strong that they make it uniquely theirs, and uniquely the Beatles. They are so good that they not only expand the idiom, but they are also able to penetrate it and take it further.
>
> ""“Back in the USSR,” this album’s first track, is, of course, a perfect example of all this: it is not just an imitation (only in parts) of the Beach Boys, but an imitation of the Beach Boys imitating Chuck Berry. This is hardly an original concept or thing to do: just in the past few months we have been deluged with talk of “going back to rock and roll,” so much that the idea (first expressed in the pages of Rolling Stone) is now a tiresome one, because it is, like all other superficial changes in rock and roll styles, one that soon becomes faddish, over-used and tired-out.""

The review hints at the nostalgic trend at the time just emerging to look back at the 1950s, which would later be typified by bands like Sha-Na-Na, and then in the 1970s, in media like *American Graffiti*, *Grease*, and *Happy Days*. In 1969, John Lennon played at a concert festival called the ""Toronto Rock & Roll Revival"" on the same bill as Chuck Berry, Little Richard, Bo Diddley, Jerry Lee Lewis, and others. They played alongside contemporary acts such as Lennon's Plastic Ono Band, Alice Cooper, and The Doors. (The Plastic Ono Band set would be released as the live album *Live Peace In Toronto* in late 1969.) The 1950s ""rock and roll revival"", a reaction to the psychedelic and early prog-rock scenes, accused by some as being too self-important, was thus already being recognized by the end of 1968. The contemporary reviews tended to look at ""Back In the U.S.S.R."" more from that standpoint, rather than from a political point of view.

On mobile, sorry if there are spelling errors.",0
"On to part 2 -- why all this matters to understand the autopsy of Charles/Carlos II. Carlos II died in 1700. I don't have ANY primary information about how or why the autopsy was performed, and none of the review articles (some of which are in Spanish) seem to have any information on that either. So I'll defer to someone else on why it might have been performed in early 18th century Spain.  Cerda, in (Rev Méd Chile 2008; 136: 267-270) suggests that it was done because he was known as ""El hechizado"", which he translated as ""The Bewitched"" (and I think should be able to be translated at ""The Cursed"" as well). Because I don't have the autopsy in front of me, this is largely speculation -- but with the caveat that I've read a lot of literature from this period. The ""clinical gaze"" of the physician who cared for Carlos -- which would likely NOT have been who performed the autopsy, I should mention -- was not looking for disease localized in specific organs. That idea would have been completely foreign to physicians of this period. And going along with that -- in later periods, the body as opened AS SOON AS POSSIBLE to catch the transition between life and death. Even in Morgagni's time, there could be a lag to dissection, though he was aware of the effects of putrefaction.

&#x200B;

Instead, he would have been looking for evidence of fundamental imbalances -- and this is speculation, of course, since my caveat above -- but even evidence that he was ""Bewitched"".  We have to interpret these colorful descriptions knowing full the context in which these physicians operated.

&#x200B;

What underlying diseases did Carlos have, then, and what did he likely die from? Modern paleopathology looks at multiple sources. The first thing to know if that Carlos was considerably inbred. Alvarez et al found that the inbreeding coefficient for Charles II was 0.254 -- HIGHER than the cofficient of brother and sister, or father-daughter! ([https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0005174](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0005174)). This would certainly put him at higher risk for recessive diseases. Hodge ([https://www.ncbi.nlm.nih.gov/pubmed/330890](https://www.ncbi.nlm.nih.gov/pubmed/330890)) looked at phenotypes of the Hapsburg's from their rural portrations, and concludes that Carlos' II shows phenotypical changes consistent with Kleinfelters (portrait: [https://en.wikipedia.org/wiki/Charles\_II\_of\_Spain#/media/File:Charles\_II\_of\_Spain\_anonymous\_portrait.jpg](https://en.wikipedia.org/wiki/Charles_II_of_Spain#/media/File:Charles_II_of_Spain_anonymous_portrait.jpg); Wikipedia example: [https://en.wikipedia.org/wiki/Klinefelter\_syndrome#/media/File:Bodymorphproj\_mkg\_modA001\_20070325\_pos03.jpg](https://en.wikipedia.org/wiki/Klinefelter_syndrome#/media/File:Bodymorphproj_mkg_modA001_20070325_pos03.jpg)), that is, having two X chromosomes. Others have suggested rarer x-linked diseases.

&#x200B;

And what did he die of? Most explanations have focused on his unusual genitalia and suggested some combination of renal colic, stones, and infections leading to his eventual death. However, before he died he apparently had a wasting disease and severe diarrhea. This has led some to suggest a more mundane (for the 17th century) cause of death -- TB enteritis.",0
"The claim that the Mafia turned the Stonewall Inn into a gay bar is a popular misconception. It is often associated with the Stonewall Riots of 1969, which marked a pivotal moment in the LGBTQ+ rights movement in the United States. In reality, the Stonewall Inn was already a gay bar when it was raided by the police on June 28, 1969.",1
"I think this is a good answer, and it's worth adding that Circe is also a goddess, Hecuba is not, but I don't think Odysseus ever has sex with her? Correct me if I'm wrong.",0
"Do you think it also has to do with detaching the Nazi period institutions from the ones of later periods? Saying Wehrmacht and Luftwaffe immediately brings Nazi Germany into minds and makes it seem like they are completely different institutions than the ""German Army"" and ""German Air Force"" of the two Cold War German republics, or the reunified Germany.",0
"It depends heavily on the method of hunting. 

Historically, there were two types of hunts. ""At force"" hunts, and ""bow and stable"" hunts. 

An ""at force"" hunt would involve a large group of men, typically dogs, and was more physically demanding.   The dogs and the men would move through the hunting area until an animal was scented or sighted, and the animal would then be chased until it reached exhaustion, or was cornered by the dogs.  One or more hunters would then finish the animal off with hand weapons, in the case of a boar, typically but not universally a spear.  

In ""bow and stable"" hunting,  the hunting party was smaller, and the hunters would typically ride (less commonly walk) through the hunting area in search of game. When the game was sighted, they would attempt to shoot it with a bow and arrow. In some cases the hunt would be conducted within a closed ""park"" where the deer could not readily escape.  


Accidents can come a number of ways:

1. any time you are riding over rough terrain, there is the possibility your horse can trip, or be startled or do something else that will forcibly dislodge you from the saddle, and that always comes with the prospect of injury. 

2. As you note, a hunter could have an encounter with a wild animal, whether it be a stag, boar or some other animal, particularly if cornered, all have the capability to injure, if not kill. 

3. When you have a group of men firing arrows at a moving target, (possibly while intoxicated), or a hidden target, there is the possibility for an errant shot. Turkey hunters famously shoot each other more frequently than most because Turkey Hunters don't have to wear orange in many states and turkey hunters tend to try to draw turkeys in with calls, and look for small motions in the brush or undergrowth and believing it to be a turkey, shoot, not realizing they're sighting in on another camouflaged hunter.",0
"In ancient Greece, the ideology and expectations surrounding sex and monogamy varied depending on the time period, region, and social class. It is important to note that the stories and myths, like that of Odysseus, often reflect a different set of values and beliefs than those held by everyday people. In general, ancient Greek society was patriarchal, and men held a higher status than women. Men were allowed more sexual freedom, especially if they were of higher social status.",1
"In Ancient Greece, there are indeed instances where unmarried pregnant women claimed that a god, often Zeus, was the father of their child. However, it's important to note that the specific circumstances and motivations behind such claims can vary. One example of such a claim can be found in Greek mythology itself. In the story of Danae, Zeus impregnates her by appearing to her as a shower of gold. Danae then conceives Perseus, the legendary Greek hero.",1
"This is great, thank you. Do either of these works have anything to say, to your knowledge, as to the efficacy of the second part of the question? Namely, that Hitler's views were ""written off"", so to speak, by the business class as empty rhetoric? It seems like they expected a return on their investment vis-a-vis Hitler and the Nazi's destruction of much of the communist/Marxist presence in Germany?

I guess what I'm trying to say is, it seems like German businessmen supported Hitler and the Nazis specifically because they agreed with his vision of terminating Marxism within Germany. Did they also believe in his message towards Jews, or did they write that part of the message off as ""too radical"" or ""empty rhetoric""? I only ask because that particular statement in the video stood out as something I had never heard before, and quite frankly, was somewhat convenient (for lack of a better term) given the current political environment.",0
"There's an adage in education that is basically, ""we measure what matters."" Which is to say, we can get a sense of what a community or society values based on the knowledge, skills, or dispositions that are assessed in schools. (There's often a disconnect between the things adults claim to value and the things we ask children to learn but that's a different conversation for a different subreddit.)

So, given that rule of thumb and an understanding of American education history, I feel comfortable saying that yes, ""dinner with the boss"" was a thing that actually happened - or teachers thought *might* happen - and the main reason is that the New York State Regents Examinations in Comprehensive Vocational Homemaking routinely asked the young women taking the exam about entertaining, including for their husband's boss. 

First, some quick context setting. NYS has the oldest formal system of education in the United States, dating back to the 1780s. This headstart meant NY policymakers and educators were experimenting with different structures long before some other states were even considering the possibility of public education. After a few different approaches to funding and curriculum, the structure that NYS fell into, and stuck with to the present day, was based on the idea that in order to ensure consistency across the state, there needed to a common measure of student learning. This measure, which was first given in the mid-1800s is colloquially known as The Regents Exams. I won't subject you to a history of the exams (as fascinating as I think it is) but basically, they're a series of exams given to high school students across the state as a way to document their mastery of content the state deems necessary before they'll award a high school diploma with a Regents endorsement.

For the purpose of your question, the most important feature was the feedback loop between NYS teachers and the exams. (At one point in the early 1900s, there were upwards of 90 different Regents exams. Schools/students could pick and choose which ones to take.) Teachers across the state determined the content for the exams and then went back to their classrooms and taught students the content that would appear on the exams. Teachers who did not participate in the writing process were given guides on what would appear (AKA standards.) They didn't know the exact questions on the exams but, for example, the teachers who taught bookkeeping knew there would be several questions where students had to solve long arithmetic problems by hand and show their work. So they taught their students how to solve complicated arithmetic problems by hand and show their work. Etc.

This doesn't mean Homemaking was offered at every NYS high school or that all girls had to take the course, but rather, there were NYS HS teachers who wanted to offer the course and felt their course content was worthy of inclusion in the pool of knowledge students learned as part of obtaining a diploma. At some point in the early 1930s, a group of NYS educators proposed courses and a corresponding exam called Comprehensive Vocational Homemaking. Their rationale and the exact year is several hundred miles away from me in the state's archive, but I know from other research that the time between proposal and exam administration was typically 1-3 years. Students then needed 3 years of courses to sit for the exam (hence ""Comprehensive."")

It's my understanding that the first administration was in June 1937 and included the note:
> The minimum time requirement [for taking this exam] is 10 periods a week for three school years with outside preparation and home project work. These three years of work must include homemaking B and D.

The exam was broken into multiple parts and the first question on Part I, section III of the 1937 exam question read:
> Suppose that you intend to invite five friends for supper, and the evening on Sunday, July 7.

> a. Write your part of a telephone conversation inviting one of these friends. [3 points] .

> b You have decided to serve a buffet supper. Write the menu for it. [5 points]

> c State your plan for the entertainment of the guests. [2 points]

So we know that from the beginning, Homemaking teachers thought teaching young women how to entertain was important. However, the exams weren't just about entertaining - there were questions about taxes (*T/F: Assessments for taxation purposes are divided equally among all the houses in a locality.*) child safety *(T/F: Instinct teaches a mother how to care for her baby*. False. FYI.), food safety (*The growth of bacteria, yeasts, and molds cause the ___ of food.*), etc. 

I don't have access to any of the exams from the 1940s but questions about entertaining appear on the 1950 exam:
> Part II, Question 1: 
> Part of a home experience might be assuming responsibility for preparing dinner for a family of four and two guests. A girl might choose the following menu:

> * tomato juice
> * broiled steak
> * mashed potatoes
> * buttered peas
> * molded fruit salad
> * baking-powder biscuits
> * butter
> * chocolate cake 
> * coffee 
> * milk 

> A. Consider all duties involved in preparing this meal. List four duties which might be done the night before, showing good management of time.

While I cannot confidently speak to what happened in other states (there's [a book coming out](https://wwnorton.com/books/9781324004493) in May on the topic that I'm very much looking forward to) but it's safe to say that the heteronormative idea that a husband would go off in the morning to an office job ([more on the history of ""9-5"" if you're so inclined](https://www.reddit.com/r/AskHistorians/comments/id2cfp/dolly_parton_had_a_famous_song_9_to_5_yet_every/g27rs9a/)) while the wife stayed home as a homemaker and at some point, ""dinner with the boss"" would happen.

Which is to say: if the calendar in the kitchen is to be believed, Mr. Hart, Vision's boss was coming for dinner on Wednesday, August 23, 1950, 1961, or 1967. I can't say it's common - hopefully, someone familiar with the history of workplace etiquette from the era will chime in - and I'm not sure when Wanda would have graduated high school or if she went to school in NYS but odds are good that if she attended a suburban white high school and was interested in obtaining a Regents diploma, she likely took a high school course that prepared her to expect the homemaking responsibility of hosting her husband's boss (or conversely be the boss' wife.) She would have been taught the content needed to answer questions like (all from Homemaker exams between 1950 and 1961):

> A homemaker on a limited budget, with only one hour to prepare dinner ... could include in the menu (1) rib roast of beef (2) stuffed onions (3) angel food cake (4) gingerbread with applesauce

~~
> Which indicates formal balance in a living room? (1) candlesticks placed at one end of a mantle and a clock near the center (2) similar chairs placed on either side of a window (3) a grouping of a desk, a chair and a wastebasket (4) a grouping of a reading lamp, a few books and a bowl of flowers on an end table

~~
> A girl's appearance is affected by her ability to choose clothes wisely and to keep them attractive. 

> A. For each of the following, give two characteristics which would indicate good workmanship, (1) a hem (2) a dart (3) a zipper

> B. Explain two ways in which the construction of a garment can affect its durability. 

> C. For each of the following undesirable characteristics of a dress, suggest one type of alteration or remodeling to make the dress wearable. (1) neckline too low (2) bustline too tight (3) stained underarm area

~~
> A person is developing emotional maturity when he (1) controls his reactions (2) requires frequent praise (3) forms many intimate friendships (4) laughs at awkward social situations

~~

> A father's change in jobs makes it necessary for his family to move to a different locality. In this family, there are the father, the mother, a sixteen-year-old girl, a thirteen-year-old girl, a seven-year-old boy and a two-year-old boy.
> 
> A. The family must first decide whether to live in the large city where the father will be working or in one of the small surrounding communities. Suggest four questions the family will need to consider in making this choice. [4]

~~

> The following is a dinner menu for a married couple entertaining the husband's supervisor. Dinner is to be served at 6 PM. Roast beef, mashed potatoes, fresh spinach, bread, gravy, (canned) buttered beets, butter, chocolate cornstarch pudding,  coffee, or milk.
> 
> List four items from the following plan which indicate good timing in preparing the meal described above. Give reason for each answer.

> Night before - make chocolate pudding and place in serving dishes.

> * 2 PM: Wash and peel potatoes and place in cold water.
> * 3:15: Place 5 lb. roast in oven at 325 °.
> * 3:30: Wash spinach and place in cool water 
> * 3:40: Set table and fill water glasses
> * 5:00: Put potatoes, spinach, and beets on stove to cook
> * 5:30: Drain and mash potatoes and place on top of a double boiler
> * 5:45: Measure coffee into pot and set water on to boil
> * 5:50: Get serving dishes from cupboard.
> * 5:55: Place meat and vegetables on serving dishes.

> Make a seating chart and diagram of one place setting for this dinner. 

(Postscript: It's fairly easy for us in 2021 to read such questions and pass judgment on the teachers who wrote the exams and the young women who took the courses. However, it's worth stressing again that the exams also included questions about negotiating a lease, first aid, disaster management, pursuing professional goals, and pulling together a sharp outfit. Also, question 48 on the 1957 exam read: *The ideal family pattern toward which most young couples strive today is (1) autocratic (2) matriarchal (3) patriarchal (4) democratic*. The correct answer was 4.)",0
"> The later Middle Ages did have knowledge of and practice rudimentary cataract surgery

***WOAH,*** hold on a second! You can't just drop that and keep going...

So in the Late Middle Ages, they were able to hold a person still, cut through the cornea, extract the lens, hope it didn't get infected, presumably just bandage over the eye until the cornea healed up? Nothing to lose I suppose in trying - severe cataract, you can't see anything, so you wouldn't get any worse. 

Is there anything more you can tell us about that, or a reference? Who would do that sort of surgery? Presumably someone who had experience with cadaver eyes. What sort of tools did they use? Was the patient sedated in any way (drunk)?

> only to find that behind them, he was farsighted

In front of them, really. The cornea does most of the heavy lifting with regard to focus. If you could get a cataract out and the cornea healed OK, it would be a miraculous restoration of distance vision., just no ability to accommodate. ",0
"The belief that women were more sexually voracious than men and that men needed to control their sexual impulses has indeed been present in various cultures throughout history. However, it is essential to recognize that cultural beliefs and attitudes towards sexuality have varied greatly across time and different societies. In many ancient societies, such as ancient Greece and Rome, there were prevailing ideas that women had insatiable sexual desires and that men needed to regulate and control their own sexual behavior.",1
"Just like medieval Europe, the Mongol rulers had a somewhat flexible attitudes towards concubinage, inheritance, and illegitimacy. But while there are some important points of similarity and even contact--yes, Mongol khans married illegitimate Byzantine princesses--the mix of politics and sexuality played out in a specifically Mongol fashion.

First, let's talk about royal women's responsibilities. Pawns though they might be in their families' use of marriage to form alliances, Mongol queens generally did not have the chance to be ornamental. The most important responsibility was running the various camps that the particular nomadic group lived in. Just like Mongol rulers had multiple wives and concubines, there would be multiple camps; just like there was a senior wife whose children had inheritance primacy, there was a senior camp.

This is actually where we get to see a little female solidarity and community, actually. The wife in charge of a camp was generally also in charge of the other wives and concubines living in it. While their sexual common denominator got bounced around from camp to camp, leaving the groups of women separate most of the time, periodically they all got together for festivals and ceremonies.

That could even be a rare bright spot for women whose marriage to Chinggis (or later rulers) was the result of the devastation and surrender of their people and slaughter of their family. Tatar princesses Yisüi and Yisügen survived the utter destruction of the royal house in 1202 to wind up as junior brides of Chinggis, who ended up running different camps but still met together. The daughters of the conquered Kereit leadership did not have husbands quite so prestigious...but they brought with them their servants, retainers, younger brothers, and a strong social and informational network.

(ETA) I should point out that above, I am talking specifically about high-status women of conquered peoples--royal women generally becoming wives, lower-ranking ones becoming concubines. However, as mentioned in a follow-up comment, medieval warfare across the tri-continental world was synonymous with victorious soldiers raping, enslaving, and/or murdering the people whose cities and villages they stormed. We have basically no firsthand accounts of surviving rape, or even women talking about surviving rape in the abstract, from the Middle Ages in general, much less from Mongol women--Christine de Pizan in 15th century France refers to rape as ""the greatest sorrow"", and there are enough traces of 'tragic backstory' sermon illustrations to show that yes, surviving rape could have a devastating long-term impact on women's lives. As far as their children went, though--remember, the medieval world had no DNA tests, and was strongly suspicious of women's sexual mores as a baseline. ""Legitimacy"" was as much a matter of recognition and acceptance as genetics.

Royal women, particularly those in charge of camps, controlled their own economic resources, including horses. They received a cut of tax revenues and of war booty, even! They could also be active in politics, especially but not only as widows. Töregene, for example, fought long and hard to keep the throne secure for her son. In other cases, women took on an intercessory, informal diplomatic or ""peace-weaving"" role. Elite women also could and did actually attend formal political assemblies, including as advisors.

It's important to recognize that not all royal women controlled their own camps. This was a question of seniority among wives, but also other factors institutional and not--social rank of their birth family, nature of the political alliance or conquest that brought them to Chinggis and the other men of his family, straight-up favoritism. The subordination of some queens to other women could be a major blow to women who had commanded significant power among their birth family and people, as they were now largely excluded from political roles.

Queens and royal concubines thus definitely had a limited sphere of action overall in heavily sexist and patriarchal Mongol society. However, they could push at boundaries and wield not just power but legal authority in their own right. And it was based at least partially on their own elite descent rather than just who their husband happened to be.

With senior wives, junior wives, concubines, kidnapping brides from conquered peoples, exchanging brides and grooms among allies, and different wives holding power over their own camps, Mongols had a roomier definition of marriage than we might expect. As shown above, this is reflected in the commonalities among royal women's lives independent of how they came to be Chinggis' partners or their place in that hierarchy.

But that hierarchy did matter, and not just in terms of who got to run the richest camp. The senior wife was the most important, and the royal succession descended through her.

In some cases, that had the result you might expect. Chinggis paid a *lot* more attention to having children with his senior wife, Börte, than he did with his other wives, even after she either hit menopause or said, ""Nope, sorry, this uterus is closed for business."" (...It's still the Middle Ages, so more likely the former.)

On the other hand, in practice the situation was fuzzier. One of his wives was probably pretty stunned to find he had *traded her*, like property, to one of his allies. One son of Chinggis, additionally, actually altered the hierarchy among his wives to make a different one senior and thus the ""real"" queen mother.

With an established principle of succession and a hazy idea of the ""sanctity of marriage,"" all the other royal children were on somewhat more equal social footing. In other words, without regal inheritance on the line, there was less of a need to draw boundaries around legitimate and illegitimate children. In fact, sometimes it was outright impossible. How these children fared was up to the present situation, needs, and desires of the parents (as in Europe, women could be very active in calculating marriage alliances for their children). There wasn't really a parallel to the western European practice of ""oh, sorry son, you can't hold the throne but have a nice cushy bishopric"", but junior sons could inherit property and participate fully in the politics of marriage.

~

I want to stress that scholars are juuuust starting to take seriously the study of Mongol women and especially queens. (Jack Weatherford was ahead of the game!) Bruno de Nicola, *Women in Mongol Iran: The Khātūns, 1206–1335* was published in 2017 and Anne Broadbridge, *Women and the Making of the Mongol Empire* just this summer. What I've related above will surely be questioned, expanded, altered, and reinterpreted in coming years. As a Latin medievalist, I especially can't wait until scholars with the linguistic chops start working on Europe-Near East-Mongol comparative queenship. But it's an excellent start.",0
"Not only strange; the question of ""should"" has some potential moral implications in other cases, and it's apt that you use that word. I recall a recent AskHistorians discussion on why we refer to WWII Nazi institutions by their German names rather than their translations- Wehrmacht, Heer, Panzer, and so on.

https://www.reddit.com/r/AskHistorians/comments/hd52ng/why_do_english_language_speakers_americans_like/fvjj0gq/

/u/Georgy_K_Zhukov discussed Richard J. Evans' point of view that ""[r]etaining the German is a form of mystification, even romanticization, which ought to be avoided"".",0
"The answer to this question really depends on how you draw the lines between religions. Depending on how you want to do the dividing, there are between one and several thousand Abrahamic religions.

As a case study, let's talk about Christianity (it's what I know best). When you talk about *Christianity*, which Christianity do you mean? Looking at the first five hundred years of Christianity, should we talk about Nicene and Arian Christians as the same religion? What about Montanist and gnostic Christians? Is Christianity within the Roman Empire the same religion as the Christianity practiced outside it? The earliest Christians were adamant that to be a soldier was a sin, as it violated the prohibition against murder. Would they have recognized the crusaders as practicing the same religion? Fast forward to the Great Schism between East and West in 1053 CE. Roman Catholicism and Eastern Orthodoxy have now been officially separated for almost as long as Christianity has been a thing, even if you start counting at Jesus's birth. At what point do we start talking about them as different religions? Can they officially start seeing other people, or is this one of those ""once a couple, always a couple"" things? If that's the case, why isn't Christianity still technically Judaism? It could be, but more on that in a bit!

Because hold on. Is Christianity even a *religion*? For a while, the Romans weren't so sure. Robert Louis Wilken documents this question well in his *The Christians as the Romans Saw Them*. Some Romans thought Christianity was a burial society, a kind of social eating club mixed with funeral insurance (you pay annual dues, the society covers your funeral expenses). Some thought it was a political organization, kind of like a grassroots faction. Some thought it was more a philosophy, and indeed, early Christian thinkers preferred the label ""philosopher"" to ""theologian,"" which had pagan undertones. Some considered them atheists because they refused to participate in the imperial cult. A lot of this came down to bad or faulty information—Romans accused Christians of being cannibals (""They have a meal where they eat someone's body and blood!"") and incestuous (""Husband and wife refer to one another as 'brother' and 'sister!'""). But the point stands that it's sometimes only retroactively that we see something as being a religion, and that definition is culturally dependent. Is a secular nation state a kind of religion? It might be harder to say no than you might think. Looking around us today, you and I might not think of football or CrossFit as religions, but an alien visiting our planet might. Give it two or three hundred years, and you might too!

OK, for the sake of argument, let's concede that Christianity is a religion. When does it start? Jesus of Nazareth and his disciples were all Jewish, they worshipped in synagogues, they observed the Passover, they visited the temple in Jerusalem. Jesus was referred to as a Rabbi and came not to abolish the law but to fulfill it (allegedly). Some supposed he was the prophet Elijah come again, or so it was said. Wouldn't his followers also be Jews? If not, when do the followers of Jesus of Nazareth stop being a movement within Judaism and start being their own religion? It's hard to say, and there is no consensus among historians about when or how to differentiate the two. A main theme of the letters of Paul of Tarsus is his perspective on the lively debate over how followers of Jesus ought to obey the law of Moses or not. Some Christians insisted that it must be obeyed in full by gentile (that is, non-Jewish) followers of Jesus. Others went so far as to claim that the God of the Jews was a different, lesser God than the one Jesus addressed as ""Father."" Accordingly, these Christians (called ""Marcionites"") rejected the Hebrew Bible as Scripture and instead created their own canon of texts. The New Testament that you and I know today has its origins as a counter-canon to the Marcionite canon. This conflict has led some scholars, like Daniel Boyarin in *Dying for God: Martyrdom and the Making of Christianity and Judaism*, to make the case that until at least the Council of Nicaea in 325 CE, what we have is one religion on a spectrum between Rabbinical Judaism on the one end and Marcionism on the other, with Nicene Christianity somewhere in the middle. It seems insane to us today to imagine Christianity not being, well, *Christianity* for several hundred years after Jesus of Nazareth's life and death, but that's just the way some things might shake out! We shouldn't sweat it though. Whenever we start to call it Christianity has much more to do with us and how we are thinking about the past and much less to do with what actually happened. Or to put it differently, labels are our way of understanding the past and making meaning from it. They have precious little to do with the way things were, one way or another.

So you see, the question of how to count religions is particularly thorny and reveals more about us than about the past or reality (whatever that is). This isn't to say that the category of ""Abrahamic"" religions isn't helpful, though! It helps us talk about the history of different movements, their origins and interconnectedness, how they understood one another, etc. But religions as a thing we can number and count, well, that's where even the category of 'religion' starts to break down. We see it first in categories like ""Christianity"" or ""Abrahamic,"" which is why this question is so instructive. Asking it helps us examine our own ideas and preconceptions, which is the first step in good historical inquiry into something as expansive as religion. So good on you for asking the question, and keep going where it takes you!",0
"> I have sometimes heard it discussed that for this reason the South's best hope for victory was British intervention. This would be somewhat similar to what happened in the Revolutionary War, where the British had complete control of the sea until the French intervened. Does this seem like a reasonable claim?

The main reason the South had such a deep belief in the possibility of British intervention was that a significant shortage of cotton exported to the U.K. was overwhelmingly economically injurious to British millworkers.  The Victorian era, however, was not known for its overwhelming sympathy to the poor (vis-à-vis workhouses) and the English were understandably leery of intervening in American political squabbles after 1781 and 1812.  

It was wildly unlikely the South could ever win, but even if they had, they no doubt would have taken absolutely any international trade agreements they could get, seeing as their economy was a one-trick pony, if you will.

There was absolutely no advantage for Britain in getting in the middle of the war, so they didn't.  The fact that some millworkers starved wasn't a major concern in their eyes; the idea that they could get lured into a military morass when they were perfectly capable of finding military commitments of their own (Second Opium War and capture of Beijing) and replacing missing cotton exports with cotton from India, which the British controlled at that time.

If you want to see why the U.K. was preoccupied, you might find [this graphic](https://www.washingtonpost.com/news/worldviews/wp/2015/09/08/map-the-rise-and-fall-of-the-british-empire/) helpful. As you can see from that map, they had a number of possessions that had nothing to do whatsoever with the United States, were more economically useful to them than the United States, and required their ongoing diplomatic and military involvement.

Sparing troops from the endeavours to protect the Empire on which the Sun Never Sets is far different from sending troops to get into what appeared to be (and in fact, was) the losing side of a war.

ETA: corrected hyperlink",0
"Paul B. Newman's *Daily Life in the Middle Ages* has an excellent selection of information about practical elements of daily life (cooking, cleaning, construction, gaming, healing, etc) if you want to read more. I've made pottage based on his descriptions of vegetables and seasonings and Christopher Dyer's reconstructed 13th century workers diets, and it's quite tasty once you eyeball the proportions correctly. I only regret not having skirrets to add to the pot!",0
"Whilst I realise that asking questions here is tantamount to perusing a line of enquiry based on a falsehood, I’m curious as to the logic behind the “Jesus as Aryan” idea. What did those writers make of Mary then? How did they explain away her Jewishness?",0
"This is, well, a very fraught and difficult question to answer, because it covers huge swathes of different historical periods and regions. As such, I can only speak to my own ""pet"" focus—namely King James VI/I of Scotland and England, a man whose sexuality has been a constant focus of historians for centuries. I want to focus on a specific example to bring up *how* sexuality becomes a very difficult topic to discuss even for people who seem more ""obviously"" gay compared to many other popular examples, and also because I would feel improper to speak for disciplines that cover times and places more accepting of same-gender sexuality.

To start broadly: European historians need to be aware of many issues when discussing sexuality in pre-modern contexts. The first is that, at least in a Western context, gayness as a property of a person rather than an action they undertake is a modern (with origins in the 19th century) concept. There's also the reality that same-gender friendships in the pre-modern era could be, by our considerations, quite effusive even when we feel confident they are completely platonic. A third consideration is that, as same-gender sex was considered shameful at best and evil at worst in early modern Europe, we cannot entirely rely on outside sources because their biases about the figure in question could easily lead them to make or refrain from making accusations of same-gender sexual activity; similarly, we cannot rely on even private letters by the alleged because they would have sought to avoid incriminating themselves.

King James VI/I (henceforth James I) gets into the thick of these issues. To briefly summarize the historical record, he was *very* close with at least three men: the Duke of Lennox, Robert Carr, and, most famously, the Duke of Buckingham. These relationships were remarked for their closeness by observers and were lengthy in nature. A hidden passage between the bedrooms of the King and the Duke of Buckingham have been uncovered. He was also, frankly, quite ""soft"", anti-war, intellectual, nervous, and reserved—stereotypes about ""gay"" behavior that have transcended many changes to gender roles over the centuries.

At the same time, however, he was reputed to have a mistress in Scotland before succeeding Elizabeth to the throne of England and, well, impregnated his wife, Anne of Denmark, over ten times. But Anne also was sent to a fairly distant palace for much of the time.

So, if we *have* to apply modern labels to James I, where do we go? He certainly never reaches explicitly sexual letters with his supposed male lovers, but of course he doesn't do the same with his supposed female lovers. Is his closeness to male figures, which begins at the age of 13, a sign of budding sexual interest in other men or merely a son overreacting to the absence of a father,  since his birth father was murdered when he was an infant? Perhaps he was simply bisexual, and interested at various times and to various extents, both men and women?

The historical record has similar confusion: while during his lifetime few close to the King dared to accuse him of gay activity, the fact that his son and heir Charles I was beheaded after attempting to bring Parliament to heel, inaugurating a brutal civil war, has led historians writing after the Restoration (and particularly after the Glorious Revolution and all the way up through the mid-twentieth century) to lean heavily into James I's purported gay activity to lend credence to the idea that the monarchy under the Stuarts was debauched, tyrannical, and, worst of all, popish. As the 50s and 60s dawned and historiography underwent massive changes, historians found themselves suddenly drawn to James I as a positive figure: pacifistic, intellectual, not given to intense religious passions, etc. But, given attitudes towards homosexuality even among many academics in that era, these ""revisionists"" were prone to eagerly deny that James I could possibly have been interested in men and decried such talk as slander.

To sum up: declaring a historical figure as straight or ~~gay~~queer can easily be presentist, typically requires leaps of faith with little concrete evidence, and often lies within the context of complicated historiography. We recognize today that it is improper to declare someone ""gay"" without them outing themselves or, at least, concrete evidence to the fact—when speaking about people who cannot speak for themselves, we must be doubly careful not to claim more than the evidence available provides.",0
"Wow thank you so much! We seem to have a common theme of answers to the question. Very much thank you! I have ordered the book you recommended! I have a friend who I met at MANifest and he is 50-60ish and told me stories of the great hedonistic sex fest that was the 1970s. And that 81-90 we’re scary dark times where people explored more personally and introspectively, leading to people rejecting the idea of “I have to act/dress this way” in large numbers",0
"Since the Church in the Middle Ages was (really) about getting people into heaven rather than keeping them out, theologians and preachers really did try to bend and flex rules when they could. So lay people could be deputized to perform baptism in extremis, although it's almost always mentioned with reference to midwives. There are even writers in the late Middle Ages who will admit that technically you *can* confess your sins directly to God right before you die, and it will count the same as the sacrament of confession...but here are all the reasons you shouldn't rely on that. (See also: purgatory.) 

Last Rites in the sense of a priest hearing a dying person's confession and giving them one last Eucharistic meal was absolutely a medieval practice--the Eucharist host in this case even had the special name ""viaticum."" (As in via-tecum, ""with you on the way"".)",0
"So the comments so far have been basically recommending ""don't read casually, read academic articles"". This is, to some extent, good advice, but it doesn't really help sort through the mountain of stuff that is online.

So, here are a few general tips to help out. unfortunately, these are not foolproof - things that are actually reliable and fine won't look like it, and things that pass these checks will actually have big problems. I'll give an example of one of those later.

1) Check the ""about"" page of the site! That should give some sort of introduction to either the organization or the person. If they're talking about how they're getting out the things mainstream academia won't talk about, that's pretty suspicious. If theyre a professional with a degree, they're probably a bit more trustworthy. This gets a shocking number of things out of the way quickly - a group like Norskk outright says ""toxic masculinity is a good thing"" which ought to be a sign that the things they claim were true of the Viking mindset are.. not.

2) obvious misconceptions. For something like ""feudalism"" or ""the Church was oppressive and anti-science"", where there are so many easily accessible and popular debunkings of the very basic strict hierarchy online, if a site is repeating the incredibly simplistic version, that suggests they aren't doing research, they're just writing what they think they know. So keep an eye on your knowledge base. There's plenty you have no reason to know, so you won't be able to detect a lot of these, but if you do see something that makes you go ""that doesn't sound right..."" Look around, see if there's a comment here or somewhere else you generally trust about it, and if the safe ground says that it's wrong, be suspicious of everything else the site says.

3) broad language. Internet sources necessarily have to simplify a lot for the sake of readability, but if they're talking about centuries of time or hundreds of miles as some kind of single unified entity, or an idea as being applicable for that, it's worth questioning that. This will often happen with the Vikings, and honestly with medieval political history broadly. A similar thing is true with something like religion: if they're framing religious groups as unified entities, even when it isn't totally inaccurate it's so simplified as to be generally unhelpful.

4) citations. Generally speaking, having more diverse and more recent citations is a sign that it's more reliable! Now, not every book is good, so this isn't a guarantee, but it at least indicates that they're trying, and if they have some sort of credential, it's often suggestive.

5) ask! If you genuinely aren't sure after those checks, ask other people! There are plenty of networks and creator fan communities with genuine experts, and talk to them to see if something is legitimate. Knowing the historiography of a field, and what scholars are discredited, what the current consensus is, etc. is not something casual readers should expect to know! For instance - ""Norse Mythology for Smart People's"" entry on Týr seems legit: while the site is a bit.. arrogant, the author is non-academic but did a lot of research, he only sometimes goes into ""Viking Values"" and other eyebrow-raising stuff, and he block-quotes academics with full citations! So it looks legit. But, on that page, the two scholars he cites to call Týr a god of Justice are Jan de Vries and Georges Dumezil. At least de Vries, and probably Dumezil too, were literal Nazis - de Vries was a registered member and Dumezil worked in the Vichy government and may have published Nazi stuff under a pseudonym. So, when de Vries says that there was no difference between war and justice in the Norse world, it hits a bit differently with that context (for the record, while he's not entirely wrong - the society resolved legal conflict through feuds and ""viga"", or battles, which Týr is a god of, justice also has an expect of mediation and settlement, which Forseti and Baldr are said to be better at in the same source).

It's a super good question to ask, and I wish there was an easier answer, but good luck and I hope this helps!",0
"It’s starts with Isiah 14:12.

In English translations, the text is translated in the King James Bible as:

“How you are fallen from heaven,
O Lucifer, son of the morning!
How you are cut down to the ground,
You who weakened the nations!”

And the Wycliffe Bible (approx. 1395):

“A! Lucifer, that risedest early, how fellest thou down from heaven; thou that woundedest folks, felledest down (al)together into [the] earth. (O! Lucifer, who risedest up early, how thou hast fallen down from heaven; thou who hast wounded the nations, fell down to the ground.)”

Both are believed to be drawn from the Latin Vulgate, a translation attributed to Jerome of Stridon, though it’s generally agreed that the work is a composite and Jerome is only responsible for parts of it.

There are multiple versions of the Vulgate, though most agree on Isiah 14:12.

Here is Isiah 14:12 from the Clementine Vulgate:

“quomodo cecidisti de caelo lucifer qui mane oriebaris corruisti in terram qui vulnerabas gentes”

The Vulgate is credited as the first known translation of the Hebrew Tanakh, more properly called Mikra or Miqra, which is a collection of Hebrew canon found in the Old Testament, though Jerome had worked on a translation of the Greek Septuagint prior, though according to Jerome, those translations were allegedly lost “through someone’s dishonesty”, translated by Wace and Schaff (1890-1900). 

Augustine of Hippo (405) states Jerome translated directly from Hebrew, though scholars, such as Adam Kamesar (1993) believe Jerome may have been working from the Greek, rather than the Hebrew texts.


The Leningrad Codex’s, a preserved Hebrew version of the Tanakh which is dated around 1000, text of Isiah 14:12 reads:

אֵ֛יךְ נָפַ֥לְתָּ מִשָּׁמַ֖יִם הֵילֵ֣ל בֶּן־שָׁ֑חַר נִגְדַּ֣עְתָּ לָאָ֔רֶץ חֹולֵ֖שׁ עַל־גֹּויִֽם׃

And the Masoretic Text (700-900), another extant Hebrew set of documents that includes the Old Testament  says:

אֵי נָפַלְתָּ מִשָּׁמַיִם הֵילֵל בֶּן־שָׁחַר נִגְדַּעְתָּ לָאָרֶץ חוֹלֵשׁ עַל־גּוֹיִם

This transliterated is:

ëykh' näfal'Tä miSHämayim hëylël Ben-shächar nig'Da'Tä lääretz chôlësh al-Gôyim

Whatever Hebrew texts Jerome used are lost to us, but the key term for us here is “הֵילֵל” which appears in both texts and is transliterated as hëylël or Hêlêl 

That is likely the term Jerome translates to  “Lucifer” if he directly translated it.

The term helel is translated in Strong’s concordance as “Shining One” and it’s coupled with “Ben-shächar”, which translates to “son or child of shächar”.

Shächar, also Shahar, meaning Dawn. Hinnels (2007) identifies Shahar as an Ugarit god of the Dawn, a twin of another Ugarit god Shalim, god of dusk.

Which explains Jerome’s “lucifer qui mane oriebaris” or “light bringer, son of the morning”.  Shining One, Son of Dawn.

It’s association with “The Morning Star” can be seen in the Greek Septuagint:

πῶς ἐξέπεσεν ἐκ τοῦ οὐρανοῦ  ὁ noἀνατέλλων συνετρίβη εἰς τὴν γῆν ὁ ἀποστέλλων πρὸς πάντα τὰ ἔθνη

Which Google transliterates as:

pós exépesen ek toú ouranoú o eosfóros o proḯ anatéllon? synetrívi eis tín gín o apostéllon prós pánta tá éthni.

The key term here is “eosfóros”, which is also transliterated to Heosphoros, also called Phosphoros, which is the Ancient Greek Term for Venus the planet as seen in the morning and a minor god who was a brother of Hesperus, which is the term for the planet Venus as seen in the evening (which parallels Shahar and Shalim as heavenly bodies and divine siblings)

Heosphoros also translates literally as “bringer or carrier (phoros) of Dawn (Heos or Eos)”, Phosphoros means “bringer/carrier (phoros) of Light (Phos or FPS) and lucifer (lux being light and fer being a diminutive of ferre, meaning to carry) is the direct translation.

So Jerome is likely agreeing with the Septuagint’s translation of the Hebrew, or he is deferring to it, depending on which reading you take on his Hebrew skills.

So Jerome is where we get Lucifer in relation to the Bible, whether as a credible translation from the Hebrew, Shiner, son of Dawn or the Greek, Light-Bringer.

As for Lucifer, the fallen Angel, The Adversary and the Devil, all conflated as one?

We likely have Origen of Alexandria to blame.

In his work “Contra Celsum” Book VI Chapter 43 he writes:

“And besides all these instances, in the book of Job, which is older even than Moses himself, the devil is distinctly described as presenting himself before God”

Origen is directly connecting the idea of the Devil with Ha Satan, the adversary in Job, which does have examples in an earlier Christian and Hebrew traditions.

He then proceeds to rattle off a number of examples of the Devil in the Bible, including “those in Isaiah, where lament is made for the king of Babylon”. (Translation by Crombie 1885)

This is the earliest example I can find of a Christian writing that conflates the Devil with the Serpent, the Adversary and Lucifer, though the figure in 14:12 would be called Phosphoros in the Septuagint Origen was reading.

While then development and conflation of those ideas is present before Origen, connecting Isiah 14:12 with these other supernatural characters of the Old Testament seems original to him.

Indeed, in First Principles (Book 1, Chapter 5) you can see the development of this thought, as he concludes the figure of 14:12 cannot be a human, but is instead an Angel of some kind, though he hadn’t fully connected this fallen Angel with Satan or the Serpent yet, though  the only extant translation is by Tyrannius Rufinius, a contemporary of Jerome’s, and apparently he modified the text to adhere to adhere to his contemporary orthodoxy (Heine 2010), so whether Origen’s connexion was closer to his writings in Contra Celsa and Rufinius edited him, or whether the idea was emerging is unknown

Origen, of course, was one of the greatest early writers in the Christian tradition, and his theology was considered a standard for Christian orthodoxy (Olsen 1999), so it’s little surprise his thinking regarding Lucifer (as translated by Rufinius, we don’t know what Origin called Lucifer in Greek).

So in summary: we know Origen, a major pillar of Christian thinking, conflated Helel Ben-Shahar, as named in the Tanakh, with Ha-Satan as well as some other satans that appear throughout the Old Testament and presented as Angels, and the serpent in Revelations and Genesis.

We know Rufinius and Jerome both choose to translate Phosphoros as Lucifer. 

And we know the English Renaissance were drawing from Origen, Rufinius and Jerome as they were conceiving their own English Vulgate which culminated in the King James Bible.

Hopefully I’ve drawn the line from Ancient Hebrew mythology referring to a non Hebrew God believed to be Venus at Dawn to renaissance Christian mythology 
of a fallen Angel that was once the brightest star in heaven.",0
"In modern times, drugs like caffeine, alcohol, and even marijuana are toward the top of the ""socially acceptable"" scale. Drugs like meth, bath salts and heroin are toward the bottom. Where would cocaine and morphine rank on such a scale compared to other drugs of the time in 19th-century Britain?",0
"The first example that probably springs to a lot of people's minds is Liberia, but that's kind of an oddball fit here. Despite a longstanding and diverse population of its own, the region we know today as Liberia was singled out by a group of free black (U.S.) Americans to be a sort of collective homeland for free and freed ""African"" Americans (whose more specific ancestral ethnic identities had been stripped away by generations of slavery in most cases). In the early/mid 19C, the American Colonization Society put forth an effort to colonize Africa with black Americans. In relation to this specific question, however, Liberia doesn't quite fit. First, the Americans who colonized Liberia were *Americans*, by and large, not West Africans returning home. Second, by the time the ACS kicked into gear, the (legal, at least) trans-Atlantic slave trade was dead.

That said:

It was undoubtedly rarer than rare, but some Africans kidnapped from Africa by white slavers did indeed return. Probably the most common way would have been to be purchased by a ship captain or merchant. But as you might expect, records of something like this require a fairly high degree of literacy, which means examples we can cite are both (a) the farthest possible thing from representative, and (b) *really great stories*.

The first one I'll mention is Jacobus Capitein (c. 1717-1747). Capitein was seized as a child in Ghana and ripped away from his home continent. Plantation slavery in the Americas would not be his fate, however. Capitein wound up being owned by a Dutch trader, who was incredibly impressed by Capitein's brilliance when he took him back to the Netherlands. Capitein received a stunning, classical education (he opens his famous thesis, which I am bummed I can't find online for you, by *quoting Cicero*) and determined to become a Christian pastor. No, not just a pastor--a *missionary*. With his owner's blessing, buoyed by a Dutch mercantile arm that appreciated a black man who was A-OK with slavery *on theological grounds*, Capitein became a pastor in the famous port city of Elmina, in Ghana. Like perhaps many missionaries, he had a lot more success spreading the benefits of education and literacy than in inculcating Christianity among his students.

My second case study comes courtesy of Randy Sparks, who discovered a set of letters in an archive that confirmed, expanded, and *made so awesome* a long-known but little-noticed story. Two elite burghers/nobles from Old Calabar in Biafra, brothers Ephraim Robin John and Ancona Robin Robin John along with Amboe Robin John, were actively involved in the slave trade *as traders* in the mid-18C when they got caught in a three-way territorial dispute among African townspeople and English traders. Amboe was murdered; Ephraim and Ancona--or as I prefer, Robin John and Robin Robin John; it's confusing, I know, but first names are too often used as a diminutive to assert power and dominance over someone else--were chained, sold, shipped across the Atlantic, and wound up on a plantation in the dreaded Caribbean. As active participants in a coastal community tied into the larger Atlantic world, they absolutely knew the sort of fate that awaited them. One really interesting point here is that their relatives in Calabar--also rich, elite, slave-trading members of the ""Atlantic world""--actually tried reaching out to secure their return, via letter writing to people they hoped might know of the Robin Johns' fate, and ideally be able to intervene. For their part, ""two princes of Calabar,"" as Sparks terms them, were bound and determined to make it home. They bounced around a little in the Western Hemisphere, from plantation to ship captain to ship captain, and somehow managed to stay together even in the face of some abusive owners. Hope, death, escape, betrayal, devastation--the story is itching for dramatization. Ultimately, a massive and prolonged letter writing campaign, and their extensive knowledge of/manipulation of legal systems, won them their freedom in England. They found their hearts strangely warmed by the fires of Methodism; they found their first attempt to return to Africa less inviting, when the drunk captain wrecked the ship. But in 1774, the brothers finally, finally, *finally* made it back home to Calabar.

Where they read the Bible to their friends and family at night, and traded other people into slavery during the days.

~~

You want to know more, I know you do, so, you can check out Sparks' book [*The Two Princes of Calabar*](http://www.hup.harvard.edu/catalog.php?isbn=9780674032057), and/or you can read the article he published first, ""Two Princes of Calabar: An Atlantic Odyssey from Slavery to Freedom,"" *William & Mary Quarterly* 59, no. 3 (2002), which is [on JSTOR](https://www.jstor.org/stable/3491465?seq=1#page_scan_tab_contents).",0
"Circe is a goddess/nymph as well. And in this case, Hermes actually orders Odysseus to sleep with her, or rather, says that to do so is the only way he'll be able to free his crew (whom Circe has turned into pigs). Odysseus himself implies that unless he sleeps with her, Hermes' magic herb will fail to protect him and Circe will turn him into a pig, too. So in this case, not only is there a threat to the hero, not only are the gods commanding him, (not only is ""willing consent"" from a modern standpoint in deep question, although obviously the Greeks had vastly different standards for what constituted rape), and not only is the woman a goddess, but it's also a feat accomplished to save his men.

As for Hecuba: In the *Trojan Women* and *Hecuba* traditions associated with the fall of Troy (two plays by Euripides), Odysseus is prophesied to capture the ever-unlucky Queen Hecuba and cart her off as a prize of war. (He also murders/sacrifices her daughter...*minor details*). The standards here are different, and once again, would be keeping in line with Greek masculinity and ""ethics"" of warfare (even though, if *Trojan Women* is designed around Hecuba and her strength of character, Odysseus is not our hero in shining armor. Virgil's version of the fall of Troy, the *Aeneid*, sure isn't too keen on him--""dires Ulixes"").",0
"Seeing the mass of deleted comments, I'm somewhat hesitant to attempt to answer this myself, but I'm sure the mods will delete it if it's not up to snuff.

I think the key thing to bear in mind is that by 1952 the monarchy had undergone a serious image transformation since the ascension of Elizabeth II's great-great Grandmother, Victoria, at the age of 18 in 1837.

By comparison to Elizabeth, Victoria's claim on the throne was far more removed. Victoria was the only child of King George III's fourth born son, her three uncles, George IV, Prince Frederick, and William IV all died leaving no legitimate children. Victoria's father, Prince Edward had died in 1820, 17 years before Victoria became Queen.

When Victoria did succeed her uncle she was unmarried and, to make matters worse, owing to the laws governing the German titles that had been in her family since George I 1714, all the Hanoverian titles went to her fairly unpopular uncle, the fifth son of George III, [Ernest Augustus](https://en.wikipedia.org/wiki/Ernest_Augustus_I_of_Hanover) creating a new German royal family. If there were any monarch of the last 300 years that would have faced massive opposition upon succession, it would have been Victoria, but even she was enormously popular in her early reign.

The thing about the (modern) Monarchy is only most legitimate choice succeeds the throne. There may well have been those concerned about Victoria becoming Queen, but what other options did they have? The only viable potential candidate left was the elderly Ernest Augustus, whose track record of [conservatism and political agitation](http://www.britannica.com/biography/Ernest-Augustus-king-of-Hanover) left a lot of people, including the Duke of Wellington, with a very low opinion of him.

Compare that to the succession of Elizabeth after the death of her father, George VI* in 1952. Her father, the second born son of George V, was not expected to succeed the throne, but after his elder brother abdicated to marry Wallis Simpson in 1936, he became King. George VI was a very popular king, and his prioritizing of his family meant they became very popular too. Whereas before the focus had been more on the King himself, George VI brought his family in to share the spotlight. As a result of this loosening of royal protocol, the royal family became much more like the one we see today, often photographed in more [informal situations](http://st3.cricketcountry.com/wp-content/uploads/2015/09/Princess-Elizabeth-the-future-Queen-Elizabeth-II-conversing-with-her-father-King-George-VI-1895-1952-in-a-garden.jpg), compared with the ultra-formal attitude of his father, [George V](https://c2.staticflickr.com/6/5499/11682897433_35d0c8c600_b.jpg). You also probably know that the Royal family [remained in Buckingham Palace during WWII despite the Blitz](http://www.royal.gov.uk/HistoryoftheMonarchy/KingsandQueensoftheUnitedKingdom/TheHouseofWindsor/GeorgeVI.aspx) and often [visited bombed areas in South London,](http://cdn.images.express.co.uk/img/dynamic/galleries/x701/70075.jpg) further boosting their popularity. [Princess Elizabeth also trained as an ambulance driver and quasi-mechanic](http://www.express.co.uk/pictures/galleries/2955/Queen-Elizabeth-II-World-War-II-pictures/Queen-Elizabeth-II-driving-an-ambulance-during-her-wartime-service-in-the-Auxiliary-Territorial-Service-on-10th-April-1945-65562) during the war, boosting her image enormously. 

After the War, Elizabeth went on to marry Lieutenant Philip Mountbatten in 1947. Philip was the former Prince of Greece and nephew of one of the most distinguished commanders during WWII, the former chief of Combined Operations, the former Supreme Allied Commander of South East Asia, and last Viceroy of India, [Lord Louis Mountbatten](https://en.wikipedia.org/wiki/Louis_Mountbatten,_1st_Earl_Mountbatten_of_Burma), himself a grandson of Queen Victoria. Not only was the match a very well made one, but it came during a time of massive post-war austerity. Rationing was still enforced, and the genera mood of the country was depressed. The Royal marriage, as it so often does, provided a welcome distraction and a reason for people all over the Empire to celebrate.

By the time George VI died in early 1952, the Royal family was about as popular as they could be. When it came time for Elizabeth to ascend the throne the public had a little over 15 years of knowing she would succeed her father, so it was not as jarring as perhaps Victoria's was. She was the daughter of a very popular king, and a very popular Princess in her own right. She had a good husband, and two children of her own, Prince Charles and Princess Anne**. Again, the question must be asked, what alternative was there? The only other close claimant to the throne was her younger sister, Princess Margaret. George VI's younger brother, [Prince Henry, Duke of Gloucester](https://en.wikipedia.org/wiki/Prince_Henry,_Duke_of_Gloucester) remained, but he had always been a private man, preferring the soldiers life to one in the limelight.

**TL;DR**

By the time Princess Elizabeth became Queen Elizabeth, the Royal family was incredibly popular, she had a good husband, and the country had been prepared for her ascension for over 15 years.  

*EDIT: From George IV to George VI.


**EDIT: Previously referred to as Anne, Princess Royal. She wasn't known as the Princess Royal until 1987.",0
"(/u/Georgy_K_Zhukov has provided a great answer to ""Who was Hitler compared to."" But as the literary and historical obscurity of many of the figures already shows, the larger question remains unanswered:

**[W]ho was used as the standard of comparison for evil before Hitler?**

I'd like to borrow from part of an [earlier answer](https://www.reddit.com/r/AskHistorians/comments/ef1eg8/since_wwii_nazi_iconography_has_been_symbolic_of/fby7ku9/):

In western (Euro-American mainstream) culture, there is loads of imagery from the Bible--Babylon, Jezebel, serpents, the colors red and black. I want to focus, however, on two broader themes:

1. Monsters, especially from the Christian Bible

2. Jews (as seen by Christians)

**Part I: Monsters**

We live in an age where you can buy a [cuddly Cthulhu](https://imgur.com/a/nFwc8jw)--a [cuddly *pink* Cthulhu](https://imgur.com/a/QEYuhuD)--a [cuddly Cthulhu *with a children's book*](https://imgur.com/a/DHrhSXb)--so I think we tend to lose sight of the, well, monstrosity of creature-type monsters (as opposed to the human variety).

But monsters were the code that the authors of apocalyptic tracts used to portray enemies and oppressors:

> Four great beasts, each different from the others, came up out of the sea.

> ...There before me was a fourth beast—terrifying and frightening and very powerful. It had large iron teeth; it crushed and devoured its victims and trampled underfoot whatever was left. It was different from all the former beasts, and it had ten horns.

> While I was thinking about the horns, there before me was another horn, a little one, which came up among them; and three of the first horns were uprooted before it. This horn had eyes like the eyes of a human being and a mouth that spoke boastfully.

> ...The fourth beast is a fourth kingdom that will appear on earth. It will be different from all the other kingdoms and will devour the whole earth, trampling it down and crushing it.

In Revelation, John does give us four ordinary people on horses. But he picks up the beast theme of Daniel 7 to describe Rome, this time:

> I saw a beast coming out of the sea. It had ten horns and seven heads, with ten crowns on its horns, and on each head a blasphemous name.

And more to the point for present purposes, the beast in Revelation doesn't just represent evil. It's also used to make otherwise-mundane imagery become evil:

> There I saw a woman sitting on a scarlet beast that was covered with blasphemous names and had seven heads and ten horns. The woman was dressed in purple and scarlet, and was glittering with gold, precious stones and pearls.

(This is, of course, also a case of Babylon being used to highlight Rome as evil.)

Medieval iconography is also keen on monsters illustrating evil. We're more used to the [Paradise Lost](https://imgur.com/a/EYC3RVj) Satan and his heirs (looking at you, *Lucifer*). For medieval Christians, demons and the devil were monsters. The nuns at the Rupertsberg [illuminated the Antichrist](https://www.medievalists.net/wp-content/uploads/2013/01/Hildegard_Bingen_Scivias_214v_Five_Ages_Antichrist.jpg), for example--the Antichrist, who most legends would make out to be a human being, yeah? We're also got plenty of [devilish monsters](http://ica.themorgan.org/icaimages/1/m153.019ra.jpg) on hand.

But devil iconography brings me to the second topic I want to look at--one I am not happy about, at all.

**Part 2: ""The Jews""**

Yes, the comparison here is to the 21st century use of [Nazi](https://youtu.be/EC9Pkz9dPMU?t=185) [imagery](https://www.youtube.com/watch?v=yys5iioLUNw), so this doubly or triply sucks.

But it's another case of associating something with The Jews to mark it as bad.

To be clear, I don't mean Jewish people/people who happen to be Jewish. (No, they just get to suffer the consequences.) I mean ""The Jews"": the racist European-American Christian *invention* of a cabal that...well, whatever they're doing, it's evil.

First, iconography.

To the 12th century, illustrations of Jewish men generally [denote them by hat](https://upload.wikimedia.org/wikipedia/commons/a/ad/GermanJews1.jpg) (from Herrad von Hohenburg's *Hortus deliciarum*, which sadly means Garden of Delights instead of Garden of Delicious Things). But as ideas of a Jewish ""race"" started to coalesce, Christian artists evolved the stereotype of the [""Jewish"" nose](https://imgur.com/a/IGCoELo).

By the late Middle Ages, [Satan and demons are depicted with ""Jewish"" iconographical features](https://imgur.com/a/YIUGo2J). Yes, mixed in with monstrosity. Association with The Jews makes the *devil* appear more evil--not the other way around.

Early modern witch hysteria gives us another example. In point of fact, witchcraft is usually an accusation levelled against Christians. But that doesn't stop artists from [adopting ""Jewish"" features to signal that a person is a witch](https://www.medievalists.net/wp-content/uploads/2012/08/Pagan-Traces-in-Medieval-and-Early-Modern-European-Witch-beliefs.jpg).

Second, conspiracy.

20th century American conservatism (20-year-rule, people) has a very, very strong anti-internationalist streak that draws on anti-Semitic associations at nearly every turn. The slight scuzziness you probably sense around the edges (or not so edges) of the phrase ""international banker,"" for example, doesn't go back to Banker being the profession that gives you the most money setting out in *Oregon Trail II*. 

For paleoconservatives' long-running fear about the US losing national sovereignty (no, really), they adopted The Jews as a quick and dirty way to express *just how evil* the people threatening to take over were. (The association of Jews and banking also goes back to the Middle Ages).

And in the early 20th century, at least, you had Jewish bankers...and then also, you had--in the words of conservative evangelical and fundamentalist Christians--""Jewish Communists."" Because ""The Jews"" were even used to make Communism seem more evil.

This didn't stop with World War II, either. Henry Ford might have distributed copies of *Protocols of the Elders of Zion* in the 1920s, but Mary Davison wrote *Profound Revolution* in 1960. And in the abominable *Left Behind* books, not only do they even turn their Rockefeller expy into a Jewish man, but they make him responsible not just for the new world order, but for their Antichrist.

...Oh, and on AskHistorians *right now*, there's [this question](https://www.reddit.com/r/AskHistorians/comments/gfi9bd/what_were_the_real_reasons_that_made_germany/):

> I just had a heated debate with my family about the real reason why Germany started WWII. They argued that Germany wanted to get rid of the Rotchilds [Rothschilds].

~~

Obviously there's a lot more to be said about the history of anti-Semitism, and Christian fantasies of Jewish association with evil and the devil. I've tried to concentrate here on examples of Jewish stereotypes used to make other things seem more evil, rather than just negative stereotypes in and of themselves.

I'd love to hear from someone who can discuss non-Western views!)",0
"If you suspect that there may be a slave grave on your property in Georgia, there are several steps you can take to confirm this and find local experts. 1. Research local history: Start by researching the history of your property and the surrounding area. Look for any historical records, maps, or documents that may provide information about previous landowners, plantations, or other relevant historical context. 2.",1
"Women were a good bit more common afloat than is often imagined. Especially on large ships of the line that could go from months in Port to months on station or on long cruises.

Below decks some could just be outright sex workers, some could be almost a type of black market sutler and seamstress. Others could be sailor's wives discretely kept aboard with outright, tacit, or without approval from above. Officer's, especially the captain or flag officers also could be often seen with their wives, particularly if the ship was bound for an especially far away station or long cruises. Say to take up a few years work in India or a new Admiral sent to the Mediterranean for a few years.

We should also note that each fleet had a different level of acceptance for conduct and environment on their ships and each captain within the service.",0
"Well, I believe I can just hang my flair up and always refer to this answer from now on, what a wonderful work. Let me add my tiny contribution to it and try to put Holmes’ addiction in the context of cocaine use at the time. *The Sign of Four* was published in 1890, which means that Doyle was writing it amidst the first few years of pure cocaine becoming the new panacea, taking the throne from opium and later pure morphine. Annus mirabilis of pure cocaine was 1884, when Sigmund Freud published *On Coca* and Karl Koller, a german physician working in Vienna, presented his findings of cocaine’s anesthetic abilities - it was pretty much the first local anesthetic which worked, and Koller utilised it in eye surgeries. Does this mean that before 1884 noone knew what cocaine was? Of course not. Albert Niemann was able to extract the alkaloid from coca leaves in 1859. The reason why it took so long for the “wonderful white powder” to gain notoriety, is one of purity and access. The amount of good available coca leaves in Europe was scarce and many experiments and findings were stiffled by materia medica which simply did not have enough of the alkaloid left to extract. Thanks to a certain gentleman called Merck and his company, which was exporting crude cocaine from Peru, the early 1880s saw a small, but stable supply of chemically active cocaine. Thus, everything which was written about coca and cocaine so far, became much more easily replicated and papers like Freud’s and Koller’s could start popping up. 

Dr. Koller was notably less “hyped up” ,however, as opposed to young Freud, who mentions the anesthesia as well, but unfortunately spends a lot of time praising cocaine as cure for morphinism (addiction to morphine). Again, this did not come out of the blue. Other important company in this story is *Park, Davis & Company* (from Detroit) which introduced a line of coca-laced products in the 1870s. As I said, coca leaves were being extracted way before pure cocaine was introduced, and it gave rise a niche on the already extremely profitable market of [patent medicines](https://www.reddit.com/r/AskHistorians/comments/4w2dga/how_common_were_early_remedies_like_childrens/). Among these, coca wines were extremely popular. I’ll quote my older Tuesday Trivia post here, about the most succesful one, Vin Mariani (a direct inspiration and competitor to Coca Cola):

>Vin Mariani was a product which was created in the 1870s and consisted mainly of (only the finest!) Bordeaux wine and extract of coca leaves. Wines of coca were not strangers in the pharmacopoeias of many countries and in fact, various medicinal wines were one of the few cases in which the temperance movements kind of squinted their no fun, anti-booze eyes (then there were the patent medicines on which I wrote before and the trick there was always to simply not say that it had a substantial amount of alcohol and/or opium in it. In that way you got teatotaller ladies that died of cirrhosis of the liver).

These were popular all over the place and one thing which was troubling about them was the amount of advertising done through “scientific” magazines, published by the manufacturers of the patent medicines: 

>Mariani had basically a little separate industry going, producing more then ten volumes of Album Mariani, where the endorsements were printed alongside biographies of the famous people. He also produced his own medical journal specifically to promote virtues of coca. It’s not quite the same, but imagine “Pepsi presents Journal of Nutritional Value of Loads of Sugar”.

The same goes for Parke & Davis and their journal *Therapeutic Gazzette*. You could not find disclosure of the relationship between *Park, Davis & Company* anywhere in this publication, but you could find plenty of articles praising their products. Including a whole series of articles about the possible wonderful cure for morphinism - cocaine. Freud quoted these very articles in his work, thus an advertising campaign had a very direct hand in spreading this belief in “miraculous” use of cocaine. Freud later also accepted an offer of money in exchange of endorsing Parke & Davis’s products, when Merck hiked up the prices and made him mad (Freud and his friends were a huge chunk of Merck’s customers). Among these products by Parke & Davis were the aforementioned coca wines, coca-laced soft drinks and cigarettes and also a[kit for injection of pure cocaine](http://i.imgur.com/yXXrAMF.jpg), containing 300mg of it, in 5 viles. I am willing to bet that Mr. Holmes was supposed to be getting high of exactly that. 

So, while the concept of addiction was out and about, Holmes was using a drug which at the time was touted as *cure* for it and coca had already reputation for providing mental and physical fuel beyond normal capabilities of the human body. More on that in [this answer](https://www.reddit.com/r/AskHistorians/comments/62vck7/what_was_the_first_incident_of_modern_style/), which is an April Fools one, but all the stated facts in it are true and paint a  picture of coca notoriety and use in the 1870s.",0
"The first literary reference to the custom of providing the dead with coins for Charon's ferry appears in Aristophanes' *Frogs* (405 BC), when Dionysus pays Charon two obols (one for him and one for his slave) to cross over the Acheron. To judge from Athenian tomb assemblages, however, the custom only became widespread in the following century, and was never universal. The coins, moreover, seem to have only sometimes been inserted into the mouth; nearly as often, they were just left beside the body. Although an obol (a modest fee, equal to about a sixth of a skilled workman's daily wage) seems to have standard, some burials have thin gold medals (bracteates) instead.

It is difficult to discern how the custom emerged. Charon, not attested in Homer or the early Greek poets, is first named in literature of the early fifth century BC (or slightly before, in the early epic *Minyas*). This does not, of course, mean that Charon was ""invented"" around this time, but it does suggest that he wasn't especially prominent in popular belief before the classical period. Why he became more prominent then is impossible to say. Diodorus Siculus, writing in the first century BC, claimed that Charon was imported from Egypt by the mythical Orpheus (1.92). Whatever his origins, it may be that the Athenian tragedians and comedians, who often made reference to the ferryman and even (as in the case of Aristophanes' Frogs) brought him onstage, made Charon a more prominent part of the Athenian imagination.

The habit of putting Charon's fare in the mouths of the dead is easier to trace. Lacking pockets, the Athenians often carried small coins in their mouths when they were going to the market. Transferring this custom to the dead was a natural leap.

Edit: u/rosemary86 has called Charon's appearance in *Minyas* to my attention, and pointed out that many references to the practice of putting an obol in the mouth of the dead mock the practice, which suggests that it was often viewed as an idle superstition.",0
"With around 2000 years of history behind these two cities, there is a lot to be said on the topic, so I will try my best to answer your question directly. 

The biggest difference between Rome and CDMX (Mexico City, DF, Tenochtitlan, etc) is that Rome has a continuous through-line of cultural and political influence, and a large part of that is due to the Papacy. While Rome has been sacked and changed hands politically in some sense, there has been a continuous Roman (Latin) culture that has predominated for most of its history. 

On the other hand, Mexico City was obviously taken by conquest, and the socio-political landscape of the city and country was completely overhauled. To put it briefly (because you seem to be more interested in the physical and architectural landscape of the city), when Cortes finally conquered Tenochtitlan, he instituted a Hispanicized Mexico that has continued to this day, where an event like this has not quite happened in Rome throughout its history. 

However, Mexico City's landscape shares similarities to Rome in terms of its ancient ruins. While there is no Colosseum dominating the visual landscape, there is just as much, if not more, ruins of the Great Pyramid as the Roman Forum. When Cortes took over the city, he, like many conquerers (include the religious ones that re-appropriated Roman temples), converted Mexica temples into churches, destroyed cultural artifacts, and even used the stones from Moctezuma's palace to build his own. We can make a direct spiritual connection between the Popes stripped Roman marble off of temples to build their Catholic churches, and temples such as the Pantheon converting to places of Catholic worship. Cortes kept the general layout of the Aztec city, and so walking its streets from the Zocalo to Chapultepec, you can easily imagine a Mexica reality to its urban landscape. 

What's interesting about Mexican history and cultural identity is that there is differing viewpoints on how to view Mexico as a nation. To your very last question, some Mexicans--including historians--would answer in the affirmative. Cortes had mestizo children, physically and symbolically ushering in a new ""people."" As preivously mentioned, the Zocalo's layout matches that of Moctezuma's city. Chapultepec Castle has defended the city from an ancient spiritual hill. The Basílica de Nuestra Señora de Guadalupe is built as a pilgrimage site where Indian Juan Diego saw an apparition of the Virgin Mary and converted to Christianity. Hell, they even NAMED the city after the people, as Mexico is the Hispanicized Mēxihcah. 

Thus, there are many who see Mexico City, as a metonym for Mexico at large, AS a continuation of indigenous culture. The city itself, while only settled about 200 years before Cortes' arrival, perhaps carries just as much of an ancient spirit as Rome does, although I am not nearly as studied on Italy as I am Mexico. As a personal anecdote (and therefore to be taken with a grain of salt), I find being in CDMX very similar to being in Rome in terms of what you're asking. 

In sum, viewing Tenochtitlan as a past reality and Rome as a continuous entity might be white-washing the issue a bit. Romans also have a bit more comfort with its evolution than Mexicans do; however, there remains a great deal of pride and acknowledgement of influence in Tenochtitlan. To put it simply, there is a LOT of Tenochtitlan in CDMX.

EDIT: Since I got here first and am the top post due to how Reddit karma works, let me add some addenda and clarifications from the conversation that has sprung from this: 

1. In this post I advocate for seeing post-conquest Mexico City as a continuation of pre-conquest civilization, not as a ""clean break."" I do believe it is important to acknowledge just how life-altering conquistador occupation of Tenochtitlan was, however. While indigenous culture prevailed in numerous ways for centuries, a centralized political power remained in the hands of the Spanish, and so this collision of cultures (resulting in the social interactions of the three social classes that resulted from that collision) completely changed the city forever, to say the least. This plays out differently for different cities and regions in Mexico at this time, but that is beyond the scope of this thread. 

2. Cortes razed the city to the ground, as u/Mictlantecuhtli points out, yet much of the surroundings of the Zocalo were rebuilt in indigenous fashion, as u/611131 points out. While this juxtaposition of Spanish and indigenous culture played out practically in the streets of the city, what we see develop over the next few centuries is an interesting interaction and mixture of these two cultures, as u/drylaw outlines. Go check out those posts for more detail! 

3. My sources, as I didn't include originally: 
Fire and Blood, TR Fehrenbach

Promiscuous Power, Martin Nesvig

Any work by Ramon Eduardo Ruiz",0
"The Kennedys are the Kennedys, so money could open doors to whatever testing they wanted. Rose wrote that she took Rosemary to multiple specialists, the best specialists she could find, doctors at Harvard, and so forth. The general agreement was that, as a child, Rosemary was mildly retarded. With immense effort on her part and support from her parents and siblings to provide the right tools for her, she attained upper-level elementary school abilities in reading and math. She also apparently had plenty of social graces and was in general a sweet and kind person. Even writers hostile to various Kennedys have suggested that her family shielded her from realizing how far behind her siblings she would inevitably end up.

All of that is very well attested--and it's not why Joseph sought the lobotomy for his daughter. About a year and a half before the devastating surgery, Rosemary had returned to the US from London, and apparently started to gradually fall apart mentally. I've read some places that the first signs of trouble had appeared when she was still in London--verbal altercations and fistfights; odd gaps in the logic and even actual writing of letters she sent. There may also have been a further deterioration in her cognitive skills, which would make sense if she was also newly having uncontrolled seizures as well. In other words, Joseph did not suddenly decide on some new experimental, dangerous procedure for his daughter on a whim. There were very real and new worries, so one can at least sympathize with the *search* for alternatives.

This is the part of Doris Goodwin's book (*The Fitzgeralds and the Kennedys*) that, I believe, has helped earn her praise for how she dealt with Rosemary from other Kennedy-orbit biographers and scholars like Dallek. In case you're interested in reading a little more.",0
"It's not a question of ""in the old days"", it's a modern practice for half the world's population. Many countries lack a refrigerated chain from abattoir to grocery, where open air butcher shops are common. Freshly butchered meat is stacked or hung in the open air, pretty much all day in hot climates. Sometimes for two days. [Here's a typical butcher's shop](https://i.dawn.com/primary/2018/03/5abc01b848e44.jpg) in Pakistan. You'll find thousands of these across south and southeast Asia, China, and other parts of the world.

It was quite fashionable in Europe to hang game birds in the cellar to tenderize the meat and improve flavor. It's the same principle as dry-aged beef that you can buy from your grocery store today. And while cellars were colder than the temperature above ground, they were nowhere near refrigerator temperatures. It was typical to hang pheasants and partridges at 55 degrees Fahrenheit for about 2 weeks at the Savoy Grill in London, for example. People paid premium prices for such poultry.

If you're wondering ""how do people survive"", the answer is quite simple - cooking. All meat at the butcher has bacteria growing on it, even the kind you buy plastic-wrapped package in the cold section of your local grocery. If you keep the temperature low enough (below 40 F), bacterial growth is slow and the meat lasts longer. If the temperature is high, bacteria grow faster. The difference isn't one of bacteria or no bacteria, but simply of bacterial load.

Cooking reduces the bacterial load by killing bacteria. Cooking also destroys most toxins produced by bacteria, with a few notable exceptions, like botulinum toxin produced by various species of *Clostridia*. Luckily, these bacteria require anaerobic conditions (low oxygen) to survive, so you won't find them in meat that's left in the open air.

Generally, meat is cooked quite thoroughly in areas where such open air butcher shops are found. You'll find a lot of meat stews, high heat *tandoor* ovens. Not so many semi-raw steaks or burgers. This makes the meat safe to eat.

Of course, food borne illnesses do happen, if the meat is cooked improperly. You'll often come across reports like ""dozens of guests taken sick at wedding dinner"" in local news. This happens because the food is cooked on site, in temporary kitchens specifically for the occasion, and mistakes happen when you're cooking for hundreds of guests in a hurry, on makeshift premises.

But by and large, cooking *at the right temperature for a sufficient period of time* renders most food (including meat) safe to eat. As I said, there are some exceptions such as botulinus, but these are more a consequence of canning techniques which create anaerobic environments, so they are more a ""modern"" thing.

Cooking can also be used to prolong the shelf life of already cooked food. For example, in places without refrigeration, people will often ""re-cook"" the day's leftovers to use the next day. This simply involves bringing the food to a boil, then cover the pot and turn off the heat. A quick form of pasteurization that resets the day's bacterial load back to zero, giving the food a few more hours of safety.

It's also worth noting that ""spoilage"" doesn't equal pathogenicity. Spoilage is simply a change in the food's appearance, taste or odor, which people find unpleasant. It can be caused by many bacteria which don't cause disease in humans. Pathogenicity is due to bacteria that are known to specifically cause disease. Sometimes, the two can work at opposite ends, for example lactobacillus (which causes spoilage) is itself harmless to humans, and even inhibits the growth of some pathogenic bacteria.

This is more a topic in biology and food science rather than history, so if you care to read more, I can point you to some resources:

* GILL, C. O., & NEWTON, K. G. (1980). Growth of Bacteria on Meat at Room Temperatures. Journal of Applied Bacteriology, 49(2), 315–323.

* CHUNG, K, DICKSON, J & GROUSE J. (1989). Attachment and Proliferation of Bacteria on Meat. Journal of Food Protection, 52(3), 173-177.",0
"When books are bought by publishers, they are at the same time sent around to all the big film companies/production houses by film agents and scouts working for the author. The film companies buy the book option, which means the right to adapt the book for a year. They can renew it, or not. Normally if your book is bought, you get around $10,000 for a year, for doing nothing, which is a sweet deal for an author who might only be getting $10,000 for writing the whole book anyway. You then get $100,000 or so if the book is made into a film or TV series. There's around a 1% chance of any optioned book becoming a film.

*Harry Potter* was in some ways unlucky. It was passed around in 1995/6 by film scouts to pretty much every film company on earth, and no film company bought it. The manuscript ended up on the piles of paper that used to be the norm in the film development world before iPads and Kindles. It was, as you've said, an unknown manuscript from an unknown author, being published by a then very small British company called Bloomsbury. Everyone passed on it.

What changed? Publishers send out books to bookshops/reviewers about 6 months before publication. Bookshops loved *Harry Potter*. The British book trade had been looking for a Roald Dahl type author for ten years, and the comparisons were made immediately. Also, the publishers leaned in heavily on the 'wrote it in a cafe' 'single mother on benefits' angle - when you're promoting a debut author, literally any hook you can get the press to report is gold, and this was perfect. 

Also, there's just the fact that JK Rowling can write. Kids responded immediately to her action and humour, and the mix of dark and light. Adults, strangely, loved it too.


With this word of mouth buzz happening in Britain, Rowling's literary agent sold the books at the Bologna Book Fair, which is the world's biggest for kids books, to Scholastic in the US, which was then the market leader across the world for children's books. The rights sold for $105,000, which would be pretty big for what's known as Middle Grade today, but was a *huge* amount then.

This American sale is what reignited interest in the film rights, as well as a few big awards in the UK, and the simply the word of mouth buzz that built up with kids and bookshops. As any person who works in books will tell you, word of mouth is basically impossible to conjure up, but when it happens (in about 0.1% of books), it happens *big*. It's the easiest (yet hardest) way to sell books. The book trade would murder to figure out how to do it more.

Heyday Productions were a small-ish British production house, which had made one film, the amazing cannibal thriller *Ravenous*. More importantly, they were the British preferred production partner of Warner Bros, who were desperate for family friendly entertainment franchises. The *Batman* films had just gone down in flames and *Superman* was in development hell. The various attempts at making franchises out of *Major League*, *Gremlins*, *Free Willy* etc had failed. Everyone knew *Star Wars* was coming back. 

*Harry Potter* was winning major book prizes in the US now. It landed on film producers doors again in 1998, now with much bigger word of mouth. It was still ignored. However, this time, the right person in the right place, a development assistant called Tanya Seghatchian, read it, forwarded it to her bosses, who then bumped it up the chain to Warner Bros corporate. The rights were locked down for around $1m, in a complex negotiation with multiple interested parties, which tends to happen when one set of film producers get interested in something.

Things could still have gone wrong. Many, many books are optioned for that amount or close and never go anywhere. There were dozens of kids books optioned by Warner Bros in 1998 for big ticket prices that you will never have heard of.

But David Heyman, producing for Warner's, hired the right screenwriter, Steve Kloves, who delivered a dynamite script. And then it still took two years from then to get the film cast, and the director assigned, and the pre-production to happen, and the script ironed out. Things could have gone wrong until the moment the cameras rolled (or even after). But enough things happened in the right order for it to work out.

So, partly Rowling's talent, partly word of mouth, partly the right people (at Scholastic, at Bloomsbury, at various UK and US trade magazines, at Warner's) liking the book at the right time for it to go ahead. Luck, lots of luck, as always.

Sources: Eccleshare, J, *A guide to the Harry Potter novels* (2004)

Smith, S, *JK Rowling, A biography* (2002)

Anelli, M, *Harry, A history* (2008)",0
"I'm not a historian, but I believe I'm qualified to write an answer to this one as an English PhD with a specialty in fantasy genre fiction. (I can't believe it! An AskHistorians question I can finally write a top-level answer to!)

Before I get too deep into specifics, I want to talk about what genre is, in a very general sense, and how it works.

Genres are ways of categorising and conceptualising media, in this case literature: as such, they are defined primary by people and by broader cultural notions of what 'fits' in various genres. This can make it very difficult to actually define any genre in concrete terms, because the boundaries are intrinsically fuzzy. You end up with a 'I know it when I see it' situation, where a person familiar with the genre tropes and conventions could tell you if any particular work fit into a particular genre with a fair degree of confidence, but still might not be able to tell you exactly what makes one work fit in a genre and another not.

Fantasy is even more resistant to definition in this way, because some of the core principles of the fantasy genre involve imagination, creative exploration, questioning of norms, and similar themes, which leads to it being even more fuzzy than other genres.

So I'd like to clarify that what I think you mean is that the popular conception of fantasy is heavily associated with the medieval / Middle Ages time period; or, again, what people *think* of as associated with that time period.

And these are important distinctions! What people *associate* with a particular time period may have very little to do with what that time period was actually like historically or what it was like to live in. A good example would be how people associate 'Victorian' with prudery and sexual repression, which is really a fundamental misunderstanding. But I digress.

In the same way, what people *think* the fantasy genre is like might have little to do with what fantasy authors are actually writing now: there's plenty of fantasy that isn't directly rooted in the romanticised medieval aesthetic you mention, and some critics who contest fantasy being defined solely as the popular genre texts, claiming that fantasy is more of a mode and stylistic approach to writing than one based on specific content.

But your overall point that the popular idea of fantasy has a strong association with that particular idea of a particular time period is basically correct. In my opinion, there are two major reasons why.

The first is that fantasy, as commonly understood, is a literature of the *past*. It's a romantic genre in the older sense of the word: it *romanticises*, it evokes adventure and mystery. It's only a mild exaggeration to say that if science fiction looks to the future, fantasy looks to the past.

That's not to say that fantasy can't comment on and interact with the present. It has to, actually! Its readers exist in the present; any work of fantasy is already engaged with both the past and the present in that sense. But there is a strong tradition of 'looking back' to a real or imagined past and of exploring history in the genre.

Why is that? The short answer is that the accepted roots of the modern fantasy genre (which is more recent than people think) lie in ancient myth and folktale, which evolved into fairytale and through admixture of other 'taproot texts' that weren't fantasy themselves but heavily informed the genre, leading to the genre beginning to emerge in its modern sense between 100 and 150 years ago. Because fantasy has always been so aware of its roots, precursors, and evolution in this way, it's self-consciously drawing on its own past and thus also on the historical past.

That being said, there are thousands of years of human history, so why the medieval period specifically? And why the specific 'with orcs and elves' imagery you gave in the question? This brings me to the second reason, which is a classic example of how certain texts can be absolutely foundational to popular conceptions of a genre.

The second reason? The Lord of the Rings (1954-55), and Dungeons and Dragons (1974-present).

Fantasy being codified as a genre in modern cultural consciousness is intrinsically linked to The Lord of the Rings, which I consider a 'pinch in the hourglass' for fantasy: it brought together everything that came before and defined everything that came after. Tolkien was a scholar of myth, folktale, and fantasy, and his work (rightly or wrongly) is considered an exemplar of what most people now consider to be medieval-style fantasy (whether Middle-Earth is actually medieval in style, nature or tone is a separate discussion). The Lord of the Rings was a cultural phenomenon that defined how people thought about fantasy for decades, and still does.

It also heavily informed the other piece of media that shaped the popular understanding of fantasy: Dungeons and Dragons (D&D). D&D copied a lot of its ideas and tropes wholesale from Tolkien: orcs, elves, and the legally-distinct-from-hobbits halflings being the most obvious examples, but it also strengthened the idea of a fantasy-medieval milieu being what I might almost call a 'play space'.

Reading a romantic drama set at the court of Louis XIV might not inspire you to take 'the highly mannered and socially complex court of an absolute monarch' as a default setting for all your future stories: you might instead focus on the actual story that happened. In the same way The Lord of the Rings on its own might not have defined 'a medieval-ish setting with fantasy creatures alongside humans' as a default setting in which a variety of stories could be told.

But Dungeons and Dragons, which by its nature required players to come up with near-infinite variations and new stories working from the same basic assumptions and setting themes, *did* encourage the usage of that milieu. It also led to an explosion not only of officially-licensed novels in various official D&D settings, but of fantasy novels that were clearly inspired by D&D.

D&D also, and this is particularly important for your question, had its roots specifically in medieval tabletop wargames such as Chainmail, where characters would control an army of knights and soldiers. The player progression in D&D shifted from controlling an army to controlling a single character, but the default assumption that this character was (a) a combatant, and (b) in a medieval or medieval-esque context was maintained.

By 1997, when Diana Wynne Jones published The Tough Guide to Fantasyland, all of these tropes, concepts, and motifs had become so established (and stale) as to be immediately recognisable and mockable: she could riff on gruel being the staple food, medieval being the presumed mode, High Priests being either good and fat or thin and evil, and how everyone in fantasy appears to wear boots but not socks, with the assumption that any fan of fantasy who read the book knew _exactly_ what she was talking about.

In summary, then: fantasy has always been associated with a romanticised past and action taking place in the past. However, fantasy in the modern sense and conception is far more recent than most people think, and the two cultural touchstones of Tolkien in the 1950s and D&D in the 1970s were what really locked in the stereotypical medieval setting and associated elements that people now consider synonymous with fantasy novels and related media.

Some sources:

Attebery, B. (1992). Strategies of fantasy. Indianapolis: Indiana University Press.

Baeten, E. (1996). The magic mirror: Myth’s abiding power. New York: State University of
New York Press.

Clute, J., & Grant, J. (1997). The encyclopedia of fantasy. London: Orbit Books.

Hume, K. (1984). Fantasy and mimesis: Responses to reality in Western literature.
London: Methuen & Co.

Jackson, R. (1981). Fantasy: The literature of subversion. London: Routledge.

Jones, D. (1997). The tough guide to Fantasyland. London: Vista.

Kenneally, S. (2016). Queer be dragons: Mapping LGBT fantasy novels 1987-2000. Trinity College Dublin. [Ph.D thesis]

Tolkien, J.R.R. (1966). On Fairy-Stories. In The Tolkien reader. New York: Ballantine Books.",0
"So Lincoln's hayseed reputation was not fake but was also politically relevant. The victory of the Republicans, despite not winning a majority of the popular vote, was driven by people who wanted to see the elite pro-slavery consensus broken. Breaking this consensus was basically the *raison d'etre* of the Republican Party and everyone knew it. There were anti-slavery elites but their failure over the past decades to make significant progress against slavery had led to a rather anti-elite mood among Abolitionists. Even as Lincoln claimed he didn't want to abolish slavery, everyone knew (or hoped, or feared) he represented a sea-change. Georgia's declaration of secession was more comprehensive than most, complaining about how Lincoln intended to not only end slavery but change a whole bunch of political pieties:

>The party of Lincoln, called the Republican party, under its present name and organization is of recent origin. It is admitted to be an anti-slavery party, while it attracts to itself by its creed, the scattered advocates of exploded political heresies, of condemned theories in political economy, the advocates of commercial restrictions, of protection, of special privileges, of waste and corruption in the administration of Government; anti-slavery is its mission and its purpose.

(One can almost hear them shouting ""fake news"" or ""discredited science"" or whatever buzzword equivalents would have existed back then.)

Lincoln's arrival to DC was marred by multiple assassination attempts. Lincoln rarely went out into the city in these early days, largely for security reasons but also because he was rarely invited out. His few social engagements were mostly put together by Republican Senators. His wife, always of somewhat delicate mental health, apparently suffered. Many of his early moves were aimed at shoring up support, especially among Republicans and Northern Democrats. But DC was a Southern Democrat society and largely closed their doors to him. This was justified by a combination of raw partisan animus and various kinds of elite gatekeeping about manners or temperament or other norms. They particularly criticized his long rambling stories, his tendency to spend time among the lower classes, and his casual manner. To quote *The Atlantic:*

>The walk was long, and the President halted a moment to rest. 'May de good Lord bless you, President Linkum!' said an old negro, removing his hat, and bowing with tears of joy rolling down his cheeks. The President removed his own hat, and bowed in silence; but it was a bow which upset the forms, laws, customs, and ceremonies of centuries. It was a death-shock to chivalry, and a mortal wound to caste. Recognize a nigger! Faugh! A woman in an adjoining house beheld it, and turned from the scene in unspeakable disgust. There were men in the crowd who had daggers in their eyes.

That quote is from later in the war, towards the end, but shows the general attitude that was even more pronounced at the start of the war.

The start of the war changed all that. The Confederates fired on Fort Sumter. In response, Lincoln called for half a million volunteers. This was actually without Congressional approval (it was granted after the fact). But the volunteers came. The people who responded tended to be highly motivated Unionists or Abolitionists willing to fight for political reasons. These people streamed into Washington, first individually and then as units.These soldiers began to (illegally) act: arresting suspected saboteurs, liberating slaves, and even getting into brawls with local law enforcement. A large number of refugee slaves also began to flee to DC. This created a growing population in DC itself that supported Lincoln while also suppressing Confederate elements that had tried to kill him. That meant he was able to go out and give public speeches to his supporters. By the end of the war they'd be a majority of the local population. But these were mostly not elites. Even many of the officers were at best locally prominent people. And among these people, including African Americans, his down home demeanor and religiosity were advantages not disadvantages. To quote the famous Frederick Douglas:

>Mr. Lincoln was not only a great President, but a *great man* — too great to be small in anything. In his company I was never in any way reminded of my humble origin, or of my unpopular color.

While the attitude no doubt had an even more profound effect on African Americans, this was also the attitude of his rural white supporters. They were used to being looked down on by the great landowners and urban elites and Lincoln was a respite from that. These people formed a lot of the core of his voting base and the Union Army.

Meanwhile, the most pro-Confederate elites... left to join the Confederacy. Naturally. But this meant they were no longer high society in DC. Instead they were high society in Richmond. Those that stayed behind, even the pro-slavery elites, had made a conscious choice to support the Union. And Lincoln was the leader of the anti-Confederate faction. Lincoln thus instantly became the factional leader of everyone except the Copperhead Democrats. The division would be solidified in Lincoln's bipartisan appointments and forming of the National Union Party by combining Republicans and pro-war Democrats.That gave him a constituency in DC high society, though Lincoln and many Republicans continued to be rejected. Famously, his Vice President Andrew Johnson was hugely bitter about the social costs of supporting Lincoln. But among the Unionists, questions about his legitimacy, not winning the popular vote, his rambling stories... All were largely dropped in the face of his growing stature. To quote a sympathetic magazine article:

>The child of the American people was their most prophetic man, because, whether as small shop-keeper, as flat-boatman, as volunteer captain, as honest lawyer, as defender of the Declaration, as President of the United States, he knew by the profoundest instinct and the widest experience and reflection, that in the most vital faith of this country it is just as honorable for an honest man to curry a horse and black a boot as it is to raise cotton or corn, to sell molasses or cloth, to practice medicine or law, to gamble in stocks or speculate in petroleum. He knew the European doctrine that the king makes the gentleman; but he believed with his whole soul the doctrine, the American doctrine, that worth makes the man.

Note how this poses Lincoln. As a common man, as a populist who is posed against the elites of the South *and the North*. It contrasts Lincoln with the more urbane and elite stock marketeers and planters and emphasizes his tendency to appeal directly to the various small classes. (Another speaker said Lincoln had a ""childlike simplicity."" There were a lot of similarly condescending notes when elite supporters spoke of him.)

Lincoln remained apart from the aristocratic norms of the time. The long rambling stories and religious references and his wife's psychic and all the other transgressions of elite manners were quietly ignored. Not by everyone. The Chief Justice of the Supreme Court, for example, was pro-slavery and never liked Lincoln socially. But his reputation shifted enough that Lincoln began to have a social life among the DC elite. That said, he continued to have a reputation as something of a down to earth country type. One anti-slavery activist from a good Boston family roughly summed up his elite supporters' feelings about him:

>I think we have reason to thank God for Abraham Lincoln. With all his deficiencies, it must be admitted that he has grown continually.

From *Reminiscences of Abraham Lincoln by Distinguished Men of His Time,*  *Life and Times of Frederick Douglass, Lincoln and the Election of 1860, Abraham Lincoln: A Life, Free Soil, Free Labor, Free Men: The Ideology of the Republican Party before the Civil War, Lincoln President-elect: Abraham Lincoln and the Great Secession Winter, Lincoln and the Democrats: The Politics of Opposition in the Civil War, The Eloquent President: A Portrait of Lincoln Through His Words, The Every-Day Life of Abraham Lincoln, Emancipation, the Union Army, and the Reelection of Abraham Lincoln, The Atlantic* (for quotes but not analysis), and *The Radical and the Republican.*

Edit: It looks like Reddit didn't take some of my formatting. I might make edits to re-separate out paragraphs etc. Otherwise I'll note if I change the body of this in any substantial way.",0
"> Is there firm evidence that this was actually the case though, for instance from earlier case law referencing common law? Because it seems equally likely that this is just attempting to justify and defend the law, by insisting that it wasn't actually anything new, but merely clarifying and upholding an older forgotten precedent.

What leads you to think that it was a forgotten precedent? There were a number of English divorce cases in the early nineteenth century that rested on men having sex with their wives' sisters, for instance, and there's ample evidence that the standard was kept alive in American common law (which was initially based on English law) until that time. Vermont's Supreme Court didn't get rid of the restriction until an 1837 case. There's more on this in Michael Grossberg's *Governing the Hearth: Law and the Family in Nineteenth-Century America*.

>Why is it then that the Victorians gave weight to Leviticus, but not to Deuteronomy? And why did they ignore the obvious reinterpretation of Leviticus in light of Deuteronomy, as referring to a man who married his brother's wife while the brother was still alive (in other words, regular adultery)? After all, the passage in Leviticus doesn't mention the original husband being dead, whereas Deuteronomy explicitly does.

I think some of this would be better asked of someone who studies the history of the relationship between Christians and the Old Testament. All I can say is that the standard of viewing sex with/marriage to affines (in-laws) has a long history in Christianity, even before/during the Reformation - Henry VIII, for instance, had to get a dispensation from the pope in order to marry Catherine of Aragon, who had been his deceased brother's wife.

One thing I would point to, though, is coverture. As I mentioned in the post, when men and women married they were considered ""one flesh"". The consequence I mentioned was that one's brother-in-law became one's brother in the eyes of the law, but there were others - married English women's property belonged to their husbands, contracts they signed could be thrown out if their husbands didn't want them to exist, and so on. (The ""one flesh"" ultimately meant that a wife became her husband's flesh.) This affinal-marriage prohibition wasn't *just* people interpreting the Bible in a particular way, but part of a general attitude toward the effect of marriage in English society and law.",0
"Interesting information, although the information about ""cereal"" being the only invented breakfast food is wrong. Cold cereal, maybe. But cold cereal is based on hot cereal, like oatmeal and cream of wheat. And those dishes have been around a long time. In America, they are based on the Dutch dish of [""suppan""](https://books.google.com/books?id=Du5Np9QJKs8C&pg=PA156&lpg=PA156&dq=suppan+milk) and the English dish of [""hasty pudding""](https://books.google.com/books?id=cINZCwAAQBAJ&pg=PR67&lpg=PR67&dq=suppan+hasty), the latter of which is mentioned in the song ""Yankee Doodle"".

These dishes were made with whatever grain was available, usually corn in colonial America, but sometimes oats or wheat. Colonists preferred eating it with milk instead of water, and sweetened it with either molasses or maple syrup.

It was initially eaten as an end of day dessert, but it's the kind of dish that you would simmer over the stove in a pot for several hours anyway, so leftovers would sometimes be kept warm overnight to eat them in the morning. 

Cider and hasty pudding/suppan had become a morning meal associated with farmers in the Northern colonies by the time of the American Revolution. When the first cold cereals were developed in the mid-1800s, the inventors were familiar with these warm grain-and-milk breakfast dishes, and their new invention was simply a ""ready to eat"" version of what had come before.",0
"> Contrary to a popular misconception, at no point did Freud theorise that children wish to have sex with their mothers.

Is there a source identifying this as a common misconception?",0
"And another neat fact, at phase changes, when heating water up, (edit: *as it starts boiling*), it doesn't increase in temperature at all, the energy 100% goes into phase change.  That's why a pot of water boiling is always the same temperature (except at different altitudes (edit: *pressures*))",0
"No, the fat isn't sloshing around, despite it being liquid - just like the ~60% that is water isn't  sloshing around. It isn't right to think of the components of an organism as discreet lumps that act like you'd expect the pure form to act out of the body, because the components are encapsulated in complex structures in the body.

In the body fat is stored in fat cells, called adipocytes. These cells are held in place in a complex web of connective tissue, with strong proteins like collagen supporting them. Inside the cells the fat is stored in tiny spheres called inclusions, which are in turn given structure by their own families of proteins, and they are held in the structural proteins of the cell as a whole.

Ultimately all this results in the fat being held pretty rigidly in place. It acts more like a gel.

Edit: Sorry I missed addressing a flaw in the question. The fat in a duck is *always* a liquid, regardless of the outside temperature - because ducks maintain their temperature at ~100f. This is really important. If the fat becomes solid it isn't as easy/is nearly impossible to move around or use when the body needs it. If the fat is solid, it is safe to assume the duck is no longer. -also- the correct word for lipid droplets in cells.",0
"Yes, other animals can be allergic to humans, just as humans can be allergic to other animals. Allergies occur when the immune system overreacts to certain substances, known as allergens. These allergens can be found in various sources, including pet dander, pollen, dust mites, and certain foods. In the case of animals being allergic to humans, it is typically a response to proteins found in human skin cells, saliva, or urine.",1
"This sounds like the answer as Granpa said it was phenol and phenol acts like the crystalization story.  It's likely that the substance had a hyperbolic reputation created by the transporters and users.  This reputation would be for fun and to make it seem dangerous and cool and also for safety reasons as phenol left on the skin will poison someone in addition to burning them badly (though it looks like there would have to be a large amount of phenol left on the skin to actually kill.)  Also, it sounds like phenol will damage someone over repeated exposures, so having a 'convenient lie' like that a drop will kill you instantly helps ensure less people will be exposed in any way.

Thoughts, /u/kraybae?",0
"From r/astronomy, should answer your question :)

[If you still have your eclipse glasses, take a look at the Sun today - there are currently two enormous naked eye sunspot groups facing our planet.](https://www.reddit.com/r/Astronomy/comments/6y16bi/if_you_still_have_your_eclipse_glasses_take_a/)",0
"Paramedic here...

The autonomic nervous system is decided into “sympathetic” and “parasympathetic” nervous responses. After this sympathetic response, your brain calms your body down through the parasympathetic response. Parasympathetic nervous system is responsible for digestion, and reproduction. Without any input from your brain balancing this parasympathetic response (it is by default “always on”) your body just assumes it is totally relaxed. So if a man has a spinal injury and his brain can’t send signals to the body telling him he’s is hurt and stressed (autonomic response), he’ll get a boner. This is known as a priapism in the medical field and has nothing to do with home being horny, he just has no autonomic nervous signals reaching his penis. So if you see a guy who fell, and he has a boner, take C-spine precessions. 

Edit: autonomic response definitely contains both parasympathetic and sympathetic. Thanks.

Also...

There a few ways to stimulate a parasympathetic response in your body. One is by “ bearing down” or pushing like you’re going have a bowel movement. This can cause sick patients to suddenly go unconscious by over stimulating this response when they are already at risk and causing their blood pressure to drop too low and not having enough output from their hearts. 

Also, when young men have sex for the first time, they can be over-excited and have an overstimulated sympathetic response. Because this lowers their parasympathetic response, they end up not be able to obtain and erection. This is ED not cause by any medical reason other than over stimulation due to the sympathetic response. Alcohol often helps in low doses, but too much cause decrease blood flow and ultimately makes ED worse. 

The human body and psyche is nuts.",0
"    import random

    def compute(num_items):
      items = set()
      turns = 0
      while len(items) < num_items:
        items.add(random.randint(1, num_items))
        turns += 1
      return turns
    
    n = 52
    k = 10000
    trials = [compute(n) for i in range(k)]
    print sum(trials) / k  # will be ~236

",0
"Yes, there is a point of no return in severe cases of hunger and thirst. When a person reaches an advanced stage of starvation or dehydration, their body undergoes significant physiological changes that can make it difficult to recover even with proper treatment. In cases of extreme hunger, the body starts breaking down its own tissues, including muscle and organ tissues, to obtain energy. This can lead to organ failure and irreversible damage.",1
"Veterinarian here, the true flu H#N# viruses are pretty common in horses, birds and pigs (hence bird flu/swine flu) though they do not always cause significant disease in those species. There are currently multiple outbreaks of canine influenza (H3N8 and H3N2) around the US though this really isn’t very common in general. 

The flu virus mutates frequently and participates in genetic recombination. This means genes from multiple flu viruses can recombine into a virus with a unique/uncommon genetic make up. This is why they make a different flu vaccine every year and why that flu vaccine doesn’t always work (they have to predict what strains will be most common in any given year). It’s also what makes the flu virus able to cross species in all sorts of different directions. 

In general, however, our pets don’t have as much of a problem with viruses like the cold and flu. (With the exception of what I mentioned above regarding canine influenza). Both of those viruses rely strongly on many individuals of the same species having close contact with each other. That just doesn’t happen as much for dogs and cats as it does for humans. Diseases that are sort of in the same vein however include kennel cough in dogs and herpes and calicivirus in cats. They can all cause upper respiratory infections and are contagious. 

Other viral, infectious diseases that we really worry about are parvovirus and the feline equivalent, panleukopenia, as well as things like distemper and rabies. Good news is there are vaccines out there for those bad boys. 
",0
"I wish they would highlight victories like this more often.  Environmentalism usually feels like a bunch of looming catastrophes that never end up really being anything.  The fact that the reason they don't end up being catastrophes is that we take action to stop them is completely lost on the average person like myself, so that the original hype ends up looking like some chicken-little sky-is-falling shit, and we aren't even told that the sky WAS falling and we legislated against that.",0
"There are mechanical differences as well as chemical differences that account for the difference in explosive power of the Fat Man and Little Boy:

**Fat Man**:

Used about 13.6 lbs. of plutonium in an implosion, caused by surrounding the plutonium in nearly 3 tons of conventional explosives. Plutonium releases about 210 MeV of energy per fission. Total explosive power of the Fat Man was about 21,000 tons of TNT, which is 5.48404 x 10^32 eV.

**Little Boy**:

Using a gun mechanism, a 85 lb. hollow ""bullet"" made of uranium is shot into a 55 lbs. mass of uranium, causing it to go critical. Uranium releases about 200 MeV of energy. Little Boy's explosive power was about 15,000 tons of TNT, which is 3.91717 x 10^32 eV.

Most likely, the initiation mechanism for the Fat Man was just more efficient, causing more atoms to undergo fission. This would make sense if you think about an implosion vs a gun barrel mechanism. Doing the math, you'd find that 2.61144762 x 10^24 atoms of plutonium underwent fission. For the uranium, it was 1.958585 x 10^24 atoms. To calculate the efficiency of the bombs, you'd have to know their relative atomic densities, but I can't find that information.

Edit: fixed notation and which mass of uranium was shot into which.",0
"Yes, there is a more or less standard way of solving this problem, but there is a lot of latitude. For instance, it's well possible that your biased coin gives you results that look perfectly unbiased for any arbitrary number of flips. So you can never know *for sure* whether your coin is biased or unbiased.

Suppose we have the following, significantly *easier* problem. We have two coins, X and Y, one of which has probability of heads *p* and the other has probability of heads *q*. But we don't know which is which. We randomly choose one coin and our goal is to determine whether our coin has chance *p* or *q* of showing heads. Note that we *know* the values of *p* and *q* *a priori*; we just don't know which coin is which.

For the solution to this problem, [you can read this post on StackExchange](https://math.stackexchange.com/questions/2033370/how-to-determine-the-number-of-coin-tosses-to-identify-one-biased-coin-from-anot/2033739#2033739). The idea is that you need to flip the coin enough times so that you are confident that both you have X and that you don't have Y. The punchline is that if the coins have *p* and 0.5 as their chance for getting heads (so we are trying to distinguish a biased coin from an unbiased coin), then the minimum number of flips needed for a 5% error is roughly N = 2.71/(p - 0.5)^(2). Note that the closer the biased coin is to being fair, the more flips we need. If the biased coin is known to have, say, p = 0.51, then we need about 27,100 flips to distinguish between the two coins.

[**edit:** Another user discovered a missing factor of 4 on the formula in the StackExchange post. I have since corrected the formula and the calculated value of n.]

However, the problem posed in the title is much different since we do not know the bias of the coin *a priori*. This means that will not be able to write down the number of required flips once and for all. It depends on how biased the coin can be. As the calculation linked above shows, we may very well require arbitrarily many flips if the bias (deviation from fair) is allowed to be arbitrarily small. If the bias is bounded away from 0, then the above analysis can be applied to give an upper bound for the minimum number of flips.

The best you can arguably really do in the general case is flip the coin with unknown bias many times and then consider a certain desired confidence interval. So let *p* be the unknown chance of getting heads on your coin. The procedure to distinguish this coin from fair would be as follows:

1. Flip the coin *n* times and record the results. Let *h* = observed proportion of heads.
2. Find the *Z*-value corresponding to a confidence level of γ. (There are plenty of calculators that can do this for you.)
3. Calculate W = Z/(2n^(1/2)). This expression comes from the fact that the standard error for *n* Bernoulli trials with probability *p* is (p(1-p)/n)^(1/2), and this expression is maximized when p = 1/2. (Remember we don't know the value of p, so that's the best we can do.)
4. The confidence interval for *p* is thus (h-W, h+W).

Please note carefully what this confidence interval means. This means that if you were to repeat this experiment many times (or have many different experimenters all performing it independently of each other), then the proportion of experiments for which the confidence interval would actually contain the true value of *p* tends toward γ. It does *not* mean that there is a probability of γ that the true value of *p* lies in this particular interval (h-W, h+W), although that is a common misinterpretation.

[**edit:** I've changed the description of a CI to be more intuitive and more correct! Thank the various followup comments for pointing this out to me.]

As a particular example, suppose you flipped the coin 10,000 times and got 4,000 heads. You want a 99.99% confidence level. So h = 0.4 and γ = 0.9999. A confidence level calculator gives Z = 3.891, and hence W = 0.019455. Hence your confidence interval is (0.381, 0.419). So if many other people performed the same experiment and you collected all of the results, roughly 99.99% of the calculated confidence intervals would contain the true value of *p*, *and* they would all have the same length. So it's probably safe to say the coin is biased. Can't know for sure though based on just one CI. But if you repeat this process and get, say, 5100 heads, then your confidence interval is  (0.491, 0.529). So it's probably not safe to say the coin is biased in that case.

In general, for this method, the number of trials required depends only on the desired confidence level. Whether you decide the coin is biased is a different question really. At the very least, you would want your confidence interval not to include p = 0.5. But this doesn't mean that can't be true. Confidence intervals are notoriously misinterpreted.

Wikipedia has an article on this very problem. The method of using confidence intervals is described. Another method based on posterior distributions is also considered, and you can read the details [here](https://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair#Posterior_probability_density_function).",0
"It’ll be about as useful as bringing an umbrella to a fire fight.

There are five hazards outside of the fireball of death. Radiation; Alpha (free helium), Beta (free electrons), and Gamma (high energy photons), and Neutrons (high kinetic energy). Those four will burn you similar to a sun burn.

The fifth and least discussed, and absolutely most dangerous, are the radioactive daughter products. The high mass fuel splits in halves, and those halves are the daughters. That can be samarium, Xenon, lead, thorium, iodine, cobalt, whatever. It comes off as a light dust (fall out). It continues to be radioactive and releasing the 4 types of radiation above. It’s that crap you can breath in, or it gets on your clothes, or on top of the soil, etc. 

Sunblock will help with photons at the UV energy level, but that’s it. Everything else will burn you, or get inside you and burn you. And the daughter products will continuously burn...

Sunblock will only help to make you sticky.",0
"Adding onto this comment, since it's not a true 'answer', but something with which I hope to provide you (OP) a bit of further insight into the strange curiosity of numbers:

There are exactly as many even numbers as there are natural numbers. Strange, you might say - 1 is not an even number, but it is a natural number - surely there must then be less even numbers than natural numbers?  

But no. That's where it gets interesting. How do we prove that there are the same amount of two things? By pairing them up - if I have apples, and you have pears, we have the same amount if we can put one of your pears next to each of my apples and have 0 left over.  

So apply this to our numbers. I put 0 next to 0 - awesome. I put 1 next to 2. I put 2 next to 4, 3 next to 6, and so on and so on. For every natural number k, I have a single paired even number - 2k. Meanwhile, every even number n must by definition be two times some specific natural number, n = 2*k, which is its pairing.  
So we've made a one-to-one pairing between the natural numbers and the even numbers - there are just as many even numbers as there are natural numbers, despite being able to provide an infinite amount of natural numbers that aren't even.  

That's pretty cool when you think about it, isn't it?

In a very similar vein I could prove to you that there are just as many real numbers between 0 and 1 as between 0 and 2, and there are just as many points on a circle with radius 1 as on one with radius 2, despite the latter having a different circumference.

Edit: Small mistake in my wording",0
"From a different source:

> Researchers have suspected a link between sonar testing and whale deaths for nearly 20 years. In 2000, the U.S. Navy said its sonar exercises led six beaked whales to fatally beach themselves in the Bahamas, and stranded whales have died near sonar-testing sites in at least five other cases since then. It hasn't been clear how the sonar disorients the animals and causes such strandings, but some marine biologists suspect that the intense sound waves force whales to shoot to the surface, and they've found evidence that tiny nitrogen bubbles expand in the whales' tissues and damage vital organs (ScienceNOW, 9 October 2003). The same thing happens when scuba divers surface too quickly--a condition known as the bends. **But a whale holds its breath when diving, preventing nitrogen buildup, so the theory didn't seem to hold water.** A group led by marine biologist Peter Tyack of Woods Hole Oceanographic Institution in Massachusetts suspected that whales alter their diving behavior in some other way.

> Whales make repeated shallow dives when trying to evade predators. The team wondered whether such behavior could be risky, especially because naval sonar--which is similar in frequency to the calls of the beaked whale's most feared adversary, the killer whale--could be forcing the whales to adopt a similar diving pattern. So the researchers mathematically analyzed dive behavior in Cuvier's beaked whales and in dolphins to test whether nitrogen bubbles could expand in whale tissue during repeated shallow dives. The team incorporated known physiological data into a model that charts how the bubble size might increase in the circulatory system, brain, muscles, and fat tissues when a whale dives repeatedly to between 30 and 80 meters for as long as 3 hours.

> During normal diving behavior, scientists believe, the lungs of marine mammals collapse when they plunge past 72 meters in depth. That ""clever mechanism,"" Tyack says, prevents nitrogen from infiltrating the bloodstream. The team's model predicts that if the whales' lungs do not collapse during a long series of shallow dives, the increased pressure can cause nitrogen bubbles to diffuse into tissues, increasing the risk of bubble formation on ascent. Limiting the duration of sonar testing may prevent the animals from diving in these harmful patterns, the team concludes in the current issue of *Marine Mammal Science*.

> Noting that diving behavior is extraordinarily difficult to study in live animals, marine biologist Terrie Williams of the University of California, Santa Cruz, calls the model ""extremely useful."" As new research shores up gaps in the model's assumptions--with actual observations to corroborate the avoidance behavior, for example--scientists can try to home in on a safe length and level of sonar exercises, clarifying the murky waters surrounding this debate. ""Now it's a question of how quickly [decompression sickness] happens,"" she says. [(Source)](http://www.sciencemag.org/news/2007/12/why-do-whales-get-bends)",0
"Studies have shown it is likely several reasons: breathing (abdominal fat pushing on the diaphragm), weakened immune system (fat cells in the spleen, bone marrow and thymus leads to impaired function), clotting (obesity increases clotting, and covid triggers clots), inflammation, and delayed care-seeking (people with obesity tend to delay seeking care).

This article in [Science](https://www.sciencemag.org/news/2020/09/why-covid-19-more-deadly-people-obesity-even-if-theyre-young) has a very good overview of the existing literature.",0
"Piggybacking off this question: 

How much water does someone REALLY need? Drinking the ""8-10 glasses"" a day gives me clear urine and over 25 trips to the bathroom, whereas if I drink 5+ glasses a day, my urine is still clear and I feel hydrated without the bladder pains/constant bathroom trips. ",0
"General surgeon here.

Larger people have larger organs in general for the reasons listed above, I’ve never read any studies on it, it’s just what I see at work.  The cavities in your bodies adjust to the need for the organs IN MOST SITUATIONS.

Symptomatic Pectus excavatum is a good example of what happens when it doesn’t.   

And what happens when you have more space that needed?  Well most spaces have atleast once size that is soft tissue that will decrease in size.  But, for instance, in brain atrophy like in old dementia patients or alcoholics, the brain is actually a little “loose”, and can slosh more.  Fluid will fill around it.  You’re body never fills empty cavities with air, it’s always fluid if anything.

Here’s another example.  In the abdomen sometimes we have to do really big surgeries like remove half the organs for a big cancer.  At the end all we do is close the abdomen as normal.  The the abdominal cavity will slowly shrink down some, all air will be absorbed (can take a month if open air, just a few days if laparoscopic).  They may get a little extra fluid in their abdomen.

In terms of “making space”. People have lots of extra space inside them, especially in the abdomen.  As people get fat, they can store so much fat inside their abdomen.  So, so much.  It can make my job very difficult.

Even on fat people, I’m talking like BMI of 60, I can still put around 3-4 liters of air in the abdomen to do laparoscopic surgery.  The inside of their abdomen will expand with their need for space as long as the need for space happens slowly (over months, not over days.  Google abdominal compartment syndrome’ for what happens if they need a lot of space over days).

Hope that makes sense.  I added paragraphs out of order so hopefully didn’t repeat myself too much 

Edit: to the dude that says that air is also a liquid and I should use the word “fluid”.......get a life.   Good lord ",0
"Zika virus is still circulating in certain parts of the world, but the global outbreak that gained significant attention in 2015-2016 has subsided. The virus is primarily transmitted through the bite of infected mosquitoes, mainly Aedes species, but it can also be transmitted through sexual contact and from mother to fetus during pregnancy. Efforts to control the spread of Zika have been successful in many regions, and the number of reported cases has significantly decreased.",1
"Overwatering causes oxygen loss in the soil as the plant and bacteria consume it with no way for gases to penetrate the water logged soil.

Lack of oxygen causes bad bacteria to start forming that will often eat plant roots and/or emit toxins that kill the plant roots so they can eat the decaying organic matter. 

This is why chronically over watered plants 'wilt', they lack enough living roots to absorb enough water. By this stage the plant will often die even with corrective treatment. 

By converse, hydroponics, the art of growing plants in water, can grow plants submerged in water because the water is constantly oxygenated. In flood setups the medium is chosen so that water drains away and is replaced with fresh oxygenated water often (a few times a day generally) ",0
"Depends on what you mean. Two different forms of your question:

**1)** If you were to start walking from mean sea level (as defined as the average position of sea level at a given point over a several year period to account for tidal and storm variability) at a specific point to a specific mountain peak now vs 100 years from now, would the total vertical distance you travel be different?

Generally, yes. Because the total mass of water in the ocean is increasing and the volume is also increasing via thermal expansion, mean sea level is rising (in most places, there are important local variations due to variations in gravity, bathymetry, and isostatic rebound in response to melting of glaciers/ice sheets) the total vertical distance between mean sea level and mountain peaks will decrease between now and 100 years from now.

**2)** Will the elevation on maps of mountain peaks change between now and 100 years because of sea level rise?

No (with a caveat). While conversationally people refer to topographic height as a value above 'mean sea level', unless you're referencing an old map, this is not really the case in that these elevations are not referenced directly to measurements of sea level. When describing a position, whether that's a horizontal or vertical position, this position needs to be referenced to something. This is equivalent to simple plotting in cartesian coordinates where everything is referenced to the origin. When talking about geographic/topographic coordinates, the reference points are called the [datum](https://en.wikipedia.org/wiki/Geodetic_datum). For heights, a [vertical datum](http://gisgeography.com/vertical-datum/) is what we're concerned with and you can see from that link that we can kind of think of three broad categories, 'tidal datums', 'ellipsoidal datums', and 'geodetic datums' (also sometimes called 'orthometric datums'). While a tidal datum is tied to actual measurements of mean sea level height in several areas, a geodetic datum is tied to a specific point, that may or may not coincide with a place where we have measured mean tidal heights. For a geodetic datum, heights are essentially [orthometric heights](https://en.wikipedia.org/wiki/Orthometric_height), so heights above the [geoid](https://en.wikipedia.org/wiki/Geoid), which is an equipotential gravitational surface which represents what sea level would be if only influenced by gravity and earth's rotation, then referenced to our zero coordinate which is specific to that datum. Ellispoidal datums are reference heights to an ellipsoid, so a mathematical approximation of the shape of the earth without topography. There are [lots](https://community.esri.com/groups/coordinate-reference-systems/blog/2014/08/14/mean-sealevel) of different vertical datums that vary by place, for the US we currently use [North American Vertical Datum of 1988](https://www.ngs.noaa.gov/datums/vertical/) which is a geodetic datum. Because elevations are referenced to the height of a specific point (with corrections for the height of the geoid as a function of location) changes in sea level have no influence on the vertical datum. Now, the caveat would be that NOAA or the equivalent body for another country could decide in the future that they want to update their geodetic datum so that they choose a new zero point based on the new sea level height in some location, but there's not really a reason to do this. Vertical datums do get updated (though not incrementally, a vertical or horizontal datum is not changed once it's established, but a new one can be introduced, an example as described on several of those pages is the switch from the tidal datum of 1929 to the geodetic datum of 1988) but this is driven by better and more precise measurements of the gravitational field of the Earth and not changes in sea level, and in fact the US vertical datum [is set to be replaced in 2022](https://www.ngs.noaa.gov/datums/newdatums/index.shtml).",0
"Electricity has been known for a long time. Egyptians noted the similarity between electric eel shocks and lightning.

Pliny the elder (and many others) noted that these shocks could be transferred, that objects when rubbed often attracted things, that so did magnets, and that the three phenomena were connected. Thales of Miletus came up with the theory that when Amber underwent friction, it became a lodestone, and if rubbed further produced lightning proving it was a magnetic force behind lightnng. Both though in terms of ""Gods"" or ""Souls"", which in terms of philosophy might be better thought of as a ""motive force without a clear origin"".

Which is a pretty solid conclusion if you discount Thales mixed up electric fields and magnetic ones. And, you know, thought everything was water (not as stupid as it sounds.)",0
"Hello,

I'm a Ph.D. student working in the field of gas hydrates. I work with methane hydrates in looking at how they form and how we can inhibit their formation.

Gas hydrates formation is a crystallization process and as such, has a [thermodynamic equilibrium line](https://upload.wikimedia.org/wikipedia/commons/b/b4/Methane_Hydrate_phase_diagram.jpg). If your system has high pressure or low temperature, driving your operating condition on the region promoting gas hydrate formation, they will form at nucleation sites. This graph however is on a pure water-methane system and the ocean contains a plethora of other gases which, when combined into your system, will have a different equilibrium curve. 

However, whatever that curve may be, an increase in temperature will lower the driving force of hydrate formation and if the temperature rises sufficiently to be above the thermodynamic equilibrium, the hydrate will melt and release the gases within. These gases rise above the sea level and reach the atmosphere where they act as greenhouse gases and further accelerate global warming. This is a feed positive system where temperature increase drives the increase in temperature to accelerate.

Things are already getting quite bad. The rate of release of these gases is accelerating and we don't have the technology to properly contain these gases and use them for our benefit. 

The permafrost region and ocean beds are riddled (sorry for the non scientific language) with gas [hydrate banks](https://alfinnextlevel.files.wordpress.com/2014/03/energy_from_ice.jpg). Estimates put the amount of gas from hydrates at 3 times as much as commercially exploited reserves in the form of natural gas, oil, and coal, combined. Unless the hydrate system is further studied and better understood, the release in methane will only further increase. 

Fun fact: Scientists lit a cave of natural gas on fire in 1971 expecting it to only burn for a few days; it still burns till this day. it has been nicknamed ""[The Door to Hell](http://i.huffpost.com/gen/1473692/images/o-DOORTOHELL1-facebook.jpg)""

I can do an science AMA on this topic with more detailed answers if the interest is present!",0
"Nerve agents such as [VX](https://en.wikipedia.org/wiki/VX_(nerve_agent))or [Novichok](https://en.wikipedia.org/wiki/Novichok_agent) (keeping it topical, no pun intended) will certainly do the job. Several types of alkaloid would be classed as deadly enough that a drop would kill you e.g. [Nicotine](https://en.wikipedia.org/wiki/Nicotine), [Batrachotoxin](https://en.wikipedia.org/wiki/Batrachotoxin), [Epibatidine](https://en.wikipedia.org/wiki/Epibatidine), [Tetrodotoxin](https://en.wikipedia.org/wiki/Tetrodotoxin). [Methylated mercury](https://en.wikipedia.org/wiki/Dimethylmercury), as another comment stated, will certainly kill you albeit not quickly. 

Very often the lethality of a substance is lower than might be expected due to poor absorption upon skin contact e.g. [Fentanyl](https://en.wikipedia.org/wiki/Fentanyl). 

&#x200B;",0
"First of all, we flush the donor organs with a preservative solution which removes almost all of the blood from the organ and  stabilize cell membranes to prevent cell death. 

When sewing organs like liver transplants  in we have to sew the inflow (hepatic artery and portal vein) and the outflow (hepatic veins or vena cava). This is done while the recipient vessels are clamped off. If we open up the outflow clamps first, blood will flow backwards through the organs, essentially pushing out an significant amount of air.  We then open up the inflow vessels after we confirm their is no major bleeding from the outflow. 

Some surgeons might leave a small hole in the outflow and vent blood through it with a clamp above it to flush air out and “purge” the system, then close the hole after reperfusion. 

Some surgeons will also distend  some of the clamped vessels with saline in order to remove air from the connections. 

Edit:  I would remiss if I didn’t take this opportunity to encourage anyone who is reading this to be sure to consider signing up as an organ donor. And tell your family your wishes. ",0
"Depends on the type of reactor. Most plants are so ridiculously automated it's not even funny. Even the older ones.

As someone stated though the lack of load would cause the generators to trip and with that happening the reactor would trip because there's nothing to take the load. Nuke plants aren't great at varying loads so a sudden drop off in load usage would cause it all to shutdown for safety reasons automatically. When we had that big power outtage many years ago on the east coast the plants all went into shut down because the systems all tripped as there was a sudden lack of load as far as the generators were concerned and all the reactors went into safety ""OH shit our powers got nowhere to go"" mode and started shut down processes. Which sometimes causes problems as the back ups for some plants are primarily fed from the grid (backups used if not acailable) but because the whole grid went down some back ups didn't do what they should have.

Source: Am Nuclear Operator 

Edit:
Few questions were asked.
1) Depending on the age of the plant, in a perfect world they should technically run without any human intervention for quite awhile. That said no plant runs perfectly so it could be as short as a day before lack of humans causes it to shut down or a few weeks. As someone said they have entire shifts of people for the reactors I'm at at all times and they're integral to making sure it runs smoothly but even without us it generally can run for awhile before issues arise and it shuts down, but it's also a much older so without us it'd fall apart.

2) The simulated load is incredibly low as the plants can't really run if there's nothing to draw the load. It's hard to just have electricity go to nothing and it's hard to pretend there's a load that can use up the pure energy a nuclear reactor puts out. Nuclear reactors do not handle adjusting their power very well and at relatively high numbers begin to poison themselves out if the level is too low. Something like 60%, I think I can't remember, reactor power causes it to be overwhelmed by it's byproducts to the point where it can't keep going and has to basically be shut down restarted after x amount of hours so that it can decay enough to not cripple the reaction. The simulated load would have to be equal to a load above poisoning levels and that's obscenely high. Generally if the generator detects no load drawing from it, it has no choice but to basically be like ""Mr reactor you need to turn off or shit going to go Cray.""

3)Most reactors built nowadays generally have a ton of safety features to hopefully power cool the reactor and poison it out to the point where the reactions stop. However... the fuel is still hot. Really fricken hot. Without the water circulating through it constantly there could be some huge issues. I work at a CANDU reactor. We use heavy water as our heat transfer medium. One of our in case of emergency cool and poison the reactor mediums is a large eater tower that gravity feeds normal light eater into the reactor as that cools and absorbs the reactor faster than the current heavy water in it. However.. It's designed for 1 reactor messing up hard and hoping people can shut the others down (all reactors are independent system wise so that faults on one isn't faults on all 4). Another feature they have assuming 100% lack of power (no back up generators for emergencies) the system is designed to go for as long as it can on a thermal flow option... like, the hot water will flow through the system cool and return back, which they got to test in real life by accident during the black out because the faults were so bad. However it only last so long. The systems probably would never breach containment if it got too hot honestly however the plant itself would be a terrible place to be with how their systems are set up. A meltdown on the levels of what has happened with 3 mile and fukushima are interesting edge cases of poor decision or poor design. Fukushima actually caused my plant to put safeties in place in case something were to happen here... Even though we are nowhere near fault lines. Meltdowns are honestly a hard thing to judge. It depends on how containment is built. It's such a plant by plant basis that it's impossible to say how every plant would react.

Edit 2:

First off sorry I don't have much for sources. It's mostly the courses we took in training and operating procedure and most of it's not really linkable.

Most plants are designed yes to just shut down the reactor if a problem arises and no human interaction occurs. The rods at most meant for poisoning the reactor out and shutting it down are gravity held up by electronic means. If no power, rods drop and kill the reaction really fast.

Also the reason the load matters isn't for the reactor itself. It's for the generators. If they aren't using the steam from the reactor to power anything there's almost no reason for the reactor to be running so it would begin to shut itself down.
 
Also my plant will never be re-tubed if that helps. Too old. On her last legs. Which is why we have to be more involved with plant operations, older plant with lots more terrible manual valves and etc.

Plants are designed to have as much automation in its processes as technologically available at the time of construction, and as such as time goes on newer plants have more sustainability assuming peak conditions.

Side note: If you want to get into it go for it but be warned rotating 12 hour shifts which we have are absolutely the worst. Anyone who says it's okay is an edge case.

Edit 3: I'm currently out, I'll try and have answers to what I can actually answer when I'm at a computer.

Edit 4:

Is CANDU the best: Eh. Depends. Its a system that works, its pretty safe, can run off not just enriched fuels, but its not necessarily the best or most efficient. It uses a Heavy Water Moderator for the heat transfer, as light water (normal ol' h20) tends to absorb a lot of the neutrons in the reaction, whereas Heavy Water does not. This is both good and bad, as the inventory of water for cooling has to be maintained and can't just be pumped from a lake (the water in most systems is never recycled back to the lakes, mind you) Edit edit: Biggest advantage of CANDU? Online refueling. We Refuel while she runs. Think of it like pushing the rods through a tube. Push one in, out comes one on other side. They very carefully balance the load with new/old fuel and which sides fueled for each tube to make sure there's no spikes in reactivity. Very neat stuff honestly.

If the plant tripped and had the resources, could we restart?: Absolutely. Most plants are designed that way. If its been down long enough, though, it has to start up -really- slowly. Most reactors take hours / days to start up and get to full power due to the nature of nuclear reactions. It has to be super controlled (which nuclear is very controlled and safe in that matter) so as to not cause problems (or to detect problems and either fix them if possible, or power back down as happens from time to time). The biggest issue is most nuclear plants don't really start up without external power from the grid kind of keeping the systems going and jump-starting what needs to be before you're getting any real power from the Generators. I honestly don't know if we could cold start, with 0 external power. That said, there's still Natural Gas and or Coal depending on where you are (no coal here) to act in the interim, so the power companies could basically shunt the power to the plants to help them start up, which is what happened during the blackout as people mentioned (Some plants were able to keep 1 or more running and used those to basically restart the others) and then from there do what needs to be done, but without any real power source the plant would be unable to keep going, let alone start up.

As for those who DO like shift work, honestly good on you. Legitimately. I found it tiring, staying concentrated for 12 hours isn't easy, and on a night shift on the last even 3 or so hours, you'll notice very few people doing anything that isn't urgent / mandatory outside of the control room.

As for water getting contaminated: I can only vouch for CANDU, but we keep our steam flow separate from the other flows. We used heavy water as mentioned in its own flow, and it basically is used to heat up a boiler, which then heats up normal light water, which then turns the turbines. The heavy water, which flows through the reactor, never leaves containment. It's not allowed to unless there's a breach of some sort, or the vacuum building (a containment device) gets triggered, and at that point there is a lot of ""oh god, we got a lot of clean up to do"" going on.... But even that is a large, sealed, concrete building. It's a lot safer than people realize. They monitor any air going into and out, all water, etc. Some newer plants don't even let you near the core itself while in operation at all, where as some older ones kind of do but for obvious reasons you don't. Very safe.

As for ""Melt downs"", it depends. Only if containment was breached (it takes a lot to breach containment under most circumstances) would there be risk to the outside, and if there was a breach, how big? There would be a lot of signs if there was, and you'd have plenty of warning. Radiation is fast, but linear in its motion. It would have to literally spill out and or explode everywhere, and exploding is something they're designed generally to not do.

Oh this post got too long, had to cut two answer... I'll post as a comment.",0
"Like all other organisms, our mating strategy is part and parcel of our overall survival strategy.

In our case, we are extreme ""K-specialists"". We devote a *huge* amount of investment and resources in our offspring, compared to, say, willows who just scatter their seed to the wind by the millions.

Our females have developped a strategy of concealed ovulation. Current thinking is that by concealing her ovulation and maintaining a perpetual state of potential sexual readiness, the human female makes it difficult for males to know whether her offpring are theirs. The male counter-strategy is to be at hand as often as possible to prevent cuckoldry. Together, this strategy and counter-strategy promote pair-bonding, monogamy and dual parental investment, thus maximising parental investment in offspring.

see:

[Benshoof, L., & Thornhill, R. (1979). The evolution of monogamy and concealed ovulation in humans. Journal of Social and Biological Structures, 2(2), 95-106.](https://www.researchgate.net/profile/Randy_Thornhill/publication/222051156_The_Evolution_of_Monogamy_and_Concealed_Ovulation_in_Humans/links/0f31752fa378bb0d42000000.pdf)

[Strassmann, B. I. (1981). Sexual selection, paternal care, and concealed ovulation in humans. Ethology and Sociobiology, 2(1), 31-40.](http://sites.lsa.umich.edu/bis/wp-content/uploads/sites/171/2014/10/Strassmann-1981-Ethol-Sociobiol.pdf)

[Buss, D. M., & Schmitt, D. P. (1993). Sexual strategies theory: an evolutionary perspective on human mating. Psychological review, 100(2), 204.](https://www.researchgate.net/profile/David_Buss/publication/14715297_Sexual_Strategies_Theory_An_Evolutionary_Perspective_on_Human_Mating/links/0deec5181791b73d35000000/Sexual-Strategies-Theory-An-Evolutionary-Perspective-on-Human-Mating.pdf)

EDIT: Thanks for /u/ardent-muses (et *alia*) for correcting the -r/-K screwup. ",0
I’m pretty sure the question was referring purely to bone structure and not fat distribution. Not sure everyone got the memo.,0
"They did extensive testing of the implosion device because they knew that the implosion must be as perfectly symmetrical as possible or else the bomb just blows itself apart. It also proved much more difficult to manufacture. Not only were the explosives required to have very specific shapes, but it was found that inconsistencies in the explosive mixture itself (air voids) were causing problems. 

In effect, the Fat Man bomb was the first application of explosives with scientific accuracy. If you just want to blow stuff up you don't need as much precision.

First they tried wrapping explosive lenses around steel tubes and detonating them. If the detonation was symmetrical then the tube would be perfectly squinched together at a point. If the detonation was asymmetrical then the tube would twist. The exact type of irregularities gave them clues to what was going wrong. 

Eventually they set up bomb test range with a high-speed x-ray camera, so they could film the interior of the implosion bomb detonating. Pretty fantastic stuff, considering the technology level of the time. ",0
"Air bubbles in the bloodstream (called air embolisms when they interfere with circulation) are a concern following organ transplantation because they can cause circulatory, and even neurological, problems. To preserve the organ during transportation, the blood in the organ is replaced with a solution designed to preserve tissue function following explantation. During the transplantation process, the organ must be connected to the circulatory system of the patient (individual blood vessels are connected through a process called anastomosis). Surgeons will connect arteries first (the inlets for the organ) before connecting the veins (the outlets for the organ), and thereby allow the patient's own blood flow to clear both the preservative solution and any air bubbles from the new organ.

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2845451/

Edit: Clarification on what qualifies as an air embolism thanks to /u/Tombomcfaren.",0
"Just because somebody is strictly tall doesn't necessarily mean they're volumetrically larger. I mean, you obviously have tall and lanky vs. short and stout.

**However**, there are physical factors that may directly affect organ size. The most obvious is the heart. In taller people, the hearts have to work harder to pump blood up to the heads, as well as bring blood back from the legs. This increased work load also ""works out"" the heart to increase its size, and is believed to produce extra stress on the heart, and may partially be responsible for [lower life expectancy associated with height](http://www.slate.com/articles/health_and_science/science/2013/07/height_and_longevity_the_research_is_clear_being_tall_is_hazardous_to_your.html).

Of course, just because something ""makes sense"" doesn't mean that's the mechanism. I'm not certain if there has been controlled studies to see if we can alleviate only the ""height factor"" to see if there's a change in heart size, or if heart size and bone lengthening are attributable to a common factor (like growth hormone). It's likely both.

Edit: on the topic of empty space in a body - in the case of organ donation (e.g. kidney, partial liver, etc.), the viscera (the abdomen) is really soft and moveable, so other organs (mainly the GI) fills in the space. There are only a few places in the body where space is ""held"" open (mainly the ribcage and the skull), and those can be filled with different things depending on the pathology.",0
"The 0.01% form spores or slime layers that are resistant to alcohols and detergents that are in the wipes. The purpose of the chemicals is to destroy bacterial cell walls. However, if the germ has a protective layer, it can remain on surfaces until it enters a more favorable environment. Not to mention, when the cleaners *do* kill bacteria, the cell remains still remain on surfaces after being broken apart, so there is still a possibility of triggering immune systems even though the bacteria can't actively grow.

C. diff is one particular organism that is rampant in hospitals for this reason (that and the patients are already ~~immuno-compromised~~ deprived of protective gut flora due to antibiotic therapy). That's why healthcare workers have to physically wash their hands by scrubbing for a period of time, and not just use hand sanitizers/cleaners.",0
"Yeah, I think lab-grown meat will replace ground meat, processed meat, and packaged foods long before we're making high-end steak.

The unfortunate thing is that that market is partially filled by the excess from high-end meat production (i.e. you can fulfill much of the demand for ground beef off of what's left over when you fulfill the demand for filet mignon, strip steak, ribeye, etc.) so it probably won't reduce agriculture demand by all that much until you *can* grow a steak.",0
"Cannabis can be detected in urine for an extended period compared to many other drugs due to its unique pharmacokinetics and the way it is metabolized by the body. The primary psychoactive compound in cannabis, delta-9-tetrahydrocannabinol (THC), is lipid-soluble, meaning it has a high affinity for fat cells. When THC is consumed, it is rapidly distributed throughout the body, including the brain and fatty tissues.",1
"Moreover, many pelagic fish have very long lifespans and mature very slowly. There have been several examples of species demographics falling dramatically within years of large scale industrial fishing beginning to target them because they needed a decade or more to reach sexual maturity. The fact that we catch species before having a complete understanding of their lifecycle is just insane. You wouldn’t expect to be able to sustainably « harvest » 30% of elephant populations every year. It’s very much the same for a number of fish species.                     
Edit for example: [example](https://www.int-res.com/articles/ab2013/18/b018p161.pdf).                
                                                                                         
Edit 2: damn, I was not expecting this comment to be that popular but thanks for the gold guys! I’m glad I could help spread this knowledge around.                                                                           
Edit 3: someone rightfully pointed out that this issue is not limited to pelagic species but may be even more pronounced in many non-pelagic ones. I only spoke specifically about the former because I had an example in mind and large scale industrial fishing does tend to happen offshore.",0
">Saltwater fish have very concentrated urine so as to retain as much water as possible

That means they have powerful kidneys, right?",0
"The surgeon claims that this will all be possible because he plans on using a super-sharp knife to cut the spinal cord cleanly. 

That's all well and good...but, axonal injury is still axonal injury. Wallerian degeneration (in which the nerve cell dies all the way back to the nerve body) will still occur. Nerves can regenerate after Wallerian degeneration, but they do so at a very slow rate. 

I can't imagine a way this would work out such that the patient wouldn't have to be on life support for at least 6 months, likely longer, *in the best case scenario*...notwithstanding the secondary infections he'd likely get which might kill him before anything good happens. 

Of course, the whole goal of this is to prove us naysayers wrong, and I, for one, would be amazingly excited to be proved wrong. ",0
"Hello! I'm a developmental biologist. Specifically, I study how the cells that become the brain and nervous system know exactly what to become.

TL;DR: cells use a variety of methods to figure out 'when' in development they are, and where they are. Most commonly, they count how many different choices have been made to identify 'when'. To identify 'where', certain cells act as landmarks, and give off chemical signals. The stronger the signal, the closer a cell is to the landmark. DNA contains specific instructions on how to interpret this combination of 'when' and 'where'. 

In detail:

Let's start with DNA. We often here DNA compared to a recipe book, with genes being recipes. I prefer to think of it as a choose-your-own-adventure. A zygote (which is what we call the cell made when an egg and sperm fuse) doesn't actually have access to the whole of its DNA. Most of it is packed tightly away. It does have access to 'chapter one', which mostly begins 'if no other chapters are open, start here'.

This cell will divide many times, and eventually change shape. In order for a cell to know what chapter to read, it needs to know two things: where in the zygote/embryo it is, and 'when' in development it is. Embryonic cells are extremely accurate at identifying this information. We don't know all the ways they do this yet. However, we know of some mechanisms.

First, there's the DNA-CYOA book itself. Every time a cell 'opens' a new chapter, with a new set of instructions, it packs away some of the old, no-longer-relevant chapters. It labels them extensively in the process, in case they're needed again. And when I say it packs the DNA away, I mean that literally. [This is a diagram](https://www.researchgate.net/profile/Kevin_Verstrepen/publication/51196608/figure/fig1/AS:276923784679429@1443035183356/Chromatin-structure-DNA-is-wrapped-around-a-histone-octamer-to-form-nucleosomes.png) of DNA in various packed and unpacked forms. At the top is the famous double helix. In reality, DNA only exists in this fully unpacked form while it's being read. Next, is 'tidied' up DNA. This is accessible, but not actively being read. Then as you go further down, it gets more and more packed away and inaccessible.

That's what it looks like when a DNA sequence is 'turned off' - it's literally filed and packed away. You may have heard of epigenetics. This is the collective term for the molecular tags that label DNA, and the molecular duct tape that keeps DNA packed up safely.

When a cell divides, it copies its epigenetic pattern out - and so the daughter cells have the same bits of the book packed away and labelled. Going back to the CYOA book, this is like a bookmark. This is one of the ways that a cell can tell 'when' it is in development. If it's at chapter 13, it knows that chapters 1-12 have already been read. 

As for 'where', let me introduce you to my favourite protein: Sonic Hedgehog, aka SHH (That's genuinely the name of the protein). SHH is what's known as a 'morphogen'. It's a signal that tells cells what to become. In SHH's case, it tells cells how far they are from a certain landmark.

It's used in a number of different contexts, but neural development is what I know best, so I'll use that as an example. Before your brain became a brain, it was a tube - called the neural tube. This runs along the whole back of the embryo, and eventually becomes the brain and spinal chord. In early development, it's the factory where the entire nervous system is made.

The cells on the very bottom of the neural tube make a ton of SHH. Other cells sense how much SHH there is around them by eating up the SHH that bumps into them at the right spots. This means that really near the bottom, there's a ton of SHH, while near the middle, there's very little. At the top, there's none at all. This is called a concentration gradient. Cells in the neural tube count how much SHH they eat, and then refer back to their DNA books. The instructions tell them to open certain chapters if they eat enough SHH, and close others. Cells also check with their neighbours, also through chemical signals, to see if they agree on decisions. The result is a very specific link between SHH concentration (and thus distance from the SHH producing cells) and cell identity. 

~~[This image](https://www.semanticscholar.org/paper/Dorsal-ventral-patterning-of-the-neural-tube%3A-a-of-Dr%C3%A9au-Mart%C3%AD/a73582b5b612510a71adeaa0045fa9527570bbaa/figure/0) shows that in practice~~. *edit: link is broken. Found the same image [here](https://www.slideshare.net/HarshaKumar2/dorsoventral-patterning-of-the-brain) and [this](https://onlinelibrary.wiley.com/doi/abs/10.1002/dneu.22015) is the original citation*. Cells with different identities produce different proteins. In this picture, those proteins have been dyed different colours, showing the pattern of cell identities in a cross section of the neural tube.

The graph on the left shows different genetic chapters, basically. Each of those horizontal stripes represents a different type of cell - a motor neuron, or a type of sensory neuron, or maybe an ~~dendrite~~ astrocyte. You'll see that they're turned 'on' at a specific distance from the bottom, and turned off further away. The combination of genetic chapters that are 'open' determines cell identity.

Finally, we have chemotaxis. When a cell is 'finished' with its time in the neural tube - ie. The factory, it will migrate out and go to where it is needed. It does this in a very similar manner to SHH signalling. Landmark cells emit a signal. Each landmark emits a different kind of signal. Cells with different identities have machinery that allows them to sense only very specific signals. They then follow the concentration gradient towards the source.",0
"> the key part of a pillow is not that it is fluffy, but that it allows your neck to rest in a natural position

The Royal Ontario Museum has a collection of ancient [Chinese stone pillows](https://www.google.ca/search?biw=1086&bih=736&tbm=isch&sa=1&ei=UQ6wWrrVAebojwSzjZj4Cw&q=stone+pillow+china&oq=stone+pillow+china&gs_l=psy-ab.3...1799.2906.0.3202.6.6.0.0.0.0.97.455.6.6.0....0...1c.1.64.psy-ab..0.4.318...0j0i30k1j0i5i30k1j0i24k1j0i8i30k1.0.JIF_xMr0hLg). I remember being shocked at the thought, but in practice it's totally correct. So long as my head has support I can fall asleep pretty easily.",0
"Spanish Flu was signficantly more deadly than any other known influenza. Where a ""normal"" flu rarely has a mortality >0.1% , that of Spanish Flu is estimated to have been > 2.5% - so significantly higher than Covid-19.

It also had a strange mortality curve. Nearly all influenza show a ""U curve"" when plotted against age. So high deaths among under 5s lowering as age increases - flattening out, then rising sharply with after 65.  Spanish flu showed a ""W"" curve - similar to a U curve , but with a ""hump"" peaking at 35. (one of the panics about early reports of 2009 Swine Flu was the number of younger people it killed)

With modern treatment, undoubtedly the Spansh Flu deaths would have been lower - one of the main causes of death was probably secondary bacterial infection as there were no antibiotics. That said, subsequent mutations were significantly less deadly, so something in the virus itself must have been responsible.

An educated guess would be that a modern Spanish flu would still be deadlier than Covid, and would likely still kill millions despite improved medical care.",0
"During pregnancy, the mother's immune system undergoes several adaptations to prevent it from attacking the fetus. One of the main mechanisms involved is the establishment of immunological tolerance. Firstly, the placenta, which is the organ connecting the mother and the fetus, acts as a physical barrier. It helps prevent direct contact between the maternal immune cells and the fetal cells, reducing the chances of an immune response.",1
"Hi, medic here! It depends. For the sake of the conversation, I'm going to assume you mean something like a run-of-the-mill, basic cut. You nicked yourself shaving, or your knife slipped a little when you were cutting veggies. Nothing that would require sutures. 

Typically, its best to wash the cut with antibacterial soap (like Dial) and warm water, that probably constitutes ""wiping it away"", but only to get any dirt/nasties out of the cut. You do this because when you DO hold pressure to stop the bleeding, you don't want to keep the trapped nasties in there. So technically, it's better to take a little of column a, a little of column b. After your cut has been wiped clean, put steady pressure on it until it clots, then put a layer of protectant (neosporin or the like) and a bandage/bandaid/gauze over it to help it heal. 

Now, if you're talking about a trauma situation, it's a little different. Say, for example, you're first responding to a motorcycle accident and this dude's calf is just wrecked to hell, blood everywhere, gravel everywhere inside the laceration- your best bet isn't to keep wiping the gash continuously, you need to apply direct pressure first and foremost to stop the bleeding. Wiping it will do nothing and not allow the blood to clot properly. Pop a tourniquet on too. (This includes keeping the nasties and gravel and all, unfortunately, the hospital will properly clean and suture when they arrive. Motorcycle dude has bigger issues than a little dirt and gravel right now.) You don't want to be exsanguinated just because you kept wiping blood. Wiping continuously doesn't really promote clotting factors, nor allows platelets to form to stop the blood. It's why you are always taught ""direct pressure"" any time there is blood leaking from a wound that shouldn't be there. 

TL;DR: when in doubt, keep pressure on it. 

I hope this makes sense and answers your question, I'm on maternity leave, pregnant AF, insomnia and heartburn have kept me up for over a day. (: 

Edit: holy shit RIP inbox, I wake up because my son decided to falcon punch my bladder after I could finally go to sleep (weird sensation from the inside out) and I have 100+ comments, replies, and messages. Obligatory thank you for the gold! Now, a few clarity points:

For my first basic scenario, I used a common example of what might happen at home/what people have at home to use. Water and really any kind of soap is fine, Dial soap is just what I've been taught to use if I have it, and many of my colleagues do too. (I HATE it though, but I'm also allergic to a lot of other cleansing agents and have to stick with it.) Personally, I don't think water temperature is a difference/makes a difference in the basic cut situation. I cut myself in the shower shaving the other day (big 'ol 32w pregnant belly over here) and I just let it run in the shower, cleaned with soap, but it didn't clot until after I got out and put pressure. EVERY PERSON'S CLOTTING FACTOR IS DIFFERENT, your mileage may vary from others. It takes me 20 minutes to stop a simple little cut like that, whereas my husband just washes and slaps a bandaid on it and forgets about it in three minutes. 

On using alcohol/sticking your finger in your mouth- as long as it's rubbing alcohol or peroxide, I don't see why you couldn't dab a bit on. We carry it in the ambulance in situations like this. (Since we don't exactly have a sink with soap and water.) If the drinking alcohol (high proof) you want to use is fresh/unopened, or ensure that nobody has drank from the bottle directly. I've never used drinking alcohol as a sanitizer, so I couldn't tell you. As for sticking your finger or cut in your mouth- I'm completely guilty of doing this too, but just keep in mind that human mouths are NASTY AF! We treat human bites wayyyy more aggressively than dog or animal bites (save for snakes, because venom). Human mouths are just... filthy. I learned how gross our mouths are and made a conscious effort to stop sticking my cut up hands/arms in my mouth. Google some human bites, and look at when they get infected. They aren't pleasant. After seeing some in the field and looking at the infections they harbor, I wouldn't ever consciously do it again- but I mean, you do you. Like I said, I'm guilty of it too, and I think it's a bit of a reflex if I'm honest. (So many people do it!) I'm too tired to do any link adding, so if someone wants to add it, I'll put it here. (:

ON TOURNIQUETS:

A little background for those that don't know me: before I got off the truck due to my little parasite I have affectionately decided to raise, I work in a huge metropolitan area with a combo critical care/911 system. I'm mainly on the CCT (critical care transport) side, meaning we transferred people from hospital to hospital, higher level of care, etc. Got to see some gnarly stuff. We also backflowed our 911 areas when they got busy, so I've seen some crap there too. 

Tourniquets are coming back into the mainstream. Using a tourniquet is simple, easy, and (in my opinion) should be part of a public safety course/first aid/CPR for civilians. You have HOURS (if tourniquet applied correctly) before you risk ANY harm to the limb. (And remember! Life > limb, regardless. So that argument is out.) 

You do not need to loosen it every fifteen minutes. That's the exact opposite of what you're trying to do. You do need to check for pulses in the extremity very frequently- if you feel one, readjust till there isn't one. Double tourniquets are a thing, I've had to do it on people before. You are restricting blood flow. A pulse means that the person has blood flow and they will continue to bleed out. Once you apply it, hospital staff need to be the ones removing it, not you. In my trauma scenario, I pictured a free flowing bloody calf that got ripped to shreds by a gravel road and is squirting blood that you can't stop with direct pressure alone. (Guess I should've said femoral, I wasn't thinking straight. Either way.) Put the tourniquet on the femoral artery, as high as you can, and cinch it tight. If that doesn't help, double tourniquet it. 

Tourniquets are a very underrated emergency medical device, in my opinion. We carry several on the ambulance. They're good use for GSWs, deep cuts, lacerations, arterial bleeding from pretty much anywhere. It's easier to remove a tourniquet than remove a limb. We might use them more than you think. Open fractures that are bleeding and you can't put pressure on because bone is protruding, etc. They're very useful and can be used for several situations. 

Another local EMS company near me actually does civilian first aid training, and they are doing a good job in removing public stigma around tourniquets in our huge metropolitan area. ""Stop the Bleed!"" is really great too. 

As for in-field debridement: if I have the chance to squirt some saline and remove the dirt from a wound (and the bleeding IS CONTROLLED) I do en route to the hospital. If they're so messed up that I got called because of a GSW or severe accident, I'm going to be a little too busy maintaining airway, keeping BP up, etc to do much. We'll be at the facility that can take their time to clean it properly versus my crude knock-up job when I really need to focus on doing other things. If I have an extra hand or some firefighters, I might ask them to help me if I have everything else under control. (S/O to my fire guys, you rock!) 

I hope I answered all the questions and cleared some stuff up in the edit. I will totally try to get back to all that commented! Baby boy is craving some tacos, now I gotta go make some tacos at 7am... my husband will be very confused when he wakes up, lol. ",0
"When you say no oxygen what you mean is no air, right?

The first Western scientist to conclude that there is a vacuum between Earth and Moon was apparently Otto von Guericke in 1650. He concluded this because he was able to construct a vacuum pump. A vacuum pump can suck the air out of a container such that there is a vacuum in the container. For a long time people thought it was impossible to create a vacuum. They thought there must always be something. And it is indeed not easy to create a vacuum but it's possible.

Now how did von Guericke conclude that there must be a vacuum in space from his vacuum pump? Well at this point people already knew about gravitation and they knew that air has some weight. If you combine these two facts you come to the conclusion that the air that is higher up puts pressure on the air below, because the air high above weighs something. This means that the air is denser near the ground because it is pressed tight. And this also means the air becomes less dense higher up.

In 1648, Florin Périer had investigated whether air is less dense up on a mountain. For that he used the recently invented barometer (from around 1640). And he found that indeed air is less dense up on a mountain.

So, combining the knowledge that vacuums are possible and the knowledge that air is thinner higher up, von Guericke concluded in 1650 that there is most likely a vacuum between Earth and Moon.",0
"HIV is a highly mutagenic virus - there are a handful of different COVID strains circulating around the world, but in HIV the virus continues to mutate readily after infection, leading to the presence of multiple strains per individual [(source)](https://link.springer.com/article/10.1007/BF00163230). This makes it a much more difficult task to come up with an effective vaccine for HIV.

As for the others, its more of an economic reason than a biological one. The scale of the COVID pandemic has caused governments and industry worldwide to pour money into the development of a vaccine -- and that level of demand just isn't there for RSV/EBV/etc. Would love for someone with more of a background in healthcare economics to chime in here.",0
"Although it sounds counter-intuitive, illusionism is gaining popularity. For example, philosopher Keith Frankish literally says he's a zombie. He claims that he doesn't *really* have consciousness. It's just an illusion.

Other philosophers strongly disagree with this. Galen Strawson called Frankish's argument ""the silliest claim ever made"", and ""a position so stupid only a philosopher could hold it"" (paraphrase).

Regarding Graziano, I don't think he would describe himself as a zombie, but he doesn't believe there is a hard problem either (he doesn't think there's any subjective experience that needs explaining).",0
"Actually the visible light spectrum is the only wavelengths that can effectively penetrate liquid water and as our ancestors first developed eyes in water we are stuck with eyes that can only see in those wavelengths.

Also only average stars output light in the visible spectrum. Larger stars output in the upper em bandwidth and small stars output mostly radio.",0
"In general, simply pressing your fingers together is unlikely to kill bacteria. Bacteria are microscopic organisms that can withstand a significant amount of mechanical force. However, the exact outcome depends on various factors, including the specific bacteria involved, the amount of force applied, and the duration of the pressure. In daily life, mechanical forces can have different interactions with microorganisms.",1
"It’ll kill some of the bacteria it comes in contact with, but no it won’t sanitize your mouth unless you intentionally swish it around for an extended period (like Listerine). Even then, it won’t kill all the bacteria in your mouth because there are so many nooks and crannies that will protect whatever bacteria are lodged there.",0
">Why does this only happen with Rh antigens/antibodies?

It *can* happen with anti-A/B antibodies, but it's rare.  Mostly, anti-A/B antibodies are IgM type, which is too large to pass through the placenta, and fetal red blood cells express fewer AB antigens anyway.  O blood types are more likely to have IgG type anti-A/B antibodies, and other blood types can have IgG type anti-A/B antibodies; sometimes due to exposure later in life.

Ironically, rh-type hemolytic disease of the newborn can be prevented by having an A/B mismatch as well - the A/B antibodies kill the stray blood cells before the mother can have an immune response to the rh.",0
"Is severing the umbilical cord still a natural instinct for humans? 

How was it likely done in early homo erectus history?

Also, another weird question: how does the placenta detach from the body. There is a major blood connection there, right? ",0
"Absolutely. Surprisingly, I learned a lot about this during my plastic surgery rotation. I got to participate in a few liposuctions. Fat is stored in cells, special cells called adipocytes, like any cell they require the importing and exporting of nutrients and metabolites etc. This means there needs to be vascularity. During liposuction you can only take out about 10 pounds of fat, the reason for this limit is actually blood loss. Back in the day the reason people would die during lipo procedures was because they would take out too much fat, which would cause them to lose too much blood and they would go into shock. Adipose tissue is a living part of you, it produces hormones and shit and it's gotta have blood supply.",0
"Surprised more people don't know this story. Sildenafil was first studied for treatment of pulmonary hypertension (high blood pressure in the lungs specifically). There is a [phosphodiesterace receptor](https://en.wikipedia.org/wiki/Phosphodiesterase) that was found to be very specific to lung tissue called PDE-5. Viagra inhibited this receptor resulted in a marked decrease in pulmonary blood pressure (hooray!). However, it was noted during this study that most of the men were reporting the same side effect. Anyway, long story short Pfizer then proceeded to make infinite billions of dollars.

So, reduced pulmonary blood pressure is the main known effect that occurs in both sexes. However, there is ongoing research on a myriad of different applications for PDE-5 inhibitors including female sexual arousal disorder, raynaud's phenomenon, and heart failure. I think the best answer to this question is that the amount of money made off the erectile effects of the drug caused most to be disinterested in further study for years, so we aren't 100% sure yet.",0
"What if, leading up to the liposuction, you drained a few pints from the patient?  Like they get their own blood back while having it sucked out.   

Also I thought this is what plasma was for but honestly I'm not really up to speed on how any of this works.

Edit:  thank you for the responses.  I was expecting some half-assed responses but got more than that.   A lot of new things to read about and learn!",0
"In short, adrenaline and cortisol. If you haven't eaten in a long time, your body doesn't relax, it instead increases stress hormones which induce the metabolism of energy stores (ie, fat tissue, glycogen) and help mobilize you so that you can find food or fight for it. 

Once you eat, stress hormones decrease, blood flow to the intestines and visceral organs is restored, and you feel hungry again (this is why appetizers induce appetite).   ",0
"One more answer which hasn't yet been mentioned.

Pee and poo were separate from the beginning. It was only much later, when vertebrates started to move to live on dry land that the exit hole for these two were united. Even in birds the organ systems that create these two are still separate, only the last leg is shared (cloaca).

Poo was invented when regurgitating ingested food wastes was no longer found efficient way of disposal. Pee on the other hand is a solution to the problem of getting rid of excess salt and ammonia from within the body. If you look at different phyla of animals you'll find very different systems for the pee problem.",0
"It does, in fact release energy, though not that much on our scales. 

""splitting the atom"" causes an explosion because you're not splitting one, you're making chain reaction that causes many many atoms to fission. For example, the plutonium core in the Fat Man weighed about 6kg, and about 15% of that core underwent fission. That's about a kilogram, and with fissile Plutoniums molecular mass of 239, that's about 4.2 moles, or 2.51x10^23 atoms.",0
"Dolphins are thought to commit suicide by not coming up for air.  This happened to Kathy, one of the dolphins who played Flipper in the '60s movie, because she was kept alone in a tank with no other dolphins.  It is thought that she became depressed in isolation and chose to kill herself.  There was also a case of a dolphin named Peter who was separated from his longtime handler and became depressed, then stopped coming up for air.

Although of course we can't know what an animal's motivations are, it seems plausible that a highly social and intelligent animal like a dolphin could choose intentionally to not come up for air.  In Kathy's case, it seemed very deliberate.  She swam into a trainer's lap, looked the trainer in the eye, and then dove to the bottom of the tank and laid on the bottom until she suffocated. ",0
"I think the core of your question has been answered, but I wanted to add one point.  Sometimes various screws are used for security reasons.  For instance on the products my company sells there are some screws we intend for the customer to undo, and we will use hex, slotted, or Phillips for these.  Other screws we don't want them messing with and may use Torx or something more rare.

In fact some companies specialize in making strange screw head shapes purely for security reasons, [as seen in this chart.](http://cdn.clm02.com/ezvivi.com/204050/6359362590222142198612765.jpg)",0
"This is the most correct answer. I don't know what the hell everyone else is on about. The hot water rapidly warms the air around it, causing the air to movie upward and out of the shower space. Colder air then rushes in, pushing the curtain with it.",0
"Women astronauts have to manage their periods while in space, just like they would on Earth. The lack of gravity does not stop menstruation from occurring. However, there are a few challenges that they may face in dealing with periods in a microgravity environment. Firstly, the lack of gravity can affect the flow of blood during menstruation. On Earth, gravity helps pull the blood down, but in space, it can float around and potentially create a mess.",1
"Alcohol/soap actually [destroy](https://www.healthline.com/health/does-alcohol-kill-germs#can-it-kill-germs) the cell walls:

'Alcohol kills germs through a simple chemical process known as denaturation.

Denaturation occurs when alcohol molecules bond with the fat membrane encasing a virus or bacteria cell. As the fat membrane is broken down, the inside of the cell — including all of its critical components — becomes exposed. It starts to dissolve, and the cell quickly dies.

This process is similar to what happens when you wash your hands with soap and water; however, soap is even more effective than alcohol.'",0
"You've got some good answers here already, but they're all leaving out an important aspect, which is how the screw and screwdriver deal with fouling. Dirt, oil, weld slag, multiple layers of paint, whatever. If you're in an environment where you don't have to worry about that, a complex geometry is fine. But on a factory floor, Phillips or torx can get irreversibly fouled. Allen head screws can be relatively easily cleaned, but the master of this is the shittiest of all screw heads, the flat head. The *only* tool you need to clear the slot of a flat head screw is the screwdriver you're going to use to unscrew it. No other screw type has that ability.",0
"Two tall trees, a birch and a beech, are growing in the woods. A small tree begins to grow between them, and the beech says to the birch, ""Is that a son of a beech or a son of a birch?""

The birch says he cannot tell. Just then a woodpecker lands on the sapling. The birch says, ""Woodpecker, you are a tree expert. Can you tell if that is a son of a beech or a son of a birch?""



The woodpecker takes a taste of the small tree. He replies, ""It is neither a son of a beech nor a son of a birch. It is, however, the best piece of ash I have ever put my pecker in.""",0
"Interesting, I looked it up just to be sure and you're right (at least according to google).  But no, fat isn't liquid in the duck.  You can see [here](https://www.youtube.com/watch?v=LeHrhQXmeq8)(Warning: duck being cleaned after being shot) for example.  You can see that there is solid yellow fat around the legs and other portions of the duck.  I don't know how the fat is processed before being sold and how it would change at a molecular level, but presumably it is somehow [rendered](http://www.dartagnan.com/how-to-render-duck-fat.html) to make it safer for storage.  So the fat used for cooking isn't the same thing you'd find inside the duck.",0
"The difference in power between the Fat Man and Little Boy atomic bombs can be attributed to their respective designs and the types of nuclear reactions they employed. The Fat Man bomb, dropped on Nagasaki, utilized an implosion design. It consisted of a spherical core of plutonium-239 surrounded by an arrangement of conventional explosives. When detonated, these explosives compressed the plutonium core, causing it to reach a critical mass and initiate a chain reaction.",1
"Agree, from the outside might looks like, because they change ""skin"" (exoskeleton) and for the naked eye it looks like two spiders are living in the same web, or one spider took over the dead spider web... Their lifespan is pretty long too (?)",0
"The reason for the different styles is cost and torque. The slotted head screws are cheap and easy to make. But they're completely useless for powered screwdrivers and you can't put much torque on the screw without it either slipping out or stripping the head (and maring the surface of whatever you're screwing). Phillips screws are self-centering, making powered screwdrivers possible. They're somewhat more expensive to produce than slotted-head. They tend to 'cam-out' easily under torque, making it hard to apply much torque. I've heard they were designed that way to prevent overtightning. However, it's not good for exposed fasteners to look stripped. Robertson-head and allen-head fasteners can handle more torque than phillips-head fasteners, but are more expensive. Because the bottom of the hole is flat (unlike the pointed end of the phillips), there's more contact area and so it's less likely to cam-out. The robertson-head is cheaper than the allen-head, but the allen-head has six points of contact rather than 4, making it less prone to rounding out the hole. The Torx-head fasteners solve the problem of rounding/stripping by having the flat bottom of the robertson/allen that reduces cam-out, but it has much better contact with the driving bit to prevent stripping the head. The points of the 'star' on the driving bit engage the recesses on the screw at nearly right angles, so it has a very positive contact. Torx is becoming more and more popular because of that, particularly in assembly-line work. Because they're less likely than a phillips to be damaged when tightening, the allen (internal hex) heads are often used for exposed ('decorative') fasteners on 'some assembly required' furniture. It's also very cheap to make the allen keys, so they usually include one with the fasteners.
",0
"Awww nobody will see this since I'm late to the party... but I work in heart surgery and can tell you how we get air out of the heart after we open it. It's also important to know that air in the arterial system and left side of the heart is waaaaay more dangerous than heart in the venous system and right heart. This is because the air goes to the lungs before it goes to the left heart. Once in the aorta, air can enter the coronary arteries that supply blood to the heart and the brain. In the heart, air can cause ventricular fibrillation and in the brain it can cause a stroke.

So to keep this from happening we utilize the clamp that is placed across the aorta (cross clamp) and vents that are inserted in the left ventricle (the chamber that pumps blood to the body) and the aortic root (where the aorta exits the heart). These vents are connected to pumps on the cardiopulmonary bypass pump (heart/lung machine). Before the cross clamp comes off the aorta, the perfusionist (person running the heart/lung machine) will fill the heart with blood. This causes the heart to contract and pushes blood out through the vents. If there is air, it hopefully exits the heart through the vents with the blood. At the same time, an anesthesiologist or cardiologist is looking at an echocardiogram, which allows them to see the air in the heart and aorta. The patient will be placed in a head up position so that the air will rise, and if there is a large amount of air, the surgeon may shake the patient. When this is done, the cross clamp can be taken off and the heart hopefully begins to resume normal function.

Edit: a word

Edit 2: obligatory thanks anonymous user for my first Reddit gold!",0
"They never work even that well. There's a temporal aspect to it as well, how long the exposure to disinfectant actually is. That quick spirt of purel is just a security blanket, possibly a net negative if you're taking a squirt from a pump dispenser in a public place everyone is touching.

As a parallel to illustrate the dynamic, in cooking you're supposed to heat chicken to 180 degrees to make it safe, but in sous vide you only heat the chicken to 150 degrees. Why?

Because at 150 it takes around an hour to kill off enough of the microbes in chicken to make it safe, vs almost instantly once the meat reaches 180.

 First experiment they had us do in microbiology lab during undergrad was called ""the ubiquity of microorganisms"". In it we stamped a media plate with our thumb, washed our hands and stamped again, did a full surgical scrub and stamped, then dunked our thumb in 70% etoh for a minute then stamped.

There was growth from all 4 thumbprints, for every student in the class. But the amount of growth was a clear progression with how intense the sanitation step was.

As someone who works in a biohazard lab, aside from physical PPE like disposable gloves and gowns olde-fashioned handwashing with soap and water is how we decon ourselves, and the mechanical action of washing our skin is doing more than any antimicrobial properties in the liquid soap.",0
"I was interested about this too so I looked it up and was surprised at how lacking the research was. This means that some of my points are educated guesses based on the mechanism of action rather than on published research specific to this question. 

Viagra inhibits an enzyme called cGMP-specific phosphodiseterase 5 (PDE5). This enzymes job is to break down a molecule called cGMP meaning that inhibiting PDE5 increases cGMP levels. It’s not important to understand that, just know that basically more cGMP means muscles become relaxed.

Similar to how it works in men, viagra increases blood flow to the genitalia by relaxing the blood vessels. In women this causes engorgement of the clitoris and labia minora. This increased blood flow will also likely result in increased lubrication. 

The other main place where PDE5 is found is in the retina. A common side effect of viagra is changes in vision, this can occur in both women and men. However, because it tends to be associated with higher dosages it may be more likely to occur in women. This is because women generally have a smaller body mass than men

PDE5 is also found in lower levels in platelets and vascular/visceral smooth muscle. This means that it causes relaxation of the blood vessels in most of the body although it is less significant than in the penis/retina. This can result in decreased blood pressure but the effects are not generally dangerous in otherwise healthy people. 
Its effect on platelets means that viagra can also reduce platelet aggregation which would impact blood clotting (also unlikely to be dangerous if you’re otherwise healthy though)

Edit: Seems to be some confusion in the comments and for whatever reason I seem to be getting a lot of attention so I thought I’d add it. Sildenafil (viagra) was originally studied for use in treating hypertension and angina pectoris. Phase I clinical trials found it didn’t have much effect on angina but it did cause marked changes in erections for the male participants. After this & the subsequent phases Pfizer decided to market it for erectile dysfunction instead.",0
"No a simple air bubble won’t kill you, either in the muscle or even in the IV. 

If somebody grabs a huge syringe and fill it completely with air and inject it in your veins, it may cause problems. But small air bubbles get commonly injected with no side effects",0
"There are generally a few reasons.  One of the biggest being that higher altitude means thinner atmosphere and less resistance on the plane. 

There's also the fact that terrain is marked by sea level and some terrains may be much higher above sea level than the takeoff strip and they need to be able to clear those with a lot of room left over.

Lastly, another good reason is simply because they need to be above things like insects and most types of birds.

Because of the lower resistance, at higher altitudes, the plane can almost come down to an idle and stay elevated and moving so it also helps a lot with efficiency. 

Edit:  Forgot to mention that weather plays its part as well since planes don't have to worry about getting caught up in the lower atmosphere where things like rain clouds and such form.",0
"1.) Saltwater fish have very concentrated urine so as to retain as much water as possible
2.) Bony fish have special cells in their gills which will pump chloride ions up their concentration gradient and out of the body
3.) Elasmobranchs (Sharks/Rays) maintain high levels of salt in their body in the form of TMAO and Urea, this keeps them isotonic with the surrounding water
4.) Some sharks (e.g. Squalus sp.) have special glands in their rectum which will secrete salts

There are probably other strategies as well",0
"It was so good, it worked to disincentivize continued production of high sulfur coal (except for export to China). When Carbon cap and trade was proposed to put the same logic on fossil fuels for global warming, the oil industry got into high gear to stop that shit. They'd seen the writing on the wall. ",0
"It's most comparable to SARS which was another coronavirus. Currently, it's less deadly than SARS but more contagious. It also seems to be pretty stable in humans, meaning not a lot of mutation has been observed (so hopefully it won't mutate into something more lethal).

Zika doesn't spread person-to-person, so it's not really comparable. edit: Zika can spread person-to-person through sex, but it's mainly through mosquitoes.

Ebola is not very contagious but can be very deadly.",0
"A slightly more mathematical explanation is that the kinetic energy of an object can be stated as K.E = 1/2 mv^2  where m is mass and v is velocity(speed) to the power of 2. This means that if you're accelerating from speed 1 to speed 2 the difference in kinetic energy between those speeds is based on mass. A really tiny mass gives a really small K.E even though the speed is squared.

This in turn means that the amount of kinetic energy being required to accelerate from speed 1 to speed 2 is really really small which means that the impact the insect feels is also really really small.

On top of this because a large drop of water will have a relatively large mass compared to the insect it will have a large K.E, which means that the insect only absorbs a tiny tiny portion of the total energy in the water droplet so the droplet in itself slows down very little.


In the example the other poster gave that's why a train going super fast hits a hair and doesn't slow down at all and the hair isn't damaged.

The actual ""impact"" the insect feels is better explained by Force but that's based on harder to explain principles but they work on the same concept of a tiny mass negating the force felt. (Force = mass x acceleration)",0
"Social species learn to differentiate between members of their own species.  It's pretty rare when one species can do that with another species.  Some animals may sound all the same to us but sound unique to individuals within that species. I'm willing to bet that, if we could talk to birds, humans would sound all the same to them.  

An interesting counter-example of that has been dubbed the ""Crow Paradox"".  You can raise a crow from the time it's a baby until adult but as soon as you set it free and it joins a murder of crows, you won't be able to tell it apart from other crows.  But, for some reason, crows are really good at telling humans apart.  Like scary good.  And no one really knows why.  

[https://www.npr.org/sections/krulwich/2009/07/27/106826971/the-crow-paradox](https://www.npr.org/sections/krulwich/2009/07/27/106826971/the-crow-paradox)",0
"Having not really looked at much of the literature, but mostly from treating COVID pts, the htn and diabetes seem to be major factors and this is likely due to clotting issues which make this virus so damn hard to treat in specific populations. But you’re exactly right, those issues just happen to be connected with being overweight. It seems to be causing ARDS in the lungs but not in the way that we normally see so our methods for treatment have not been super successful. Vent management (I’m an RT) is the most troubling part of this whole equation. In March we were seeing like 70% mortality in patients that were intubated. Things have gotten much better now but we’re still seeing a lot of the same issues in vent management",0
"We often call the process of ""speaking"" a language [language production](https://en.wikipedia.org/wiki/Language_production). It's way more than just speaking.

The first thing to keep in mind is that using language keeps your brain very busy. If you ever thought that the whole ""10% of your brain"" thing had even conditional truth to it, go take a look at fMRIs of people doing simple linguistic tasks — real talking uses most parts of your brain, as you're generally engaging spatial, quantitative, emotional, and all sorts of other reasoning on the fly as you're conjuring up words and orchestrating your various meat-flaps to represent them.

When you're understanding a language, the number of steps you have to do is cut down dramatically. Your ear picks up acoustic signal; you bucket into phonemes, which if you're proficient is going to be sensitive to the language you're listening for; you build those sound-idea strings into morphological segments, which you gradually slot into phrases and sentences; and then, in normal listening, as you *infer* enough, you're pretty much done—you get the idea and it's just refining. There's time in there to do some quick spot-translations, to do some on-the-fly word order swaps, and other touch-up work, especially if the speaker isn't going at all-out max speed at you.

Contrast that to language production. Hoo boy. First you have an idea; Sapir-Wharf debates notwithstanding, the very early concepts of what you're trying to say are at least *unspecialized* to a language's specific quirks. From the idea, you have to create a sentence structure around it. Maybe you normally use SOV ordering but you're trying to make an SVO sentence. That's a cognitive overhead tax on you. Then you start trying to find the right words. More overhead. Now you're trying to recall the right sounds to go along with it; more tax. Finally, while you're generally still trying to juggle all of that, you're trying to orchestrate those meat-flaps to make the unfamiliar sounds. More tax. It adds up! Without practice, it's just too much to do at full speed. And practice, in the meaningful sense, is an extremely contextual thing; replying to the questions and answers fixed to a topic in your textbook is going to only have a small crossover with the highly-inferred, highly chaotic banter you get in a real, typical conversation.

Interestingly enough, there's an extra sociolinguistic aspect that kicks in where the self-awareness of speaking ""indirectly"" levies yet more of a cognitive hit, which helps explain why some people will objectively speak a language better when they're a bit drunk.

So, tl;Dr? ""Speaking"" is a whole lot more than speaking, and it's hard work until you've done a lot of it in very representative environments—listening has many fewer and much more predictable difficulty variables in the speed and complexity, while production is just way more nuanced and complicated.
",0
"You can definitely overwater some of the columnar cacti and cause them to get too ""fat"" and split. Most cactus species will just rot and die if they get overwatered, but some will try to store it, get too fat, and split between the ribs. I've seen saguaro and Mexican fencepost do it.",0
"Genital warts are caused by hpv, which is a DNA virus, so similar to herpes.",0
"Mechanic here. The whole 3000 miles / every three months is now a myth used to sell more oil. Back when cars didn't have oil filters you had to change it every 500 to 1000 miles, later filters became a standard feature on engines, but because the motor oil of those days was... simple (read shit), you had to change it frequently. Modern oil has advanced leaps and bounds over the early days of motoring, and you can say it's high tech. Conventional motor oil can easily last 7500 miles or longer and synthetic oils can easily cover 10000 to 15000 miles or more. Not just that but a quality filter can withstand at least 10000 miles if not 20000, safely too. 

I also see a lot of people saying that the sole job of oil is to lubricate. That's simply not true. The oil in your engine lubricates, yes, but it also regulates temperature, cleans the motor, seals the motor, and provides corrosion protection. 

Bonus fun fact: Old synthetic oil used to leak because the molecules are much smaller in synthetic oils and unlike regular oil, it didn't saturate the seals, letting them dry up, and break/crack causing the leaks further. Modern synthetic oils contain seal conditioning additives so it simply isn't an issue any more. You can also go from synthetic to conventional and back, or mix and match with no issue - that is unless your car requires synthetic oil, in which case DO NOT put regular oil in it.  

EDIT 2 - u/logicblocks pointed out that I didn't explain what happens to the oil. That's my bad. 

SO, what happens to the oil when it reaches its life expectancy, be it 3K or 30K Well it's not the oil that goes bad, it's the additives. The additives break down faster than the actual oil. The tricky part is that it is the additives that extend the life of the oil. The additives break down, they no longer keep the oil viscous and 'slippery.' The lubricant part is simple enough. The oil stops being an effective lubricant. The viscosity is a bit more complicated. As the oil gets 'used up' it no longer maintains the viscosity required by the engine. Most engines have a range of use, such as summer and winter oil. As it breaks down, oil thins out, meaning it no longer moves through the engine at the required pressure to ensure proper lubrication. If the oil is not used up, but old, it thickens up and effectively becomes grease, which your oil pump would struggle to push said clumped up oil, burns out, no oil anywhere, good bye engine. If your oil pump is an absolute badass and pushes the thickened up oil into the valve train, shit goes south in a hurry, too. To sum up, you want your oil to be flowing at a specific rate to ensure that it goes everywhere. Too thin, it moves too fast, it doesn't stick to surfaces and it doesn't do much - you might as well be running water. Too thick and you add unnecessary stress to the engine, ruining the fine tolerances of the motor. 

EDIT - Some people pointed out about burning oil and pre-existing leaks. One VERY important detail about going longer than your 'dealership' interval... CHECK YOUR OIL LEVEL!!! Especially with aging cars, it is NEVER a good idea to fire and forget. The one big advice I can give to anyone of any skill level. KEEP UP WITH YOUR MAINTAINANCE !!!! You can check your oil level, your tire pressure, and other minor things that will keep your car running for much longer.

I may make my living working on cars, but I care about cars more people at times, so it's not fun when I see car that hasn't seen the most basic of care.

Gold edit: Thank you for the gold! I like helping people with whatever knowledge I have, but the gold is nice. Thank you. 

Also I now understand the RIP inbox thing. I'll try to reply as best as I can to questions and concerns.",0
"Animal fat is not just chunks of lipid lying around but connective tissue composed mostly of lipocytes, or fat cells. These specialized cells store the actual lipid molecules as small droplets. You could similarly ask why all the water in animals (70-80% of body weight) is not just sloshing around :)",0
"One that no one is mentioning is potentially the most likely and damaging.  BGP is the protocol that handles routing on the internet and is what enables the internet to be decentralized.  BGP is largely trust based, and there have been cases of companies saying they “own” IPs that they do not.  There have been several instances of countries trying to censor sites like YouTube. Generally this is done by “black holing” IP subnets. So for example, in that country, all traffic destined to You Tube would simply be discarded and your request would never make it to YouTube.  Since BGP propogates routes automatically and is latgely trust based, there have been times where these “null routes” escape from the country they are meant for, and impact global traffic.

There are of course many mitigations to this, but its conceivable that a specially crafted BGP hijack could significantly disrupt global traffic (as has already happened several times over the years).  I would definitely say BGP is right now the achilles hell of the internet, much more so than DNS (its just that many non-networking folks have likely never heard of it, while many people are aware of DNS)

Speaking of DNS, another risk to worry about is a DNS hijack(which are generally much less impactful than BGP hijacks), discussed in some other posts.  We are starting to see more of these schemes (sometimes in conjunction with a BGP hijack to point endusers DNS traffic to nefarious servers), and sometimes these schemes are designed to steal cryptocurrency.  As there is money in this, I would expect to see more and more of these types of attacks, especially if crypto prices go back up.

See more [here](https://en.m.wikipedia.org/wiki/BGP_hijacking) ",0
"Yeah, but a real device necessarily would carry some coefficient of fart drag, which would decrease efficiency. ",0
"In addition, many antibiotics are bacteriostatic - that is, they it don't kill them, but just stop their replication. For example, tetracycline, sulfonamides, macrolides. If you stop them early, well, time to start replicating again!",0
"At first, I thought you meant a sausage with lab-grown lean tissue but with animal-sourced fat.  
Then I realized that what you mean is a lab growing lean muscle tissue in this vat and fat tissue in that vat. Mix the two together and you've got something.  
I like this thought. Everyone's obsessing over getting a steak, but that's a hell of a lot harder than getting a hot dog.",0
"Boiling water is an effective method for disinfection and can help kill or inactivate many types of microorganisms, including bacteria, viruses, and parasites. When water is heated to its boiling point (usually 100 degrees Celsius or 212 degrees Fahrenheit), the high temperature kills most pathogens that may be present. Boiling water works by denaturing and destroying the structural proteins and enzymes that microorganisms rely on for their survival.",1
"What first comes to mind are the [millenium problems](https://en.wikipedia.org/wiki/Millennium_Prize_Problems): 7 problems formalized in 2000, each of which has very large consiquences and a 1 million dollar bounty for being solved. Only 1 has been solved.  


Only one I'm remotely qualified to talk about is the Navier-Stokes equation. Basically it's a set of equations which describe how fluids (air, water, etc) move, that's it. The set of equations is incomplete. We currently have approximations for the equations and can brute force some good-enough solutions with computers, but fundamentally we don't have a complete model for how fluids move. It's part of why weather predictions can suck, and the field of aerodynamics is so complicated.",0
"The big fuss is that when people say ""radiation"" they are conflating anything that emits/radiates energy (i.e. anything but the cold vacuum of space) with ""ionizing radiation""  - x-rays and gamma rays. The normal stuff like light, infrared, UV, radio is so common and harmless, we don't think of it as radiation, except when speaking scientifically. 

The reason ionizing radiation is dangerous is that high concentrations of ionizing radiation are so powerful they penetrate all but the most dense matter (ex. lead). Ionizing radiation has so much energy, when it's traveling through matter, it smashes through it, breaking apart molecular bonds. When these molecular bonds are in your DNA, your DNA can get messed up and that cell in you body won't function properly any more. A few cells here and there, your body can handle, the cells self-destruct or are otherwise cleaned up. But if too many get messed up DNA, they get out of control, these cells run amok. We call that cancer.

[Also, here's a handy chart from XKCD explaining the scale and levels of dangerous ionizing radiation.](https://xkcd.com/radiation/)",0
"The easy answer is no. If you mean combustion (or burning) of the bread, then there would be less calories because once combustion occurs (even partial) the byproducts are either indigestible or barely so. 

If you mean dark toast, the kind you might get at 6 on the toaster, it has the same calories. The Maillard reaction is what drives browning and it is a complex process where proteins denature and bind to other proteins as well as carbohydrates and so forth created an amalgam of mixed molecules. Essentially this is what leads to that caramel/nuttiness you get when things are browned. However, this conformational change and denaturation does not decrease the calories because the overall building blocks are the same and still digestible.

However, if let’s say a byproduct of a Maillard reaction is an indigestible molecule that was previously digestible, you could argue that it is now lower in caloric value because it is no longer bioavailable energy. 

Side note, a lot of people are talking about measuring calories by using a bomb calorimeter aka burning the item. This is no longer the method used for finding caloric value of food. Instead they find the net average of Atwater bioavailable nutrients and then use standardized values (e.g. 4 Kcal/g for Carbohydrates) to calculate the assumed caloric value. Again, this is obviously dependent on bioavailable sources of energy, not overall stored energy.

A perfect example of how a bomb calorimeter is not a feasible option, is Lettuce. Excluding the water (which is 95% of the material) lettuce is primarily fiber. Insoluble fiber in this case or in other words fiber we cannot breakdown (Cellulose). This material has no caloric value to us because it is not bioavailable (aside from small amounts created by gut fermentation thanks to helpful bacteria). So a piece of lettuce has a net caloric value of basically 0 in the Atwater system. In a bomb calorimeter however, it might have a much higher value because inside each of those cellulose walled cells is stored sugars, proteins, and so forth. Additionally, cellulose is essentially a starch made up of Beta-Glucose, however Beta-glucose is in a different conformation than Alpha-Glucose in starches we digest which means it is incompatible with our enzymes. However, combustion wise, cellulose and amylose (Alpha-glucose polysaccharide aka starch to most people) are equivalent in “Calories” in the context of a bomb calorimeter.  

Again, this is not the case in bioavailability. The only animals that can actually get the full caloric potential from plant material are foregut fermenters and hindgut fermenters, aka Cows and Horses. This is why they need multiple stomachs or a large cecum, in order to host helpful microorganisms to breakdown cellulose. Even Termites are not able to digest cellulose, but usually carry symbiotic organisms that can. 

https://www.scientificamerican.com/article/how-do-food-manufacturers/

Addl. Note: /u/chuggsipas pointed out the fact that to be totally accurate about this discussion we have to really highlight that a Calorie at its base definition is the amount of energy required in order to raise one gram of water, by one degree Celsius. It’s important to distinguish this because while I do mention that a bomb calorimeter is not used for nutritional labeling values, it is the correct way to calculate calories in its true context. Another thing chugg brought up, and I absolutely agree with, is the fact that nutritional calories are a terrible measure of how our body uses energy. We do not just ingest and combust whatever is bioavailable, there are a multitude of processes that are dedicated to metabolism, storage, availability, etc that are not taken into account by flat caloric values. In fact evidence builds every year that quality of foods and caloric sources are more important than the overall calorie value. However, on some very basic level you can get a vague idea of your energy intake with the Atwater calorie system. 

Edit: Added some clarification in regards to glucose in Cellulose.

Edit2: Fucked up and did L/D-Glucose instead of Alpha/Beta. Corrected that :X 

Edit 3: Just wanted to say thank you to anyone who challenged or questioned anything I wrote. I definitely needed to add some information and make changes here and there. I appreciate it, especially since that’s what healthy discussion is about, and no one can be 100% correct, 100% of the time without some input from others! ",0
"We use moles instead of mass since it accurately shows how many molecules of a substance we have. The chemistry behind reactions is dependent on the number of molecules present, not their mass. 
To put more simply, it's more important to know many ingredients you have for making a hamburger, then it is to know how much the ingredients weigh. It's more important to have two buns instead of just knowing you have 100g of buns.

Edit: Forgot to mention that the OPs question is not stupid, and is completely reasonable. As some others pointed it, it would be a good opportunity for the teacher to emphasize the importance of moles vs mass.",0
"OK, 5 things need to be separated:

1.  Temperature is temperature.  It's a description of how hot it is.  Its thermal energy.  It's not describing where the heat came from.  Insulating it more would increase its temperature, but that wouldn't change the rate of heat from nuclear activity.

2.  Spontaneous radioactive decay is where an unstable isotope is undergoing spontaneous decay via half-life alone.  This always generates heat, but too little to use for power generation, and it's uncontrollable by definition, as it's spontaneous.  Some fission byproducts have very long half-lives.  Many also are only part of  a ""decay chain"" as their new child isotopes may also be unstable and have a half-life.  This is unlike fire in that letting the heat from spontaneous decay build up and raise the temperature does NOT increase the rate of spontaneous decay.

3.  Fission nuclear REACTIONS mean a neutron from one spontaneous decay, or from another fission event, struck another atomic nucleus of a fuel isotope in just the right way to cause a ""neutron capture"" and cause fission in that atom, which also spits out one or more neutrons which might be able to cause another fission reaction.  Most fission byproducts CANNOT participate in a reaction however, only the original fuel (uranium).  Not all radiation is neutrons.  Alpha, beta, and gamma can result from spontaneous decay or from fission, they are dangerous radiation and create heat, but cannot cause a fission *reaction* at any intensity.  FUSION reactions (helium, tritium) are different than fission and temperature matters, but it does not occur in any commercial reactor and would not occur in any ""meltdown"" situation of any sort.

4.  Fission ""critical mass"" means that the overall scenario is such that the neutron(s) emitted by a fuel nuclei undergoing fission will, on average, cause at least one fission of another fuel atom.  This requires a fuel isotope to be able to create more than one neutron upon fission to be possible to sustain itself as some neutrons will always be lost.  This is not fire, and high temperature is not necessary nor does higher temp increase fission reactions.  The layout is very important.  Not only is packing a lot of fuel in close proximity essential, but neutrons are often ""too fast"" to capture and cause fission, thus they must be slowed down by a moderator or they will go right past the fuel atoms and fail to cause a reaction.

5.  A nuclear EXPLOSION is a critical mass operating at an exponential increase in neutrons causing fission reactions that the system cannot possibly contain, and its rise is almost instantaneous.  The ""blowing apart"" aspect is due to pressure created by its fantastically high temperature.  Spontaneous decay (#2) cannot cause a nuclear explosion, as there is no exponentially growing reaction, and it's by definition not a reaction.

Chernobyl will always be undergoing lots of spontaneous decay of its fission byproducts (#2) for thousands of years.  This will make it warmer than its surroundings (#1).

Chernobyl was entombed with much of its uranium fuel.  SOME fission REACTIONS (#3) are still occurring, but they are rare.  Because the neutrons have to hit the fuel just right to cause fission.  It has been broken up, spread out, and no longer a ""critical mass"" (#4) that sustains a reaction of neutrons-causing-fission-causing-more-neutrons once it starts.  Far from it.

It cannot do a ""nuclear explosion"" because it's far from a critical mass.  If you put a decaying isotope (#2) in an insulated, sealed bottle with a small about of water, it could, over time, elevate its temperature (#1), boil the water, and create enough *steam pressure* to cause the container to explode and that  would spread the isotope around.  However, this is not a *nuclear* explosion (#5).  It is not a critical mass (#4) and fission reactions (#3) are not even occurring.

We are concerned that, if not cooled, Chernobyl could do that.  Water could leak in, get inside that huge pile of nuclear trash and get trapped under tons of concrete floors and dirt and stuff, heat up, and build up trapped steam pressure.  It's not really plausible to make enough steam pressure to go ""boom"" anymore, but it could build up some pressure and steam/water could break a barrier and rush out through a crack carrying dangerous isotopes.

EDIT: wow, 3000 points, 36 awards and climbing!  I know it's not gonna pay any bill, but internet points are fun!  Thanks guys!",0
"When a 20-year-old receives an organ transplant from a 50-year-old, the transplanted organ does not age at the same rate as the recipient. The biological age of an organ is determined by various factors, including the donor's age at the time of donation and the recipient's overall health and lifestyle.",1
"There's a u-shaped sort of curve for walking. It's more biomechanically efficient at slower speeds (becoming less efficient at very low speeds as basal metabolism becomes significant given the amount of time). [This](http://fellrnr.com/wiki/Calories_burned_running_and_walking) has a whole bunch of info, and takes slope into account. Pretty cool. According to their data, a 14 min/mile is roughly the crossover point where running starts to be more efficient, but that depends on the person a bit (different sized legs).

Any running motion is pretty much steady-state as far as energy expenditure/distance.",0
"Not sure about ease of birth, but waist-to-hip ratio is generally correlated with various measures of health:

From same author:
http://psycnet.apa.org/record/1993-45219-001
http://faculty.bennington.edu/~sherman/sex/whr-singh2002.pdf

Child cognitive ability correlated with mother waist-to-hip ratio:
https://www.sciencedirect.com/science/article/pii/S1090513807000736?via%3Dihub

https://www.sciencedirect.com/science/article/pii/S0191886904003617",0
"Geologist graduate here: 
Before Pangea, we had a supercontinent called Rodinia, and another prior to it (evidence gets weaker over time due to crust destruction). Depending on the direction and movement of plates, some continents will collide again, and some will tear apart (east Africa).
The process of moving the plates relies on how much the mid ocean ridges are pushing out new oceanic crust, how quickly the old oceanic crust is getting sucked under bouyant continental crust, and movements in the asthenosphere. 
To be honest, i have no idea how long away the next supercontinent is. Pangea was approx 200mya, Rodinia approx 750mya. Rodinia also hung around for a longer period of time than Pangea.
I hope I helped answer some of your questions. 

Fun fact: they believe the initial move to break up Pangea was caused by insulation under the land mass, which heated up, allowing magma to melt above crust and swell and push the land masses apart. ",0
"Most hand sanitizers use alcohol, which kills indiscriminately.  It would kill us if we didn't have livers to filter it, and in high enough doses will kill anyway.  Some germs survive due to randomly being out of contact, in nooks and crannies and such, not due to any mechanism that might be selected for.",0
"There's at least one element we never really discovered until we observed the decay of uranium

Astatine is a highly unstable element and is thought to only have 30 grams in the entire earths crust. It's never been observed by the naked eye.

With such small amounts available for study it unlikely we have confirmed all of it's chemical properties though we likely have a good model from what they are.

https://www.sciencealert.com/meet-the-rarest-natural-element-on-earth

Francium is also ridiculously rare 
https://en.m.wikipedia.org/wiki/Francium

I believe that as long as a planet has uranium on it it'll be guaranteed to have all decay elements between it and lead.",0
"PCR based assays are very susceptible to contamination, which is the current testing methodology. 

Viral transport media where the swabs are stored contain antibiotics and fungicides to kill off any bacteria and fungi to maintain the viability of the virus. 

Also no specimen processor wants a lunch bag full of your spit lol

I haven’t done a COVID test but I’ve used some of the commercially available PCR tests for other viruses. Swabs are vortexed in reagent so I think the difficulty of applying the sample to the reagent would have to be considered too.",0
"Haha damn that's a pretty rough opinion of Griffiths. In your opinion, what would be a better text to use for E&M, especially for undergrads? ",0
"I recall reading an analysis of this phenomenon many years ago in scientific american. The shower head does generate airflow, but also in a hot shower the warm air goes out over the top, sucking in cold air at floor level, causing the curtain to blow in. 

I tried to find it again, but failed. Instead, [here](https://www.scientificamerican.com/article/why-does-the-shower-curta/) is a modern equivalent analysis of the airflow using a finite element simulation - it seems comprehensive enough.
",0
"Okay, let me introduce you to r vs K selection.

All living creatures have to have babies, to pass their traits on to the next generation, ensure that their species survive.  But there's 2 general strategies, r and K

r selection are the creatures that have many offspring at once.  Think of insects or fish.  They have lots of babies, and most of these babies will die.  They put very little effort into raising these young.  Basically they reach sexual maturity early, have a hundred or more babies at a time, and sometimes die soon after, leaving room for their offspring to take their place.  Think of salmon spawning in a river then dying.  Think of insects, which typically have one large clutch of eggs then die soon after.  These species are short lived, but just spew out their offspring, most of whom will die.  If ever there's a favorable change in the environment more of their offspring will survive and can very quickly take over.  If the environment changes for the worst, well they had hundreds of offspring, one or two might make it.

K selected have fewer offspring.  They devote a lot of attention to these offspring, investing a lot of time and effort into teaching them how to survive.  Their plan is to have fewer offspring, but invest a lot of effort into protecting and teaching their offspring so that most will survive.  Because the parent(s) need to invest so much effort into raising their young, the parents have to be long-lived to allow time for them to pass on their knowledge and provide protection.  Think of humans who's offspring are completely helpless for the first 2 years or so, and are not ready to take care of themselves until they are teenagers (or in the case of my nephew, 30+ years old).

Elephants are highly K selected.  It takes a lot of effort to raise a young elephant, so they don't reach sexual maturity before they are teenagers.  When they become pregnant it's 22 months, almost 2 years, before they give birth.  They only give birth to a single offspring.  This offspring is dependent on it's mother for at least 5 years, and up to 10 years.  A baby elephant nurses for at least 2 years, it can't survive without it's mother's milk, and if orphaned before this will die of starvation.  After giving birth the mother will not be able to get pregnant again for at least 4 years, so their first child will be almost 6 years old before there is a chance at a brother or sister.  An elephant can become pregnant well into their 50's or even 60's, although fertility drops after age 30.  All of this mothering gives baby elephants a high chance of survival.  70% of elephants will live to the age of sexual maturity.

For all of these reasons, an elephant has to be long-lived.  It, like humans, invests so heavily in raising their offspring that they have to be able to stick around for years.

Now giraffes, on the other hand, are kind of in between r and K.  They reach sexual maturity at around age 7, half the age of an elephant.  They are pregnant for 15 months, and give birth to a single offspring (usually).  The offspring are weaned off their mothers milk at age 1, although they can begin to eat solid food at around age 4 months.  Baby giraffes are reliant on their mothers for less than 2 years, generally becoming independent at 15-18 months.  Meanwhile the mother giraffe can become pregnant again almost immediately after giving birth, so she can be nursing one young while pregnant with it's sibling.  Sadly, only about 50% of giraffes will survive infancy.

For a mammal that's even more r selected than a giraffe, look at a dog.  Dogs are pregnant for 63 days, and give birth to multiple offspring.  They can get pregnant at 10 months, and the puppies are weaned at about 30 days.  And dogs only live about half as long as giraffes.

Clarification: I stated that elephants have to live a long time because they are highly K selected.  In actuality it's more of a ""there's a genetic push for an elephant to live longer lives.  A long lived elephant, since they remain fertile right up almost to the point of death, will have more offspring.  These offspring will in turn inherit their parent's long life, so there is a selective pressure for longer lives in elephants, which allows them to devote more effort into raising their young.""",0
"EDIT: The response I give below may actually be incorrect dogma. Thanks /u/Supp_Carries_U for pointing that out. It appears that the reason this advice is given to patients is because it is simple and works, but is based on old fears. That said, you are still better to finish your course, as it is known that shorter courses of antibiotics are associated with failure to treat.

&#x200B;

The primary reason is because failure to finish antibiotic treatments leads to the development of antibiotic resistant bacteria.

All drugs take time to work. As you start the treatment the concentration of the drug in your blood starts to rise, and it may continue to rise over several days. Furthermore, even if the drug instantly reached an effective concentration, it would take time for the drug to act. What this means is that over the course of days antibiotics are starting to kill the bacteria in your system, but they don't all drop dead instantly. Which bacteria die first? Well the ones that are the most sensitive to antibiotics. And who does that leave behind? The ones that are the LEAST sensitive.

What makes some bacteria more sensitive to antibiotics than others? Well if we limit our scope the bacteria that could be killed by the antibiotic (some bacteria just don't have the required processes to be killed by particular antibiotics), we see that bacteria have special pumps that remove antibiotics from them. Others have small mutations that make the antibiotic less effective, and some bacteria produce enzymes that break down antibiotics.

Those bacteria usually aren't completely immune to antibiotics, they're just harder to kill. So when you start taking antibiotics, you might have a mixture of all kinds of bacteria, but after the antibiotic has started working, if you have any resistant bacteria, they will survive longer. And if you stop early, now those bacteria might be all that is left. And without any competition from the sensitive bacteria \[i.e. the ones the antibiotics killed\], their population will increase. If you'd finished your course, those resistant bacteria would have all been finished off, but by stopping early, you turn yourself into a factory for producing antibiotic resistant bacteria.  This is bad for society, but it's also bad for you. Not only are you selecting for antibiotic resistance bacteria, but your providing pressure for them to become even MORE antibiotic resistant, to the point that the antibiotic doesn't hurt them at all. At that point, if you get sick again, the antibiotic you took last time wont do anything. And by the time doctors realize that, you're usually very sick.

So finish your course of antibiotics.",0
"The gag reflex is a protective response of the body to prevent the passing of food, drink, or other substances from the oral cavity (the mouth) into the pharynx (the throat). The primary role of this reflex is to protect the airway from invasion, thus reducing the risk of aspiration or choking.

When we swallow, however, a different reflex is triggered: the swallow reflex. This occurs when the tongue voluntarily pushes food or drink (called a bolus) toward the back of the oral cavity. The bolus activates some combination of nerves in the back of the tongue, the pharyngeal arches, uvula, and posterior pharyngeal wall to initiate the swallow reflex, which enacts a series of involuntary muscle movements designed to close off the trachea (windpipe) and direct the bolus into the esophagus.

When swallowing foods and liquids the swallow reflex typically activates instead of the gag reflex, thus allowing people who have a gag reflex (about 1/3 of the population do not) to swallow easily. Some people have an atypically sensitive gag reflex that can preclude swallowing as it activates prior to the swallow reflex  when a bolus is propelled toward the back of the oral cavity. These people are treated by a team that often includes an otolaryngologist (ENT) and speech-language pathologist to minimize the sensitivity of the reflex to allow for more effective swallowing.

I would also like to clarify on some points I have seen so far in this thread. 

First, the swallow reflex can still be activated even if the food is not sufficiently masticated (chewed) into small enough pieces to pass safely through the pharynx and esophagus. The gag reflex is designed to decrease the risk of this happening, but it is not always activated in these circumstances and (again) not all people have one.

Second, the larynx does not open up during the gag reflex. Instead, the vocal folds (vocal cords) close and often the aryepiglottic folds (false vocal cords) close as well. This protects the airway from invasion in case the gag reflex is not successful in clearing the material from the pharynx back into the oral cavity.

Third, the absence of the gag reflex has been shown to be associated with dysphagia (disordered swallowing) [source](https://dx.doi.org/10.1007/s00455-017-9826-y). However, the loss of a gag reflex is only one reason among many that increase a person's risk for dysphagia with advancing age. More common reasons speech-language pathologists see older individuals for dysphagia evaluation and treatment include: decreased muscle tone or control of the tongue, larynx, or pharynx; decreased pharyngeal or laryngeal sensation; anatomical changes (e.g. cancer, surgery); and changes in cognitive abilities. Many of these are secondary to diseases and conditions that become more likely with advanced age such as dementia and Parkinson's disease.

Edit: deleted ""overrides"" in favor of ""activates instead of"" in first sentence of third paragraph as a more accurate, clear description

Edit 2: 1st sentence - replaced ""designed"" with ""a protective response of the body""",0
"Most of the major points regarding refeeding syndrome and hyponatremia have already been covered in this thread. So I'd like to point out a different issue with severe dehydration. As the body becomes increasingly dehydrated, it becomes increasingly difficult to actually get water in. 

This is especially true when you consider one of the most common causes of dehydration is fluid loss due to GI illness. (vomiting and diarrhea). This is even doubly so with infants. It's not possible to rehydrate simply by drinking. You cause more vomiting, more loss, and risk aspirating and drowning. And with dehydration comes hypovolemia. There's so little volume of blood in your veins that they are virtually impossible to start an IV without blowing the vessel. 

The most extreme treatment for this is [Intraosseous Infusion](https://en.wikipedia.org/wiki/Intraosseous_infusion) , where fluids are injected directly into the bone marrow. In laymen terms, we jam a straw into your shin and pour saline/meds/etc.

There isn't really a point of no return on life, so long as you're still alive. But that's not to say you won't have lasting ill effects. Lifelong dialysis due to kidney failure is a bitch.",0
"ICU doc who treats COVID-19 and research on COVID, and published on COVID both original research and editorials in reputable medical journals.

We are much better than previously.

Regarding COVID-19 specific therapies:
1. The UK RECOVERY trial demonstrated a mortality benefit in intubated patients who received dexamethasone. There are some flaws with the study, and the exciting finding may not be methodologically robust, but generally, people are using it.
2. The NIH ORCHID trial demonstrated no mortality benefit in ICU patients with hydroxychloroquine.
3. The ACTT-1 trial suggested a shorter recovery time in hospitalized patients with COVID who received remdesivir. This is the least solid data of the three studies I mentioned.
4. At present, there is no compelling data to suggest convalescent plasma, or any other drug, will benefit patients with COVID. Despite this, many physicians, including my colleagues, still administer it and other unproven therapies. Additionally, there isn't compelling data about use of these therapies as a preventative, or administration in mild disease, although the RECOVERY trial suggested that mild disease night do worse with dexamethasone.

There is a desire among physicians who are desperate to try any plausible therapy. But these are unproven and may actually be harmful. We don't know. A few years ago, there was a big splash about a possible therapy of vitamin c, thiamine, and steroids for sepsis. The original study was intriguing, but methodologically flawed. Many docs gave the cocktail anyway, thinking it couldn't hurt, and might help. A few years later, several better studies have been done, and a large one is still underway. There is no evidence of benefit, and some evidence to suggest harm. So the docs you see giving convalescent plasma, hydroxychloroquine, and beta agonists are really practicing magical thinking, not science.

The reason the UK was able to conduct so many studies with larger numbers despite having fewer COVID patients than the US is because there was effective scientific leadership to encourage patients to join trials. We struggled to enroll patients in trials in the US because no patient wanted to be randomized-- they wanted to be sure that they would get the magical hydroxychloroquine, and many docs capitulated to these requests, instead of standing firm and saying, ""We don't know if it works, which is why we're studying it.""

But the most important benefit to how we treat COVID is better supportive care. Some of this has to do with less resource strain than was present in March, especially in Italy, Spain, New York. Hospitals relied on just-in-time inventory supply chain models and refused to acknowledge problems despite China giving everyone a 2 month head start. Most countries have a reasonable testing/contact tracing program. Even the US, which has done a piss poor job, is doing better than March/April.

With the supportive care, the whole world lost its damn mind with treating COVID. People were pushing crazy ideas like COVID was high altitude pulmonary edema. This theory was espoused by a sea-level emergency doc who was familiar with neither ARDS nor HAPE. People thought that these patients had better lung compliance than traditional ARDS and thought to perhaps give larger tidal volumes on the ventilator. They thought that these patients had higher rates of clots (might be true, but the reported rate is comparable to rates seen in similarly critically ill patients with septic shock or ARDS), and started administering therapeutic doses of anticoagulation despite no evidence of clots. Plenty of non-intensivists would report with amazement discoveries obvious to most seasoned pulmonologists or intensivists, like standard ventilator management worked after trying unproven modes like APRV or known harmful ones like oscillatory ventilation, or that less sedation and less paralytics had quicker recovery times. I helped write up some of the Lombardy Italy experience, and the docs there were throwing everything at patients, based on tweets they read. Give Ace inhibitors. Don't give them. Give hydroxychloroquine, kaletra, remdesivir, tocilizumab, plasma, etc, paralyze all these patients for weeks at a time, and then wonder why all the survivors were so profoundly weak. Things have calmed down to the point where these unproven meds aren't given as routinely as before.

Additionally, we have a better idea about transmission, and are more willing to let people use high flow nasal cannula instead of early intubation. In the US, there was a misconception that many patients needed early intubation. Now, most places will treat COVID like any other severe ARDS and intubate accordingly.

The biggest improvement in the past four months is that docs have calmed down and realized that the right course of action is to provide the same supportive care that we typically do for ARDS instead of relying on witchcraft.",0
"Not quite taste, but still relevant: asparagus makes your pee smell weird, but only a small fraction of people can actually smell it:

>The asparagusic acid in asparagus produces many sulfurous byproducts that give your pee a rotten-like smell.

>The smell can be detected as early as 15 minutes after eating asparagus and may last up to 14 hours.

>However, not everyone produces the smell, and the majority of people can’t smell it due to a specific genetic modification.

Source: https://www.healthline.com/nutrition/why-does-asparagus-make-your-pee-smell#bottom-line

I'm one of the unlucky people who can smell it. Sometimes I'll eat a salad that has a little bit of asparagus without realizing it has asparagus, and an hour later I'll realize it when I go to the bathroom.",0
"I'm just speculating here, but maybe the reason it took so long for you was because you had a low ""fat-turnover"" rate due to being so lean. You weren't really storing fat or utilizing stored fat all that much, the fat cells you had weren't really changing or being used between when you were a heavy chronic user and when you were trying to clear your system, and thus the THCCOOH in your fat wasn't being liberated and then excreted. And then also, I guess if you have few fat cells, then each individual cell would be more saturated with THCCOOH for the same amount of THC consumed, which coupled with this low turnover, could further contribute to taking a while to clear out.

That speculative explanation makes sense to me, but I might be misunderstanding how metabolism works.",0
"Lab-grown meat, also known as cultured meat or cellular agriculture, is indeed chemically similar to conventionally produced meat. Both lab-grown and conventional meat are composed of muscle cells, connective tissues, and fat, which contain proteins, lipids, carbohydrates, and various other compounds. However, there are a few key differences between the two. One distinction is the way lab-grown meat is produced.",1
"I appreciate the clear explanation made easy to understand, slut_4_cum. So could you say gyres and ocean deserts are interchangeable terms?",0
"It doesn't.

This is part of the reason why newly pregnant women suffer from morning sickness; the brand new fetus (or whatever it's called at this early stage) secretes a hormone that suppresses the mother's immune system to stop it attacking the tiny bundle of foreign cells.

Ultimately it's not (usually) a problem, because the mother and fetus have completely separate, independent circulatory systems so the mom's white blood cells can't get to the baby and attack it. This is what the placenta is, it's how the baby's and mom's blood vessels get close enough for oxygen and nutrients to be exchanged, but without actually sharing blood.

Edit: stupid autocorrect

Also edit: Anonymous Silver?! Cool! I'm not sure what that means, but I'll take it. Thanks, unnamed benefactor!

Edit part 3: And my very first Reddit Gold, too? What a day, what a day. . .",0
I found [this article](https://www.smithsonianmag.com/science-nature/everything-you-wanted-to-know-about-dinosaur-sex-173015/) from the Smithsonian that elaborates some on what we know.  it's an short interesting read. ,0
"Damn, son! That was awesome. I've seen the same behavior in my coconut oil and suspected it had to do with packing efficiency, I just couldn't figure out why. Your explanation was super effective!",0
"So couple things:

1. Exercise physiology is stupid complicated. Lactate formation is a piece of muscle fatiguability, but it's not the whole story.
2. It's actually crazy useful to make lactic acid. In doing so, you get a couple quick and dirty high energy electrons to use to make ATP **without needing oxygen**. The heart can't do that, which is why cutting off the oxygen supply for even a couple seconds causes crushing chest pain.",0
"Fun fact: a fair number of Philips screws are NOT Phillips, they are JIS, a slightly different standard that Phillips head screwdrivers do not fit worth a damn. 

JIS are designed not to cam out in factories and is a very secure drive.. unless you use a Phillips screwdriver on them, then its 3x worse then Phillips.

JIS screwdrivers however do fit Phillips very well. That said the JIS screwdrivers I bought cost $60 for a set of 4. 

Most things from Japan (Such as yamaha motorcycles) use JIS. Sometimes they will have a small dot on the head to indicate its JIS but not always. I HIGHLY recommend buying JIS screwdrivers. (I have the Vessel JIS set from ebay, VERY good quality)

Between a good set of JIS screwdrivers and a manual impact screwdriver (Did I mention one of the Vessel JIS screwdrivers is a manual impact driver+regular screwdriver?) and penetrating oil, I have not stripped a JIS/Phillips head screw since buying them.

That said, I also reach for the manual impact screwdriver as the very first thing whenever I meet any large head JIS screws... and replace them all with SHCS (Socket head cap screw). ",0
"This is a amazing question and one I cannot fully anwer. The sheer complexity of the nervous system is mind boggling. From my understanding you phrasing however is somewhat flawed. Prosthetics don't 'interface', for lack of a better word, with nerves. They instead use the output of said systems. You're nerves send a signal to your muscles and the muscle responds in a way it's biologically designed to. We can read that response and use it as input.

What is mystifying to a certain degree is that even though you would expect our nerves and muscles to be hard wired from brain to muscle this is actually not the case. Even though the nerves or wires run from A to B they don't seem to be hardset into performing a single function. The human body has adopted to be more flexible. 

In essence a baby is a prime example. They will hit themselves in the face trying to get their arm movements right. It's like a robot running through calibration to asses their range of freedom. Given enough time a adult could rework their nervous system into signaling a appendage to respond to their desires.

But to boil it down. Do people born without a arm lack the nerves to control a prosthetic? Biologically yes. Could they adapt to using a prosthetic arm? Yes, but it becomes harder with age.

Don't have the time to provide sources atm but I hope others will do so as well and challenge my views on this topic. It's interesting to think about.",0
"TMAH [(tetramethylammonium hydroxide)](https://en.wikipedia.org/wiki/Tetramethylammonium_hydroxide) is a nasty chemical that a spill roughly the size of your hand will kill you. 

Like start making phone calls to the people you love sort of kill you. It is also used in industry so it is possible that he was hauling it.",0
"Yes, absolutely.

A friend had a parrot that lived inside, learned to swear and was generally hilarious. He was allowed outside, would hang in the trees with the wild birds chatting away, then fly back for the evening.

There is nothing quite like a tree full of parrots all screaming F**K in unison.",0
"I am writing a PhD thesis on the ocean response to tropical cyclones. Contrary to what some people have suggested here, the energy of cyclone (hurricane) winds powers ocean currents well over 100 meters below the ocean surface. This has been observed directly using autonomous ocean profilers that measure temperature, salinity and velocity at different depths:
https://journals.ametsoc.org/doi/full/10.1175/2010JPO4313.1

Now, the answer to your question varies according to what you consider ""deep sea."" However, a particular process does come to mind that can reach down hundreds of meters deep and impact marine organisms: hurricanes power vertical currents that lift up water towards the surface.

 If a hurricane's eye moves slowly (say 1m/s), water may be sucked up from 150 meters deep all the way up to the surface ([source]{https://journals.ametsoc.org/doi/full/10.1175/2009MWR2863.1}). 

However, if the hurricane moves much faster, it will create a large underwater wave: deep, cold water will be carried up and down by as much as 80 m and with a regular frequency in time. These are called near-inertial internal waves (NIWs) and they are an effect of the Earth's rotation. Across the global ocean, these waves help transport energy from the ocean surface towards deeper regions and thus play a role in shaping climate.

Now... These effects are mostly localized along the hurricane track, and are likely to be very weak to the sides of the track. This explains why a submarine could navigate ""below"" a hurricane and not have much trouble. However, a submarine navigating right below a hurricane eye is about to get into A LOT of trouble, as it's going to lose (to some extent) the ability to control its depth.",0
"There's two aspects to this - the first is that direct venom injection will (as one might expect) have a pretty significant effect on the animal's health, especially because a relatively large amount of material needs to be injected in order to induce a robust immune response and obtain good yield.

The second is that development of an antibody is not immediate, nor does it necessarily convey full protection to the foreign substance in question. In most typical horse [antivenom production](https://webcache.googleusercontent.com/search?q=cache:FpfLrxzuIZAJ:https://bezoar.georgiapoisoncenter.org/wp-content/uploads/2012/06/Chapter_115a_-_Antivenom_Spider1.pdf+&cd=12&hl=en&ct=clnk&gl=us) operations, peak antibody response is observed on the order of weeks, which reflects the fact that the proper B cells need time to be identified/selected/proliferate to produce appropriate antibody. While this is happening, the horse will continue to be subjected to whatever negative effects the venom may exert.

As a side note, presence of antibody alone does not guarantee full neutralization of a foreign substance. Older, so-called ""third-generation"" HIV tests looked for the presence of antibody, because a person would only develop antibodies targeted toward HIV if their immune system had been exposed to the virus - but we know that HIV infection essentially cannot be controlled without external intervention.

As for why brown recluse toxin in particular is so deadly to the horses (most antivenom production does not result in such dramatically harmful effects to the horse) and humans, it is reflective of how dangerous the toxin itself is. One of the major components of brown recluse toxin is sphingomyelinase D, which breaks down a component of cell membranes and leads to widespread cell death. This can lead to very serious [complications](https://www.sciencedirect.com/topics/neuroscience/sphingomyelin-phosphodiesterase-d) such as breakdown of red blood cells and platelets, which can lead to even worse multi-organ complications such as kidney failure and so-called disseminated intravascular coagulation (out-of-control bleeding and clotting due to the consumption of clotting factors resulting from microscopic damage to blood vessels).",0
"Yes. The classic examples are spumaviruses (“Foamy Viruses”), members of the retrovirus family that are widespread among animals (though there doesn’t seem to be a true human version). The most studied (though “most” is relative, since these don’t seem to cause any disease there’s limited interest in them) are simian spumaviruses, since these occasionally infect humans - still, apparently, with no symptoms at all. 

>	FV [foamy virus] is considered non-pathogenic in natural and experimental hosts but systematic, longitudinal studies have not been conducted to verify the apparent non-pathogenicity. Humans can be zoonotically infected with a variety of SFVs originating from Old World monkeys and apes (OWMA) through occupational and natural exposures but demonstrate an apparently asymptomatic though persistent infection

—[Wide distribution and ancient evolutionary history of simian foamy viruses in New World primates](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4627628/)

The reason these viruses seem to be so harmless is that they infect cells that are about to be shed anyway, so they don’t end up significantly changing the natural biology. 

>While FVs share many features with pathogenic retroviruses, such as human immunodeficiency virus, FV infections of their primate hosts have no apparent pathological consequences. ... We show that superficial differentiated epithelial cells of the oral mucosa, many of which appear to be shedding from the tissue, are the major cell type in which SFV replicates. Thus, the innocuous nature of SFV infection can be explained by replication that is limited to differentiated superficial cells that are short-lived and shed into saliva. 

—[Replication in a Superficial Epithelial Cell Niche Explains the Lack of Pathogenicity of Primate Foamy Virus Infections](https://jvi.asm.org/content/82/12/5981)",0
"In relation to the questions about vaccine derived virus and the replies, it is worth pointing out that the polio virus is spread  by fecal-oral transmission or when infected shit enters the body via the mouth.

Thus the mention of poor sanitation, no flushing toilets and no water for washing. Thus from dirty hands to mouth, either directly or via food or drinks contaminated during preparation.",0
"As far as I am aware, the amount of pressure you can apply is pathetic relative to that needed to kill your average bacteria.

Quick math: surface area of your finger tips = 1cm^2
Amount of force you can apply with a finger, 200 newtons, so that's a pressure of 2 Mega-Pascals (290 PSI).

[It looks like](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3736669/) you need to get to pressures of about 200 atmospheres (20 Mega-Pascals) before you start to slow the grow of E Coli, and that certainly isn't killing them outright, just slowing them down.

So I'm pretty sure they're safe from your fingers.

Of course this makes sense. If the cell walls of bacteria are like a brick wall, then the walls of your cells are like pieces of paper. Bacteria are just built more robustly than you and I.

",0
"Others have calculated the energies of the projectiles, though for u/SocomTedd: the Mark 7 gun fired explosive shells, whose *explosive* energies far exceeded the kinetic energy of the projectile (~6000 MJ explosive vs. ~40 MJ kinetic.) The kinetic energy of the projectile, while large, is insignificant compared to the actual payload.

I actually wrote an executive-level summary of railgun physics and their operation for a high-level program manager working for the Navy. (None of this involved classified material.) One potential issue is that railguns tend to use very dense materials for their projectiles, which are liable to penetrate through targets rather than deliver energy to them. This is easily solved by modifying the projectile to *scatter many smaller projectiles* near the target. 

As u/jehan60188 says, 4 MJ/kg is the threshold for railgun superior effectiveness. Once the projectile is fast enough it is more effective to use the kinetic energy of the projectile rather than use (chemical) explosives. As kinetic energy scales essentially without limit with respect to velocity, railguns are inevitably more effective. They are not, *yet*, more effective, which is what motivates continuing naval research on the subject. 

Chemical explosives provide a fundamental limit on the effectiveness of traditional guns, either through explosive or kinetic payloads. (There is a similar limit to the effectiveness of chemical rockets.) Electromagnetic propulsion, on the other hand, suffers from no such limit. ",0
"Yup. We activated the seed oil synthesis and accumulation pathways in leaves. They make enormous amounts of oil which is basically liquid fat. We're now up to seed levels of 40% by dry weight in leaf. 

https://onlinelibrary.wiley.com/doi/full/10.1111/pbi.12131",0
"You've already had this covered well enough, but...

Think of it this way:  I have my vagina.  It's a tunnel a few inches long.  It's the place where the fun happens when sexy times are going on. :)  My cervix is  around the end of this.  The cervix connects  the vagina to the uterus.  The cervix is basically a muscle ring that spends it's whole life clenched tightly.  It's job is to keep stuff--except sperm--out of the womb.

The womb is there the baby making happens.  think of it as a sack, or a balloon or something. It's got space on the inside.

When we have a period, what's basically happening is that the inner layer of the womb is peeling off.  This inner layer has a lot of blood vessels in it, because it's supposed to be ripe delicious ground for a fertilized egg to rest.  So when it starts peeling off, there's a lot of blood and juice and stuff.

So, it's basically an open, weeping wound. Heavy days are when the bleeding is heavier.  Lighter days are when the bleeding is lighter.

There are some actions we can take that can ...  adjust the rate at which blood leaves our body, so to speak.  For example, laying down allows blood to pool, both in the womb and in the vaginal canal.  Blood flow is lessened as a result, however, when you stand up, gravity WILL kick back in.

(for comedy, this can result in, in the morning, laying stone still as one feels that there is a lot of blood inside them....  then the QUICK leap up out of bed, clutching one's crotch and fleeing to bathroom.)

Actions that involve contracting the womb can make more blood come out--as the womb already contracts itself to try and push blood out.  excersize, activity, orgasm and similar can have the same result.

Being still can result in blood pooling inside.  Every womb-owner has experienced, say, going through a class, and feeling fine in regards to if they need to change their tampons, then standing up, and realizing that no, they need to go NOW.

but we can't really effect the heavy/light-ness of our period.  We can't control how bad the wound is bleeding, so to speak.  Typically, you start with light days, then heavy days, then light days again.

so, again, think of it as a water ballon or something, attached to some flexible tubing.  the water's leaking out at a relatively set rate (that may be light, heavy or anything in between) ... and the water finds all sorts of places to go, and changing the orientation of the tubing can result in pooled fluid being released from the ""tide pools"" so to speak.",0
"I'll start by saying that we actually don't know if this is a new human species or not. The authors who found this fossil argue that it is because:

> ""A combination of primitive and derived features in the Harbin cranium establishes a good set of diagnostic features that were used to define a new Homo species."" via Shao et al: https://www.cell.com/the-innovation/fulltext/S2666-6758(21)00056-4, published yesterday.


Basically, there are certain features that tend to be specifically diagnostic of species difference when we look at hominids. Things like brain size, face length, brow size, tooth differences, cheek bone size, etc. The reason we look at these things is because they tend to be variable between species, but fairly consistent within species. And, even in cases where one or more of these traits is variable (e.g., a baby born with microcephaly), all the concomitant traits probably won't change much. For example, the authors note that this fossil has a mix of features that are commonly seen in more human-like hominids, and those more common in ""primitive"" hominids:


Like humans:
> ""large cranial capacity, short face, and small check bones""


More ""primitive:""
> ""low vault, strong browridges, large molars, and alveolar prognathism"" via Ni et al: https://www.cell.com/the-innovation/fulltext/S2666-6758(21)00055-2, published yesterday.


Because this combination of traits isn't known in any other hominid, and because it doesn't seem to showcase signs of any known disease or deformity, it's unlikely (although never impossible) that this is just, like, some guy who looked real weird. 


Still, the leap that this is a brand new species just from one skull is problematic. First of all, there are known species of hominid that have unknown skull structures. The article you linked to mentions Devisovans as an example of this. They are a close relative species of ours, and some people of east Asian descent have their DNA in their genomes! We only know they exist because we sequenced the DNA of some non-skull bones, so we have no idea what their heads would have looked like. That means it's possible that this is just a skull of a species we already knew about. Even if this is the case, that's still a really cool find! 


I should also mention that even if this is a species we already knew about, it's almost certainly not *H. sapiens*, as the time and place it was found doesn't match up with human migration. Present evidence suggests that modern humans probably didn't make it to East Asia until about [50,000 years ago](https://en.wikipedia.org/wiki/Early_human_migrations), and this individual lived in present day China over 146,000 years ago.",0
"If a fish was in milk or another liquid with the correct oxygen concentration, yes, it could *respire*. That's not a dig on your vocabulary, that's all I think a fish could really get out of another liquid: just enough oxygen to technically be able to do body processes like exchanging CO2.

Fish also experience osmotic stress, ph stress, turbidity stress, viscosity stress, chemical (ammonia, nitrite, nitrate, and other) mineral/hardness stress, heavy metal stress, etc. Putting a fish in milk for more than a few seconds would likely put strain on its gills because of all the fats, proteins, minerals etc, that the gills have to filter out. You couldn't survive in air that had oxygen, but was also filled with ash or small particles, either.

Long term, fish also rely on their environment to not be too salty or not salty enough, and not too acidic or alkaline. Over time, this fish in oxygenated milk would experience stress because the milk didn't have the correct amount of salt, or was too acidic.  You couldn't survive in air that had oxygen, but was also part damaging chlorine gas.

Fish are whole animals and have more requirements other than just not suffocating or desiccating out of water. They  rely on their environment that completely surrounds them to take care of the same body processes that we have. It just happens to be water for them, and air for us. You would need to find another liquid that didn't have anything else suspended in it that would strain the gills, was the correct ph, salinity, and be free of chemicals or minerals that might poison it.Source: graduating tomorrow with a fisheries degree

TL;DR: yes, if you forget every technicality.  


Edit: multiple people have pointed out milk is actually slightly acidic (close to ph=6.6) I originally wrote it was too alkaline for fish. My bad! I will point out fora the sake of technicality that some fish do thrive below a ph of 6.6 (fish that prefer blackwater like some rasbora sometimes like water with a ph as low as 5.0!) but it was merely an error. I learned something today!",0
"I hate being this guy but according to Google there's somewhere between 10^78 and 10^82 atoms in the observable universe and there are only 10^67 shuffle outcomes so technically you have a better chance to have a perfectly synchronized shuffle but it's still pretty much a crap shoot. I liked your comparison though, really gives a ""tangible"" sense of how many shuffle outcomes there really are. 

Even when someone goes to explain it in a way like 'if we shuffled once a second for a million billion bazzillion years then it would still take...'

Edit: 10^67",0
"There's a lot of speculation in the comments, and a lot of the information in the comments is outright wrong/dangerous. Please don't take medical advice from any of these comments...

In short, pathogens cause the release of cytokines, which are inflammatory modulators that in a broad sense do various things to help fight infection. Some of these cytokines are pyrogenic (IL-1, IL-6, TNF, IFN). These act in many ways, but one of them is acting at the level of the hypothalamus to raise the body's 'set point' temperature via PGE2, similar to how a normal thermostat works. This causes a number of physiologic changes eg. you vasoconstrict in the periphery (so your limbs feel cold), and we're behaviourally programmed to decrease exposed surfaces - wearing more clothes, getting inside, reducing activity. You might also shiver.

Fever generally makes us feel terrible because of the above. It also increases baseline O2 consumption, can induce mental changes, and it can also exacerbate cardiac or pulmonary disease. 

There is evidence that an elevated (febrile) temperature in animal cells IN TEST TUBES is beneficial, via a heightened immune response and increased bacteriacidal killing (PMID 12015457). HOWEVER there are no studies showing that fever itself facilitates any faster recovery from illness or adjuvants the immune system. There is isolated evidence in the context of influenza vaccination that treatment with antipyretics can actually boost anti-influenza antibody levels (PMID 7746030). We're pretty sure that treating fever symptoms with antipyretics does no harm and also doesn't slow recovery. 

Exogenous heat exposure/production in an uncontrolled fashion can override the body's ability to lose heat and cause dangerously high (read: you could die) internal temperatures (ie. heat stroke). The thing we worry most about in the context of the acute illnesses that we're talking about from a temperature perspective is high fever, because we know that this results in bad things happening (some mentioned above) - and potentially seizure, coma, death. 

Our bodies are well-oiled machines, and for the most part, your body knows what it's doing. Don't go messing around with trying to increase your body temperature on your own, because that is perhaps the most dangerous thing you can do. 

tl;dr - We don't really have evidence that tells us whether temperature alone changes how the body manages infections. We know for a fact that artificially altering your body's temperature, particularly attempts to raise temperature, is dangerous.

This is not medical advice, and if you want medical advice then you should go see a doctor.

Edit: spelling and more pointed summary
Edit 2: Thanks for the gilds!",0
"No, an obese person does not typically have more blood in their body than a person with an optimal body mass index (BMI). The volume of blood in the body is largely determined by factors such as body size, not body composition. While obesity is characterized by an excess of adipose tissue (fat), this does not directly translate to an increased blood volume. Blood volume is primarily determined by factors like height, weight, sex, and overall body surface area.",1
"Surgeons take several precautions to minimize the risk of air bubbles in the bloodstream during and after an organ transplant. These measures include:

1. Proper positioning: The patient is placed in specific positions to prevent air from entering the bloodstream. For example, the Trendelenburg position (head down, feet up) may be used, as it helps prevent air from entering the large veins. 2. Minimizing air exposure: Surgeons take care to minimize air exposure during the surgical procedure.",1
"Long-running Canadian science show Quirks & Quarks covered this about 6 weeks ago. He interviews the people that publish papers and make things more relatable.  You can read the story below but it's best to click the ""listen"" button and get the interview.  

[Quirks & Quarks](https://www.cbc.ca/radio/quirks/jun-6-detecting-covid-in-sewage-spacex-s-crew-dragon-pet-dogs-fail-at-rescue-and-more-1.5598598/the-key-to-early-detection-of-covid-19-outbreaks-might-be-in-sewage-1.5598610)",0
"And if this organ doesn’t sense enough fluids amongst those blood vessels, it’s why you feel pain in the form of a headache, no?",0
"So the gag reflex can be overactive or underactive, indicating a sensory/neurological issue. I believe underactive gag reflexes are usually due to a sort of sensory overload and the brain is like ""This again? No, it's not a threat. No gag.""

Not having a swallow reflex or having a diminished swallow reflex is a form of dysphagia, which is a fancy word to describe a problem with eating. It can happen due to congenital disorders like cerebral palsy, or acquired disorders from stroke to TBI to gradually worsening degenerative nervous system diseases. Spinal surgery on your cervical vertebrae may cause problems with the swallow mechanism which can slow the swallow reflex.",0
"Evolution isn't intelligent. While we often say that parts of our body are designed for certain things, the reality is more like they were designed, and ended up being useful for those things

There is no engineer to improve the body, and as long as the funny bone weakness doesn't actively kill us or keep us from reproducing, it's not gonna just get fixed",0
 Birds excrete concentrated urine and poo out the cloaca. This concentrated nitrogen and phosphate makes an excellent fertilizer hence the guano trade.,0
"Boiling water doesn't ""clean"" it. It does, however, kill certain harmful bacteria, which results in water that is safer to ingest.",0
"Most vaccines given these days are not attenuated virus but dead virus, or just particular proteins from a virus. I'm not sure if there are any others that you can spread this way but in general it is not a concern. The issue with the polio vaccine is that the oral polio attenuated virus vaccine is very cheap and easy to store which makes it the best choice in places where catching polio is a high risk and medical care is not widely available. The tricky thing is deciding when to stop using it!",0
"If it kills another member of the species, it should kill the same creature. The body probably isn't going to respond differently to the same venom because of where it came from, still the same batch of chemicals. ",0
"If Randall Monroe reads /r/askscience, and I'm guessing he does, then hopefully he reads this suggestion: if ever you need a guest contributor for What If, hit up /u/VeryLittle. 

Seriously man, you're awesome. Even just skimming through your history it's clear you put a lot of effort into answering questions in this sub in a light-hearted and undaunting way. Thank you, this is how we bring science to the masses. Combine fart jokes and math. ",0
"The separation of urine (pee) and feces (poo) is a natural process that occurs in the human body during digestion. After food is consumed, it goes through the digestive system, where nutrients are absorbed and waste products are formed. The process starts in the stomach, where food is broken down into smaller particles and mixed with digestive juices. From there, it moves into the small intestine, where further digestion and absorption of nutrients occur.",1
"There is a real, but very very remote chance of reinfection. If you got COVID-19 and are not immunocompromised your risk of reinfection is vanishingly small. 

That being said, while the risk is damn near zero, it is not actually zero. Even if you had COVID-19 and recovered you’d shouldn’t be totally careless.

[Coronavirus Reinfections Are Real but Very, Very Rare—NY Times](https://www.google.com/amp/s/www.nytimes.com/2020/10/13/health/coronavirus-reinfection.amp.html)",0
"Lot of the initial data we got from China wasn't super helpful. We knew it was contagious, deadly, And had a brief idea of what symptoms looked like.

At first, treatment was shifted towards early intubation (no bipap, no hiflow oxygen) but patients were found to have a difficult time being extubated. Now we tend to delay intubation and try hiflow oxygen (talking 60-100% blend of oxygen at 60-80L of minute, a truly massive amount of oxygen therapy. 

Medication therapy has shifted as well. Initially it was thought steroids (traditionally used in ARDS treatment) was harmful in this type of patient, where as now they are given religiously. We also no longer give hydroxychloroquine as the rhythmn issues were found to be more harmful than helpful. We have remdesivir as an antiviral for treatment which has shown an increase in favorable outcomes, albeit this medication can also come with other dangers and certainly isn't a cure all.

Convalescent plasma is also available which has shown some benefit as well, but really isn't truly studied well enough to say how much. 

I'm just nurse, so if any physicians or other providers have any corrections or anything I missed,  please feel free to chime in.

Edit: forgot to mention hypercoagulopthy. Its now understood critically ill patients have a significantly increased chance of blood clot formation, significantly increasing risk of stroke, pe/dvt, limb/tissue ischemia. Patients are now started on prophylaxis if not already taking something (like xarelto/eloquis/Coumadin etc.)",0
"In theory, the stomach will be fuller if you drink water, yes. And cold water will take longer to pass. But it's not that simple once you add food and hunger to the equation.
First, the food you are eating is likely warmer than your body, so when mixed with the cold water, both will countereffect each other.
And, more importantly, how much you want to eat doesn't only depend on how fast you fill up your stomach. It depends partly on how your body interprets what it is eating/drinking. It is a very complex process that involves taste; your brain; the fat, protein, and starch content of what you eat; the spiciness of the food, what foods are available, etc. It's very hard to predict whether drinking cold or hot water is going to make you eat more or less without actually studying in different conditions.

Edit: it turns out that it's not even clear that cold water takes longer to pass. Some studies show that cold water at 4C (close to ice temperature) can pass faster than water at 20C (at room temp).",0
"Interesting question.

According to [a paper published in the The Journal of Neuroscience](http://www.sfn.org/Press-Room/News-Release-Archives/2008/One-Sleepless-Night-Increases-Dopamine-in-the-Human-Brain), one sleepless night increases dopamine in the human brain. An increase of the neurotransmitter was found in the striatum, involved in reward/motivation, and the thalamus, involved in alertness. The researchers concluded :

> The rise in dopamine following sleep deprivation may promote wakefulness to compensate for sleep loss. “However, the concurrent decline in cognitive performance, which is associated with the dopamine increases, suggests that the adaptation is not sufficient to overcome the cognitive deterioration induced by sleep deprivation and may even contribute to it,” said study author Volkow. 

This would serve an evolutionary advantage to early humans who felt they needed to stay awake for extended periods of time, e.g. for hunting food. This contrasts with exercise-induced tiredness because, as we'll see, exercise does not necessarily cause cognitive impairment.

[Another study in *Perceptual and Motor Skills*](http://pms.sagepub.com/content/84/1/291.full.pdf+html) sought to establish the effects of physical exhaustion on cognitive functioning. They had 13 fit men pedal on stationary bikes at different intensities, and had them perform a series of short-term memory tests.

> It appears from our findings that the extent to which physical effort affected cognition depended
on the intensity of the session and on the set size of the decision task.

They also referenced other papers that addressed neurochemical changes within the brain.

> Finally it may be worth considering our results in the context of the biochemical changes
brought about by physical exercise. Indeed, it has been argued that these changes may interact
with cortical activity during strenuous effort (Hebb, 1955). Peyrin, Pequignot, Lacour, and
Fourcade (1987) reported an activation of the catecholamine system resulting from strong physical
work and suggested the existence of a positive relationship between adrenomedullary activation
and mental performance.

So sleep deprivation-induced exhaustion and physical exercise-induced exhaustion are similar in the sense that they cause an increase in catecholamines (i.e. dopamine, norepinephrine, etc.).

However, with physical exercise, it appears that an increase in mental performance is possible, whereas we already saw sleep deprivation can be cognitively impairing:

> Comparative discussions of the present results with those of previous studies are daicdt
because of the different operarionahzarion of fatigue across studies and the specific interpretation
of results. Nevertheless, it has been already reported that treadmill exercise conducted at
high physiological activation (94% of maximum heart rate) significantly enhanced mental performance
(McGlynn, Laughlin, & Rowe, 1979)

**Edit**: [Also understand that exercise uses up glucose stores in the muscles](http://www.bd.com/us/diabetes/page.aspx?cat=7001&id=7516) and your body begins to burn fats as fuels, which can contribute to the feeling of overall fatigue if too much glucose is used up. This is a problem particularly in diabetics. [Here](http://www.ncbi.nlm.nih.gov/pubmed/3315761) is a paper that establishes the relationship between hypoglycemia (low blood sugar) and levels of alertness.. ~~I do say anecdotally that I don't think sleep deprivation has much effect on blood glucose levels. But let me look for a source on that.~~

**Edit 2**: [This paper](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1991337/) evaluates the effect of sleep deprivation on glucose metabolism.

> The research reviewed here suggests that chronic partial sleep loss may increase the risk of obesity and diabetes via multiple pathways, including an adverse effect on parameters of glucose regulation, including insulin resistance, a dysregulation of the neuroendocrine control of appetite leading to excessive food intake and decreased energy expenditure. 

This is a different mechanism than by physical exercise-induced alterations in glucose metabolism. While your body knowns when to use glucose as energy while exercising, it appears that sleep deprivation results in dysregulation of neuroendocrine control of appetite and insulin resistance. In other words, tiredness from sleep deprivation is different from tiredness from physical exercise because sleep deprivation essentially results in bodily malfunction. Yet another reason to get enough sleep at night!

**Edit 3**: Increased clarity and tried to point out more differences. Perhaps someone with more expertise in physiology can chime in?

**Edit 4**: Thank you /u/whatthefat for the input:

>It should be noted that sleep deprivation specifically causes a reduction in ATP stores of neurons.

>http://www.jneurosci.org/content/30/26/9007.short

***TL;DR***

Tiredness from strenuous physical activity appears to be from your body using up its glucose and ATP energy stores. Tiredness from sleep deprivation is a result of your body going into overdrive mode: there are anomalies in the amount of neurotransmitters in the brain such as dopamine, and it has adverse effects on glucose metabolism and energy expenditure.
",0
"We gave people who recieved the J&J shot a booster here in Iceland after many of them were infected with symptomatic covid, including serious symptoms.

All of the data is here: www.covid.is

It includes a full breakdown of all of out breakthrough cases.",0
"Well, two reasons: there is technically eight types of it (A-H) of varying lethality (H being the most poisonous known substance on the planet, 2ng, or 2 x 10^-9 grams is enough to kill a human being). We use a version that is slightly less poisonous (actually two types, A and B) and only inject very little of it. Remember: everything is poisonous and nothing is, it all depends on the dose.",0
"We can do a very(!) rough back of the envelope calculation.

Assume a 100kg person (I like round numbers). Assume that they're all water, so we have 100 litres. Assume that they're at body temperature, so about 310 Kelvin.

Now you drink 0.2 liters of ice cold water, 273 Kelvin.

Since both are water, they'll have the same heat capacity and the end temperature will be just a simple weighted average:

T = (100 * 310 + 0.2 * 273) / (100 + 0.2) = 309.93

so it's almost negligible, like a 0.07 degree drop.

If you wanted to be more accurate, you could use the average specific heat capacity of the human body. I can find it via google, but that would take the fun out of computing it. You'd use a weighted average of the capacity of water (60% of human body is water) and of things like proteins, fat, bones.

It wouldn't _drastically_ alter the equation though, the fact that the drop in temperature would be small will remain. 

Like, let's use the factor 0.5:

(50 * 310 + 0.2 * 273) / (50 + 0.2) = 309.85, so now we're looking at a .15 degree drop. Still negligibly small.",0
"The ""empty"" space between the camera and the atom is not actually empty. It is filled with air molecules, which are constantly moving and colliding with each other. These air molecules consist mostly of nitrogen, oxygen, carbon dioxide, and other trace gases. While they are invisible to the naked eye, they are present in the atmosphere and fill the space between objects, including the camera and the atom.",1
"Skydiver here. The water always hits you from the direction you are falling. From the moment you exit the aircraft, you are ""falling"" forwards at around 70-90 knots typically. That is already fast enough that you will be striking water on the side of you facing the relative wind. From there, the direction of your fall becomes more vertical (referred to as the slope) while constantly accelerating up to terminal velocity. So, you end up catching up to more rain droplets from the direction you are falling. This happens until the parachute is deployed and your descent is slowed sufficiently that rain starts falling faster than you. 

I'm avoiding the words belly and back as that can be confusing - it is possible to skydive in many orientations. Belly down is the basic ""box man"" position, but skydivers also backfly, go head down or head up. Terminal velocity varies from 120-180mph and is dependent on the style of flying. 

Skydiving in even just light rain is actually so uncomfortable that your face often turns red from being battered by raindrops (if wearing an open face helmet). It feels like coarse sand thrown at you at 100mph. ",0
"There are a lot of factors here.   Slotted screws are easy to damage if the screwdriver slips or doesn’t fit perfectly.  

Phillips head screws were designed to limit how much torque is applied by “camming out” in the factories before power tools were torque limited. 


Now that power drivers are designed with torque limiting clutches the emphasis is on securely fitting the driver without slipping off.  

Everything in engineering is a trade off and screw heads are a perfect example. ",0
"That isn't really messing up the spelling of a word though. When you mess up the spelling or letter order of a word, you are still using valid letters in the language. If you throw random strokes all over the place, you are likely no longer using valid radicals.  
  
Instead of messing up stroke position, consider simply messing up a radical. For example: 情, 请, and 清 are all phonetically similar; a native speaker could recognize the intended meaning and also the mistake.  
  
Chinese can also misunderstand the intended character for a word they frequently hear. For example, on a shopping list, my mother in law wrote ""豆付"", which is a misunderstanding of the second character ""豆腐“ (tofu). And no, it wasn't an intentional abbreviation. It was more like when an English speaker says ""mute point"" instead of ""moot point.""",0
"Some great answers about the physical / market aspect of this question, but there's at least one other reason:

Sometimes the person installing screws *does not want* just anyone to be able to remove them. A property management company I worked with used a seven-pointed screwbit / screws on all of their work because they didn't want tenants to be able to take the screws out, and they wanted to know if any of them were replaced.

If you look at the screws used in some public restrooms, you'll see a version of this as well. They'll usually be non-symmetrical screwheads that allow them to be screwed in with a regular flathead, but not screwed out. This is intended to reduce vandalism.",0
"There are multiple factors there.

First, chips are made out of a single large wafer. Usually, each wafer (currently most famous ones are 300mm in diameter) will make tens to hundreds of chips depending on the die size. The smaller the die, the more chips you can make.

So a large die like a big GPU will need a lot of space on the wafer, the whole wafer which cost can be anything between $3K to $12K depending on quality and the target process required will have much fewer chips than a small die chip like a mobile SoC. for GPU's you might get something like 150\~170 320mm² high-end GPU's out of a single wafer, but smaller low-end GPUs are designed to be small, so you can get hundreds of them on a single wafer. A typical low-end GPU might be 77mm² which will give you roughly 810 dies. So this is one reason why a high-end product which tends to be large as it much more expensive to make, here you can see almost 5 folds the number of chips per the same wafer for just different die size.

Then you have things called yields and defects. But let's start with defects as it's just a sad part, while they always make these chips in very clean rooms, small defects particle will still find it's way into those wafer while in the making. So let's assume that 30-40 small dust particles stuck on the wafer, on the large dies, the high-end ones, this will basically make at most 40 dies not working properly, so out of those 150 dies, you can get only 100\~110 working chips. While on the smaller dies, you already have 810 chips, so you might get away with 760 chip already.

&#x200B;

That's why, while making chips, especially large ones, the designers will make the design flexible, they can completely disable parts of it and still make use of the chip, this can work like magic for things that contain a lot of similarly designed blocks, like GPU's, or Multi core CPU's, as when a defect is affecting a part of the GPU cores/shaders, or the CPU cores you can just disable that part and things will work. But if the defect happens to a crucial part that the GPU/CPU can't just work without it (like the scheduler) then that chip will be dead.

&#x200B;

Some times, the chip designer will intentionally make extra logic just to increase the working chip yields, or they will just assume having less than the actual hardware logics so they can increase the yields of qualified chips. For example the PS3 Cell processor actually have 8 logics called SPE, but the requirement for the PS3 is just 7 SPE's, so and chip with at least 7 SPE's working is qualified to to be tested (other factors includes clocks, power, temps, etc..). This made chips that have either 7 or 8 working SPE's are qualified which will be much better yields than only 8 working SPE's.

&#x200B;

For other consumer grade products, partially defective chips can also be sold under other product segments. for example GeForce 1080, 1070 & some 1060 are all based on the same die called GP104, while the larger die called GP102 is used to make the 1080Ti, Titan X, and Xp. The GP104 is the same chip here, just the 1070 is using a partially defective chip, so NV just disabled some shaders and other logics and re-used the chip as 1070. If the chip contains more defect, it can be disabled also and used as 1060 as well.

&#x200B;

The same can be applied to CPU's, now CPU's have many cores, and we have many segments also, so if a CPU die have one or two Cores not working properly then it can be used for a lower segmented CPU. Both Intel and AMD do this actually, some i5's are using a partially defective i7 die actually.

&#x200B;

But some times the die might not be defective, it might be working, but it's of a low quality one, this is called binning, usually on the wafer, the dies closer to the center have better quality or to say characteristics than the ones which are on the edge, these qualities are like ability to work faster using lower voltage/power/temps, better overclockability. etc.. This what make it different for products like an i7 8700K and a regular i7 8700, or like Ryzen 7 1800X and Ryzen 7 1700X or Core i5 9600K and Core i5 9400, Both are the exact same chips but the former can be clocked higher on stock while maintaining the required voltages, temps and power consumption, or it can also overclock better too, some differences can be small like Ryzen case but some differences can be big like the i5 case where the product is marketed with a different name.

Edit: small corrections (including the main typo: the wafer diameters 300mm not 300m), and the Ryzen example.  


Hell thanks alot for those Silver, Gold & Platinum !! all are my first ever !.",0
"In theory yes, in a genetically identical twin (so complete HLA match at all loci). This is based on what we now do in treatment of cell therapy for cancer patients (it also happens to be my own research :) where we take patient's own anti-tumor T cells expand and infuse back to them 

You can do it two ways. 


1) Find and isolate  T cells reactive against the virus or infection, expand them in large flasks and infuse them back into acceptor. Advantage here is that you can take a pool of reactive T cells so you get broader repertoire targeting the infection. But this has to be like you said HLA matched so your acceptor T cells don't kill donor T cells, so genetically identical twin might work

2) If you already know the specific protein and Epitopes from the infection you are targeting say SARS COV2 protein S1 some epitope restricted by patient's HLA A0201 (one type) for e.g. you can also isolate the specific T cell receptor (TCR) gene responsible for the targeting and engineer your acceptor twin's healthy T cells to express the TCR. *This is called TCR therapy*. The advantage here is that you control the specificity since you know exactly what you're targeting, so there is less chance of off target effects. Now the con is if you're targeting say SARS COV2 you likely need multiple proteins targeted on the virus and you lose the advantage of the broad repertoire that's available in #1 (each TCR from a T cell only recognizes one epitope from a given protein region).  #2 however has the huuuuge advantage that you don't need to have perfect HLA match since you can still use the acceptor's own T cells. All you're engineering into it is the actual TCR gene that is targeting the infection. *So forget your identical twin example, if I currently have a disease and you have already recovered from it, if you and I share even 1-2 HLAs it's possible we can take your TCR against the virus and genetically engineer the TCR into my T-cells*. Cool huh.

However, I have to warn if you're thinking in terms of COVID19 you likely have to intervene much much earlier in their disease course. The reason being, the disease causes severe respiratory failure like ARDs and symptoms are similar to when you have cytokine release syndrome in patients undergoing T cell therapy. So it unclear if you will be helping the patient if you give this amount of cells at a late stage of the disease, since the first stop all the billions of T-cells you infuse into patients are going to go straight to guess where : Lungs. They might severely damage the lungs which is already fighting an infection. Nevertheless it's unexplored and opportunities abound.

Edit: minor details",0
"It is important to finish the whole course of antibiotics because doing so helps ensure that all the bacteria causing the infection are completely eradicated. When you start taking antibiotics, they begin to kill off the bacteria causing the illness. However, not all bacteria in the body are killed at the same rate, and some may be more resistant to the antibiotics. If you stop taking the antibiotics prematurely, even if you start feeling better, there is a higher risk that some bacteria are still present in your body.",1
"Rabbits have a condition known as GI Stasis, where food stops being digested, due to a combination of factors, including eating less, less movement of material through the system, and buildup of ""bad"" bacteria in the GI tract.

Humans suffer a superficially similar problem, known as gastroparesis, where the muscles in the GI tract are not working as well as they should, thus material moves more slowly, and less nutrients are absorbed.

So even though you eat something, you may not be able to get any nutrition from it. And that is just one of many mechanisms, you could have issues with production of digestive enzymes, bile, stomach acid, etc, which can stop or slow down absorption.

Another way to look at it, though, is this; if you have a person dying of starvation (or rather, massive organ failure, because of a lack of nutrients), who just died, but their cells are mostly still alive, if you pour food and water down their throats, do they come back to life? Of course not. It's just a question of what fails and how, and that point of no return is probably highly variable, and perhaps subject to what sort of medical care a person can get.",0
"For the implied backup question: why did they go with one design over the other?

The explosives needed to make Fat Man work had to be incredibly precise. If they all didn't go off in the exact correct shape at the exact right time, it would have shot the Plutonium out of the shell like a cannon instead of compressing it. Frankly, the explosives were possibly a greater engineering challenge than the nuclear part. 

Little Boy was much simpler, engineering-wise. They both took an astronomical amount of resources to build, but Little Boy they were almost positive would work. Fat Man was much more efficient, but required testing and had a higher potential failure rate. We went with Fat Man style bombs going forward, hence all the nuclear testing, but we had Little Boy as backup to make sure *something* went off.",0
"The extra 71.7 hours, known as the leap hour debt, is not accounted for by the addition of an extra day in February during leap years. Instead, it accumulates over a much longer period of time. The Earth's rotation is gradually slowing down due to tidal interactions with the Moon and the Sun. This is caused by the transfer of angular momentum from the Earth's rotation to the Moon's orbit, resulting in a lengthening of the day.",1
"The amount of water makes no difference. Only the height of the column of water. The taller the column the higher the hydrostatic pressure at the bottom. 

Not sure how much you could contain with your finger. Maybe 50-100 psi. 

For a column of water weighing 8.3lb/gal pressure=0.052 x 8.3 lb/gal x depth

Assuming you can hold 100 psi then using the above formula, anything over 231 ft deep you couldnt hold back the water pressure any more.

These are calcs I use in the oilfield all the time.",0
"Kinda like a continous batch.

The stomach doesn't digest the food though. The stomach has the essential enzymes for protein digestion (Pepsin, Kathepsin) which cracks the polypeptides in small digestable chains. Fat basically just passes through the stomach but get made more liquidous through peristaltic. Carbohydrates are also just passing the stomach since the enzymatic digestion of the alpha(1-4)glycosidbinding of amylose through alpha-Amylase gets inhibited through the low ph Value of the stomach. 
Which is the next function of the stomach. It inhibits the growth of a lot of harmful microorganisms.

The stomach passes the ""mash"" to the intestine. The pylorus takes care that everything stays long enough in the stomach so that it get ""worked up"" enough and supply the intestine slow and continously.",0
"Ok, so I work in the industry of antimicrobial testing, and no, it's not a legal disclaimer, we test and see how much of a log reduction a product gets and we literally scrub the shit out of the device or surface or whatever, so no, it's not a CYA claim, it is based on actual FDA or EPA regulated testing on very specific strains of bacteria, fungi or viruses, or appropriate surrogates. The remaining bacteria or other microbes not killed may very well develop resistance and there are even concerns that some previously non-pathogenic strains of E. faecium and faecalis have evolved to be resistant to alcohol based sanitizers. And yes, sterilants can kill even super bad spore strains, but the contact times are like 20 minutes for even the best, trendiest ones used on devices. And those sterilants are pretty nasty. Basically, don't overuse any antibacterial substance, and know that manual scrubbing of contaminated surfaces with any cleanser or disinfectant goes a long way, a product just works better when you scrub and use at the proper contact time. ",0
"I'll jump in from a physical therapist perspective.

There are two types of ""mechanical"" headaches we see and treat: tension-type and cervicogenic. By mechanical I just mean to differentiate from something neurological like a stroke or a migraine (which may have more mechanical factors involved than we used to think, but I digress..).

Tension-type refer to any muscle in the neck or jaw ""referring"" pain into the head. One theory is the ""trigger point"" theory, which says an ""active"" trigger point (one that is actively causing you pain) can send said pain elsewhere. The most common headache muscles are:
Upper fibers of trapezius - generally refer to temporal aspect
Sternocleidomastoid (SCM) - similar pattern to trapezius
Suboccipitals (4 muscles on both right and left) - occipital headache and/or ""behind the eye""
See: https://www.ncbi.nlm.nih.gov/pubmed/16863699 as one study discussing trigger points and headaches.

Cervicogenic headaches (see https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3201065/) are caused by a few different mechanisms. One is the proximity of the trigeminal nerve to the nerves of the upper neck - perhaps as neck pain occurs in this area it ""overflows"" to the trigeminal nerve and into the head. The other is similar to trigger points in that our spine facet joints can refer into the head.

In answering the original question, the area of the headache can give some insight to the origin of the headache, but oftentimes patients have a combination of the two types and blending occurs.

Headaches can also occur from post concussion syndromes and vestibular issues, neither of which garner much information at all from location of headache - unless again you're dealing with a multifactorial issue like a concussion that had all types involved in some way or another.",0
"You mean ""micro gravity"" as ""anti-gravity"" is more of a science fiction term ([micro-gravity is NASA's term](https://www.nasa.gov/audience/forstudents/5-8/features/nasa-knows/what-is-microgravity-58.html) about living and working in/on spacecraft). Or more likely just ""weightlessness"".

Female astronauts have the option of carrying their preferred pads or tampons while in space ([Popular Science 2016](http://www.popsci.com/brief-history-menstruating-in-space)). If they choose to not get their periods they can medically induce amenorrhea, or stop their periods from happening ([The Atlantic 2016](https://www.theatlantic.com/health/archive/2016/04/menstruating-in-space/479229/)). There was [a scientific paper](https://www.nature.com/articles/npjmgrav20168) written on medically induced amenorrhea in female astronauts in 2016. 

There are several ways to induce amenorrhea, through hormonal pills, injections or intrauterine device (IUDs). While it is safe to have your period in space, it may not be all that convenient to switch pads or tampons and deal with disposal. The water treatment is also set up to recycle water from urine, but not menstrual blood so that is also an issue ([American Chemical Society 2014](https://www.acs.org/content/acs/en/pressroom/presspacs/2014/acs-presspac-april-9-2014/recycling-astronaut-urine-for-energy-and-drinking-water.html)).

According to National Geographic (link below) if a woman spent three years in space she would need about 1,100 pills, which adds some weight to a mission but is less unwieldy than many tampons. Alternatively, three years worth of tampons (or pads) still weighs more than three years worth of pills.

Female astronauts' periods have a long history. National Geographic has a great opening paragraph in one of [their articles](http://phenomena.nationalgeographic.com/2016/04/22/how-do-women-deal-with-having-a-period-in-space/) on the subject:

>Sally Ride’s tampons might be the most-discussed tampons in the world. Before Ride became the first American woman in space, scientists pondered her tampons, weighed them, and NASA’s professional sniffer smelled them—better to take deodorized or non-deodorized?—to make sure they wouldn’t smell too strongly in a confined space capsule. Engineers considered exactly how many she might need for a week in space. (Is 100 the right number?, they famously asked her. No, Ride said. That is not the right number.)",0
"The immediate ""obvious"" answer most people will give is that no, you will not move. This is obvious because the blower will experience a backwards force and when that air hits the umbrella it will experience a forwards force that cancels out. Conservation of momentum and all that.

What's obvious in an ideal world doesn't actually hold true in the real world. There are two problems with the above explanation.

1. The backwards force experienced by the leafblower is not necessarily equal to the forwards force experienced by the umbrella. Fluid dynamics isn't very nice at being symmetrical, see the [Feynman sprinkler](https://en.wikipedia.org/wiki/Feynman_sprinkler) as an example.

2. Conservation of momentum actually states that the net force is *not* zero. The air starts stationary behind the blower, and ends up with some velocity after it hits the umbrella. Either some air gets redirected back or some air makes it past the umbrella. With a correct shape and large enough area, the situation described will actually cause the skateboarder to move forward.

For a real life analogue of the OP's situation, see [reverse thrusters](https://en.wikipedia.org/wiki/Thrust_reversal). Those are identical to the situation described, yet they clearly cause a net force in the direction of the ""umbrella"".",0
"Viagra is prescribed to women all the time under its other common market name - ~~Rovatio~~ Revatio . It's mostly a semi-specific vasodilator so it's used to treat pulmonary hypertension and right heart failure. There is no special relationship between the penis and Viagra other than the penis has blood vessels.

EDIT: Now that I have more time and a real keyboard. Viagra is also a fairly potent performance-enhancing substance for endurance sports. For one of my research rotations, we tested it in a group of athletes that included men and women. We eventually stopped calling it Viagra and just said sildenafil because the guys wouldn't stop making jokes and the women got freaked out and either dropped from the study or it affected the results. In both men and women, it improved performance about the same amount, they both felt accelerated heart rate over what they typically experience at those intensities, flushing in the face and neck, and lightheadedness after the tests. None of the subjects reported sexual side affects (we specifically asked).",0
"I did take my typhoid vaccine orally, and polio is/used to be delivered the same way, so it works for some formulations of vaccines.

However, others are too fragile to endure stomach acid. This is why some oral medications have a special acid-resistant coating on them and “DO NOT CHEW” on the instructions!  But it doesn’t work for all of them. Typhoid is foodborne, so it makes sense that it would survive the stomach.

Edit: the oral polio vaccine was discontinued in the US in 2000, but oral typhoid was still in use here as of April 2019",0
"There's a few potential causes, but I should preface this with stressing that immunology is a poorly understood and extremely variable area of medicine - meaning this answer is not going to be all encompassing and the theories presented may not be the most up to date information available.

Generally, there's two main schools of thought and they're both closely related.

1. Sensitisation

Over time, your body learns and adapts it's immune system. If it just so happens that eventually it decides that something you've been exposed to is a threat, it will mount a response.

This response is reinforced on repeat exposures, strengthened and ultimately becomes an ongoing allergy.

Why now? Why in this form? Essentially bad luck combined with repeat exposure. Since the pollen is present often, it can be thought of as ""only a matter of time"" before this process occured and it happened to be now.

I dislike this theory because as much as many of the body's processes have a random element, it seems unlikely that developing an allergy could be entirely random.

Which leads to

2: Misassociation 

Think scenario similar to the above, but involving a concurrent infection, virus or pathogenic substance.

Your body goes on alert after an exposure to a severe virus, it wants to make sure that whatever caused it never happens again. It just so happens that every time your body is exposed to that virus, it's also exposed to the pollen (because it's pollen season).

So your body mounts a response against both and you develop an allergy as a result. Exactly how this association gets made isn't terribly clear, but my understand is that this is the prevailing theory.

You can see good examples of this when a tick bite can trigger an allergy to red meat - even if the tick was picked up in an area with no cattle for any related proteins to be carried over.

As a sneaky third option:

3. Your body hasn't formed a new allergy, there's just a new pollen or funghi in your area now that you hadn't been regularly exposed to before.

Maybe you moved recently, maybe there's a new weed growing. I couldn't tell you, but it's a bit of an Occam's razor solution.",0
"Yes it did, the numbers will never be known though since the government had no testing capabilities like we have today.
Asymptomatic infections happen because of the  slight differences in the immune system from person to person that are caused by genetic variation.
Some people are just bound to have a immune system that has a better handle on the disease than average. The same thing can be seen with most viral or bacterial infections, and has been observed even in people with HIV.",0
"Oh shoot!  As a geoscientist and a huge Subnautica fan, I'm sorry to come in late on this.

**No**, the lava depicted in the lava zone is completely unrealistic (but *so* cool.)  Let me comment on the pieces of the answer that people have already given:

As /u/Little_Mouse points out, real underwater volcanism on Earth doesn't have much glow to it: the water cools the lava so fast that it's almost all dark except for a few glints of red.  Their video was taken at shallow depth by a scuba diver: here's a video from 1 km deep, similar to the lava zone in Subnautica:

https://www.youtube.com/watch?v=hmMlspNoZMs

No glowing pools, no red lava falls.  Water is a fantastic reservoir for heat, and the fact that warm water rises lets it carry away heat by convection really *really* well.

/u/PresidentRex has a great analysis of pressures and the phase diagram of water, but there's one thing they didn't realize: **hot supercritical water is always less dense than cold**, as shown in the graphs [here](http://www1.lsbu.ac.uk/water/water_density.html).  Thus, there will be no ""stable layer of supercritical water"": it would be buoyant, rise, and be replaced by cool water, carrying away heat by convection.

What if the layer of water near the lava surface had a ton of salts dissolved in it, so it was denser?  As /u/Bassmanbiff points out, the thermal radiation law applies to *everything*, not just rock: the supercritical water layer *itself* would glow.  But that's clearly not what we see in Subnautica, and in any case the water above this layer would still convect, rapidly cooling it just as if it were lava itself.

Finally, as /u/UniqueUserTheSecond points out, there's a thermometer in the game, and it reads 70 degrees C in the active lava zone.  That's probably a reasonable temperature, actually -- note that in the video I linked to, the submersible isn't damaged by the volcano's heat, and /u/Little_Mouse 's video was taken by a scuba diver swimming just a few feet from the lava!  But this is nowhere near the temperature at which stuff starts to glow -- no matter what stuff.

As a side note, several people are commenting on air pressure and O2.  One thing's for sure: the way Subnautica handles air and breathing at depth is completely wrong, and trying to dive the way you can in Subnautica would kill you dead.  Nobody in the real world has done a dive on pressurized gas to a depth greater than 700 meters, the people who've done it to a depth below 100 meters only do so with hours of preparation, a special gas mixture, and slow cautious pressure changes, and even then many people who've tried to dive below 300 meters have died.  The vehicles and seabases behave as if they are at sea-level pressure (if they weren't, they wouldn't implode if you take them too deep), but you can't just hop from 800 meters of pressure into your sea-level pressure vehicle without dying immediately.  And let's not even talk about how moonpools work....

Of course, a realistic approach to lava and air pressure wouldn't make for nearly as fun a game!",0
"Bloodsucking insects will become engorged after feeding. Mosquitos, bed bugs, lice, ticks, fleas, ticks (arachnid), are visibly larger after feeding.

Perhaps more relevant is the Fat Body.

Insects have an organ called the Fat Body. Lipids (fats) are stored here in adipocytes in the form of triglycerides (same way mammals store fat, essentially). These lipids are consumed during periods of high energy demand (like when flying), and are replenished in periods of food abundance. 

Some insects have been shown to increase the size of the Fat Body in the winter, as a mechanism to enhance survival. Other insects (like house flies) don't seem to be able to store extra fat. 

edited to add some sources: 

Study on fat storage in Culex pipiens: https://www.ncbi.nlm.nih.gov/pubmed/2769712/

review paper on the insect Fat Body:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3075550/",0
"Splat and then burn (pretty much happening at the exact same time).

Ok, first some pedantry, melted rock at the surface of the Earth is called lava, not magma (magma is the same material when still beneath the surface). Two things to consider, lava is quite dense and viscous (i.e. it's thick). Both of these actually vary as a function of composition, but if you have an exposed pool of lava (these really aren't that common) it's likely a [lava lake](https://en.wikipedia.org/wiki/Lava_lake), which is usually [basaltic](https://en.wikipedia.org/wiki/Basalt) lava. The density of basaltic lava is in the neighborhood of [2700 kg/m^3](https://en.wikipedia.org/wiki/Magma#Density) and viscosity is around [10^1 -  5x10^2 Pas](http://courses.washington.edu/ess439/ESS%20439%20Lecture%203%20slides.pdf) (depending on temperature). For reference the density of water is ~1000 kg/m^3 and viscosity is in the neighborhood of 1x10^-3 Pas (again depending on temperature), the viscosity of basaltic magma is more akin to [somewhere between honey and peanut butter (1 CP = 1 x 10^-3 Pas)](http://www.vp-scientific.com/Viscosity_Tables.htm). The average density of a person is also around 1000 kg/m^3 give or take. So, you would be jumping onto a very dense (compared to you or liquids we're use to jumping into) and very thick material, so you would definitely splat. 

The complication to your splat is that basaltic magma is REAL hot, around [1300 degrees Celsius](https://en.wikipedia.org/wiki/Magma#Composition,_melt_structure_and_properties), so you would catch on fire pretty much immediately, likely even before you hit the lava surface because the air immediately above the lava is also going to be very hot. A good analogue of what would happen can be viewed [in this video](https://www.youtube.com/watch?time_continue=1&v=kq7DDk8eLs8) in which some volcanologists through a bag of wet, organic trash into a lava lake. The response of the lava (i.e. the fountaining) is because of the addition of water (which would also happen if a person jumped in, as we are pretty water rich).

EDIT: To add, while stable pools of other kinds of liquid lava generally don't exist on Earth, even if they did, the answer would be the same. The density of more silica rich lavas (like andesitic or rhyolitic) is slightly less than that of basalt and their temperatures are not as hot, but still much denser than a human and still really hot, and as a bonus these silica rich lavas are actually much more viscous so you would be met with an even thicker material.

**EDIT 2:** Myself and others have responded to some of the (very frequently) repeated questions, but here are answers to a few.

**Lava vs magma distinction:**  Location is the primary difference, but the difference in location leads to different properties that make the distinction useful. One of the biggest is the absence vs presence of dissolved gases. Magma, as it is still underground and under pressure, can have a decent amount of gases (water, carbon dioxide, various nitrogen and sulfur compounds, etc) dissolved in it. For basaltic magma, the concentration is low, around 1-2%, for more silica rich magmas like rhyolitic magma, it may have upwards of 8% dissolved gases. The presence of these gases changes the composition of the material and also the types of minerals that can/would form if you crystallized the melt. Once it reaches the surface and the pressure is released, these gases come out of solution, and you would now call it lava. An imperfect analogy (especially the temperature aspect) would be the important distinction between if someone asked you if you'd like a soda, you would probably assume that they were giving you a cold, bubbly beverage (magma), but if instead they handed you a warm, flat soda that had been sitting on the counter open for a few hours (lava), you would question as to why they had not specified that it was a warm, flat soda, but in the process perhaps better understand the magma-lava distinction.

**Viscosity of lava:** We have both calculated and measured the viscosity of lava directly. [This PDF (jump to the eight page, to the Rheology section)](https://pubs.usgs.gov/pp/1801/downloads/pp1801_Chap9_Cashman.pdf) from the USGS describes this a bit for Hawaiian basaltic lava. In a similar vein, for the few asking, yes, lava is a non-newtonian fluid, this is also talked about a bit in the pdf.

**Location at which you catch on fire:** All of the people convinced that you would catch on fire before impacting the lava and all of the people convinced that you would only catch on fire after impacting the lava should 1) stop messaging me and 2) start messaging each other. In all seriousness, the heat of the air around lava is going to depend a lot on the details. This [page from the volcano group at Oregon State](http://volcano.oregonstate.edu/how-close-can-i-get-lava-and-will-it-hurt-or-kill-me) does a good job of describing the factors that make approaching lava safe vs unsafe. Basically, if there is an intact crust on top of the lava, you would probably not catch on fire until you broke through that crust, but this is not a definitive answer.

**What about this depiction of lava in this movie:** They're mostly all wrong. Gollum would not sink. The dude from Volcano in the subway would not melt. You could not drive a car across an active lava flow. You (or not even Chris Pratt) could not outrun a pyroclastic flow (or survive being engulfed in one). Etc.",0
"> Robertson-head and allen-head fasteners can handle more torque than phillips-head fasteners, but are more expensive. Because the bottom of the hole is flat (unlike the pointed end of the phillips), there's more contact area and so it's less likely to cam-out.

While I thank you for bringing up Canada's contribution to the fastener world, a couple of things... Robertsons aren't Square-Drive.  While they're square in cross-section, the driver and the hole in the screw are slightly tapered, and the bottom rounded. This makes it much harder to strip a Robertson. 

It also means that the screw can stick to the driver even when both are non-magnetic, which is really handy. ",0
"In general, a single failed transistor is unlikely to kill the entire CPU. CPUs are designed with redundancy in mind, meaning they have multiple transistors performing the same function. This redundancy helps ensure that if one transistor fails, there are other functioning transistors that can take over. Modern CPUs also employ error detection and correction mechanisms to identify and correct errors caused by transistor failures.",1
"[Here's a diagram of a TRRS audio jack](http://www.circuitbasics.com/wp-content/uploads/2015/03/TRRS-Wiring-Diagram-300x224.png). You'll see that the connector is divided (separated by insulators into distinct conducting strips). The reason this is called a TRRS audio jack is that it's broken into 4 different conducting strips, called Tip, Ring, Ring, Sleeve. There are also TRRRS jacks which have an extra ring and thus 5 conducting strips in total.

To do mono audio, you need 2 conducting strips (audio + ground). To do stereo audio, you need 3 conducting strips (left audio + right audio + ground). If you have 4 or more conducting strips, then you can have stereo audio plus some other form of communication. The diagram I linked to you has the 4th strip be a microphone, but some smartphones will use the 4th conducting strip to send control information such as ""pause"" and ""play"" commands.

Unfortunately there's no one standard for how TRRS and TRRRS jacks are used. Different devices and different headphones will make different (incompatible) decisions on what to do with the extra strips. If you're going to buy headphones with a TRRS or TRRRS connector, you just have to check beforehand whether it's coincidentally going to be compatible with your phone.

The most common protocol used by phones is called CTIA~~or OMTP~~. (Edit: upon further research, CTIA and OMTP are 2 different standards, but seem to be largely compatible in this area). It's defined by the Cellular Telecommunications Industry Association. Note that other audio and video equipment will use the same jacks but be electrically incompatible in the higher rings of the jack.",0
"Hey, I work in pancreas development so this is a topic I know fairly well, but don't directly study.

Anyways, pancreas transplants. First an important note- to treat diabetes, you don't need a whole new pancreas. In fact, you only need two percent of one! The pancreas is comprised of two tissue types- exocrine tissue, which aids in digestion, and endocrine tissue, which secretes hormones to regulate blood sugar levels. The vast majority of the pancreas is exocrine tissue, while the endocrine portion, comprised of approximately one million micro-organs called ""islets of Langerhans"", is the only part that secretes the hormones you need to ""cure"" diabetes (including most notably insulin). The two components function largely independently, and it is rare that patients with diabetes have exocrine dysfunction, so actually you only need those darn islets and not the whole pancreas.

So, what then are the barriers to islet transplantation? The major issues are two-fold. First is getting (enough) islets. It is a non-trivial task to harvest intact and functional islets even in laboratory animals (mice), to say less of a much larger and rarer human pancreas. Remember how the vast majority of the pancreas aids in digestion? Well it does that by making digestive enzymes, and you can imagine what happens when a potential donor passes away- those enzymes get released willy-nilly and start breaking up anything nearby, including our precious islets. It's estimated by the NIH that [only slightly more than half of decedent donors, which are rare enough already, are viable for islet transplantation](https://www.niddk.nih.gov/health-information/diabetes/overview/insulin-medicines-treatments/pancreatic-islet-transplantation). Even if you get a good donor, islet recovery rates aren't perfect and you usually get maybe half of the islets available, sometimes far less. This then necessitates pooling of islets from multiple pancreata (the fancy scientific plural of pancreas), which currently averages out to I believe two good donor pancreata per recipient. It also makes living donation of islets sub-optimal, since again a single donor is unlikely to provide enough viable islets.  

The other major issue is engraftment. To start with there are the usual complications with allo-transplantation- potential rejection, a need for immunosuppressants, etc. By contrast, one pro with islet transplants is that you don't have to connect a bunch of complicated blood vessels like you do with whole organ transplant- we've injected islets into the liver, kidneys, under the skin, even under the capsule of the eye, and had them engraft successfully. However, a great number of the transplants fail, and we don't know completely why- this is one of the hot topics of pancreas research. Perhaps it's poor islet [re-vascularization](https://www.ncbi.nlm.nih.gov/pubmed/24106517). Perhaps it's [islet inflammation destroying islets](https://www.ncbi.nlm.nih.gov/pubmed/25777058). Or a thousand other things acting in concert- I study a molecule called Hmgb1, [which is elevated in islets that engraft poorly](https://www.ncbi.nlm.nih.gov/pubmed/25058889). The current debate is whether this is causational- do transplanted islets secrete Hmgb1, causing islet failure, or are islets secreting Hmgb1 because they are injured? Research seems to be leaning toward the former.

So to summarize- you don't need the whole pancreas, just the islets. However, it's hard to get enough islets for transplantation, and it's hard to get the islets to function properly/not die in the recipient. On a final note, this is why xenotransplantation (islets from other animals, usually pigs) and stem-cell derived islets are such hot topics in the field. They circumvent the first problem and in the case of stem-cell derived islets, a lot of the second. 

edit: Several posters have astutely pointed out that getting new islets would likely not circumvent the autoimmune disorder that underlies most Type-1 diabetes cases, and would eventually result in transplant failure in that subset of cases. I didn't discuss that issue because 1) it's far outside my specific expertise, and 2) I didn't think of it. /u/SPACE_CHUPACABRA has a great comment below [addressing some of these issues](https://www.reddit.com/r/askscience/comments/8nhhi7/why_cant_we_perform_a_pancreas_transplant_for/dzvynnk/). 

edit 2: Another common question is why not just transplant the whole pancreas then, if it's so difficult to extract islets. Whole pancreas transplant as you can imagine is a major surgery with major risks. Often patients suffering from advanced stages of diabetes or other pancreatic diseases are in bad shape, which increases the risks for complications even further. By contrast, islet transplantation is usually done through catheterization, sticking a long tube into your liver, which doesn't even require general anesthesia. ",0
"The most commonly accepted idea is that we don't actually have any concrete evidence of what our consciousness actually is. Closest scientific explanation would be that the human/homo sapien brain and intelligence was the driving force behind our species' evolutionary split from our other common ancestors. Consciousness would most likely be the product of our increased brain development/evolution through natural selection. Each generation was smarter than the last with primitive homo sapiens having their increased cognitive abilities being naturally selected for with their reproduction. This process was working for a long time, starting as early as early as 3.8 million years ago with our oldest common ancestor, and continued through around 300,000 years ago for homo sapiens to first appear. 

It's thought (or at least assumed) that our egos and consciousness came about from the increasing improvements to the human brain structure as it evolved throughout the millennia. The part of our brain responsible for our own feelings of self is the [Default Mode Network.](https://en.m.wikipedia.org/wiki/Default_mode_network#:~:text=In%20neuroscience%2C%20the%20default%20mode,cortex%2Fprecuneus%20and%20angular%20gyrus.) The DMN is a series of unique connections in the human brain that help link our different senses and experiences together. While this isn't completely unique to the human brain, it's significantly more developed than in any other species we've seen. We've seen how this part of our brain can be effected by clinical research and brain scanning of people experiencing different psychedelics that can make you experience ""ego death,"" (where you don't feel connected to yourself) because they reduce and suppress the effects and control the DMN has over your consciousness. 

It is still one of the biggest mysteries in the universe along with why there is even a universe we live in.",0
"**Short answer:** Yes. Flatulence would propel an astronaut forward very slowly, but if you used the gas as fuel for a combustion reaction the astronaut could get going much faster.

**Longer answer:** Gas diffusing will carry a small amount of momentum backwards, so it must exert a force on the person, pushing them forward. Essentially, farts are rocket fuel. So let's figure out how much and how fast a person farts, to figure out how fast an astronaut can get moving in space.

Anyway, [this paper abstract gives us a good idea of the average volume of gas produced by a person in a day.](http://www.ncbi.nlm.nih.gov/pubmed/1648028) They give it somewhere between 476 to 1491 mL, and [another paper](http://www.ncbi.nlm.nih.gov/pubmed/9176210) gives the composition as a mixture of methane, nitrogen gas, hydrogen gas, and carbon dioxide. Let's say the average person produces 1 L of gas each day and we'll [guess that this gas mixture is about 0.5 grams/Liter, which is not entirely unreasonable given the known masses of the gasses in the mixture.](http://www.wolframalpha.com/input/?i=1+mole%2F22.4+liters+*+1+liter+*+%2810+grams%2Fmole%29) That comes out to 0.5 g of flatulence every day for a normal person. 

Now, let's guess that a fart leaves the butthole at about 1 m/s - again, not entirely unreasonable. So putting all this together, we can find that a day's worth of farts carries backwards momentum equal to

    (1 m/s)(0.5 grams) = 0.0005 kg m/s

so for momentum to be conserved, the astronaut will now be traveling [7.7x10^-6](http://www.wolframalpha.com/input/?i=0.5+g+m%2Fs++%2F+65+kg) m/s forward, [which is only about 1000x faster than hair grows.](http://en.wikipedia.org/wiki/Orders_of_magnitude_%28speed%29) If an astronaut in space farted every day, it would take 10,000 years for him to get up to a normal highway speed. 

This is incredibly inefficient, but luckily, there's a better way. The gasses I listed above are combustible - specifically methane. Just spewing the gas backwards to get a push forward would be like putting your SUV in neutral and trying to propel it forward with a supersoaker that sprays gasoline backwards. Instead of *throwing* it backwards, you can *explode it* backwards to generate thrust, like a real rocket. After all, every 14 year old knows you can light a fart on fire, but if the astronaut did this the gas behind him would expand in all directions, not giving him much of a push. Instead, we need to harness this energy for a jetpack, so that all the exhaust goes backward.

If we take the methane to be about 1% of our flatulence, and the energy of combustion to be 890 kJ/mole, then we find that the [chemical potential energy of the gas is about 100 million times greater than the kinetic energy backwards.](http://www.wolframalpha.com/input/?i=%280.01+Liters%29+*+%2822.4+moles%2FLiter%29++++%28890+kJ%2Fmole%29). If we had one of those fancy [gas backpacks that they put on cows](http://www.springwise.com/img/uploads/2014/05/cowbackpacks.png) to harvest the methane from their farts and a jetpack to burn it, then [this gas would be enough to get a particularly flatulent astronaut up to highway speed in a day.](http://www.wolframalpha.com/input/?i=%282*%280.01+Liters%29+*+%281%2F22.4+moles%2FLiter%29++++%28890+kJ%2Fmole%29+%2F%2865+kg%29%29^%281%2F2%29) 

(Edit: /u/throwaway_MZ3Ji8yc offers a good discussion of the practicality of such a rocket in the [comments](http://www.reddit.com/r/askscience/comments/3569v1/if_you_farted_hard_enough_in_space_could_you_move/cr1mnpp) below.)
",0
"The ability to produce human speech involves a complex combination of anatomical, cognitive, and neural factors. While primates, including humans, possess the necessary anatomical structures such as larynx and vocal cords, their vocal capabilities are limited compared to humans. One key difference lies in the structure of the vocal tract. The human vocal tract, including the pharynx and oral cavity, is uniquely suited for producing a wide range of complex sounds.",1
"It's really, really difficult to sequence RNA and really easy to sequence DNA. Most RNA sequences are made by first converting the RNA into DNA by using enzymes that are RNA-dependendent DNA polymerases, also called reverse transcriptases. 

Once you've made the corresponding DNA (which is the reverse complement of the RNA, and is called cDNA), you just sequence that instead.

It's not perfect and it misses some stuff (like RNA editing), but it's generally really accurate and way the hell easier.

ETA: Thanks for the award. It's a bit...spooky, given the current time. But I guess cyber high-fives don't spread corona, right?",0
"Excellent Minutephysics video explaining exactly this. [Why is the sky dark at night?](https://www.youtube.com/watch?v=gxJ4M7tyLRE)   

Summary:  

* Universe had a beginning so there aren't necessarily stars in every direction
* Some of the far away stars light hasn't reached us yet
* The really far away stars light is red-shifted towards infrared (not visible to the naked eye) because of the expansion of the universe. 

Edit: To add in some points from the comments.  

* Yes some of the light from distant stars is blocked by dust and other objects in the way. The dust tends to absorb visible wavelengths and re-emit in the IR range which we can’t see but that wasn’t in the video so I didn’t include it in my summary.   

* Inverse-square law for light intensity. Intensity reduces massively over interstellar distances but that doesn’t really help answer the question because every star does this. Multiplied by an infinite number of stars in every direction, suddenly that tiny bit of light from each star adds up and the night sky should be far brighter than it is. For why it isn’t, I refer you back to the video and my original 3 points.",0
"No, the melting point of duck fat does not mean that it automatically melts at 57 degrees Fahrenheit. The melting point indicates the temperature at which solid duck fat transitions into a liquid state. On a 90-degree day, a living duck's fat would not be sloshing around as you suggest. The duck's body maintains a stable internal temperature through thermoregulation mechanisms. Ducks have feathers that help insulate their bodies, preventing excessive heat gain or loss.",1
"This happens with stars too; almost all stars are far too small for even our best telescopes to resolve them as extended objects instead of point sources, so we just see them as pin-points of light and how large they appear is just down to how bright they are rather than how big they are.  [Betelgueuse](https://en.wikipedia.org/wiki/Betelgeuse) is an exception, and we're able to image it's surface since it's fairly close and really huge to produce pictures like [this](https://en.wikipedia.org/wiki/File:Betelgeuse_captured_by_ALMA.jpg) and [this](https://en.wikipedia.org/wiki/File:ESO-Betelgeuse.jpg), though to the naked eye it's still just a point-source like that light-emitting atom.",0
"Reposting my comments from the last time this came up
----

I did a breakdown below to help people with the scale and context: https://www.reddit.com/r/science/comments/86bthl/great_pacific_garbage_patch_is_16_times_bigger/dw4kdwg/

In short, **if you cleaned up every spec of plastic in the entire 1.6 million square kilometers, and dumped it all into a Walmart, it would fill the Walmart 1 foot deep**.

That's it? Yep, that's it.

Still awful, and half of it is made from fishing nets, but, context is important to avoid sensationalizing things.

Some interesting tidbits because I hear about this all the time but never get a chance to grasp the scale:

- **92% of the plastic mass is large chunks**, baseball or bigger, but it will all eventually break down into tiny pieces.

- 1.8 trillion pieces of plastic currently. That's 250 pieces per person on the planet they say. That's sensationalist rhetoric. Most of the pieces are miniscule. **They know the reader will think about every person throwing 250 water bottles or toothbrushes into the ocean every year as a ""piece"", but, in reality a single water bottle might break down into 4000 micro pieces that they're counting**. While 92% of the mass is huge, 94% of the piece-count is rice-sized. This number is completely meaningless because if you took each piece and broke it in half and in half again, you'd have 7.2 Trillion pieces. Is that any worse? It's the same mass. The number of pieces is interesting maybe, but doesn't mean anything other than perhaps the degree to which the plastic is broken down.

- **46% of the mass of the plastic is fishing nets**. I'd never heard that before. HALF the mass is just fishing nets. That's where it's coming from. Nets are shitty for entanglement reasons too, obviously.

- There's ""only"" 80,000 tons of it in total. That sounds like a big number, but let me frame that in context. **That's only half of what an average landfill ads in a year**. An average landfill in the USA ads 150,000 tons a year, and they're usually around for 50+ years. The page says it's 500 jumbo jets. Well 500 jumbo jets is actually shockingly small, that's one jet, then a 2 hour drive to find the next closest one. **Or, think of a giant redwood tree, it's only 40 of those for the entire mass of the patch**. Think of seeing a giant tree, then driving 8 hours to the next nearest. To me, it's a shockingly small amount of garbage. This relatively small amount of garbage is dusted over an area half the size of the entire USA.

- Broken down (by me), while there's 250 pieces per person on earth, by mass, **there's 10 grams of plastic in the ocean per person on earth. Your share of that is about 2 plastic bottlecaps worth**. That actually seems like a lot.

- Volume-wise, **the size of all the plastic in the entire Great Pacific Garbage Patch, is about 3% the size of a single Walmart**. Think about one cube of plastic, 129 feet (43m) square. That's it. That's the entire patch. **If you ""cleaned up"" every scrap of plastic in the entire 1.6 million square km of the patch and threw it on the floor of a Walmart to house it, it would only reach half way to your knee**. It's really just, not that much plastic.

...

My concern is, **can it ever completely break down, or, what's the end-game of it?** I've heard that it will become microscopic in size, continue to poison or bioaccumulate in fish. But then what? Will sunlight/abrasion ever completely break it down like ocean water does to everything else, atomizing it?

Scooping it up while it's large definitely makes sense, as does not putting the stuff there in the first place, but, if half of it is fishing nets, presumably they just tear off on their own.

Overall I think this story is generally overblown because the dramatic name ""Giant Pacific Garbage Patch"" leads you to think of a country-sized landfill floating in the ocean. Still something worth addressing though.",0
"So true, especially your point about chikungunya. I actually just submitted a paper on a vaccine we developed against another alphavirus and one reviewer shit all over the fact that with so few diagnosed cases (disregarding the fact that they typically occur in remote regions without access to modern medicine to properly diagnose) that the pursuit of further development was not realistic. Wanted me to put in the paper something like ""hey, we did this and it was very effective, but you shouldn't care about it because this will never make it to market"". 

Of course we submitted to a neglected tropical disease journal. Like, I get their point, but damn, unless you have points of scientific merit, having your major issues with the paper be feasibility in go-to-market is totally asinine in peer-review.",0
"Edit: My answer below covers the mechanistic reasons for baldness (because I'm biochemist and that's the portion I know about) and why it occurs mostly to men. I'm not aware of definitive research on the evolutionary reasons for baldness so I've stayed away from speculating on that and tried to stick to what biochemistry/physiology does know. You are free to speculate about the why as much as you'd like, hopefully someone with a good understanding of hominin anthropology can likely fill in such details. Note that not all traits are positively selected so Male Patterned Baldness may just be a non-deleterious side-effect of sexual maturation. 

Hair follicles are mostly switched on by the presence of androgens (i.e. testosterone and dihydrotestosterone) and the follicles have two important reaction parameters; a testosterone sensitivity threshold and a kind of response strength.  The sensitivity threshold level sets how much testosterone must be circulating before a follicle switches over to producing mature hairs. Head and eyebrow hairs are examples of follicles with exceptionally high sensitivity. Very, very, very little testosterone/DHT is required for the follicle to switch on, mature and start producing hair. And this is why male and female infants quickly start producing mature head hairs. On the other hand pubic, underarm and beards hairs have low androgen sensitivity and this is why they do not switch on until the increases in testosterone/DHT levels seen at puberty.

Alongside this follicles have a response strength that dictates how vigorously the follicle produces hair once they are activated. Beards hairs have high response levels, eyebrow and arms hairs not so much. So beard hairs come in fast and thick. Scalp follicles also have a very strong testosterone/DHT response but they don't undergo significant changes at puberty as they are already fully mature when puberty arrives.

If just so happens that there is a loose correlation between this response strength and testosterone/DHT toxicity. Essentially the more strongly a follicle reacts to testosterone the more likely it is to die off after chronic DHT exposure. I guess you could think of it like the follicle being ""overworked"" but it is a little more sophisticated than that (see first link). As men produce the most testosterone their most sensitive and strongly reacting follicles are at higher risk of this toxicity, and these happen to be the ones on the scalp. And this appears to be the driver for Male Pattern Baldnss. The mechanism for this are not completely understood but this is a nice easy to read summary

http://www.medicalnewstoday.com/articles/68082.php

As I recall this is also a great review of the effects of androgens on hair development and it covers a lot of detail on the biochemical science of follicle maturation.
http://onlinelibrary.wiley.com/doi/10.1111/j.1529-8019.2008.00214.x/full",0
"Depends on the cancer's ability to metastasize. One can live for many years with small carcinomas in thyroid nodules that don't really do anything much, for example. And many people are diagnosed with latent cancer on pathology after they have died of something else.

If you get to the point where you are diagnosed, though, treat your cancer or it will probably kill you.",0
"I just read on Wikipedia that drinking five to six ice-cold glasses of water would burn an extra 10 calories a day. That would take 6 months to burn 1 lb. of fat.

I don't know about anyone else but that sounds like a lot to me.",0
"Hormone therapy can indeed have different effects on different parts of the body depending on the type and duration of treatment. In the case of hormone therapy for transgender women (male-to-female individuals), the primary goal is to suppress testosterone and promote the development of feminine secondary sexual characteristics. During hormone therapy, the clitoris can undergo certain changes due to the increased levels of estrogen. Estrogen can stimulate the growth of erectile tissues, leading to an enlargement of the clitoris.",1
"The purpose of utilizing different types of screw heads, such as Phillips, flathead, Allen (hex), and others, is to provide versatility and convenience in various applications. 1. Phillips head: The Phillips screw head has a cross-shaped recess and is designed to be self-centering, which reduces the chance of the screwdriver slipping off the screw head. It is commonly used in manufacturing and assembly lines because it allows for faster and more efficient tightening. 2.",1
"A transplanted organ will function according to the donor, so in this case it would be like an 80 year old organ. 

One exception to this is the liver. It had unique regenerative qualities not present in other organs. This is why it's possible to grow a full liver from a single lobe, the liver has 4 lobes I believe.",0
"It stimulates the olfactory bulb which send signals to the amygdala and the  hypothalamus. This may explain the euphoric effects of catnip, which would be mediated by the emotional centers in the amygdala. Activation of the hypothalamus can lead to species-specific instinctual behavior, such as feeding or mating.

Edit: [forgot the source](https://www.labroots.com/trending/cannabis-sciences/14044/catnip-weed-cats)",0
In obstetrics pelvimetry is used assess whether a women's pelvis is thought to be clinically adequate for vaginal birth. Typically the physician measures the diagonal conjugate (symphysis pubis to sacral promontory) and estimates interspinous distance (distance between ischial spines). Generally the limiting dimension for delivery is considered to be the ischial spines. The true limiting factors anatomically for vaginal birth are the bony parts of the pelvis that can't be seen externally. So someone with wider hips won't necessarily guarantee that they will have an easier birth. Not to mention the other two P's that affect ease of birth. The passenger (size of baby) and the powers (strength of uterine contractions). ,0
"The same way they kill their prey, and bug humans. They sting and bite.",0
"Your stool turns brown in the large intestine (colon). The brown color is NOT from bilirubin directly, but from a breakdown product of the bilirubin known as stercobilin. 
Stercobilin is produced by bacteria in the gut once they have broken down bilirubin (which is broken down from the iron-containing part of your red blood cells that gets released when a red blood cell is destroyed, also known as heme). When red blood cells are injured, heme is released, and it gets absorbed by white blood cells (macrophages, basically these big dumb goons that float around eating whatever doesn't belong in the body) where it gets processed and then sent back into the blood to the liver. The liver will further process the Particle Formerly Known as Heme into bilirubin, which is then stored in bile and released into the duodenum (the part of your digestive system immediately after the stomach). 
As the bilirubin progresses through the gut, it will be converted by bacteria into something called urobilinogen. Urobilinogen can be released in the blood or stay in the digestive system for excretion. The urobilinogen that remains in the gut will get eaten by bacteria in the colon; the waste product from these bacteria is stercobilin, and that's what gives poop its brown color.
Whether you have an iron deficiency or not doesn't affect the color of your stool very much, unless the bleeding is in your actual GI tract which can cause black or tarry stools (from the clotted blood).

TLDR: Colon 

edit: Wikipedia is a surprisingly excellent resource for medical knowledge. Sometimes the explanations are difficult to understand without background knowledge, but usually they're pretty good

https://en.wikipedia.org/wiki/Stercobilin

Double edit: Thank you so much for my first-ever Reddit gold! I always knew it'd be for talking about poop. And the follow-up questions are awesome too, thanks guys",0
"Even then, it's worth considering what triggered the positive diagnosis.

There will be many 74-year-olds who have had Covid but were never tested - whether their symptoms were mild to non-existent, because they refused testing, or because there were no tests available. Where the test is prompted by exposure, rather than concern for the individual's health, the unadjusted odds of survival will be significantly above 92%. Nonetheless, it's likely that the number of undiagnosed cases in 74-year-olds will be smaller, as percentage of the total, than in the population at large, because of the greater likelihood of severe symptoms.

On the other hand, as you say, any weight gain, cardiovascular issues, or diabetes, would also depress the survival rate.",0
Wouldn’t Saturn lose a bunch of its atmosphere (and therefore mass) as the sun goes red giant? Would the increase in radiation from the sun strip it away?,0
"I did my PhD thesis on appetite regulation. There are multiple targets in the regulation of appetite hormones, but because of the annoying/beautiful complexity of the whole system, there is often compensation when one specific thing is targeted. Or, like some drugs that have been tested, your gut transit is slowed too much and you projectile vomit. Which is one way to lose weight, I guess 😆",0
"You did not ask a stupid question. When trying to understand these conventions of science, you pretty much can't ask a stupid question. In fact, I would argue it was an **important** question, and the teacher wasted an opportunity to stress the usage of the mole to the class. 

The mole refers to a number of things, just like a dozen. You can have a dozen eggs, but also you could have a dozen molecules of caffeine. You could have a mole of caffeine, but you also could have a mole of eggs. This is important because chemistry cares more about the *number* of molecules than the weight of those molecules. 

Furthermore, consider the following balanced equation: 2(H2) + (O2) -> 2(H2O). Given 2 moles of H2 and excess oxygen, you know you can produce 2 moles of H2O. Using moles allows us to compare the actual quantity of molecules, whereas with weight it would be difficult to compare in such a neat fashion. Given 200g of H2 and excess oxygen, you have to do some annoying math to first convert to moles, then convert back to grams. 

Mass is, like you noted, more useful because it's easier to measure. You weigh chemicals with mass because it's easier, and because we're capable of converting to moles. That said, it's not uncommon to have percentages which are based on weight. Mass by mass, mass by volume, and volume by volume (m/m, m/v, and v/v respectively) are all common, with the first being solids in solids (e.g. alloys), the second being solids in liquids (e.g. solutions), and the third being liquids in liquids (mixtures and some solutions). ",0
"tl;dr: We don't know but it could have something to do with reduced energy supplies, a build-up of waste metabolites and reduced synaptic pruning  (impairing removal of old connections to make room for new ones).


We know that people need sleep as all animals do it or at least have some equivalent of sleep. For example, insects don't have REM and dolphins sleep by turning off one brain hemisphere (since they breathe voluntarily, one hemisphere must be active at all times to avoid drowning). The thing is, we know a lot about what happens during sleep, in terms of the electrophysiological, biochemical and psychological markers, but not an awful lot as to why we actually need it. There is no single theory that explains why exactly we need it, but the most popular ones tend to revolve around fighting infections, reducing energy consumption and clearance of waste products from the brain.

Off the top of my head I can think of three pieces of evidence for this (I'm sure there's more). The first two are pretty obvious. Firstly, we fight off infections best when we are asleep and we consume less energy while asleep. In terms of clearing waste products, changes in neuroglial behaviour suggest that they clear waste products from the brain while sleeping, as many regulate cerberopinal fluid; the main mechanism of removing waste metabolites from the brain (as this organ requires different conditions from the rest of the body). It's perfectly possible that all these theories are correct and we need to sleep for all three reasons.

Another more recent theory suggests that synaptic pruning occurs during sleep, whereby unwanted connections are removed from the brain (this also happens in babies: they are born with something like 10 times the amount of neurons they and only the strongest neurons and connections between them survive into adulthood). So for example, it may not be necessary for me to remember a certain conversation I had with a friend that day, so the synapses conveying that information may be pruned during sleep. 

So I can think of two reasons as to why cognitive performance declines when sleep deprived. The brain could be working inefficiently when sleep deprived due to an energy deficiency and build up of waste metabolites which screw up the carefully designed molecular machinery that keeps your brain functioning. It could also (or additionally) be due the reduced synaptic pruning consequent of sleep deprivation: the unnessecary synapses don't just create clutter, they take up space and this may make it harder for new synaptic connections to form. This would impair cognition as a large chunk is dependent on synaptic plasticity.",0
"CMB radiation coming from the front is blueshifted, and that from the back redshifted.

CMB is coming from every direction, so you'll have a sunset-like colour gradation of the sky from 'blue' to 'red'.

CMB is not visible to the naked eye, but if you're traveling fast enough you'll shift it into the visible and beyond. A splotchy rainbow ring should appear around the direction you're heading in (with invisible UV and gamma death at its center).",0
"Blinking is a motor function controlled by the facial nerve, the seventh cranial nerve. Cranial nerves come directly from the brainstem, bypassing the spinal cord. Cranial nerve reflexes are often used to assess levels of brain function (diencephalon, mesencephalon, and medulla). ",0
"Lots of great information in this thread. I did a bit of research into this a while back - specifically about how altitude while hiking influences the boiling guidance, so my focus is on common stuff you find in water while out in the wilderness. (You'll see my seemingly arbitrary use of 14,000 feet below. Mt. Whitney, the highest peak in the lower 48, is 14,505'.)

tl;dr: Boiling your water for any duration gets rid of lots of nastiness that you'll commonly encounter while hiking.

The CDC suggests that:
""Common intestinal pathogens are readily inactivated by heat. Microorganisms are killed in a shorter time at higher temperatures, whereas temperatures as low as 140°F (60°C) are effective with a longer contact time. Pasteurization uses this principle to kill foodborne enteric pathogens and spoilage-causing organisms at temperatures between 140°F (60°C) and 158°F (70°C), well below the boiling point of water (212°F [100°C]).

Although boiling is not necessary to kill common intestinal pathogens, it is the only easily recognizable end point that does not require a thermometer. All organisms except bacterial spores, which are rarely waterborne enteric pathogens, are killed in seconds at boiling temperature. In addition, the time required to heat the water from 60°C to boiling works toward heat disinfection. Although any water that is brought to a boil should be adequately disinfected, to allow for a margin of safety, boil for 1 minute. Although the boiling point decreases with altitude, at common terrestrial travel elevations it is still above temperatures required to inactivate enteric pathogens. To conserve fuel, the same results can be obtained by bringing water to a boil and then turning off the stove but keeping the container covered for several minutes.""

(See http://wwwnc.cdc.gov/travel/yellowbook/2016/the-pre-travel-consultation/water-disinfection-for-travelers)

The Wilderness Medical Society Practice Guidelines for Wilderness Emergency Care states: ""As in pasteurization, temperatures above 160°F (70°C) kill all enteric pathogens within thirty minutes, and 185°F (85°C) is effective within a few minutes. Thus, disinfection occurs during the time required to heat water from 140°F (60°C) to boiling temperature, so any water brought to a boil, even at high altitudes, is safe.""

It's interesting, then, that the CDC recommends one minute at sea level and three minutes at elevation in their general guideline, which seems to contradict both their own guidance about 'common terrestrial travel elevations' and the Wilderness Medical Society. Which is correct?

Of the water-borne bacteria, viruses, and protozoa listed by the World Health Organization, only Hepatitis A and Poliovirus 1 survive instantaneous temperatures greater than 80°C for more than 1 second. The highlight of the word instantaneous is deliberate. The inactivation is a function of both temperature and time, as suggested by the Wilderness Medical Society. To reach a boil, time has been spent at each previous temperature. At 70°C for example, Giardia is inactivated at the rate of 99% for every 600 seconds. According to the EPA, water is considered potable with a 3-log (99.9%) removal of Giardia lamblia and 4-log (99.99%) removal/inactivation of other viruses. At an instantaneous temperature of 80°C, Giardia is inactivated to a safe level in 60 seconds and is instantly inactivated at 87.7°C (see Kerasote, Ted:  Great Outdoors; Drops to Drink.  Audubon, July 1986).

Let's looks specifically at those viruses which are the most heat resistant.

Hepatitis A
Hepatitis A can be encountered in water that is contaminated by human waste, so it is possible to encounter this in wilderness situations.

Bidawid et al (see http://www.ncbi.nlm.nih.gov/pubmed/10772219) found the following for the heat inactivation of Hepatitis A:
Hepatitis A 5-log reduction (99.999% reduction)
85°C in less than 30 seconds
80°C in less than or equal to 41 seconds
75°C results in a 1-log reduction for each 10.2 seconds
Note that temperature exposure is instantaneous. That is, the samples were immersed directly into a water bath at the stated temperature. The higher temperature does not, therefore, take into account any time of exposure at any lower temperatures. Note also, that Bidawid's study focused on Hepatitis A in dairy products and concluded that fat content affects the survivability of the Hepatitis A virus.

Parry and Mortimer (see http://www.ncbi.nlm.nih.gov/pubmed/6094725) found the following for the heat inactivation of Hepatitis A:
Hepatitis A 5-log reduction (99.999% reduction)
85°C virtually instantaneous
80°C in 5 seconds
75°C in 30 seconds

In either case, based on Bidawid or Parry and Mortimer, achieving a boil at any elevation up to 14,000' eliminates the risk of Hepatitis A.

Poliovirus 1
Poliovirus 1 is also possible in water contaminated by human waste.

Strazynski et al (see http://www.sciencedirect.com/science/article/pii/S0168160501007085) found the following for the heat inactivation of Poliovirus 1:

Heating at 72°C for 30 seconds was a reliable method of inactivating polioviruses present in water, milk, and yoghurt reliably. Also, heating at 55 °C for 30 min resulted in complete inactivation of polioviruses, regardless of the suspending medium.

Similar to the Hepatitis A virus, achieving a boil at an elevation up to 14,000' eliminates the risk of Poliovirus 1.

See also:
http://www.who.int/water_sanitation_health/dwq/Boiling_water_01_15.pdf
http://cid.oxfordjournals.org/content/34/3/355.full
",0
"Would the fact it was buried under a trench create a high-pressure environment that would amplify the damage? Would it be possible to replicate in the place Fawkes' gunpowder was at? Black powder is much more slowly burning than TNT, and how sealed the environment is could be crucial to determine the built up pressure, and thus the damage.

There is a historic basis how meaningful this is, albeit on a smaller bomb scale. During the [20 July plot](https://en.wikipedia.org/wiki/20_July_plot), several German officers tried to assassinate Adolf Hitler using a briefcase bomb. To avoid setting off metal detectors, they had to use plastic explosives wrapped in paper rather than a metal casing, despite the fact that at the time of WWII, plastic explosives were not as advanced or high-pressure as later explosives like C4. They expected Hitler to have a conference in a bunker and had the bomb placed there, where the sealed environment would act like one big casing, allowing the bomb to build up pressure that would kill everyone inside. But instead, Hitler had the meeting in a regular building, with windows and other gaps. As a result, the detonated bomb dissipated its explosive force, and Hitler survived the explosion, albeit with some injuries like a shattered eardrum.",0
"EDIT: A few responses in this thread say the same thing ""we don't know what sleep is for"", which is a true statement. However, we *do* know what it is *not* for, hence my responce below. 

_______________________

What is clear is that sleep primary function is not energy conservation. What isn't clear is if it adds to, or plays a role in sleep at all. 

It is clear from various proofs that energy can't be the primary or even a major factor is *why* organisms sleep (or have sleep-like states). To give some quick ones:  

* Hibernating animals wake up intermittently to both sleep and pee.  e.g. squirrels, don't continuously hibernate. They get out of their hibernation a few times and [in those times they spend most of that asleep^1, ^2]( http://ajpregu.physiology.org/content/276/2/r522).   

* For humans the energy savings of sleeping vs laying awake but resting is the equivalent to a slice of bread (~50 calories). This is hardly worth losing vigilance for 1/3 the day for.   

* The correlation of [body mass and sleep amount is weak at best^3]( https://www.dovepress.com/cr_data/article_fulltext/s80000/80731/img/fig3.jpg). Although generally smaller mammals, for example, sleep more than larger mammals, you can't really guess at the amount of sleep an organism has by their size.  e.g. Lions, Platypuses & chipmunks all sleep around the same amount of time^4.  

* Almost every organism appears to have a sleep-like state^5. Even single cell organisms. With these organisms it's unclear if ""sleep"" is even happening, but it's doubtful that reduced stimuli responsiveness does much for their energy expenditure.   

The third point, though, show that there is a bit of correlation (even if weak), which indicate that perhaps there are some side benefits of energy conservation in sleep.  

I've only highlighted some of the key evidences about energy conservation. There are many, and there is wide consensus that energy conservation isn't the point of sleep.  I will say though, that I think we often make a mistake by trying to see a function of sleep as a ubiquitous thing that *must* be the same across all species (but that is just my thinking on this)   

_________________________________          
^1 ^http://ajpregu.physiology.org/content/276/2/r522  
^2 ^http://physrev.physiology.org/content/83/4/1153  
^3 ^https://www.dovepress.com/cr_data/article_fulltext/s80000/80731/img/fig3.jpg  ^(from #4)    
^4 ^https://www.dovepress.com/the-influence-of-gravity-on-rem-sleep-peer-reviewed-fulltext-article-OAAP    
^5 ^(Sleep-like rather than ""sleep"" since we often define sleep as we see it in humans --i.e. laying down, less responsive to stimuli and brain changes-- but in many organisms, we can't see most of these with the exception of stimuli responsiveness. Hence ""sleep-like"" rather than ""sleep"".)   
",0
"Cardiovascular physiologist with extensive exercise physiology experience here.

Couple of things first - cardiac muscle is fundamentally different compared to skeletal muscle. Although certain contractile proteins are similar, in terms of energetics (energy production and consumption), cardiomyocytes (heart cells) are very different. 

The main reason why cardiomyocytes are so resistant to fatigue is because they contain almost twice the amount of mitochondria. Mitochondria are the **aerobic** cellular powerhouse. We know this by looking at the content of [citric synthase which tracks very well with mitochondrial content.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4121645/#!po=35.0000)

The heart is very metabolically flexible in terms of fuel. [It consumes glucose, free fatty acid and **lactate.** Yes.. you read that right. It consumes lactate (so does skeletal muscle).](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2290415/) This is especially pronounced at high exercise intensities.

Finally, cardiomyocytes are very well vascularized and since they have more mitochondria are incredibly good at extracting oxygen and using it for aerobic respiration. In fact, even at the rest heart muscle pretty much extracts most usable oxygen from blood which means the only way for the heart to improve oxygen delivery is to improve flow (as it cannot improve on extraction).

These are just some broad concepts but I recommend taking a look at some exercise physiology texts that will help.

Obligatory edit - Woah! this blew up. RIP my inbox. 

Thanks for the kind words. Going to try to answer the most common questions asked here.

**One common question is - what would happen if we replaced all muscle in the body with cardiac muscle?**

**Answer** 

Lots of bad things. Cardiac cells talk to each other through a structure called the intercalated disk which allows all cardiomyocytes to beat synchronously to produce an effective beat. Further, cardiomyocytes are self-excitatory i.e. they contract even without nerve supply (that's why a transplanted heart with its nerves cut still beats - a lot faster rate too because the heart needs the vagus nerve to rein it in). Obviously both of these would be very bad for skeletal muscle as these are incompatible with voluntary, purposeful movements. First, it's precisely because muscle fibers in a motor unit are isolated from others that we're capable of effective movements otherwise a contraction that started in one fiber would rapidly spread to others. Secondly, you can only imagine how bad it would be if skeletal muscle cells started to contract by themselves without any nervous system input. Ninja edit - oh and one more thing, the [metabolic rate of cardiac tissue per unit mass is almost 35-times greater](https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-6632.2000.tb06470.x?sid=nlm%3Apubmed) than skeletal muscle (440 kcal/kg per day vs 13 kcal/kg per day). If we replaced skeletal muscle with cardiac muscle our daily energetic needs would skyrocket given that skeletal muscle is a substantial percentage of our body mass/weight.

**Another common question - some variation of; does the heart only have a set number of beats and if I speed my heart up with exercise am I draining it?**

**Answer**

Not true and please don't stop exercising. If anything, exercise revs up your heart (esp cardio; weight training has very modest cardiac effects) during exercise and that's a good thing (if your heart rate doesn't go up that's bad and called chronotropic incompetence). That's because over time you get what's called eccentric hypertrophy so your heart can pump more blood out per beat (increased stroke volume). Further, regular cardio also increases vagal tone (the vagus is the nerve that slows the heart rate) and this in combination with the increased stroke volume means you get a very nice resting heart rate. [Low resting heart rates](https://heart.bmj.com/content/99/12/882) and [high cardiorespiratory fitness] (https://mayocl.in/2WzpBok) VO2max (or VO2peak) are associated with significantly lower risks of heart disease and all-cause mortality.",0
"fart in the bottle, light the gas, take the boost, *then* throw the bottle",0
"No,  this has been proven.  

Per Stanford study:[ Running elicited a significantly greater total energy expenditure than walking on both the treadmill and the track (P < 0.001) for both genders (Fig. 1a). On the treadmill, the males expended 520.6 ± 27.6 kJ for 1600 m; this was significantly higher (P < 0.05) than the energy expenditure by the females (441.1 ± 25.6 kJ). For the walk, the males expended 370.4 ± 17.7 kJ, and the females expended 309.6 ± 17.2 kJ for 1600 m (P < 0.05 between genders). When energy expenditure was adjusted for fat-free mass, the gender effect disappeared, but running exercise still required more energy than walking (P < 0.01; Fig. 1b).](https://web.stanford.edu/~clint/Run_Walk2004a.rtf)",0
"**Short answer:** Oh no. Oh God no. You're so dead. It's not even really the UV rays that do the damage.

**Long answer:** The important thing to know up front about 'radiation' is that it's a bit of a catch all term, and many of the uses have almost nothing to do with each other. To 'radiate' just means to give off energy. Sometimes that energy is good- the sun is radiating electromagnetic radiation, like visible light and infrared and UV. Other times, nuclear fallout is radioactive and emits electrons and alpha particles, which are incredibly dangerous inside your body. So that's takeaway number one- there are different kinds of 'radiation' and it's a bit of an overused buzzword at this point. 


Now let's go back to your question. I'm going to give you two answers, one about atomic bombs and one about a reactor meltdown. Bombs first though, because that's more fun.

Nuclear explosions tend to kill in 3 ways, depending on your distance from ground zero. The first is the fireball itself. That's the central explosion part. If you're near that, you're incinerated. Full stop. Nothing except a bunker under meters and meters of concrete will save you.

Going farther out, the next things to kill are the overpressure and thermal radiation. Out here, the shockwave from the nuclear blast can rupture organs, but more likely it'll make a building fall on you. And similar to the fireball, the thermal radiation zone is *hot.* Like, imagine the sunrise on the horizon got bigger until it occupied 100x more of the sky than it used to, getting hotter and hotter until everything is on fire. That's what a nuclear bomb is like. Here, it's photons of all wavelengths that are impinging on you, burning you to a crisp. Sunscreen just filters some of the UV rays from the sun- it'll do nothing to stop you from cooking in this. 


Last, of course, is the nuclear radiation that you asked about. In fact, this part of the answer is the same for the bomb and for the meltdown, which is why I saved it for here. Nuclear fission, whether in a bomb or reactor, makes a lot of radioactive nuclei which will decay and emit electrons (beta) and high energy helium nuclei (alpha), which produce a lot of damage in biological tissues. Other sources of the 'radioactive' kind of radiation include spontaneous fission and neutron emission from other radioactive nuclei. After bombs and meltdowns this stuff spreads, and if you're inhaling this stuff in any considerably amount you're pretty much gonna die a horrible painful death. Sunscreen is a glorified Maginot line. 

And on a funny historical note, Edward Teller (physicist from the Manhattan project) actually brought sunscreen to his viewing of the first nuclear detonation, the Trinity test. Even in retrospect, that's pretty amusing. ",0
"We don’t ~~know~~ *have safety data to support doing that*. It is probably not a good idea. 

*For instance*, The second dose of ~~all of~~ the *Moderna* COVID vaccine had more severe reactions than the first dose. 

However, and most importantly, we don’t have data on ~~it~~ *switching Covid vaccines mid stream, or taking different forms*, so no one can tell you it has been proven safe. It might not kill you, but for instance, one of the patients treated with the second dose of the high dose of the Moderna vaccine ran a fever of 103. You probably don’t want something like that to happen.

If you start a 2 shot regimen~~t~~, you should get both of the same. You should probably not get another vaccine unless you test negative for antibodies later. (Doctors can do this for other vaccines too—they wear off then they give you a booster)

**Edit:** changes marked with strikeouts and italics for clarity and validity",0
"That article repeats the *claims* of the team that found the skull ""*It could be a new species of human*"", but also puts it into the context of other old human remains discovered in the region.

>Dragon Man joins a number of early human remains uncovered in China that have proven difficult to categorise. These include remains from Dali, Jinniushan, Hualongdong and the Xiahe jawbone from the Tibetan Plateau.  
>  
>There has been a fierce debate about whether these remains represent primitive examples of Homo sapiens, Neanderthals, a human group called the Denisovans, or something else entirely.

So it's one piece of pretty interesting evidence, the hard part is determining where it fits in the human evolution story and it's offshoots, predecessors and extinct or assimilated competitors etcetera. Because yeah, it's just one skull.",0
"The cranial nerves:

* CN I: olfactory - smell
* CN II: optic - vision, pupil control 
* CN III: oculomotor - most muscles of eye movement, pupil control, some eyelid control
* CN IV: trochlear - eye movement
* CN V: trigeminal - facial sensory
* CN VI: abducens - eye movement 
* CN VII: facial - facial motor, some taste
* CN VIII: vestibulocochlear - balance and hearing
* CN IX: glossopharyngeal - oral sensation, taste, salivation
* CN X: vagus - parasympathetic innervation to the body, many many functions
* CN XI: accessory - shoulder shrug
* CN XII: hypoglossal - tongue movement

Testable reflexes: 

* Pupil reflex - nerves 2,3 - diencephalon
* Corneal reflex - nerves 5,7 - mesencephalon
* Dolls eye/caloric testing - nerve 8 - mesencephalon
* Gag reflex - nerves 9, 10, 11 - medulla
* Spontaneous breathing - brainstem/ medulla ",0
"Every TAS gene is about this.  That's the whole point of the TAS(te) classification.

Tas2 is just a type 2 taste receptor.  

The shape of the question is problematic - almost everything has a slightly variant taste.  What you seem to want is night-and-day differences.

Those ones, like the cilantro, cucumber, and brussels sprout examples so common in here, tend to be that someone has inherited a defective taste gene that fails to pick up a noxious chemical.

This is actually quite common.  One of the human superpowers is we're really, really good at ignoring poison.  There's a reason you can't feed your pets half the stuff you eat.

And so, as our liver ranks, our tongues have learned ""oh, guess we don't have to worry about furolinase anymore"" over the years.  

Those are mutations that lose tongue protections we used to have, but don't need anymore, because what used to be poison has gone through the liver and is now food.  And so it should be tasty instead of noxious now, because as animals in the forest, we need all the food we can get.

Those mutations aren't universally distributed.

Those foul chemicals you're tasting were dangerous to our weaker ancestors.  They're mostly natural pesticides.

Caffeine, THC, nicotine, opium, capsaicin (hot peppers,) theobromine (tea,) and theoxanthan (chocolate) are all poisons meant to keep insects in check.

Delicious, delicious poisons.",0
"No. Fish can’t even live in water that has the wrong amount of salt or dissolved oxygen in it. Putting a fish in a liquid other than the correct water would be like putting a human in the Venus atmosphere. Sure it’s “air” but the concentration of oxygen, nitrogen, CO2, etc are all wrong. Every animal, especially fish, are evolved to very specific environments, and putting them in something they are not adapted to survive in will kill them",0
"Cannabis doesn't actually have a trace profile significantly longer than any other substance, it's a bit of myth cooked up by adverse parties. 

[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2763020/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2763020/)

THC is broken down quickly and excreted in comparable rates to other common substances, but  11-nor-9-carboxy-Δ9-Tetrahydrocannabinol (THCCOOH), formed by the breakdown of THC, lasts significantly longer. Most urine tests are for THCCOOH.

The industry standard for THCCOOH is 50 ng/ml, which occasional smokers (1-2X a week) can achieve in a little under three days. 

The long detection times you are referring to occur in chronic users because, as the other guy said, THC is fat soluble. The body clears THC at a more or less constant rate, but ingesting THC at a higher rate than it can be cleared results in THC being saturated more fully over your fat cells. 

Even with that in mind, studies show that the average chronic user, with a low BMI, can clear out THC stores and return to the under 50 ng/ml limit in just over a week. Though there are many who may retain THC in their urine for close to a month.",0
"Slightly off-topic, but that first researcher you cited, Nora Volkow, is a rock star in dopamine research, especially with respect to drugs of abuse (e.g., cocaine, heroin, etc.). I highly recommend reading some of her work on that topic. Some of it is very eye-opening in understanding why drug addicts behave the way they do.

That's all. Have a nice day, everyone!

edit: spelling

edit 2: [Here's a link to one of my favorite papers by NV.](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC155054/)",0
"If you get within 0.5 nm (in ~200K years) you'll probably just get sucked the final distance by intermolecular forces, so you have that to look forward to.",0
"A penis and a clitoris come from the same embryological tissue. Testosterone therapy makes a clitoris grow because it begins signaling the body to change the clitoris into a penis. Of course, that change is far more complex than simply ""growing"" into a penis, but it still causes hyperplasia of the clitoral tissue.

It won't make a penis grow larger because the body has already signaled the growth of a penis; there is no conversion to occur.

We can see the same change in men transitioning to female; estrogen therapy and testosterone blockers will cause a penis to lessen in size.

I hope that helps.",0
"> If an astronaut in space farted every day, it would take 10,000 years for him to get up to a normal highway speed.

But he would have to be naked or at least have an opening for his butt, right?",0
"The use of different screw types serves several purposes, including:

1. Torque transfer: Different screw heads are designed to efficiently transfer torque from a screwdriver or wrench to the screw, ensuring proper tightening without slipping. For example, Phillips head screws have a cross-shaped recess that allows the driver to apply more torque compared to a flathead screw, reducing the chances of stripping the screw head. 2. Application-specific requirements: Different screw types are designed for specific applications.",1
"Suffocation doesn't kill you by keeping you from breathing air, it kills you by keeping you from reoxygenating your blood which starves your brain of oxygen. Fish still need oxygen, they just get it through a different process than breathing. ",0
"I work for a Chinese company and the Chinese here drink hot water year round. In the summer, they say they drink it because it makes their body work harder to cool down. This sounds completely asinine, am I the dummy? Wouldn't it make more sense to drink cold water so your body can focus on cooling extremities? ",0
"The scientists fly in. I was a sailor on a supply ship. We went in summer time and the seas weren't so bad. Although being so used to it, swells no matter what size are smooth and waves are rough. Then we hit ice and go through it until we can't.  Then we waited for the Russian IceBreaker to come lead us in. They are awesome by the way, professional and kind.",0
"So when the duck dies, the proteins break down quickly? Or how does it get from the living-fat state to the melts-at-57F state?",0
"There's a misconception here I want to acknowledge. 

When someone with a penis begins taking estrogen, it doesn't reduce the size of their penis. What *will* happen is that they experience fewer spontaneous erections (morning wood, ""why do I have a boner right now"" erections, and hair trigger responses to attraction). This will cause atrophy over time, but common wisdom is that if you provoke an erection (assuming you can provoke an erection after some time of taking estrogen) often enough you won't experience atrophy, and your penis size will remain the same.

It's a semantic difference, but functionally estrogen isn't reducing the size of any penises.

Source: I'm a transwoman with a bunch of estrogen coursing through me; I know what's happening to my penis. And I've done far more research than is necessary.",0
"The two are almost entirely unrelated.

Diseases mutate on their own. Their lifecycle is rapid. It can take a million generations for a species to mutate, and for humans who don't have kids until we're 20 years old that means a long time. When you reproduce as quickly as viruses do, it's much faster.

Any virus will mutate given enough time and hosts. The existence of a vaccine isn't directly relevant. What vaccines can do, is eliminate hosts and restrict time. If enough people had gotten vaccinated earlier, there would have been less colonies of coronavirus having millions and millions of offspring, and thus the odds of a stable mutation cropping up would be far, far lower.

So, in short, if vaccines are used properly as medical experts suggest, they can kill off a virus before it mutates into something even deadlier, or something the vaccine won't stop. The existence and administration of a vaccine will basically never make a pandemic worse.",0
"Case in point, for anyone wondering if this really happens: in 1985, the pilots of China Airlines flight 006 reacted incorrectly to an engine failure and allowed the thrust imbalance to turn the plane upside down. The plane fell, turning over and over, for _5.7 miles_ straight down before the pilots managed to recover and land the heavily damaged aircraft in San Francisco. No one died, but had they been flying at a lower altitude, everyone on board would have been toast.",0
"**Ph.D. in Vaccine Development here.**   
Short answer: Many many reasons   
Slightly longer answer: 

* Combination of impact and design difficulty - Understandably, vaccines were developed for the most common/most impactful diseases (the need) + a combination of how easy/straightforward it was to design the vaccine, based on what we knew about the pathogen, its structure, the immunodominant antigen, and most importantly the correlate of protection. So you could think that a lot of the impactful diseases, that have the advantage of straightforward design - e.g. low mutation ability, limited immunodominant antigens, clear correlates of protection, e.g. antibodies have been addressed - Rotavirus, pneumococcal pneumonia, Measles, Mumps, Rubella, etc. etc. 
* Others are ones that have high impact but pose considerable scientific challenges - Flu, HIV etc. There's been lots of work and $ put into them but their high mutation rates mean they are scientifically hard to crack. That's why while we have \*generally effective\* flu vaccines, we still need to rely on strain coverage and annual seleciton. We've been making steady progress towards a universal flu vaccine, but it's still a while away. For e.g. for flu, we know the correlate of protection, but don't have a stable/conserved immunodominant antigen yet... for HIV - mix of both, some potential immunodominant antigens, but the high mutation rate is a fucking nightmare. RSV is similar - there has been a lot of interest due to the high impact, but the concern is scientific (antigen selection and immune response) than lack of resources 
* Then there is the need aspect, in the other direction - Vaccine development is expensive, and takes time. So you need resources, and in current society - conpanies investing in this need to recoup their costs - so this goes into the argument on funding. Someone else in this thread brought up chikungunya - Yep, decent example - mostly a disease affecting developing countries, but other than a few labs, and a few startups focuisng on emerging diseases, there wasn't that much effort in there - from the U.S. side - till recently. It has been in development in other countries, but like I said, this takes time.... and resources 
* mRNA vaccine tech - This tech has been in development for the last 18 years or so. It is not new! The problem is, as most of the ""easy"" targets were taken up by other vaccines as mentioned earlier - they had a higher bar to prove. For a new tech, they need to convince regulators, that their vaccine is as good/better than established ones, as opposed to just proving they work. Plus the cold chain concerns etc. are a factor. But the key advantage for them was always the speed of reactivity - Once a pathogen is identified/sequenced, it's much quicker to design an mRNA vaccine than a protein/live inactivated etc. so companies like Novavax, Moderna, Biontech have been working on mRNA vaccines for a while as the USP is ability to react quickly to novel antigens - as what happened here. 
* This is a litmus test for new vaccine tech, and it will be fascinating to see where we go from here. As this tech takes center stage, we can expect more investment in this tech for other diseases.",0
What if you took out more fat but gave a blood transfusion?,0
"> Phillips screws are self-centering, making powered screwdrivers possible. They're somewhat more expensive to produce than slotted-head. They tend to 'cam-out' easily under torque, making it hard to apply much torque. I've heard they were designed that way to prevent overtightning.

If you look up the patent for the phillips head you will find that the inventor did talk about ""cam-out"" as a benefit, but he was talking about the ability of the drive bit to force debris out of the screw head. It had nothing to do with ""cam-out"" as currently understood. It's a very common misconception.

See Chapter 2:

https://theses.lib.vt.edu/theses/available/etd-42698-205111/

",0
"The patterned baldness I guess might be a result of the ~~extra~~ added testosterone. It would be hard to say with a sample of just 3 people.

wrt their body and facial structure I don't really know enough about testosterone's other systemic effects to comment.",0
I wonder if this is why cats often cock their heads 90 degrees to the side when they are observing things. It gives them a new range of focused sight.,0
">do I make their hand dirtier or do they make my hand cleaner?

Those aren't mutually exclusive, so it could be both. The question would be whether or not you used enough hand sanitizer to destroy the bacteria on both of your hands.  

It's like asking, ""If I pour hand sanitizer on my left hand and rub my hands together, am I making my left hand dirtier or my right hand cleaner?""

If you used just enough for his hands, it's likely that the sanitizer took care of the bacteria on his hands, but couldn't kill all of the ones on your hands. If you put too much on his hands, and it was enough for both of you, then ... it's enough.",0
"When researchers give chimps eggs, they often search the area for a particular plant that they like to eat with eggs. They gather some leaves, put the whole raw egg in one cheek, and a few leaves from the plant in the other cheek. Then they crack the egg inside their cheek and mix the egg and leaf together in their mouths, I would consider that a sort of spicing.

>[Often leaves are added to soft fruits that have been crushed against the
ridged palate of the chimpanzee, and sometimes to eggs and meat. This mixture of leaves and other foods forms a ""wadge"" that is sucked for 10 minutes or more to extract its juices. A wadge may be held in the mouth as the chimpanzee moves to another feeding site (Goodall, 1986).](https://nagonline.net/wp-content/uploads/2013/12/Chimpanzee-Nutrition.pdf)",0
"Thank you! 

Follow up question: why is experiencing weightlessness stressful to the body? Wouldn’t it be easier on your joints etc? ",0
"The language you speak can indeed have an impact on the shape of your palate. The shape of the palate, which is the roof of the mouth, is influenced by various factors including genetics, diet, and oral habits such as thumb sucking during childhood. However, linguistic factors can also play a role. Different languages have distinct speech sounds or phonemes, and the way these sounds are produced can require specific tongue and mouth positions.",1
"> At higher levels chess is largely considered a draw as there are many many ways to cause a draw  
  
It's important to note that higher level computer chess games can be much much longer than human games though.  
[Take this position for example.](http://puu.sh/wg1RM/eef48bcda7.png)  
If both white an black play perfectly then white will checkmate 546 moves from now. Note that a full-time control game usually lasts for around 50 moves or less and rarely goes over 100.  
  
A comment by the chess legend Gary Kasparov on this.  
>The one thing for people to understand is that chess is, you may call, mathematical infinite game. The number of legal moves is more than number of atoms in the solar system. So machines cannot solve the game. You cannot expect machine to play e2-e4 at move one and announcing mate at 16,455 moves. But machines could work the game of chess from the end.  
  
>Now we know that machines mathematically solved all positions with four pieces, like king and queen, versus king and rook. All positions with five pieces, all positions with six pieces, and now seven pieces.  
  
>Seven pieces, it’s on the way. I’m not sure it’s all solved. We’re talking about 100 terabytes. Obviously, eight pieces will be already just insane number, and the game of chess’s ultimate endgame with 32 pieces. That’s why, maybe, machines will get to eight or nine moves, but that will probably be the end, even for the immense computing power that you can expect in next five, ten, twenty years.

> . . . 
  
>In some of the positions, like there are certain seven-pieces positions, when the win — and we’re talking about a forced win — can be reached within 500 moves. Now, 500 moves, I remember, I looked at some of the positions. Even at six-pieces positions . . .  
  
>**COWEN:** It’s not intelligible, what’s happening, right?  
  
>**KASPAROV:** It’s no intelligence at all. It’s just pieces moving around. There’s a certain position with king, two rooks, a knight on one side, and king, two rooks on other side. It said mate in 490 moves, first mate.
Now, I can tell you that — even being a very decent player — for the first 400 moves, I could hardly understand why these pieces moved around like a dance. It’s endless dance around the board. You don’t see any pattern, trust me. No pattern, because they move from one side to another.
At certain points I saw, “Oh, but white position has deteriorated. It was better 50 moves before.” The question is — and this is a big question —** if there are certain positions in these endgames, like seven-piece endgames, that take, by the best play of both sides, 500 moves to win the game, what does it tell us about the quality of the game that we play, which is an average 50 moves?**  
  
[From this interview.](https://medium.com/conversations-with-tyler/garry-kasparov-tyler-cowen-chess-iq-ai-putin-3bf28baf4dba)",0
"Yes.  [Here](https://m.youtube.com/watch?v=x1SgmFa0r04) is an excellent map showing accurately modeled atmospheric levels of CO2 from satellite and ground measurements taken during a year, for example.  You can easily see humans emitting it, and then forested regions sucking it up.  Unless it’s winter in that hemisphere, in which case it just swirls around until spring.  Other gas levels show similar seasonal patterns.

(Edit: changed to specify that it is a model based on continuous samples.  They obviously can’t sample the entire atmosphere at once every day.  And CO2 isn’t bright red.  Among other points people apparently felt necessary to clarify.)

(Edit again:  wow, I was not really expecting so much karma and a double-gold for this.  The question just reminded me of this cool map I once saw.  I bet it's even a repost!)",0
"Ok, this will likely get buried, but I'll give an explanation.

After you ingest (swallow) water, it goes to your stomach. There, it is slowly released to your small intestine to be absorbed and pass to your bloodstream.
It takes water between 10 and 40 minutes to pass to your small intestine and be absorbed. The speed of this depends on how much water, the temperature (cold water will be released slower), if there is something else in the stomach, etc.
But, in general, it doesn't matter whether you chug a bottle of water down in 10 seconds or slowly sip it in 10 minutes. It will all end up in your bloodstream in about the same time.

The only thing that can make a difference is whether you vomit the water that you are ingesting. You stomach vomits its contents in response to a series of things (whether it senses that something you ate may be rotten or dangerous, whether there is a bad smell, etc), and one of those things that the stomach takes into account is how full it is. So ingesting water in small sips can help avoid vomiting, and that is what's recommended when for example children have gastroenteritis. But if you don't vomit the water that you swallow, it doesn't matter whether you swallowed it in 10 seconds or 10 minutes.

Edit: Wow, thanks for all the upvotes. Turns out it didn't get buried after all! This is 1/4 of my total karma right there! Glad you liked it!

Edit2: it's actually not clear whether cold water will be released (and thus absorbed) slower. As someone pointed out below, studies on this seem to be contradictory.
",0
"It's not a void. The fat cells expand with fat, and shrink with less fat. The exoskeleton keeps hugging the fat cells. ",0
"You know, it's pretty incredible that our bodies are efficient enough to only put out the heat of a single 50W lightbulb.

Above and beyond basic locomotion (moving a large mass semi-constantly with lots of actuating joints), digestion (converting 2000 kCal of organic compounds into ATP, glycogen, adipose tissue, and other useful compounds), and the many assorted autonomic functions - it also houses a ludicrous parallel processing super-computer.

Nature is seemingly pretty damn good at energy conservation.",0
"I think there are a few ways this question can be answered. The short answer is ""no"" but the long answer is ""not exactly.""

Chloroplasts are plant organelles that can undergo dramatic changes as they develop from proplastids. When developed in the dark, proplastids develop into amyloplasts: chloroplast derivatives that stockpile starches in huge quantities. These amyloplasts are abundant in starchy roots known as *tubers* (think potatoes) which are (in simple terms) swollen roots that store energy. This is roughly analogous to fat reserves on an animal.

Very well-fertilized and watered plants have more succulent tissues than plants grown under more limited conditions. In gardens and fields, this can result in plants that are more susceptible to pathogens, as their tissue is rich in nutrients and water. This is roughly analogous to the propensity of the obese to suffer from health complications.

TL:DR: plants will not get ""fat"" but there are some aspects of plant physiology that could be considered similar to obesity/fat storage.",0
"Your teacher is a dick. It's a great question to ask.

Moles allows you to do calculations with the actual number of molecules.

There's a reason why people don't go around saying ""I need 200 kg worth of guys to help me out here!"" It wouldn't be very practical to use mass in that situation. However, saying ""I need three guys here!"" makes total sense.",0
"The laser bouncing tunnel is called LIGO, each arm is 4km in length and there's 2 arms (at a 90 degree angle)

There's also 2 LIGO locations, the first one (whichever happens encounter it first) detects gravitational waves and the second verifies that it wasn't an anomaly. 

Sorry I fuckin love ligo it's so cool

Edit: the two I'm referring to are in Livingston, LA, USA and and Hanford, WA, USA , however I believe that there are two in Europe and all of them share data, which is wonderful. ",0
"> I suppose a mushroom starts to die after it releases spore.

I lack formal qualification, but I'm something of an amateur mycologist. I like hunting and identifying mushrooms. I think they are cool little creatures, and can be quite tasty if you know what you are looking at. So take the below with a grain of salt. It's not well-researched, more a collection of my own obversations and understanding given a few years of learning about and cataloging mushrooms as a hobby.


You started to distinguish between a mycelial network and a mushroom, but didn't really do that great a job, IMO.


Okay, so this guy made clear that mushrooms are just a part of a mycelium. Specifically, they are sort of like ovaries. The job of a mushroom is to produce spores and spread them on the wind (or through water back into the soil, or even through an animal back into the soil).

Mushrooms only sprout naturally during certain seasons. In greenhouse conditions, you can keep a mycelial network constantly spawning new mushrooms by controlling moisture, temperature, and light conditions. Most mushrooms that people eat are gilled mushrooms. The gills produce spores constantly while the tissues are alive.

The tissues are delicate. They require moisture and the mycelium requires food to generate spores. The mushroom can continue to produce and drop spores for hours, even days after it has been picked. Spores are the mushroom's answer to sperm. (or more accurately, sperm is the animal answer to spores)

Spores are hardy. They can survive for years on end. But mushrooms aren't. They require moisture and key temperature, and being rather delicate, they are quite prone to bacterial and fungal infection that quickly causes parasitic fungi and bacteria to reduce mushrooms to slime within a matter of weeks of sprouting.

From the minute that a mushroom sprouts, it begins to die. It will hold out for a short time, but eventually the cells will either dessicate and wither, or the mushroom will break down. Most often, this is accelerated by insects and animals feasting on the flesh of the mushroom.

As for when a mushroom dies... That's a complicated question. Cellular activity in mushrooms is an awful lot like cellular activity in other plants and even animals. The tissues can still differentiate when fresh. Think of it like stem cells. They can become just about any kind of cell. Once the mushroom starts to ripen, though, the cells have lost their ability to vegitate (spawn a mycelial mass). At this stage, I would argue that the mushroom is now dead.

In animals, this line is pretty easy because we have brain activity that drives the body. Fungi don't have this distinction. The mushroom can separated from its mycelial mass and recuiltivated elsewhere, effectively cloning the original mass. Also, the mushroom can rot without it really dying (as the spores exist as clones of the mushroom.).

As for a direct answer to the OP's question: Slicing won't kill a mushroom. Picking won't kill a mushroom. Refrigeration won't kill a mushroom. Digestion will kill the mushroom, but the spores may survive digestion.

The only real way a mushroom dies is when it matures enough that it loses the ability to regrow the entire mycelium it spawned from. This only happens with time.

Even worse, this varies from species to species because of the chemistry behind cooking. Some mushrooms are harvested as buttons, others are harvested at maturity, and others are harvested ripened. It really depends on the species and what goes on inside of the mushroom chemically during development. Some mushrooms are quite foul-tasting as buttons, but not as adults, and others are the opposite. (Better yet, some mushrooms are actually harvested specifically for their foul-tasting compounds. Mushrooms aren't just food, they can be recreational.)

Some mushrooms are already, at least by my definition, dead by the time you pick them. Others are quite far from it when picked.

One thing that picking a mushroom does do, however, is it severs the mushroom from many of the special tissues that the mycelium maintains for digestion, respiration, and distributing moisture. Mycelium are a bit less specialized than animal cells, but they are specialized just the same. If you cut an organ out of an animal, it starts to die then and there. It's not dead the instant you cut it out, but it doesn't have long. Mushrooms are never meant to have very long. They basically have a 1-2 week window of life when they sprout to when they are dead. Unlike cutting the heart out of an animal, by cutting and storing a button mushroom, you may actually be able to lengthen the functional lifespan of the mushroom by doing so (with refrigeration and moisture).

**TL;DR:**

So... It's kind of a mixed bag, but the best answer you can really get for the question: Either before you cut it, or some point after you cut and store it, depending on conditions. Harvesting the mushroom doesn't really factor into killing the mushroom. Once they have begun to drop their spores, death doesn't really apply to a mushroom. It's more like decay than death. The answer to ""When does a mushroom decay?"" is much more obvious.",0
"The particle itself was never of any particular relevance, except for potential weeding out potential grand-unified theories.  The importance of the discovery of the boson was that it confirmed that the Higgs FIELD was there, which was the important thing.  For about the last 50 years, particle physics has constructed itself upon the un-verified assumption that there must be a Higgs field.  However, you can't experimentally probe an empty field, so to prove it exists you must give it a sufficiently powerful ""smack"" to create an excitation of it (a particle).  

So the boson itself was pretty meaningless (after all, it was at a pretty stupid high energy). But it confirmed the existance of the Higgs field and thus provided a ""sanity check"" for 50 years of un-verified assumption.

Which for particle physicists was something of a bittersweet sigh of relief.  Bitter because it's written into the very mathematical fabric of the Standard Model that it must fail at SOME energy, and having the Higgs boson discovery falling nicely WITHIN the Standard Model means that they haven't seemingly learned anything new about that high energy limit. Sweet because, well, they've been out on an un-verified limb for a while and verification is nice.",0
"Better to be prepared and have too much than too little. They also were uncertain of the medical issues of menstruating and whether or not the uterus would be able to expel the shed lining in zero gravity or if it were possible for the lining to move up into the fallopian tubes (retrograde menstruation) ([Popular Science 2016](http://www.popsci.com/brief-history-menstruating-in-space)). Luckily for women astronauts, things still work as intended in space. ",0
"I made a quick Python script to test the number of trials needed: 

    import random

    cards = 52
    deck = []
    turns = 0

    while len(deck)<cards:
        a=random.randint(1,cards)
        if a not in deck:
            deck.append(a)
        turns +=1

    print ""Turns = %s"" %(turns)  


If you want to try the experiment a certain number of times and you just want a list of trials it took every time, you can use this, just replace ""k<3"" with k lower than the number of times you want to repeat the experiment, to see how many turns it takes each time!  

    import random
    
    cards = 52
    deck = []
    turns = 0
    trials = []
    k=0
    
    while k<3:
        while len(deck)<cards:
            a=random.randint(1,cards)
            if a not in deck:
                deck.append(a)
            turns +=1
    
        trials.append(turns)
        turns = 0
        deck = []
        k+=1
    
    print trials  


I tried 10 times and the result is the following: 
[239, 216, 256, 191, 289, 223, 209, 221, 239, 216]

I will try more times and make a graph soon!  

**EDIT:** If you want results to be displayed line by line for easier graphs in Excel or whatever, replace the last line (""print trials"") with the following:  

    for n in trials:
        print n  

**EDIT 2:** I repeated the experiment 1000 times, and the results are these: 

Steps:    
   
52-100------------0  
101-150---------40  
151-200--------279  
201-250--------347  
251-300--------205  
301-350---------77  
351-400---------33  
401+------------19  
  
  
  
**[PICTURE](http://i.imgur.com/KU27fWf.jpg)**    

[Thanks to /u/vennith that pointed out in a message a mistake i made: I jumped from groups of 50 elements to groups of 100 elements. I fixed it now]

The average is 233,7. Really close to our 236!

As we can see, most of the times we need from 201 to 250 days, followed by 151 to 200 days.  
What's amazing is that it never took less than 100 steps to complete the 52-deck of cards, that's because the probability of completing the deck in so few steps is very small!  

**EDIT 3:** A more compact code based on the one /u/Felisens made:  

    from random import randint
    trials=[]
    for k in range(10):
        ndays = 0
        deck = 52
        while deck:
            if randint(1, 52) <= deck:
                deck -= 1
            ndays += 1
        trials.append(ndays)
    print trials  

Instead of 'for k in range(10)' you can put any number inside the parenthesis, the number being how many times you want to repeat the experiment. As before, if you want each number in a new line replace the last line with:  

    for n in trials:
        print n  

**EDIT 4:** I tried 10000 times and the result is this:  

**[10000 TIMES](http://i.imgur.com/ziDTKOx.jpg)**    

**Lowest value:** 88  
**Highest value:** 802  
**Mean:** 235.5 (Extremely close to 236)
",0
"To expand on this a modem takes binary (1,0) and turns it into audio signals. It uses the full range of frequencies that can be sent over the telephone line to get the maximum data throughput. This is why dial-up has a fundamental limit of 56kps. 56kps is the most data you can push through a phone line without violating phone line specifications. So that sound you're hearing is the data being sent over the wire. The computer at the other end “hears” that sound and use its modem to translate it back into 1’s and 0s. In fact, very old modems actually did literally hear the sounds. Google acoustic coupler modem if you want your mind blown.
or just watch this shit. https://www.youtube.com/watch?v=X9dpXHnJXaE

note that these things had a pathetic data transfer rate. less then 1kps",0
"When our body is deprived of water, our blood volume decreases while our blood salt concentration increases. This change is detected by pressure receptors in our cardiovascular system and salt-concentration system in our hypothalamus.

Vasopressin/antidiuretic hormone (ADH) containing neurons receive this information about the changes and respond by releasing vasopressin, which directly acts on the kidneys and leads to reduced urine production and water retention.

The kidneys release an enzyme, *renin*, promotes the conversion of *angiotensinogen* (protein from liver) to *angiotensin I*, which gets degraded to *angiotensin II*. Angiotensin II directly effects the kidney and blood vessels to increase blood pressure. Angiotensin II is also detected by the *subfornical organ,* in the telencephalon. This area activates cells in the lateral hypothalmus, producing an overwhelming thirst to promote drinking.

To some extent, our brain is controlled by our kidneys and I think that is somewhat ironic and fascinating.",0
"There are several research projects afoot that deal with reclaiming phosphorus (and nitrogen, while we're at it) from human urine. In the longer term, this is almost certainly the solution.

It's worth noting that if we suddenly had zero phosphorus, it would probably cut our worldwide agricultural yield by as much as 90%. That's how important fertilizer is to worldwide agriculture.",0
"Although they are capable they probably wouldn’t. For men viagra doesn’t produce a permanent erection like in movies. It still requires a stimulatory input from the nervous system for it to work. On a molecular level this is related to my explanation earlier but slightly more complex. 

To cause an erection the brain will send a signal to the penile smooth muscle via the nervous system. This causes the production and release of nitric oxide. Nitric oxide causes the activation of guanylate cyclase in the corpus cavernosum (part of the penis). Guanylate cyclase is an enzyme that converts GTP to cGMP. As mentioned before cGMP causes muscle relaxation.

That’s pretty confusing but basically without stimulation from the brain there’s no nitric oxide which means no cGMP which means no relaxation which means no erection.

Edit: changed ‘still requires sexual stimulation’ to ‘still requires stimulatory input from the nervous system’. As u/BannanasAreEvil pointed out this is a better way to phrase it.",0
"It also depends on which method of meat generation is used. Basic collagen reconstructive methods do indeed lack fat and peripheral tissue for the most part. However, there are new methods of generating tissue using stem cells that can create MORE than lean tissue, but indeed a more chemically similar natural meat structure. This tissue is able to differentiate itself from 'lean muscle' and include adipose tissue and even skin (crispy). With regard to contaminants, this is negligible due to the fact most meat is farmed. The most optimal juicy lucy should be made with the latter, stem cell meat. Thanks. ",0
"Current estimates put the overall percentage of Americans that have been exposed at around 6-7% from the CDC.   


The problem is that we really aren't testing people unless they're showing symptoms, or are on someone's contact trace. Hell, I have someone in my house showing symptoms and currently awaiting test results and I was told that I'd have to wait until they pop positive. As such, my work is telling me to come in and expose my coworkers potentially. It's messy and its dumb. So yeah, we have definitely seen about 6-7% tentatively. The problem is, with that number, a lot of deniers will say we are fine then! But that couldn't be further from the truth. If it took this long to get 6-7% infected, we'd have to repeat the last 90 days about 10x to reach herd immunity for this one strain. And that's assuming it doesn't mutate its S-protein and fool our immune systems and restart the process. It's already done it once when in Italy. The Italian strain is what is dominant now due to it being 10x more infective than the original Wuhan strain.",0
"Not only [can fish fart](https://www.theguardian.com/education/2005/mar/11/highereducation.research), but those farts were a source of a [bit of diplomatic tension between Sweden and Russia](https://www.maritimeherald.com/2019/how-fish-farts-almost-caused-a-diplomatic-accident-but-founded-the-most-liberal-city-in-the-world/). The sound of flatulence was misidentified by the sonars of the Swedish Royal Navy as a Russian submarine.",0
">Hairstyling demonstrates relatively sophisticated tool use

That depends on what you mean by sohpisticated, but not all styling requires tools.

Styles like mudding of the hair, or ""dredding"" of the hair, that we still see in African tribal cultures don't require tools, and are likely some of the earliest ""styling"" technologies, though I'm not sure there's much evidence to back up that claim.  

Braids and rope are essentially the same technology; they don't require tools although combs make them easier.  I can imagine them developing before carved or constructed combs, since the human hand can suffice as a rudimentary comb, as could an antler.  We have debatable evidence of braids from about 30,000 years ago, in Austria, with the [Venus of Willendorf](https://en.wikipedia.org/wiki/Venus_of_Willendorf) and about 25,000 years ago with the [Venus of Brassempouy.](https://en.wikipedia.org/wiki/Venus_of_Brassempouy)  But early hominids would be mostly if not completely gone by this time, making extrapolation difficult.

Burning is another technology applied to hair styling that could be an early development in the same era fire production was being cultivated.  It wouldn't require additional tool development beyond  fire-making, and could have been used by early hominids with the tech for carrying fire.

Shaving and hair cutting could have come with just the simplest stone tools, near the very beginning of tool use in hominids.  But it's difficult to attribute.  Even the [Châtelperronian industry](https://en.wikipedia.org/wiki/Châtelperronian) is still controversial, though we do have some evidence that the tools and body ornamentation happening there was related to Neanderthals.

https://www.sciencedaily.com/releases/2016/09/160920090400.htm

We have unearthed fairly sophisticated hair combs in Africa around 5000 BCE, and can probably push their development back a bit in time, but how distant would be a guess.

A lot of information we have about early homind lifestyle is happening with chemical analysis of food proteins left on teeth, pollen analysis, and something called [Peptide Mass Fingerprinting](https://en.wikipedia.org/wiki/Peptide_mass_fingerprinting) for rapid detection of hominid remains.  It's teaching us a hell of a lot about neaderthals.  But it's really deep analysis of such ephemeral residues, there remain limits to our reach into the past.",0
"Yes, insects can indeed become ""fat"" in the sense of storing excess energy. However, their method of storing energy differs from mammals. Unlike mammals that store energy in the form of fat cells, insects primarily store energy in the form of glycogen, which is a complex carbohydrate. Glycogen is stored in specialized organs called fat bodies, which are located in the abdomen of insects. These fat bodies serve as the primary energy storage and metabolic centers for insects.",1
"There are two main factors. Most antennas people notice are for xm radio that works at 2.3 ghz instead of 100 mhz that standard fm radio operates. The wavelength is directly related to the antenna size and the wavelength of 100mhz is roughly 10 feet were 2.3ghz is 5inches.

Secondly electromagnetic modelling software has made amazing jumps in the ability to model complex structures, like HFSS and Feko. So designers can embed antennas in places they never could before, like most cars have the fm antenna in the wind screen. 

There has been a lot of talk about impedance matching, although helpful not really that important in this case. Just because I can impedance match a beer can to 50 ohms doesn't mean it propagates worth a damn.

Source: I've been rf and antenna designer for 15 years",0
"Brown color also comes from bilirubin which is a byproduct of hemoglobin from your red blood cells breaking down. Bilirubin is processed in the liver and excreted in bile. It's also filtered by your kidneys, making urine yellow. ",0
"I like it all and I guess the definition of ""chronic"" user or ""heavy user"" should be more measurable. With variations in strains and delivery route effecting potency. Would consider my self to have been a heavy chronic user and took 42 days to clear my urine and pass a test. Also I was a personal trainer at the time and upper 20s BMI with body fat around 8%",0
"To add to what other people have said, while many individual cells don't ""rest"" they also die pretty quickly in comparison to our lifespan. 

In a matter of speaking, they burn the wick at both ends of the candle until they die, getting replaced by a younger version. And on and on. 

Some have shorter lifespans than others. Colon cells die after a few days. Sperm cells last a few days. Skin cells last a few weeks. Red blood cells last a few months. White blood cells last about a year.",0
"Diseases to ruin your civilisation 101:

* Must have high R0: ie. each infectee must infect multiple others.  Either be outrageously contagious or have a very long asymptomatic-but-infectious period.  If there is existing immunity in the population you must evolve around prior immunity (like flu) or be infectious enough to spread though a disparate vulnerable population (measles, pertussis, chickenpox)
* Be lethal.  This conflicts directly with point 1 because dead people are not very sociable.

Therefore it is hard for any pre-existing human disease to meet these requirements, as high lethality is maladaptive for germs.

It is therefore most likely for a pandemic to be an emergent zoonotic infection; an animal germ that is newly learning to infect humans effectively. Given that we are then inventing a hypothetical pathogen, you can kind of choose whatever you want as the next pandemic-causer.

The most probable infections are going to come from diseases whose current hosts are in close proximity to humans (farm animals) or have virulent living conditions (eg. bats).  You would also want a germ that targets a receptor that is relatively well-conserved (ie. similar) across humans and the target population -- such as the ACE2 that COVID targets.

Honestly, nothing is really that probable, which is why lethal pandemics have been rare in history.  The only families of germs with proven potential to cause pandemics are those that we have already seen achieve it, so I would say high risk families are flu, coronaviruses and coccobacilli.

At a lower tier, we may worry about things similar to HIV (Retroviridae), ebola (filoviridae), leprosy (mycobacteria , as is TB) and rabies (Rhabdoviridae); pathogens that are nasty and known or suspected to be zoonotic. However they each have their own impediments to future relatives becoming pandemics, chiefly that they are not sufficiently infectious.",0
"In the vacuum of space, there is no air or medium to transmit sound waves or gases like on Earth. So, if you were to fart in space, the gas would not disperse or push against anything to generate any movement or propulsion. To move around in space, astronauts rely on the principle of action and reaction, as described by Newton's third law of motion.",1
"This is a really good question, one that comes up all the time in my profession as an ICU physician. As /u/SGDJ points out, blood flow is key to not only the bodily repair process, but also to its normal homeostatic function. One way to gain insight into what the body “prioritizes” as vital for function is to note what happens when the body is in a shock state. This occurs for example when there is massive blood loss (i.e. hemorrhagic shock). The body will attempt to compensate for diminished oxygen delivery to the tissues by shunting blood away from less essential organ systems to those that are ‘vital’ in the immediate sense. What we see in this scenario is that blood shunts *away* from skin, kidneys, liver, intestines and *towards* three main organs: **the brain, the heart, and the adrenal glands.**



The reason for this priority is abundantly clear. The most important organ to ultimate survival is the brain. It is also the most sensitive to hypoxia injury. Therefore, the heart must be able to sustain cardiac output and oxygen delivery. And the adrenal glands that secrete vasogenic hormones like epinephrine and dopamine are necessary to regulate it all.



There is constant debate regarding which organ systems should be deemed ‘vital,’ but in actuality, blood flow is preserved to those three organ systems in a shock state in an attempt to preserve immediate survival.",0
"The use of adrenaline or other stimulants to wake people from comas is not a viable option for several reasons. First, it's important to understand that comas are complex medical conditions with various underlying causes, such as traumatic brain injury, stroke, or metabolic disorders. In most cases, the primary issue lies in the brain's function or structure, rather than a lack of arousal.",1
"And one was able to witness the process. All of the sudden, a classmate would disappear. The news had photos of the patients in the iron lungs. IF they returned, one saw the after effects, including them struggling in heavy braces. It's hard to doubt when it is all around you. 

The first vaccines were given with glass syringes with what seemed like long needles, especially into a child's tiny arm, but still the lines were willingly there. The follow up doses were given orally on a sugar cube. 

Money was donated and collected for the fight with dimes and it seemed to be defeated relatively quickly because the scare was real and in one's face.",0
"I actually wrote my thesis along these lines
 I was studying Antarctic yeast species, which live in cold, dry environments and are exposed to incredible amounts of uv radiation. In other words, very similar conditions to space.  Numerous studies have found that they in fact can survive in space, so it's entirely possible that other microbes could survive the trip to Mars. The yeast I studied ate rocks, do they may even be able to reproduce on Mars as well.  We try to sanitize most stuff that gets sent to space, because on the off chance there is native alien life ( bacteria and what not) we don't want to accidently kill it off with an invasive species",0
That is so damn cool. Thanks for sharing!,0
"My first fire in the department I volunteer for was a lightning strike. I couldnt tell for the life of me what started it. The investigators showed up, walked to the collapsed chimney, said ""yup, lightning strike"" and basically left. I asked them how the hell they ID'd it so fast and they basically said that one big indicator would be the damm lightning storm that just passed and that the sand in the mortar and bricks of the chimney had turned to glass from the intensity of the lightning. So simple, but it blew my mind.",0
"[“What does this mean for the pandemic vaccine effort in general? The first big take-away is that coronavirus vaccines can work. I have already said many times (here and in interviews) that I thought that this would be the case, but now we finally have proof. The worst “oh-God-no-vaccine” case is now disposed of. And since all of the vaccines are targeting the same Spike protein, it is highly likely that they are all going to work. There may well be differences between them, in safety, level of efficacy across different patient groups, and duration, but since all of them have shown robust antibody responses in Phase I trials, I think we can now connect those dots and say that we can expect positive data from all of them.”](https://blogs.sciencemag.org/pipeline/archives/2020/11/09/vaccine-efficacy-data)
- Derek Lowe

If the remaining trials are successful we could get billions more doses for 2021.

Looking good.",0
"Oral polio vaccine (OPV) contains an attenuated (weakened) vaccine-virus, activating an immune response in the body. When a child is immunized with OPV, the weakened vaccine-virus replicates in the intestine for a limited period, thereby developing immunity by building up antibodies. During this time, the vaccine-virus is also excreted. In areas of inadequate sanitation, this excreted vaccine-virus can spread in the immediate community (and this can offer protection to other children through ‘passive’ immunization), before eventually dying out.

On rare occasions, if a population is seriously under-immunized, an excreted vaccine-virus can continue to circulate for an extended period of time. The longer it is allowed to survive, the more genetic changes it undergoes. In very rare instances, the vaccine-virus can genetically change into a form that can paralyse – this is what is known as a circulating vaccine-derived poliovirus (cVDPV).

https://www.who.int/westernpacific/news/q-a-detail/what-is-vaccine-derived-polio",0
"Comas aren't just a form of deep sleep. In fact, sleep is a complex and specific pattern of brain activity that requires a healthy brain to perform it (and just happens to produce unconsciousness as a side effect). Your brain just temporarily switches off consciousness - and various stimuli can make your brain switch it back on. A sufficiently loud noise, a certain amount of physical touch or movement of the body in space, a shot of adrenaline as in your question, etc. will all send signals to that switch and flip it back to the ""on"" position.

A coma is a *lack* of activity. The consciousness switch (parts of the ascending reticular activating system) is broken, or the wires leading it to the machinery of consciousness (other parts of the ARAS) are not working, or the machinery itself (cerebral cortex) is hopelessly damaged. This damage can be due to lack of oxygen (suffocation, drowning, opioid overdose, stroke) or due to mechanical injury, but in all cases, the neurons are severely damaged or dead. In some cases a signal can't even get to the ARAS. Even if it can, the ARAS and/or the cortex can't respond like it should. That's the entire reason the coma is happening, and it's the reason that playing Justin Bieber at full blast or jostling the person won't wake them up either.

Tl;dr: a coma is what happens when your on/off switch is broken or disconnected. Trying to hit the on/off switch won't solve the problem.",0
"Cancer itself can cause weight loss because of the metabolic expenditure (see: other comments explaining cachexia).

Cancers in the GI tract especially around the stomach, pancreas and initial small bowel can cause anorexia (lack of appetite), early satiety (feeling full quickly) and nausea, through tumour mass taking up physical space in the gut, through chemical release and through invasion of the nerves supplying the gut. It is because of these effects that unintentional weight loss over a relatively short period of time sets off alarm bells and is seen as a ""red flag symptom"".

Treatments of cancer can also affect weight:
- chemotherapy can cause horrendous nausea and vomiting but can also strip away the lining of the bowel, causing inadequate absorption of your food. 
- chemotherapy is also very metabolically demanding; by essentially damaging cells throughout the body (hence why hair falls out), the body is forced to replenish them, those using energy
- radiation treatments of the mouth, throat and chest can cause burns and scarring of the oesophagus and make it difficult or even uncomfortable to eat
- plus surgeries etc are hugely demanding, not including bowel surgery affecting ingestion and absorption of food!!

Hope that helps.",0
"This is a quickly moving pandemic - our knowledge of the virus is shallow and its potential to evolve with time is greater than 0. That said, the initial outbreak in Hubei province allows for a detailed breakdown of mortality by age and sex. The following comes from [a paper in the China CCDC weekly](http://weekly.chinacdc.cn/en/article/id/e53946e2-c6c4-41e9-9a9b-fea8db1a8f51) that evaluated 72,000 case studies, along with about [56,000 case studies from the World Health Organization](https://www.worldometers.info/coronavirus/coronavirus-age-sex-demographics/#ref-2) (WHO). The full integration and interpretation of these two sources can be found [here](https://www.worldometers.info/coronavirus/coronavirus-age-sex-demographics/). A partial summary follows.

By age:

|Age|Death Rate|
|:-|:-|
|80+ years|14.8%|
|70-79 years|8.0 %|
|60-69 years|3.6%|
|50-59 years|1.3%|
|40-49 years|0.4%|
|30-39 years|0.2%|
|20-29 years|0.2%|
|10-19 years|0.2%|
|0-9 years|no known fatalities|

&#x200B;

And by sex:

|Sex|Death Rate|
|:-|:-|
|Male|2.8%|
|Female|1.7%|

&#x200B;

It is unclear why men and women are affected, but an intriguing possibility is an X-linked gene that creates the ACE-2 receptor, which is exploited by some coronaviruses to enter cells. Females are XX, while males are XY, meaning they only have one copy of the gene and a subpopulation may be more vulnerable. Primary research can be found [here](https://www.nature.com/articles/s41421-020-0147-1) and [here](https://www.nature.com/articles/s41368-020-0074-x), with a more accessible summary [here](https://www.the-scientist.com/news-opinion/why-some-covid-19-cases-are-worse-than-others-67160) This matches a similar pattern with the 2003 SARS outbreak, where 13% of females died while 22% of men did.

The origins of the virus are unknown, but genetically it is [very close to a similar virus in pangolins](https://www.nature.com/articles/d41586-020-00548-w), which unfortunately are poached for traditional medicinal purposes. While it is not definitely known where the virus comes from originally, it is most likely an episode of zoonosis - where a virus spreads to a new species from another. Many human pandemics have their roots in animal transfer, including influenza (chickens), Ebola (chimpanzees), HVI/AIDS (chimpanzees again), measles (cattle), among hundreds of others. If you are interested in the history of these kinds of disease species jumps, I recommend [Spillover](https://www.amazon.com/Spillover-Animal-Infections-Human-Pandemic/dp/0393346617) by David Quammen.

My background is as an archaeologist, and I've been researching the emergence of epidemics for the past few years (no pubs on the topic though, sorry). If you are interested in this too, I recommend William McNeill's [Plagues and Peoples](https://www.amazon.com/Plagues-Peoples-William-H-McNeill/dp/0385121229). Historically, diseases seem to have gone through an initial high mortality phase, followed by a more contagious phase. This is only an hypothesis - and by a non-specialist at that - but I wonder if the SARS to COVID-19 infections follow that pattern. The difference being that a modern medical system can isolate the more lethal first step (SARS). Unfortunately, for more contagious diseases (like COVID-19), only massive containment measures are likely to be effective. Not all governments are capable (or willing) to do this.

Lastly, some may read the low infection rates for young people as a reason not to worry. As a parent of an infant, I take some solace on them. But keep in mind that even if you get a mild infection, you may spread it to someone you care about who will have a much harder time with it. While masks (N95 or greater) may be effective at preventing contamination, the most effective measures will be simply hygiene like frequent showers and washing your hands thoroughly. WHO recommendations for hygiene habits to reduce disease transmission can be found [here](https://www.who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public).

Lastly, I caution agains the [normalcy bias](https://en.wikipedia.org/wiki/Normalcy_bias). It is easy to assume that this will blow over like so many other supposed crises have. But the only comparable disease in the modern era is the [1918 influenza epidemic](https://en.wikipedia.org/wiki/Spanish_flu) that infected 500 million, and killed 40-50 million. If the numbers from Hubei are representative of the infection (and keep in mind - that is with extraordinary containment measures), then a 2.8% mortality infecting 500 million today would be 14 million people. It is unknown how international travel through airports and crowded cities of 10+ million will factor into this pandemic - we don't have a historical precedent for that. We also don't have a precedent for how pandemics interact with modern medical systems. There are a lot of unknowns. Wash you hands.

I hope this helps. Stay safe everyone.

ps. sorry for the weird text changes, wrote this in markdown first, which did not turn out well",0
"A very general answer to your two questions - absolutely not a guarantee but yes it is possible, and a LONG time. The land masses we know today have come together and separated more than once over the last 4.5 billion years, and could come together again as tectonic plates continue to interact with one another - pulling apart, pushing together, and/or sliding past one another.  However, there's no guarantee they'll simply meet up on the opposite sides, as there are complex and varying forces acting on the tectonic plates, so we can't, or shouldn't, assume their trajectories after pulling apart will be linear over the following hundreds of millions of years it would take for them to move towards one another once again. And just a tad more about how long it could take - the tectonic plates containing the US and Europe are currently moving away from one another at a rate of approximately 1 inch or 2.5 cm per year, roughly at the pace at which our fingernails grow. While some plates may move more quickly, others can move even more slowly, so again, the theoretical timeline for another supercontinent is a long ass time.",0
"The reproductive behavior of dinosaurs is a topic that scientists have been trying to understand based on available evidence. While direct observations are not possible, researchers use various methods to infer how dinosaurs may have reproduced. Based on the fossil record, it is believed that dinosaurs likely engaged in sexual reproduction. Similar to modern reptiles and birds, dinosaurs probably had separate sexes, with males and females possessing distinct reproductive organs.",1
"The color in poop comes from the breakdown of red blood cells. Red blood cells contains iron binding proteins and the color comes from the degradation. 

“The non-iron portion of heme is degraded into the waste product biliverdin, a green pigment, and then into another waste product, bilirubin, a yellow pigment. Bilirubin binds to albumin and travels in the blood to the liver, which uses it in the manufacture of bile, a compound released into the intestines to help emulsify dietary fats. In the large intestine, bacteria breaks the bilirubin apart from the bile and converts it to urobilinogen and then into stercobilin. It is then eliminated from the body in the feces. Broad-spectrum antibiotics typically eliminate these bacteria as well and may alter the color of feces. The kidneys also remove any circulating bilirubin and other related metabolic byproducts such as urobilins and secrete them into the urine.”",0
">You can see good examples of this when a tick bite can trigger an allergy to red meat - even if the tick was picked up in an area with no cattle for any related proteins to be carried over.

That's because it's not an allergy to red meat in the culinary sense, but rather a broader sense which refers to meat from any mammal. The allergy is specifically to the carbohydrate galactose-*alpha*-1,3-galactose (""alpha-gal"" for short), which is present in all mammals' cell membranes except for those of primates. ~~When bitten by a tick, your immune system may also come into contact with remnants of its last meal, including any alpha-gal that would be present if it had recently fed on any number of mammals.~~ EDIT: It looks like I was mistaken about the mechanism for exposure to alpha-gal. The ticks which are known to cause alpha-gal allergies just happen to have this compound as a component of their saliva, probably through an evolutionary coincidence.",0
"Geology PhD here, I will add citations and extra info if this comment gets interest, I'm just about to get on a plane so that'll be in several hours but a lot happened 3 Ma, during that period the Earth transitioned from a ""greenhouse"" earth to an ""ice house"" Earth as the Northern hemisphere glaciated. This was due to the closure of the Isthmus of Panama which altered ocean currents and allowed the Atlantic meridional overturning circulation to form (it may have existed in a weakened state before however). This cooled the northern hemisphere allowing ice sheets to form. The ice sheets of Antarctica had already formed at this point however around 34 Ma. 

In short the CO2 levels were higher due to a number of reasons but one was the planet was still in a warm ""phase"" with high levels of atmospheric CO2 because there was less area of ice sheet to store CO2 and cool the planet allowing more CO2 to be stored in the oceans

Disclaimer: this is all I can remember off the top of my head, please correct me if any of it is wrong :-)

Edit: My flight is delayed! Good news for you all but bad news for me haha, anyway, promised citations from a presentation I did on the topic recently:

• Bartoli, G. et al. 2005: Final closure of Panama and the onset of Northern hemisphere glaciation. Earth and Planetary science letters. V. 237, 1-2, p. 33-44. http://dx.doi.org/10.1016/j.epsl.2005.06.020

• Filippelli, G. M. and Flores, J-A. 2009: From the warm Pliocene to the cold Pleistocene: A tale of two oceans. Geology. V. 37, no. 10, p. 959 – 960. doi: 10.1130/focus102009.1

• Grotzinger, J. P. and Jordan, T. H., 2014: Understanding the Earth. W.H. Freeman and Co. (seventh ed.)

• Haug, G. H., and Tiedemann, R. 1998: Effect of the formation of the Isthmus of Panama on Atlantic Ocean thermohaline circulation. Nature. V. 393, p. 673 – 676. doi: 10.1038/31447

• Schneider, B. and Schmittner A. 2006: Simulating the impact of the Panamanian seaway closure on ocean circulation, marine productivity and nutrient cycling. Earth and Planetary science letters. V. 246, p. 367 – 380. http://dx.doi.org/10.1016/j.epsl.2006.04.028

• Linthout, K., Helmers, H. and Sopaheluwakan, J. 1997: Late Miocene obduction and microplate migration around the Banda sea and the closure of the Indonesian Seaway. Tectonophysics. V. 281, no. 1, pp. 17 – 30. 

• Lisiecki, L. E., and Raymo, M. E., 2005: A Pliocene-Pleistocene stack of 57 globally distributed benthic 18O records. Palaeoceanography. V. 20. doi:10.1029/2004PA001071

Edit 2: If you thought the Pleistocene had high CO2 then google the paleocene eocene thermal maximum, I'm happy to add info about that after my flight (once I can access my computer)

Edit 3: thanks for the gold! My first :-D boarding now (delay was minimal) I'll be back in a few hours with more cool palaeo stuff.",0
"**Short answer:** For any given water molecule, the odds are basically negligible. But the odds that you've drank at least one water molecule twice are pretty much 100%. 

**Long answer:** Think in terms of the numbers of water molecules on earth. In a cup of water there are about 10^24 water molecules (100 g / 18 amu ~ 10^(24)).

The total mass of water on earth is approximately 10^24 g of water, which works out to about 10^46 water molecules on earth. 

So if you pick 10^24 molecules out of 10^(46), put them back into the 10^46 and mix them back up, and randomly choose another 10^(24), what are the odds you'll pick at least one atom twice? We can approximate it in the same way we do the [birthday problem:](https://en.wikipedia.org/wiki/Birthday_problem#Approximations) P = 1-e^(-n^2 /2m ) where n=10^24 and m=10^(46). Turns out this number is basically equal to 1, so the odds are *almost certain* that any two glasses of water will have at least one atom in common. This generalizes between *every* cup of water - in that cup of coffee you're sipping right now, the odds are good that it has shared atoms with basically every person to ever live. 

It's pretty cool how Big Numbers^TM work out. A tiny probability, given sufficient chances, becomes a certainty. 


",0
"Whenever you mathematically ""ask"" the Standard Model for an experimental prediction, you have to forcibly say, in math, ""but don't consider up to infinite energy, stop SOMEWHERE at high energies"".  This ""somewhere"" is called a ""cut-off"" you have to insert. 

If you don't do this, it'll spit out a gobbledygook of infinities.  However, when you do do this, it will make the most accurate predictions in the history of humankind.  But CRUCIALLY the numbers it spits out DON'T depend on what the actual value of the cut-off was.  

If you know a little bit of math, in a nutshell, when you integrate things, you don't integrate to infinity - there be dragons - but rather only to some upper value, let's call it lambda.  However, once the integral is done, lambda only shows up in the answer through terms like 1/lambda, which if lambda is very large goes to zero.

All of this is to say, you basically have to insert a dummy variable that is some ""upper limit"" on the math, BUT you never have to give the variable a value (you just keep it as a variable in the algebra) and the final answers never depend on its value.

Because its value never factors in to any experimental predictions, that means the Standard Model doesn't seem to suggest a way to actually DETERMINE its value.  However, the fact you need to do this at all suggests that the Standard Model itself is only an approximate theory that is only valid at low energies below this cut-off.  ""Cutting off our ignorance"" is what some call the procedure.",0
"The difficulty in developing a pill that effectively helps with weight loss stems from the complex nature of human metabolism and the multifaceted factors that contribute to weight gain and loss. While there are medications available that can aid weight loss to some extent, they are typically only prescribed for individuals with certain medical conditions or obesity-related complications. One of the challenges in developing a weight loss pill lies in finding a mechanism that can target fat cells specifically without causing detrimental effects on other tissues or organs.",1
"Yes, fish can indeed ""fart"" in a sense. When fish consume food, they produce waste in the form of feces, similar to mammals. However, unlike mammals, fish do not possess an anal sphincter to control the release of gas. Instead, gas can be released from their digestive system through their anus, but it is not accompanied by the distinctive sound or odor associated with mammalian flatulence.",1
"Not just corneas; sclera is also transplanted, either 1/4, 1/2, or whole globes are used.

Source: I work in organ and tissue donation.",0
"It is correct that exposure while young will help build your initial immune capability and that building phase is finite. You reach a point where your immune system has a pretty good database to work off of after a few years of encountering infectious or inflammatory stuff. 

What people don’t consider is that on a daily basis, our bodies are launching an immune response to literally millions of attacks that we don’t even know are going on because not every immune response causes symptoms. (Fun side fact, the symptoms of an illness are not caused by the pathogen, they are the result of your body’s immune response. Fever for example is your body raising its own temperature in an attempt to kill foreign bacteria or viruses). Even with lower socialization in quarantine, our bodies are still inundated with countless immune triggering pathogens, bacteria, spores, allergens, even our own cell mutations. Just because you haven’t gotten a head cold in a year or dodged the flu doesn’t mean your immune system isn’t still being put to the test. The world is a filthy place. 

That being said, unless kids have been in hyperbaric chambers all 2020, they’re probably still being exposed to enough immune triggers to keep developing healthy and normal immune responses.",0
"The unsatisfying answer is ""it depends."" Here on Earth, the temperature you experience is largely determined by the ambient temperature of the matter (air) around you. But in space, you would be in near vacuum. The random particles around you might have some temperature, but there simply won't be enough of them to transfer a meaningful amount of energy to or from you. 

Instead, your temperature will be determined by radiation: how much sunlight are you absorbing, and how much infrared are you radiating away from yourself. When the magnitude of both of these are equal, that will be your equilibrium temperature. These will be determined by your albedo (e.g. what color are you wearing), your geometry (e.g. how fat are you), how well heat is conducted within your body, and how much heat are you generating yourself. In other words, it's complicated. But, to give you some idea, an astronaut in a normal space suit in Earth orbit can't spend much time in direct sunlight without getting cooked.",0
"Well then. Birds are dinosaurs, so everything we know about birds falls under the purview of your question. However, for extinct forms, we can also make inferences using a technique known as [phylogenetic bracketing](http://en.wikipedia.org/wiki/Phylogenetic_bracketing). 

Dinosaurs are [archosaurs](http://archosaurmusings.wordpress.com/what-are-archosaurs/), the two living representatives of which are crocodylians and birds (see also our [FAQ on why birds are dinosaurs](http://www.reddit.com/r/askscience/wiki/biology/birds_are_dinosaurs)). If there's a character that both groups have, it was likely present in their common ancestor. Things like a four chambered heart (which evolved independently from the mammalian heart), [unidirectional airflow in the lungs](http://mappingignorance.org/2013/03/29/triassic-lungs-unidirectional-flow-in-alligators-breathing/), and nest-building/parental care are present in both birds and crocodylians, so they were probably present in their common ancestor. That means extinct dinos likely had those traits or lost them secondarily. We have fossils that confirm these some of inferences, like brooding of nests. 

Interestingly, we've also recently found that alligators are monogamous over multiple mating seasons, as are many birds, so that could have implications for how we look at extinct archosaur behavior. Alligators will also show nest site fidelity, coming back to the same or nearby areas over multiple nesting seasons. Many crocs have [complex mating rituals](http://www.wired.com/wiredscience/2011/05/alligator-mating-physics/) as well, so these also seem to be ancestral to archosaurs. 

As far as dinosaur reproduction goes, we've found a lot of similarities between the reproductive tracts in birds and crocs. For example, [alligators and birds form eggshells in similar ways](http://www.ncbi.nlm.nih.gov/pubmed/1547315).

Most [""reptiles""](http://en.wikipedia.org/wiki/Sauropsida) have [hemipenes](http://en.wikipedia.org/wiki/Hemipenis), which are paired copulatory organs that are everted for mating. This is not true of archosaurs. Most birds have lost their penis, but some retained it (ducks and ratites like ostriches and emus are two examples). I don't know of any fossil dinosaur genitalia, but birds (those that have a phallus) and crocs each have a single phallus rather than the hemipenes of extant lepidosaurs. That's likely what other extinct archosaurs probably had. However, given the range in variation that we see in living birds alone, I'm sure dinosaur genitalia existed in all shapes and sizes.

In short:

- Dinosaurs probably ancestrally had penises similar to crocodylians and some birds, but they could have been lost in lineages like they were in many bird groups.

- At least some brooded their nests.

- They probably had mating displays like birds and crocs do.

- Some may have been monogamous over multiple mating seasons like many birds and crocs.

[This article](http://blogs.smithsonianmag.com/dinosaur/2012/02/the-anatomy-of-dinosaur-sex/) similarly covers these topics.
",0
"Most phones have about a  3000mAh battery, Those charges are energized up 3.6 volts (give or take), which means the battery can supply 10.8 Watt hours. See as a Watt is Joules per second, and there are 3600 seconds per hour 10.8 joules/second \* hours  \* (3600 seconds/hour) = 38,880 Joules of energy. Lets just call it 38 kJ of energy.

&#x200B;

So, how much energy does it take to start a car? Lets say you're car is nice and new, so you might need to run the starter motor for half a second.  Your battery should be charge to 12.6 volts. But how much current will the starter draw? A hell of a lot is the answer. The inrush current to a starter motor might be 700 amps, and then settle to about 200 amps. But lets average it out at 400 Amps for half a second, at 12.6 volts. So 12.6 volts \* 400 Amps = 5 kW, for half a second is 2.5 kJ.

Really? I'm shocked by that. So it seems your cellphone battery does have enough energy.

However, you couldn't do it directly. Different batteries have different ""C"" ratings, which decide how much current they can pass without risking blowing up, and I expect the C rating on cell phone batteries is quite low. Which is to say, the cell phone battery can't supply the 200-700 Amps you need to spin the motor. Furthermore, and more fundamentally, your cellphone battery is too low a voltage to get the motor moving. So you would need to have a circuit that probably was composed of a DC/DC converter, that boosts the voltage of your phone battery, and then 6 or so super capacitors to store the energy that your phone battery trickle into them. I reckon your phone battery can probably supply about 1 amp of current max, \[totally guessing that number\] at 3.6 volts, which is 3.6 watts, or 3.6 joules per second. So in order to charge up this hypothetical device to 2.5 kJ of energy it would take 700 seconds, or 11 minutes.  Give or take. And this is all assuming your car starts in half a second, and 100% efficiency. In reality, it might take a good second or two, and you'd be lucky to get 80% efficient. In which case it might take over an hour.  


So, TLDR: Most modern cell phone batteries have enough energy to start a typical car motor, certainly at least a couple times, but they can't supply enough current at a high enough voltage to do it. You might be able to build a device that could allow your cellphone battery to start your car, but it would take a bare minimum of 11 minutes, and perhaps as long as an hour, for your phone battery to charge the device.",0
"Viagra (sildenafil citrate) is a medication primarily used to treat erectile dysfunction in males by increasing blood flow to the penis. Its mechanism of action involves inhibiting an enzyme called phosphodiesterase type 5 (PDE5), which helps to relax and widen blood vessels in the genital area. In biological females, Viagra does not have the same direct effect as it does in males. However, some studies have explored the potential benefits of Viagra in women with certain conditions.",1
"Microwaves are high-frequency radio waves. They heat food by being absorbed (by the water in the food, mostly).  An antenna, and in particular a metal antenna, will absorb this energy much better.

In metal, the shapes that make good antennas are much more common than the shapes that do not make good antennas.  The walls and oven racks for example are carefully designed not to be good antennas.  

But a randomly shaped piece of metal will probably absorb most of the energy in the oven, and do what antennas do:  convert it to electricity.  

There is so much energy in a microwave oven that if it is converted to electricity, it will arc and generate lightning.  Or get melting hot.

Even if the arcing does not catch things on fire, the little bolts of lightning will damage the walls of the oven, which are also metal, and although the walls start out designed to be bad antennas, the arc damage can cause small antenna sites which will suck energy out of the oven over time.  Or worse.",0
"Artificial selection has transformed many animals (and plants) to similar degrees as dogs:

* Consider goldfish - originally a drab Asian Carp shaped like any other small fish - now have a [wide range of shapes, sizes, and colors that look nothing alike](https://en.wikipedia.org/wiki/Goldfish#Varieties)
* Consider turkeys - originally a flying bird - bred to be too fat to fly.
* Consider Corn - [originally small bunches of seeds that looked more like a stalk of wheat](https://geneticliteracyproject.org/2017/08/09/ancient-corn-genome-unlock-secrets-crop-diversity-adaptation/)
* Or [Aurochs - the 7-foot-tall horned ancestors of cows](https://en.wikipedia.org/wiki/Aurochs) 
* or [Silkie Chickens that look more like a furry pokemon](https://www.thehappychickencoop.com/silkie-chicken/) than their [red junglefowl ancestor](https://en.wikipedia.org/wiki/Red_junglefowl)
* or [cauliflower, broccoli, brussel sprouts, kohlrabi, ""tree cabbages"", kale, and cabbage](https://en.wikipedia.org/wiki/Brassica_oleracea) - all the same species bred by people for different foods
* Does [this pig](https://en.wikipedia.org/wiki/Wild_boar) look anything like [this pig](https://en.wikipedia.org/wiki/Domestic_pig) that humans bred from the former one?

And circling back to OP's question of cats....

Considering that [domestic cats can breed with Asian leopard cats,  Margays, fishing cats, caracals, oncilla, and other similar species](https://en.wikipedia.org/wiki/Felid_hybrid#Domestic_%C3%97_wild_hybridization), if people chose to do so, they could breed a much broader range of cat-like hybrids.",0
"Marine mammals have evolved several adaptations to maintain proper hydration despite living in saltwater environments. Here are some ways they stay hydrated:

1. Kidney adaptations: Marine mammals, like whales, manatees, and seals, have highly efficient kidneys that can concentrate urine and conserve water. They have the ability to excrete highly concentrated urine while reabsorbing water back into their bodies. 2. Salt gland: Many marine mammals have specialized salt glands that help them get rid of excess salt.",1
"In Computer Science, we like to quantify algorithms based on how their running time is affected as the input size grows.  Some algorithms run at the exact same speed regardless of input size, while others become significantly more complicated much quicker as the input size increases.  By way of an example of an easy case is pulling a value out of an array -- it doesn't matter if we ask for array item 2 or array item 29 756, the speed of doing so is constant.  A more complicated case would be something like chess -- we can calculate all possible moves on a smaller chess board, but as the board gets bigger we get into a situation where calculating all possible games would require every computer mankind ever manufactured to date to run until the heat death of the universe...and it still wouldn't complete.

So we have a notation for describing an algorithms runtime complexity ([Big O notation](https://en.wikipedia.org/wiki/Big_O_notation)), and we can put problems with similar runtime constraints into a _complexity class_.  And there are two very important complexity classes called 'P'  and 'NP' that many algorithms fit into^0.  

Algorithms that are part of 'P' have two important characteristics: the time they take to run can be described as a polynomial (that is, by an equation of the form ""an^k + bn^k-1 + cn^k-2 ... +xn^2 + yn +z""^1 ), and the time required to verify the solution can also be described as a polynomial.

Algorithms that are part of 'NP' also have a similar pair of characteristics.  Like problems in 'P', the solutions can be verified in polynomial time.  However, their runtime to calculate the solution in the first place only runs in polynomial time on a _non-deterministic_ Turing machine, which may be worse than polynomial time when run on a _deterministic_ Turing machine^2.  You don't have to worry about the details of what that means -- but generally it means that these are problems where we can verify the result in polynomial time (or ""poly time"" for short), but where the computation itself may not be computable in poly time.

Using the above definitions, it's not hard to see that every problem in P is also in NP.  If you were to draw a Venn diagram, P would be a circle entirely inside NP.  All P problems can be verified in polynomial time, and all of their runtimes can be run in poly time on a non-deterministic Turing machine (as well as running in poly time in a completely deterministic Turing machine).

So here is where the unsolved equation comes in:  we know that P is inside NP.  However, is P = NP?  That is, can _every_ problem in NP be reformulated such that it would also be in P?  Or are there problems in NP that can't be reformulated to also be in P?

This has been an open question in computer science for much of the past century, and currently there is no proof either way.  Many computer scientists _believe_ that P ≠ NP, but there is no actual proof one way or another (on a side note, some feel that P = NP, however some in that camp feel that any conversion of a NP problem into P would be non-constructive^5).

Okay -- so what is the point of all this über-nerd gobbledygook?  It reads like a whole bunch of mental masturbation for eggheads -- is this important in the real world?

The answer to that decision problems is a big YES.  Some extremely important algorithms that people rely upon in their daily lives currently rely on the assumption that P ≠ NP.  One of the most important of these is _encryption_.  Decryption can be thought of as a decision problem -- given an input (the encrypted data), we can quickly verify if our ""solution"" is correct (that is, did the decryption work?  Did we get the right decrypted data back?).  But how useful would decryption be if we could also decrypt _any_ data (without the decryption key) in polynomial time on any computer?  What would happen if it was also very easy to decrypt any encrypted information without a password or encryption key?  Right now the whole contract of encryption is that it is very easy to decrypt data if you have the proper encryption key, but that without the encryption key decrypting the data is more difficult, and gets more and more difficult as the key size increases.  Decrypting data with a 2048 bit key would require more time in the average case than the expected lifetime of our solar system.  Proving that P = NP, and then finding a constructive solution to convert an NP decryption algorithm such that it is also in P would likely break the way we encrypt data.  This could have serious repercussions to how virtually all commerce and personal privacy on Earth works.^6

At the same time, it could make a lot of problems that are very difficult to solve computationally more efficient.  This could have all sorts of positive benefits (that outweigh the negatives of breaking encryption).  The Knapsack problem^7, for example, is in NP and is thus more and more difficult to solve as the number of items you could potentially put into the knapsack increases.  But if we had an efficient way to convert this problem such that it was also in P it would potentially have all sorts of positive benefits in the real world^9.  All of the world shipping logistics for example could be significantly improved -- the Knapsack Problem isn't any different than figuring out what sets of items to pack into a shipping container to maximize the weight and number of items being shipped -- companies that were able to efficiently compute this for each cargo container, and for each ship (as you can think of assigning containers to ships as an instance of the Knapsack Problem as well!).

This problem is so important that is it one of the seven [Millennium Prize Problems](http://www.claymath.org/millennium-problems).  I'd also argue that it's the most important problem, as if you could solve it and prove that P = NP, [it may mean that a computer could generate proofs for all of the other Millennium Prize Problems](http://www.claymath.org/sites/default/files/pvsnp.pdf)^10.  So if you can solve this one, you _might_ also be able to efficiently solve all of the _other_ major mathematical problems of our time.

How cool would _that_ be?  HTH!^11

-----
^0 -- 'P' and 'NP' problems are formulated as _decision problems_, that is problems where the result is YES or NO.  Conceivably, we can generally take problems and convert it into a decision problem -- a sort algorithm, for example, may be reformulated as a sorting algorithm where at the end we ask ""Is this list sorted?"", and we get back a YES or NO response.  I'm trying to keep things somewhat simple to understand for laypeople, so I'm not going to deal with these specifics in this post.  
^1 -- I would have preferred to use the same letter for the term multipliers with subscripts, but AFAIK Reddit doesn't permit subscripts, only superscripts.  So please don't take my use of a, b, c, x, y, and z to imply that there are only 26 terms in the polynomial form.  There could be just one, or there could be thousands.  
^2 -- Ugh, I've been trying to reformulate a way to discuss this without getting into the differences between deterministic and non-deterministic machines, or what a Turing machine is^3.  The simplest explanation is that the computers we run are all like deterministic Turing machines^4; a non-deterministic Turing machine is one that you can think of is allowed to ""guess"" at answers.  
^3 -- at its simplest, it's a simple mathematical model of a computer used to prove what computers can do, and what they can't do.  
^4 -- You can think of ""deterministic"" to mean that given a series of instructions, the machine will run the instructions one at a time, and won't just decide to go and do its own thing once in a while.  
^5 -- a [_non-constructive proof_](https://en.wikipedia.org/wiki/Constructive_proof#Non-constructive_proofs) is one that doesn't create or provide an actual object to demonstrate the proof.  So in this case, it would be a proof that doesn't actually show _how_ to convert a problem from NP into P, and which doesn't provide an example of converting a problem in NP to also be in P.  
^6 -- There are some conditions here.  I've been somewhat hand-wavy concerning some of the specifics of the runtime constraints for a poly time algorithm.  Most people wind up thinking that ""poly time"" means _fast_, and everything else means _slow_.  That isn't necessarily the case -- n^10 is polynomial, and ~~has~~ can have worse runtime characteristics than an algorithm that runs in exponential time of 2^n (for _some_ values of n).  However, algorithms that have such massive poly time exponentials are pretty rare, so we don't run into cases like this very often.  So while not a universal truth, in most _known_ cases problems in P run faster than problems in NP that are not also in P.  
^7 -- the Knapsack Problem is pretty easy to visualize.  Say you have a knapsack, and a bunch of items of different weights^8.  The Knapsack Problem asks:  which items should you pack such that you get closest to some fixed maximum weight value?  
^8 -- you can also think of items with different volumes if you prefer.  In fact, a multi-dimensional Knapsack problem could look at both the volume and mass of the items, as well as potentially other factors (such as their monetary value).  
^9 -- other than being very useful for your next camping trip.  
^10 -- On the positive aspects of proving that P = NP: ""For example, it would transform mathematics by allowing a computer to find a formal proof of any theorem that has a proof of reasonable length, since formal proofs can easily be recognized in polynomial time. Such theorems may well include all of the CMI prize problems."" ([S. Cook, _The P vs. NP Problem_, Clay Mathematics PDF](http://www.claymath.org/sites/default/files/pvsnp.pdf).",0
It's because everybody has Philips head screwdriver and so the screw is ubiquitous and cheap.,0
"Excellent explanation, and thanks for pointing to some good resources. I just went through 12 hours of diagnostic evaluation with a new psychologist, and I was impressed with how much the exercises obfuscated the goals for measurement. Some things were more obvious, like the impulsivity test. However, that one is impossible to fake. I smacked that spacebar every goddamn time I saw the ""x"". Anyway, not sure what the point of this comment is.

*Edit: Thanks for all the info about the test! Fascinating.",0
"Bill & Melinda Gates Foundation article covers it pretty well 

https://www.gatesnotes.com/Health/How-to-respond-to-COVID-19

“There are two reasons that COVID-19 is such a threat. First, it can kill healthy adults in addition to elderly people with existing health problems. The data so far suggests that the virus has a case fatality risk around 1%; this rate would make it several times more severe than typical seasonal influenza and would put it somewhere between the 1957 influenza pandemic (0.6%) and the 1918 influenza pandemic (2%).

Second, COVID-19 is transmitted quite efficiently. The average infected person spreads the disease to two or three others. That’s an exponential rate of increase. There is also strong evidence that it can be transmitted by people who are just mildly ill or not even showing symptoms yet. This means COVID-19 will be much harder to contain than Middle East Respiratory Syndrome or Severe Acute Respiratory Syndrome (SARS), which were only spread by those showing symptoms and were much less efficiently transmitted. In fact, COVID-19 has already caused 10 times as many cases as SARS in just a quarter of the time”",0
"They didn't develop it in 8 months. The Moderna vaccine has been in development since 2013 for a different strain of coronavirus; they reworked it. Here's a video from the MIT course on the pandemic from one of the researchers who made the vaccine, Kizzmekia Corbett.  [https://www.youtube.com/watch?v=xpqfdr9FPWM&feature=emb\_logo](https://www.youtube.com/watch?v=xpqfdr9FPWM&feature=emb_logo) The MIT course also has other researchers (one studies HIV) who discusses how and why there is no vaccine for HIV because how quickly it mutates. (Full MIT Course here: [https://biology.mit.edu/undergraduate/current-students/subject-offerings/covid-19-sars-cov-2-and-the-pandemic/](https://biology.mit.edu/undergraduate/current-students/subject-offerings/covid-19-sars-cov-2-and-the-pandemic/) )",0
"Sanitizers almost always use alcohol, which bacterial cells don’t really have any cellular means of developing resistance against.  You may as well worry about developing resistance to having a nuke dropped directly on your face.  Alcohol essentially saps bacterial cells of all moisture instantaneously, and to combat that they would need to develop characteristics which would essentially make them not even bacteria anymore (like a plant-like cell wall or a eukaryote-like complex cell membrane)

EDIT:  I got a few things wrong, thanks for pointing them out everyone! (no sarcasm intended).

-  Alcohol doesn’t work mainly by sapping moisture, it actually causes the bacterial cell membrane (and eukaryotic cell membranes also) to basically dissolve.  We can put it on our hands because of our epidermal outer layer of already-dead cells which basically doesn’t give a fuck about alcohol.

- Some bacteria actually can develop resistance to low to moderate concentrations of alcohol, by devoting more resources to a thickened cell membrane.

- Look up bacterial endospores.  These can survive highly concentrated alcohol solutions and cause surfaces to be re-colonized under the right conditions.",0
"I don't think this is 100% true. At least, the food bolus doesn't turn brown immediately after entering the small intestine. It's more of a yellowish color at that point. It turns brown in the large intestine, possibly due to metabolism by the bacteria that live there. 

I am basing this on what I see in lab mice, anyway. Stuff in their small intestine is yellowish, but their poop in large intestine and beyond is brown like humans. Maybe if a gastroenterologist is around, they could clarify this is also the way it works in humans. ",0
"Hi /u/kabir9966!

Quantum entanglement is a phenomenon, in which the measurement results of two entangled particles are correlated. I.e. if  I measure the spin of 100 pairwise entangled particles along the same axis, the results of the entangled pairs will always correlate. In other words, when one measurement gives spin up, measuring the other will always give spin down. This holds true, no matter how far the two particles are apart, or how short the time between the two measurements is. 

One possible explanation of this phenomenon goes as follows: The measurement results follow a secret plan that is created together with the entangled pair. That is, the measurement results are deterministic. You can imagine this like hiding a small item in one of two identical boxes. Then you take one of the boxes to the moon and open it. If you find the item, you instantly know that the other box is empty. This would be a very neat solution, as no signal would have to be exchanged for you to gain this information, thereby side-stepping the problem of relativity.  Furthermore, this theory is [realist](https://en.wikipedia.org/wiki/Realism_in_physics#Local_realism), in the sense that the state of each object is well-defined at all times. 

This is called a [local hidden-variable theory](https://en.wikipedia.org/wiki/Local_hidden_variable). Here, the term ""local"" signifies, that this theory holds on to the constraints of relativity, any object can only influence its immediate surroundings. This constraint is also called ""locality"". The idea of this theory is, that the measurement result of all quantum mechanical particles is pre-determined from the moment of their creation in such a way, that conservation-laws are respected. When we measure one particle of an entangled pair, we get the secretly pre-determined measurement result, and thereby instantly know the state of the other particle, without the need for  any signal to be exchanged between them. 

As it turns out, we can test whether or not such local hidden variables exist using the  [Bell inequalities](https://en.wikipedia.org/wiki/Bell_inequalities): Veritasium has made a [pretty good explainer how this test works](https://www.youtube.com/watch?v=ZuvK-od647c).

The bottom line is, that such a hidden-variable theory would lead to different outcomes that what we measure. 

Consequently, the local realist theory described above cannot be true. We have to let go of at least one of these constraints: The universe can respect realism, but not locality; or it could respect locality, but not realism; or it could respect neither. 


A theory that respects locality but gives up local realism would mean quantum states really remain in an undetermined state of superposition until they are measured, and in the moment of the measurement, the wave function of both particles instantaneously collapses (according to the Copenhagen Interpretation anyway). There are no hidden variables pre-determining the outcome of these measurements, and no signal is exchanged faster-than-light.

The Nobel price was given for experimental evidence that realism does not hold locally.",0
"Mute individuals, also known as people with mutism, have difficulty or inability to speak. This can be due to various factors, including physical, neurological, or psychological causes. It's important to note that not all mute individuals are completely unable to make any oral sounds. Physical factors, such as damage or malformation of the vocal cords, can prevent someone from producing speech.",1
"Not compared to mammals. They actually produce very little urine and the ions are being pushed down their concentration gradient (mostly) in the kidneys, not up. Take a look at this: http://web.utk.edu/~rstrange/wfs550/html-con-pages/u-osmo-kid.html",0
"Yeah, aren't all human embryos female during early developmental stages? When the genes that dictate it will be male are triggered, only then do the genitalia start developing into a penis. 

If I remember correctly this is what causes, to be blunt, that line that runs through the scrotum to the butthole. I speak under correction though...",0
"You can be allergic to eggs, which would make you allergic to mayo (real mayo at least...). If you were allergic to butterflies, how would you know? How often do you physically encounter them, not just look at them? There's lots of people allergic to mold.

The allergies you hear about are ones that involve things people commonly interact with, like food, pets, and medication. You might be allergic to some weed that grows on a mountain in Tibet, but if you never encounter it, you'll never know.",0
"Rather than a drug which increases metabolism (or decreases appetite) could you not instead somehow retard digestion or calorie absorption such that a proportion of the calories you eat come straight out as a massive shit instead of heat?

That way you'd still feel hungry as normal, still be able to eat as normal, and your body would burn the calories it *does* absorb as normal. You'd just pass much of the calories straight through without absorbing them.

---

EDIT: Or something that simply slows down digestion so you eat less because you feel full for longer.",0
"Yes, Greyhounds require a different anesthesia protocol than other dogs due to this very reason.
Source: former vet tech

edit for clarity: The reason being their high metabolism. When they said ""sleep for days"" they meant sleep forever. Certain barbituates can kill them. Instead of just inducing with propofol and keeping under with sevo, they require additional meds like acepromazole and atropine. My doc would pop em with ace and telazole, atropine,then keep them under with sevoflourane gas... you get the picture. Extra work and meds indeed, a few breeds required this protocol",0
"> migraine are often lateralized

In some senses that is a statement of definition rather than a characterization.

http://www.etymonline.com/word/migraine

> migraine (n.)
> late 14c., megrim, from Old French migraigne (13c.), from vulgar pronunciation of Late Latin hemicrania ""pain in one side of the head, headache,"" from Greek hemikrania, from hemi- ""half"" + kranion ""skull"" (see cranium). The Middle English form was re-spelled 1777 on the French model. 
> 

Arguably one that isn't ""lateralized"" is not, by the origin and  meaning of the english  word ""migraine"", a migraine.

So an  interesting question would be ""Are there any headaches having the same physiological basis as a migraine, that are not lateralized?""
",0
">We see in humans that people who miss out on REM sleep (because of interruption, sleep disorders, etc.) will over-compensate for missed REM when conditions are favourable, with what we call 'REM rebound'; i.e. instead of 30 minutes of REM every two hours, they might exhibit, say, two, one-hour REM periods.

Is this why daily consumption of THC is said to negatively affect memory? I used to stop dreaming completely when smoking weed daily and would get phases of extremely vivid and intense dreaming when quitting for a couple weeks. Definitely didn't enjoy the rebound especially because I hate dreaming in general. Never felt that it impacted my memory though and don't see much difference since quitting for good.     
    
Sorry if this is kind of unrelated.",0
"The unpredictability of breaking a dark chocolate bar can be attributed to a few factors. Firstly, the structural properties of chocolate play a role. Although chocolate appears homogeneous, it actually consists of numerous small particles suspended in a fat matrix. These particles can vary in size and distribution, leading to variations in the internal structure of the chocolate bar. Additionally, the presence of deep grooves on the surface of the bar can impact how the bar breaks.",1
"http://news.nationalgeographic.com/2015/08/150819-whales-dolphins-bends-decompression-sickness/ 

>Researchers from the University of North Carolina Wilmington investigated how marine mammals’ tissues—specifically, fat deposits in the jaws of toothed whales that are used in echolocation—absorb nitrogen gas, one of the gases that contributes to the bends. They found that the makeup of the fat affected how much nitrogen gas dissolves in it—and that different species had different fat compositions.

 >Once, scientists thought that diving sea creatures like the elusive, deep-diving Cuvier’s beaked whale were resistant to the bends, but mounting evidence suggests that this may not be entirely true.

>In 2002, international navy sonar exercises were linked to a mass stranding of 14 whales in the Canary islands. The whales had gas bubbles in their tissues, a sign of decompression sickness.",0
"HPV also causes actual cancers. 

Almost all cervical cancer is caused by HPV. Some cancers of the vulva, vagina, penis, anus, and oropharynx (back of the throat, including the base of the tongue and tonsils) are also caused by HPV.",0
"As an addendum to the information already provided here:

Its not just visible light that'll produce a tangible thrust.  Any wavelength of light will.   This becomes a problem for space probes, because electronics and power supplies turning on and off create heat in the infrared spectrum, and these infrared photons cause a small thrust which over long periods of time will cause the probe to veer off course.

There's actual computer simulation and modeling done at NASA to account for this infrared-thrust effect when setting probe trajectories and course corrections.",0
"There’s a book I heard about on NPR called Does It Fart? And yes it will answer your fart questions of animals. The book includes creatures with a standard anus as well those with different butt holes. The best part about the story on NPR was a kid had called in and asked when fish fart, does it make bubbles “like when in the bath?” Kid’s giggling and the NPR folks start laughing and it was so wholesome.",0
"The presence of click sounds in languages is a fascinating topic in linguistics. Click consonants are produced by creating a suction in the oral cavity and releasing it, resulting in a distinctive sound. While it may seem that click sounds are rare, they are actually quite prevalent in the world, although they are mainly concentrated in southern Africa. The reason for their concentration in this region is due to a combination of historical, geographical, and cultural factors.",1
"In the deep ocean, where light penetration is limited, seasonal variations as experienced on the surface are not as pronounced. However, there are certain factors that can influence seasonal patterns in the deep ocean. One such factor is the seasonal mixing of water masses. In certain regions, particularly in the polar regions, surface waters become dense and sink to the deep ocean, causing vertical mixing. This can introduce nutrients and oxygen to deeper waters, influencing the distribution and abundance of marine organisms.",1
"tl;dr: If you're near Earth's orbit, you're already way too close. You'd need to drift outward quite a bit.
 
On Earth, if you leave something lying in the sun indefinitely, it will heat up until the heat it loses to the surrounding air (and the ground) is balanced with the heat it absorbs from the sun. In space, it works the same way, except that without air, the only way for an object to lose heat is by radiation (infrared, at normal body temperatures).

If we want to keep our body temperature roughly constant, the heat we radiate needs to balance out the heat we absorb from the sun plus the heat generated by our own metabolism.

I found [a page estimating our own power output as 100W](http://www.physlink.com/education/askexperts/ae420.cfm). Let's say we are a perfect blackbody radiator with a surface area of 2m^2 and our skin were to be 307K - we're in ""spherical cow"" territory here, but this should be the right order of magnitude.

The power our skin radiates is then given by the [Stefan-Boltzman law](https://en.wikipedia.org/wiki/Stefan%E2%80%93Boltzmann_law) as 5.67e-8 Wm^(-2)K^(-4), or 5.67e-8 * 2 * 307^4 W = about 1000W. (So the good news is, we're losing more heat than we produce on our own, even without air-cooling. Without the sun, we'd freeze instead of overheating.)

So how much sun do we need? This one is a bit more complicated, because it depends on how we're oriented. The bigger our cross-section facing the sun, the more we absorb.

The [total power output of the sun is 3.828e26 W](https://en.wikipedia.org/wiki/Solar_luminosity). To get around 1000W of that, we need to capture 1 part in 3.828e23. So if we present a cross-section of about 1m^2, then our distance should be the radius of a sphere with a surface area of 3.828e23 m^2.

Result: 1.75e11 meters, or about 9.7 light minutes. To give you a picture, Earth's and Mars's orbits are about 8.3 and 12.5 light minutes from the sun on average, so we'd have to go (very, very roughly) a third of the way from Earth to Mars orbit to feel comfortable.

(Not double-checked; it's entirely possible there's a massive error in the above calculation, on top of all the ballpark guessing.)

Edit: Mind you, the ballpark guessing already introduces some *wild* inaccuracy. If we drop the cross section to 0.5m^2, suddenly we're closer to 6.9 light minutes. And we're not perfect blackbody radiators, anyway.",0
"I see someone else already commented about [2,4-DNP](https://en.wikipedia.org/wiki/2,4-Dinitrophenol). That's off the market because it can easily kill you. But if you can figure out how to make it safe, you'll probably get a Nobel prize for it, OP. Because someone already gave the fun answer, I'll follow with my more boring one: 

  
Part of the main difficulty in losing weight is curbing appetite. Weight loss is a simple equation: calories in, calories out. While you'll never be able to outrun the fork, exercising or moving more technically isn't even necessary as long as you're eating less calories than you'd naturally burn in a day. Likewise, as the viral ""Twinkie Diet"" experiment proves, what you eat isn't as important as how much you eat as far as strictly shedding weight goes. Of course, eating all junk food can cause malnutrition and other problems, but strictly losing weight while eating nothing but junk food has been and can be done.   


There have been drugs that can curb appetite, but they're usually reserved as prescription only, as a last resort for the most critically obese patients. The Food and Drug Administration (FDA) has approved these prescription appetite suppressants:   


* Liraglutide (**Saxenda**®).  

* Naltrexone-bupropion (**Contrave**®).  

* Phendimetrazine (Prelu-2®).  

* **Phentermine** (Pro-Fast®).  

* **Phentermine**/**topiramate** (**Qsymia**®).  

* **Diethylpropion** (Tenuate dospan®).

  
The problem with these drugs is that they directly affect your brain chemistry. They control how your body and brain produces and processes hunger signals, and altering that natural process is pretty dangerous. There is no one size fits all approach, so dosages must be monitored by a professional. Typically, these drugs are prescribed as an attempted corrective measure more than anything. Patients that are morbidly obese have already altered their natural production of Ghrelin, or hunger inducing hormone, to extreme levels, and so prescribing an appetite suppressant is a last ditch effort to block the brain from processing those hunger inducing signals, and bring Grhelin production back down to normal levels. They are not guaranteed to work long-term, as there are many ways they can easily fail. The patient can build up a tolerance to the drug, eat out of habit, social construct, or boredom, eat oversized portions when they do feel hungry, or any number of other reasons that would ultimately cause the drug to be ineffective as far as inducing or maintaining weight loss goes. If that happens, going off the drug can actually make the initial problem worse, because the brain will no longer be shielded from the overload of extra Ghrelin that the patient is now producing to counteract the effects of the drug.",0
"Botox (**bot**ulinum **tox**in) works by paralyzing the muscles. Believe it or not, botulinum is incredibly poisonous. It's said about a gram of it could kill a million people. You may have heard of botulism, which is where botulinum toxin can paralyze the respiratory system. 

In this case, the muscles are incorrectly contracting, so the Botox makes them stop doing that by making them incapable of contracting.",0
"I caught the Swine flu when I was around 20 and it felt like there was a real chance I might actually die. I always get a flu shot now. People act like it's inconvenient but you can get one at most pharmacies (or any doctor's office), generally in 15 minutes or less. It's much better than going through days worth of hell.",0
do genital warts do the same thing as hiv?,0
"70,000 bees is a lot for a tree nest, but not insanely high for a large managed hive.  As to your question, I can tell you this:  beekeepers will sometimes combine two weak colonies together to make one strong colony that will make it through the winter.  You kill one queen, and then put all of the bees together.  If you combine them immediately, they'll often all kill each other.  However, if you put one hive box on top of the other separated by a sheet of newspaper, the bees will get used to each other's smells over time and when the newspaper starts developing holes in it, the bees often won't sting each other and will instead work together to clean out the newspaper.  When it works, that's how you combine colonies.  So to apply that to your question: if a tree had two bee-occupied cavities in it with a small amount of rotted wood in between the two colonies, and one queen died... I guess maybe they could combine?  But I certainly wouldn't expect it.",0
"> I imagine seals, dolphins and other sea mammals drink seawater...

Actually, no! Most marine mammals get all the freshwater they need from their diet; water is a common by-product from the metabolic processing of fats and carbohydrates\*. The salt content of their blood and internal tissues is pretty much the same as any terrestrial mammal, and so they'd dehydrate and perish in the same way as, say, us if they were to regularly drink the saltwater they swim in.

Of course, plenty of it does ends up inside them incidentally as they eat anyway, but they're capable of filtering much of this excess salt out by utilising their modified kidneys to produce very concentrated, very salty urine. In which case:

> how good are their kidneys?

Not as good as you'd think they'd be! Surprisingly, for most tasks, they're structurally not much better than those of any terrestrial mammal. The real difference comes in *how* they use them; it's changes in hormonal regulation that helps them rapidly produce concentrated salty urine. The modifications we see are likely more to do with their need to dive deep (and also their large body size, in whales) than anything to do with their salty marine environment.
_____


^(* Indeed, I've heard it said dolphins are unable to distinguish between feeling hungry and thirsty as, to them, they're the 'same thing'. As such, it's said one could (cruelly)^) ^(have a dolphin drink from a hosepipe and think it just had a full meal.)

___

^**Reference:** [^(Ortiz, R.M. (2001)^) ^(Osmoregulation in Marine Mammals. *Journal of Experimental Biology*. 204, 1831-1844)](http://jeb.biologists.org/content/204/11/1831.short)",0
Piss is the answer. Buckminster Fuller called it and designed a reclamation toilet to harvest it. We are great phosphate making machines.,0
"Corona and influenza viruses are single stranded rna viruses. Once they get into your cells they start replicating, which your immune system is very good at recognizing and killing. 

Herpes is a double stranded DNA virus. It can get into your cells and just sit there not replicating, so it is hard for your immune system to see. It will become active and replicate periodically, but then it infects other cells  where it will just sit dormant again. So your immune system is playing whack-a-mole, which isn't very effective. 

Hiv is a retro virus, so it actually integrates into your cells own dna. You basically can't get rid of it without killing the host cell, and the cells it primarily infects are t cells, which are immune cells that  are generally tasked with antiviral immunity. So it basically infects and can kill the very cells responsible for killing it. It can also hide in other cells that create a reservoir for it to become infectious again, which makes it even more difficult to treat.",0
"Mma fighters who a use testosterone replacement therapy such as Dan Henderson, randy couture, or the commentator Joe Rogan all go bald and get big fat heads. They look weirdly similar after they do that. 

Why is it? ",0
"I love this information. I had not considered research facilities. I'm sitting here thinking about enormous warehouses and indoor football fields  and crap. 

However, I'm not sure they sculpted the building's structure specifically to the curvature of the earth for the precision required. That seems like more a ""we have this really cool laser that has the most stable legs, and most motion dampening arms, so we know exactly where it's supposed to go.""

Just seems like if the moon's gravity DID shifte one side more than the other, an alarm should go off, and the collider should not fire, until the correction has been made (either mechanically, or manually). I imagine it probably runs this safety check every time it asked to fire.

",0
"Screw Reddit, eat the rich -- mass edited with redact.dev",0
"Little longer analogy I heard. There are two kinds of ways to kill bacteria. Deactivate or destroy. It's like trying to disable a car. If you are rooting around in the guts trying to remove a key component or make them not function, the car manufacturer can change the design to make that more difficult. If you are shooting it with a tank, there's only so much armor they can add, and even that won't stop the biggest guns, like alcohol.",0
"Yes! Hawking Radiation.

https://en.m.wikipedia.org/wiki/Hawking_radiation

I'll defer the calculation to someone more mathematical than me (not on a phone), but:

>black holes of mass M in grams evaporate via massless electron and muon neutrinos, photons, and gravitons in a time τ of

8.66 * 10^-27 * (M/g)^3 ",0
"I think there's a common misconception that needs to be addressed here- something that makes a lot of people believe that vaccines can cause mutations in viruses. Since antibiotics can cause resistant bacteria to evolve over time, it's easy to think that something similar can occur with viruses and vaccines. However, this is a fallacy. Unlike antibiotics, vaccines don't create selective pressure for resistant strains of a virus. At least no more-so than naturally acquired immunity does. 

This requires some explanation. Bacteria are living organisms that reproduce on their own. Bacteria that can cause infection in humans can also exist and grow in any suitable environment. Antibiotics are chemicals which can kill certain species of bacteria but which are not harmful to human cells. As enough bacteria are exposed to an antibiotic, occasionally one might have a mutation which gives them a resistance to it, and this resistance allows that bacterium to outcompete their sisters which do not have that gene, and eventually become dominant, thus making an antibiotic less useful over time. 

On the other hand, viruses are not living cells. They cannot reproduce on their own. Instead, they reproduce by attaching themselves to another cell and injecting genetic material into it. This material hijacks the cell's protein and RNA or DNA making machinery and turns it into a ""virus factory"", and preventing it from doing its normal job. The cell then releases the viruses into the host's body and then viruses can infect other cells. In the human body, your immune system identifies infected cells and kills them. It also creates antibodies which can bind to virus particles and destroy them. But it takes time for your immune system to ""learn"" how to make the proper antibodies for a given strain of virus. During this time, many cells become infected, creating more viruses and damaging tissue. And as viruses are created, occasionally your cell's machinery leaves a transcription error, or ""mutation"", which can change the way the virus attacks the body. Usually the mutations are irrelevant or cause the virus to be unable to infect a cell. However, very rarely a mutation can cause a virus to be able to do something very different than previously possible- like infect new types of cells or even jump species. Or, in some cases, to evade antibodies which were effective against prior strains of the virus. 

A vaccine gives your body a chance to recognize proteins in a certain virus and make antibodies without actually infecting you with the virus. This way, if you actually are exposed to the virus, you will fight it off without it having as many chances to reproduce. Fewer reproduction events means fewer chances to create a mutation which will evade the vaccine. Vaccine derived immunity is very similar to ""natural"" immunity. It's not doing anything to the viruses that your immune system wouldn't have done anyway, but gives it fewer chances to mutate.

Lastly, I want to highlight the fact that vaccines kill viruses in the exact same way as your immune system already does, so there's nothing special for them to develop resistance to versus natural immunity. Antibiotics are a completely separate mechanism. You can kill a petri dish full of streptococcus with some penicillin, and the bacteria can also evolve resistance in said petri dish. If you take a vaccine and mix it with a vial of virus particles, it will have no effect on it. In fact, some types of vaccines are designed to PRESERVE virus particles so that they can be put in your body without being destroyed.

Edit: Please don't treat this post as authoritative in any way. I am not a virologist, and this explanation is based on mostly general knowledge, and may have errors. This comment was inspired by a now deleted comment that suggested that the existence of vaccine-derived variants is propaganda and misinformation. I was trying to point out a logical fallacy explaining why antibiotics are not analogous to vaccines at all. I didn't expect to get so much attention, and some of the responses correctly pointed out that vaccines actually can and do create selective pressure on viruses in certain circumstances. However, for various reasons, from a public health perspective, it's better for everyone to get vaccinated while it's better to limit antibiotic usage as much as possible. There has been a lot of great discussion generated from this post, including from actual virologists who you should all take with more confidence than what I've said.",0
"It is important we don't anthropomorphise too much.  Apes that have been taught to sign for example have never asked a question.  They understand the concept of a question as they will answer one.  But they have never asked a human a question.  So if they are not asking us questions, what if they don't ask themselves questions?  ""Is that going to happen to me?  What is death?  What happens?""  It seems impossible for us to imagine thought without introspection and questioning the nature of existence and life, but it is possible.

Edit: I'm going to hijack my own post since it is getting popular.  If you want to feel how a gorilla probably does when confronted with human language I can provide that experience: quantum mechanics.  In quantum cause does not always lead to effect.  Our basic logic, the basic way we think, doesn't work.  

>measurements made on photons in the present to alter events occurring in the past, this requires a non-standard view of quantum mechanics.

https://en.wikipedia.org/wiki/Delayed_choice_quantum_eraser

We are not stupid, nor are Gorillas, we might just be showing them a part of the universe in speech that they are incapable of intuitively understanding, much as we are incapable of intuitively understanding how what happens now, might impact events in the past.",0
"It’s exceptionally rare.  Metastatic disease is usually caused by seeding through the blood supply, or, invasive growth into new organs. 

Because mums and bubs don’t actually share a blood supply (their supplies run very close together in the placenta, facilitating transfer of nutrients etc, it’s very hard to seed from one to the another, and invasive growth is pretty damn hard through amniotic fluid. 

There’s a couple of dozen of “proven” cases, mostly leukaemia and melanoma (while melanoma is a solid tumour, it’s insanely invasive), and leukaemia is a liquid tumour which would facilitate the transfer from mum to bubs blood.",0
"Antibiotics kill bacteria or inhibit their growth. Antiseptics destroy vegetative cells and discourage growth, on a biological surface. Disinfectants are the same as antiseptics, except not meant for a biological surface. Sterilization removes all microbes. Sanitization only reduces the number of microbes, and typically is used on an eating or drinking utensil. 

In the end, they get used interchangeably at times. 

Antibiotics are the good/bad ones, that are great for dire circumstances but can breed resistant bacteria when they’re overused (generally if you’re not prescribed an antibiotic, don’t use it. It’s not that it’s hurting you, it’s that by using it you’re helping select for the most resistant bacteria)",0
"Have some filters from an old astrronomy class for my telescope; i recommend anyone that can get some, do.

When you can look through a telescope at it, the sun ia more than a big ball of fire, shits dynamic.",0
"I'm sorry to hear that your chemistry teacher responded that way. Your question is actually quite valid and not at all stupid. The concept of the mole is an essential part of chemistry and allows us to work with large quantities of atoms and molecules more easily. The mole is a unit used to measure the amount of a substance. It provides a convenient way to express the number of atoms, ions, or molecules in a sample.",1
"I'm a palliative doctor (admittedly not a researcher), but this is the core of how we view the cachexia syndrome. Sadly, many weight gain agents (e.g. corticosteroids, megesterol) cause fat gain but not lean mass gain. Cachexia (what you seen in cancer, COPD, heart failure, TB, etc.) is different from starvation in that you don't go the usual carbs --> fat --> protein loss pattern. It's why cancer patients look skinny in a very different way from just thin people; they're losing a lot of muscle mass.",0
"ELI5: Down syndrome is a genetic condition caused by the presence of an extra copy of chromosome 21. This extra genetic material affects the development of the body and brain, leading to certain characteristic features, including a distinct facial structure. Typically, when a baby is conceived, they inherit half of their genetic material from their mother and the other half from their father.",1
And good sex on drugs can feel like a minute but actually be an hour!,0
"We can only speculate, but it probably complements our ability to store fat well.",0
"EDIT:  i am amazed and humbled by the kind words and beautiful stories shared here.  they are treasures.  please write them down so your sons and daughters can read them.  I threw this list together really fast and posted it.  imagine my surprise when i saw my in box and almost fell over.  so i made some changes, mainly in explaining more thoroughly and in more detail to ""explain like i am five"".  i also added a few, that i had forgotten.

the list was very errr, cold and clinical and i did not convey my love for this work.  some of your responses reminded me i need to address this.  we live in a ""youth worship"" western country, often where the wise elders are forgotten, death and dying are hidden, not discussed, taboo, whereas at one time in our past, death was as normal as birth, with the working close to the land and animals, we knew and were familiar with the natural swing of life and death.  we run from death, we do everything we can to make ourselves younger, and to avoid that inevitable experience we will all have.  working for hospice allows me to become a part of an elder's life, to bring my support to them when it may be needed most.  i consider it a calling to bring more awareness of the normal reality of death.  and more awareness of the incredible sweetness of being able to support a loved one through their passage.  i cannot convey to you the blessing of being able to walk with a patient from the beginning of their journey, to the end of their life.  it is the most vulnerable, most tender of mercies to be able to have the privilege to witness that passage.  if you have the opportunity to be present for an elder during this journey, please try to.  It may change you forever in a good way.  It is the most spiritual of moments I have had in my lifetime.  You may want to even volunteer for hospice to be able to serve our wise elderly and walk with them through this time, to acompany them at a time they may be abandoned (it happens).   You will be rewarded greatly.
______________________________________


Hospice worker 26 years.

1. Withdrawing from interaction.
2. No interest in food or water (beware, pt can aspirate at this point if forced to eat or drink).
3. 02 (oxygen blood saturation level often called ""sats"" or ""sat level"") levels dropping (70 and below),  Normal is 90s.  I find using the oximeter is very helpful.
4. Color changes.. skin can go very pasty and grey
5.  Mottling of knees, elbows.  Mottling is a kind of blue/ white effect on the skin that may look a bit like bruising.
6.  Blueness of fingernail beds, slight facial blueness around nose, mouth.  Blank staring at ceiling or corner and or talking to a family member long deceased.
8.  Muscle wasting at temples and eyes sinking into boney orbits
9.  Apneic and or shallow respiration.  Near the end a patient will often start to breathe irregularly.  This is called apnea and or cheyne stokes respiration.  what you will see is the patient breathes, then pauses.  then breathes again.  this can go on for a while or it can be near the end.  as the patient gets closer to passing, the pauses will start to be longer than the breathing.  i.e. starting... 10 seconds of pause, 50 seconds of respiration.  then 20 second pause, 40 seconds of respirations (i use the second hand of my watch to count).  As the time passes, the pauses become longer and longer.  This is the place of truly near death.. as the pauses become longer and longer, the pauses will gently last until the patient gently passes.  it can be an almost seamless and very peaceful thing to watch.  they drift into death, and you wait for the next breath, and it just doesn't come.  these are the deaths one hopes for, the good deaths with family around the bed, as the patient literally just slips away.
10.  Sweaty, hot skin and or clammy cold skin.
11.  Small dove like sounds on exhalation.
12.  Inability to interact verbally.
Remember the sense of hearing lingers .. your loved one will hear you to the very very end despite showing no outward signs or inability to move or respond verbally.
13.  Cooling of extremities (hands and feet).
14.  Phlegmy sounds.  This is what is commonly called the ""death rattle"".  Pt cannot swallow their oral secretions at end of life and these may build up in the throat.  The sound you hear is the sound of the inhalation and exhalation air going over around and through those secretions in the throat.  Lay the patient on their side and the secretions can come out.  There is also a medication that can be used to help with this (drops) (can't remember name of it sorry) but the medication can take a while to take effect.  I have been told that though this is difficultf for us to hear, it is not painful for the patient.  This can be hard.  Sometimes the nurse can drain the mucousy secretions with a machine, but it is said that the more the machine is used, the more secretions are manufactured.
15.  There is a type of respiration that is often seen near the end, and it is best described as breathing ""like a fish out of water"".  Patient will often use auxillary muscles (like their shoulders and upper torso) to try to breathe.  
 
16. Pain.  
A)     Physical pain.   What I have found is that it is very very difficult to die if you are in intractable pain.  the muscles are tense and hard, the body is wracked, the pain becomes all encompassing.  the goal is to make the patient comfortable and to ease pain.  this could mean trying differrent medications to assist with pain reduction.  I cannot tell you the many times that finally, once the patient is relieved of physical pain, they can RELAX and let go. 

B)    MYTH.. we do NOT kill our patients with medications.  this is ILLEGAL.  What we try to do is find the balance of pain relievers that will allow the patient to be pain free, and also allow them to still carry on with their normal activities and at the end, be able to speak or communicate if they are able.  example:  some patients are in so much pain when they come to us, they are unable to do anything.  with the right meds, one lady was able to take her crafts to her regular craft fair out of state with her hubby in their RV, for the last time, to say goodbye to all her craft fair colleagues she had known and worked with for 30 years.  another man was able to go spend a last visit at the cabin he built up in the mountains.   Another patient was able to go to disneyland with her family, one last time.  Medications are a wondrous thing.  We learn what the patient would like to do and we try to find the medications that will allow them to do that.  We will often pay for a final wish like this.

C)     Emotional Pain.  Sometimes called spiritual pain.  releasing emotional pain can come with counseling with spiritual counselors and social workers (both available through hospice).   At end of life there may be issues that are unresolved with family members that have caused patient deep regret and grief.  EMOTIONAL PAIN CAN TURN INTO PHSYSICAL PAIN THAT NO PHARMACOLOGICAL MEDICATION CAN TOUCH.  Although sometimes resolution may be impossible, our counselors do their best to help patient resolve conflicts and issues that are a burden to patient and a burden to the family members.  NOTE:  these resolutions could mean the diffference between a peaceful death and a difficult death.  

There is something we can see, a definite change of pallor, expression, something hard to explain, that can tell an experienced eye that it is soon.

It is very hard to tell you exactly when it will happen but the above are some     things to watch for.
______________________________________

Many of your comments were around hearing at end of life.  Here's just one article.
https://news.northwestern.edu/stories/2015/01/family-voices-and-stories-speed-coma-recovery",0
"ELI5: Dentists recommend flossing or using interdental brushes because they help remove plaque and food particles from between your teeth and along the gumline, where a toothbrush can't reach easily. Flossing and interdental brushing are important for maintaining good oral hygiene. Mouthwash, on the other hand, can be a helpful addition to your oral care routine, but it's not a substitute for brushing and flossing.",1
"Human cells can replace themselves, this is correct. But they need a scaffold to replace themselves ON for them to be in the right place. And the nature of that scaffold is why scars stick around forever.

Let's compare our bodies to a multi-floor brick building that King Kong or Cloverfield or Godzilla or something punches a big chunk out of.

You have a couple choices to do something about that building before the weather gets in and wrecks it worse. But a feasible one of them isn't a complete tear-down and rebuild using scaffolding and heavy construction to recreate the building properly. People have got to go on living in there and there's not enough free spending money around to do it.

So you patch that hole as best you can and maybe brick up the opening, and that's good enough for people to keep living in it. But it leaves a not-very-pretty gap in your building. It's functional even if some of the electrical stuff or elevators don't work due to the still missing area, and it looks ugly because you couldn't quite get everything perfect without bringing in super-expensive heavy machinery and shutting everything down, and the bricks don't match. So you're left with a serviceable building with ugly spots that you can't ever afford to make perfect-looking again.

Scarring's the same. The body doesn't have the ability to regenerate huge missing areas because it can't create scaffolding once you're out of the womb. All of the 'heavy equipment' necessary for it is no longer available. This wasn't critical enough of a skill for us to evolve as a species because enough of us survived and had kids even without it to take over the world. So the body goes with a ""walling off"" strategy without coming with a bunch of perfectly set-up scaffolding to build new clean supporting structures for the new cells to grow back into their perfect original shape. 

And those wall-offs are dead 'hard' tissue that is permanently set into their walled-off shape and can't be replaced. Again, perfect-looking repairs weren't necessary to the survival of our species so we didn't evolve them.

",0
"Precisely.

Eat normally (""balanced diet"", occasional treat, not all junk food, etc. etc.) and every nutrient is in your food enough for you to live perfectly healthily.  The fact that you need only mg of vitamins tells you this - it's not used much and is in your blood, but you're consuming kgs of food every week so it's comparatively miniscule.  It's ""necessary"" but it's present in your body through incidental means anyway (your food isn't scientifically sterile and only has vitamin X in it) and your body has stores because it \*is\* vital.  Just don't let those stores deplete, but that can take weeks or months.

It really comes from ""recommended daily allowance"" which is a simplified stat to try to be indicative, not definitive, for an average over a long period, not a particular day.  You can't say ""50kg of fat per year"", because people will read it to mean they can eat nothing then have 50kg of far over one week and kill themselves.  But similarly, you can't say ""5mg of Vitamin X per day"", because it's a running average and you can go days / weeks without any at all.  The latter is safer to state, and it's easier to know what you eat per day, than per year.

Eat vaguely normally, avoid malnutrition (sorry, but a vitamin deficiency is basically indicative of malnutrition or a serious medical issue - taking supplements is like hiring an exercise bot to do your push ups for you rather than just doing some push ups), and you'll live as long as anyone else will.

And, no, taking more of them won't make you ""healthier"" any more than breathing more oxygen will.  And, yes, taking more of them than recommended can actually have detrimental effects in some instances.

Eat, drink, don't be an idiot.  You'll live into your 90's and beyond, barring anything else getting in the way.",0
"Yeah man, I think it's been long enough, we could all use a Unidan 2.0 situation. I wanna start seeing cheese fact guy pop up in all the default subs with interesting and whimsical cheese facts. After a while, he'll ascend to near-pop-star levels of reddit fame, then we'll meet him on Ellen, Jimmy Fallon, and Eric Andre (in that order).  He'll be a bit reserved, yet witty, and with the fire of passion in his eyes when he starts rattling off cheese trivia for an adoring fanbase.

Months will pass, the cheese fact guy memes will make their rounds, and he'll become a permanent fixture in every frontpage thread and we'll all love him for it. But then... One day... A thread gains traction on /r/quityourbullshit. A mod from /r/terraria claims cheese guy was banned for curating multiple accounts he uses to upvote himself and reply to himself. But no! No, we cry! It can't be true! But it is. The mod posts screenshots of DMs, another mod from /r/f7u12  comes forward and admits to bribery via lavish cheese gift basket.

For about 18 hours, Reddit is in shambles. Why, cheese guy? You didn't need to manufacture love; giving a shit about you was the easiest upvote we could bestow in any thread. Some laugh, some cry, some rage, but most people just upvote the torrent of relevant memes.  After which 98% of us move on with our fucking lives and continue to use this site as an outlet for jokes, light discussion, and faux outrage. A few weirdos continue their pointless crusade against cheese guy while the rest of us struggle to remember what the drama even was before shrugging and scrolling to the next meme. 

Before disappearing forever, cheese guy posts a shameful and short 2 sentence apology that is buried and forgotten under the deluge of pandas clumsily falling out of trees.

But in the end, we don't care. We miss our champion of cheese. We just want to exhale through our noses slightly as he fumbles his way through a Joe Rogan interview, as an obvious lackadaisical redditor. We just want to open a dumb thread about a cat getting tangled in some yarn and upvote a vaguely comical comment about certain extra-sharp cheeses triggering Scottish Folds to sneeze repeatedly.  We just want some random guy to take us by the hand and lead us all on a slightly different Reddit adventure today, but they won't. Lest they become the next Unidan.",0
"Brain: Hey body you awake?

Body: ....

Brain: Body?

Body: ...

Brain: OH SHIT DAWG YOU'RE FALLING!!!

Body: ...fSADFASDJFKASDF.....?????

Brain: Okay cool you're not asleep yet.",0
"When a baby is inside the womb, they receive oxygen from their mother through the placenta. They don't need to breathe air because their lungs are filled with fluid. However, during labor and delivery, various changes occur that prepare the baby's lungs for breathing air. As the baby passes through the birth canal, the pressure squeezes the fluid out of their lungs.",1
"ELI5: The temperature of 200C/400F is often used as a standard cooking temperature because it provides a good balance between cooking food thoroughly and preventing it from burning. When you cook food, you want to heat it enough to kill any harmful bacteria, cook it through so it's safe to eat, and develop desirable flavors and textures. However, if the temperature is too high, the food may cook too quickly on the outside while remaining undercooked on the inside.",1
"Good answer except that this: 

>P.S: Jet engines work entirely differently, even if they do have fans at the front they're for compressing air into the engine, not generating thrust.  

Isn't true for most turbofans anymore. Big, high bypass engines get more thrust from the fan (which is sortof like a prop in a tube) than the core engine.",0
"Yes! In fact, the ability to multi-crop rice is largely why China was able to support hundreds of millions in population centuries before Europe.

EDIT:  Some have asked for more info on population.

In the 1600s, Ming China had an estimated population of [~200 million](https://www.popline.org/node/331311), give or take a few tens of millions. The combined population of Europe peaked at [~80 million](https://www.enotes.com/homework-help/what-was-population-europe-1600s-what-significant-468026) before the Thirty  Years War swiftly brought that number down.

By 1800 Europe's combined population reached ~[150 million](http://www.thuto.org/ubh/ub/h202/wpop1.htm), while In China reached ~[300 million.](http://afe.easia.columbia.edu/timelines/china_modern_timeline.htm)

TL; DR Rice is fucking ***awesome***.

https://ricepedia.org/china

https://www.popline.org/node/331311

https://www.enotes.com/homework-help/what-was-population-europe-1600s-what-significant-468026

https://afe.easia.columbia.edu/timelines/china_modern_timeline.htm

https://www.thuto.org/ubh/ub/h202/wpop1.htm",0
"Thanks for your expert reply /u/poopsinurinals! But seriously, if you can't control your internal sphincters at all, then why can you sometimes pee on command like if you have to give a urine sample?",0
"Quite a few birds only eat fish small enough to swallow whole. They do take care to swallow the fish head-first, as fins and other projections can be very pokey going the other way.

Some species of bear that feed on salmon only eat the skin, which is loaded with fat. In more spare times they may also eat the flesh, which isn't as fatty.

That said, there are probably plenty of instances of fish-eating animals seriously injuring themselves, as JerseyWiseguy suggests.",0
"**\[EDIT2\] This got a little more complicated the more I did research. So for a 5 year old:**

The space station doesn't run out of air because NASA got really really good at recycling the things they already have on the station. They have some magic machines up there that can turn water into air. So then where do they get the water, you ask? From the astronauts pee, from their showers, from their sweat, from the humidity in the air. They turn that gross, weird water into drinkable / air-making-able water. BUT eventually they'll run out. SO they send up a fun little gas called hydrogen (which is a thing that's in water, but way way lighter than water and therefore cheaper and easier to send up) and they combine that with the stuff the astronauts breathe out (CO2) using yet another magic machine that makes water.

**Full explanation:**

Electrolysis of water (H2O) is the main method to generate oxygen aboard the ISS. Water is split into oxygen (O2) and hydrogen (H2). The oxygen is vented into the breathable cabin air system, known as the Oxygen Generation System, while the explosive hydrogen is vented externally. \[EDIT1\] Check the edit below, it seems like they don't vent the hydrogen anymore and instead reuse it to make more water.

The station’s football-field-sized solar arrays are the power source to electrolyse the water. Each day the OGS continuously provides between 2.3 and 9kg (5 to 20lbs) of oxygen. The OGS is a component of the ISS life support system, known as ECLSS or Environmental Control and Life Support System, located in the US Destiny module. The Elektron system aboard the Russian Zvezda service module performs the same vital electrolysis service for the ISS crew. The Electron system was also used aboard the Russian Mir Space Station.

Pressurized oxygen storage tanks replenished by visiting unmanned cargo ships provide a backup to the electrolysis method. Finally, the crew can also generate oxygen chemically by igniting Solid Fuel Oxygen Generation (SFOG) canisters comprised of lithium perchlorate. Each canister provides the oxygen needed to support one crew member for one day.

From: [https://www.spaceanswers.com/space-exploration/do-they-make-oxygen-to-breathe-on-the-iss/](https://www.spaceanswers.com/space-exploration/do-they-make-oxygen-to-breathe-on-the-iss/)

As for where the water comes from, after reading an article on NASA.org, it's mostly recycled. They have systems that efficiently gather all the water. This includes moisture in the air, sweat, urine, and shower water that was unused. ~~There's also apparently another Russian space station that has a large stock pile.~~ They don't usually ship more water from earth because it's so expensive but they can and do sometimes.

The NASA article: [https://science.nasa.gov/science-news/science-at-nasa/2000/ast02nov\_1](https://science.nasa.gov/science-news/science-at-nasa/2000/ast02nov_1)

**\[EDIT1\]** I think the idea is the recycling is hyper optimal and also the water that is recycled is all drinkable. I just read some other article (which I closed out of) which mentioned the water tastes like bottled water, although it came from urine.

Also /u/ubik2 just posted a good explanation of how the system kind of feeds into itself and how they can get around needing to ship up heavy water to the space station.

>Since 2010, they’ve also been using the Sabatier reaction. That consumes the hydrogen from electrolysis and the CO2 from breathing, producing methane and water (which can then go back into electrolysis). This means you don’t have to keep shipping up a bunch of water. Unfortunately, that reaction needs more H2 than it frees (to make the methane), so we still ship that up. Fortunately, hydrogen is very light.

As for ""where does the nitrogen come from"" which is necessary for the air, it seems like they ship that up there (couldn't find any magic processes that produce it).

>One system is to have oxygen delivered from [Earth](https://science.howstuffworks.com/environmental/earth/geophysics/earth.htm) via spacecraft. This oxygen is stored in external tanks; similarly, these spacecraft deliver [nitrogen](https://science.howstuffworks.com/chemistry-channel.htm) gas, which makes the ISS air supply.

From [https://science.howstuffworks.com/international-space-station2.htm](https://science.howstuffworks.com/international-space-station2.htm)

BUT it seems like you don't actually need that for breathing. Super interesting!

>Nitrogen, a gas that makes up 78 percent of breathable air on Earth, is inert and can therefore be safely stored onboard spacecraft. Despite its high concentration within ambient air, nitrogen serves no particular physiological benefit to humans and only serves to keep the Space Station pressure at 1 atmosphere (14.7 psia). Prior to extra vehicular activities (EVA or space walking), astronauts purge nitrogen from their blood supply to prevent decompression sickness (“the bends”). It is neither important nor practical to reclaim this nitrogen.

From: [https://www.nasa.gov/pdf/146558main\_RecyclingEDA%28final%29%204\_10\_06.pdf](https://www.nasa.gov/pdf/146558main_RecyclingEDA%28final%29%204_10_06.pdf)

**\[EDIT3\]** From /u/acorz on how often they fly out the resupplies (unchecked source but they mention that work at the [Johnson Space Center](https://www.nasa.gov/centers/johnson/home/index.html) aka NASA).

>We fly N2 and O2 tanks 2 or 3 times a year. We fly most of our water once a year on [HTV](https://en.wikipedia.org/wiki/H-II_Transfer_Vehicle). In 4 years, I've only seen small hydrogen tanks for a science experiments, not resupply.

**\[EDIT4\]** /u/sharfpang has a really [awesome comment](https://www.reddit.com/r/explainlikeimfive/comments/b2505l/eli5_how_does_the_iss_never_run_out_of_fresh_air/eiso5j9/?context=3) explaining how the Sabatier process isn't actually *currently* used and that they do actually ship the water up. I'm actually not totally sure this is true but they sounded confident (and [this article](https://www.nasa.gov/home/hqnews/2010/oct/HQ_10-275_Sabatier.html) suggests Sabatier is used). BUT their comment goes into detail with the potential problems that come out of the Sabatier process and some other cool science stuff, so it's worth checking out.

**\[EDIT5\]** Just a funny tidbit in this research is the Russian side of the ISS doesn't actually use their urine in the recycling process, but the US side does. I feel like I read it in a more credible source, but here's just a [random article](https://www.theguardian.com/science/2015/aug/26/us-astronauts-recycled-urine-international-space-station) I Googled that says the same thing.

**\[EDIT6\]** Alright back to business. So the plot thickens, /u/sharfpang sourced an [exchange thread](https://space.stackexchange.com/questions/30269/does-iss-get-a-surplus-of-oxygen-or-water#comment90086_30269) comment that makes it sound like Sabatier was a ""success"" which was actually a failure they silently decommissioned.",0
"Neuroscience major here. Covered this in my physiology class last semester in uni. /u/PavlovsHumans is partially correct. Theres a 3-point model:

1. Younger children have a lack of vasopressin release
2. Younger children tend to have poor bladder control, either due to low capacity (they tend to hold in pee as long as they can) or have overactive bladders (uninhibited bladder contractions)
3. Children have a developing central nervous system and spinal cord, meaning they may not have the full capacity to interpret and respond to a full bladder as well as an adult

Interestingly, children who are born premature or have low birth weights have higher rates of bed-wetting for longer periods of time. This again is probably due to the developing central nervous system.

&#x200B;

\*\*\* Edit: It looks like PavlovsHumans answer has been drowned out by the other comments. Vasopressin is a hormone that makes you produce less pee (anti-diuretic). Sorry if that was confusing!",0
"Love people who know their shit talking about said shit on Reddit. 

Thanks for the read!",0
"Cow or horse manure should last a LONG time. If it breaks down it would take years and years, longer than you'd ever likely have a pile of shit hanging around. :)

It will just keep composting until it's inert and then it's just soil.",0
This guy knows the shit out of wood.,0
"In order for venom to be dangerous, it must enter the bloodstream.  Many venomous toxins specifically target red blood cells themselves and break down those cells which results in organs being unable to receive oxygen.  This leads to inflammation and necrosis of the cell tissues.  Organ failure then leads to death.  (Not all venom works this way.  Sometimes venom attacks the nervous system instead.)

When consuming anything, animals have several defense mechanisms in the digestive system.  The first defense is the mouth itself.  If the lips start burning, the animal might not swallow.  Then, the taste buds help the predator know whether this should be swallowed if it tastes bad.  Next, the saliva in the mouth and chewing action helps break down molecules and foods into smaller pieces.  Saliva is also full of white blood cells (leukocytes) that attack harmful items (such as bacteria) that might enter your mouth.

After the mouth, the item enters the stomach.  The acids, heat, and enzymes in the stomach are capable of denaturing proteins.  For proper protein function, the proteins must consist of specific shapes.  By denaturing potentially harmful proteins, they are rendered harmless in many cases.  They are also broken apart.  [Gastric acid](https://en.wikipedia.org/wiki/Gastric_acid) in the stomach is normally between 1.5 and 3.5 PH.  (Stomach acid can be incredibly acidic!)  So, if it reaches this point, the venom is most likely going to be broken down in the stomach.

However, if the venom happens to make it to the digestive tract intact, then it could potentially leech into your system.  But, if it is possible for such a substance to reach this point, it is not actually a venom.  It is a poison!

To explain further, for an animal to be venomous, it must inject its venom by way of a sting or bite.  For an animal to be poisonous, however, it does not inject the toxins in the same manner.  Instead, you are harmed by swallowing the poison.  So, if a snake was commonly eaten and it resulted in people dying, the snake would be considered poisonous not venomous for this reason.  If this same snake could also harm people by biting them and injecting venom, the snake would then be poisonous and venomous!  Most snakes are simply venomous.  That is, consuming their venom is usually harmless.  It is not recommended that you do so, though, since it could potentially be toxic if it passes all of the digestive defenses, and no matter what you consume, if you consume enough of it, it will kill you.  The dose makes the poison.

""All things are poison, and nothing is without poison, the dosage alone makes it so a thing is not a poison."" -- Paracelsus

Edit: Clarification that saliva itself does not break down the proteins.  That starts in the stomach.

Note: Bloodstream is a simplified term and circulatory system would probably be a better term to use.  Venom initially enters a different system designed to transport white blood cells and remove toxins from the body (lymphatic system).  For some venom, the molecules are too large to enter the bloodstream itself.  Comments from /u/KingKongBrandy and /u/Maj3sticCr0w led me to make this note.

",0
"ELI5: The recent discovery of a new organ called the ""interstitium"" might seem surprising, but it's important to understand that it's not a completely new structure that scientists just stumbled upon. The interstitium was actually hiding in plain sight, and its true nature was not fully understood until now. The interstitium is a network of fluid-filled spaces that exists throughout our bodies.",1
"It's not, the slogan, ""Breakfast is the most important meal of the day,"" was invented in the mid 19th century by Seventh Day Adventists James Caleb Jackson and John Harvey Kellogg to sell their newly invented breakfast cereal food stuffs. Yes, *that* Kellogg. Then the bacon market jumped on that bandwagon, got 5,000 doctors to sign off on the health benefits of eating protein in the morning, and got news papers to publish the signatories as though it were scientific research.

It was all founded on bullshit. It was a weird time, where people were moving into factories and people were getting all up about indigestion. Kellogg was also weird about the morality of breakfast and thought bland food would curb masturbation. If you look into this period, it gets weirder than that. I blame all the lead and mercury in the water. That shit makes you dumb and *fucking crazy*.",0
"Awesome explanation.
My experience:
My buddy invited me up to his cabin for which I thought would be fly fishing and beer. I arrived and he handed me a shovel. I looked confused as hell. He told me we are building a well. Middle of nowhere in the mountains in Colorado. We dug this fucking well for hours, he put a tube down, and then did the rest of the world. Bam! We had a well and drinking water.
The rest of the trip we drank beer and fly fished haha",0
"Personally I'd argue this is a 'reclassification' rather than a 'discovery'. We've known about the interstitium for a long time. The idea that it should be viewed as an organ it its own right (rather than as part of the organs its found within, or as an extension of the lymphatic system) doesn't make it a new discovery... just a new way of thinking about things. 

The media storm sounds very similar to earlier this year when they said we'd 'discovered' the mesentery (basically a tissue structure that attaches to your gut)... again this wasn't the discovery of a new structure... rather a proposal to think of the structure as an 'organ' rather than an extension of the gut. ",0
"When machines start up, they goes through a routine to put everything in the right place.

It’s like when you wake up for school, brush your hair, put on clean clothes, and eat breakfast. You are ready for anything.

Well some days are very bad. Maybe you fell in a puddle, ruined your clothes, got your lunch wet. You never could have prepared for this and now your day is ruined.

Nothing you do will salvage this day.  You can’t get the mud out of your clothes. You can’t eat that lunch now.  Sometimes the best thing is to just call it, go home, and start over again fresh tomorrow.",0
"We don't conclusively know.

We do have a few indicators.

Cartilage usually attaches to bone or connects in such a way that leaves marks.

Beyond this we can look at their closest relatives.

Dinosaurs were the ancestors to birds, which have no ears.

Dinosaurs were cousins to lizards and other large reptiles, who again have no ears and kind of suck for hearing.

While they may have had ears in the sense of audio sensing organs, they almost certainly did not have ears as we recognize on mammals. 

Edit- Officially my highest rated comment ever",0
"When you boil water, it reaches a high temperature that is effective in killing most bacteria and viruses. However, boiling a sponge may not effectively sanitize it for a few reasons:

1. Porous Material: Sponges are made of porous materials, which means they have tiny holes and spaces that can trap bacteria deep inside. Boiling water may not be able to reach and kill all the bacteria hiding within these pores. 2.",1
"One's face is exposed at all times.  Skin on your face is incredibly tough.  It heals very quickly.  It has high blood flow.  It's skin turn-over rate is very fast.

You could drag your face in gravel and it will heal up in a couple days.  A skinned knee takes well over a week to heal.

It's just how we're built.  When you use a fractional resurfacing laser, you run much higher energies on a face than you would use on a chest treatment for example.  Chests are thin and weak.

Story related: My brother broke my nose and took all the skin off the right side of my nose and part of my cheek with a stick when we were scrapping around.  Huge scab.  I looked like a monster. He hit me on a Thursday, by Sunday the scab came off and underneath was fresh skin.  Acne is a tradeoff for the durability.

I'm glad I wear glasses.  I could have lost an eye if I didn't. To be fair, my weapon was a shovel handle so we were evenly matched.  His stick broke when I blocked it and the end piece is what hit me.  He didn't mean to hit me in the face.  In the end it was funny.  There was so much blood that he ran away thinking that I was going to kill him.  I had to yell at him, ""I'm not going to hurt you, I need a rag to stop the blood.""  It worked out fine.

TLDR; Faces have high blood flow, heal fast, and are super tough.",0
"This was an album that brought a very different and original sonic landscape to people who were NOT used to it. Imagine waiting for months for the next Beatles album and listening to THIS. Just imagine waiting and lusting for the follow-up to Revolver with its black and white artwork and getting this colorful sleeve work that features the Beatles as you had never seen them before: long hair, moustaches, in those weird military band uniforms.   


And that's even *before* you put the stylus over the record...  


Flanger, echo, stereo imaging, distorted guitars, orchestra-driven tracks, tambouras and tablas, the whole this-is-not-the-Beatles concept, even the colorful gatefold sleeve with its who's-that trivia.  


Try to get a hold of a list of the singles and albums that Sgt Pepper was competing against in the famous Summer of Love and you'll understand what kind of departure it was.  


Jimi Hendrix and Beach Boys were giving the Beatles a run for their money, but this album was a huge step forward.  


Now, check the kind and size of influence this album had in the world by checking the kind of songs, artwork, fashion, words (slang even...""turn you on..."") that came AFTER Pepper.  


One of the things that will stick in my mind FOREVER is the use of the word ""clutching"", in She's Leaving Home. Have you heard such an usual word in a song ever again?  


For me, personaly, the very first bars of A Day in the Life are hauntingly beautiful. Lennon's voice is just... different. He has such a eerie delivery never again heard or matched (by himself, I mean).  


If you play guitar, for instance (although bass, drums, piano, or singing certainly apply) and try to learn and play these songs, you will even find yet another layer of complexity and appreciation.  


Sometimes you need to tune your strings higher just to be able to match some solos, not to mention you will have a blast (and a hard time) trying to match the sounds you hear with the help of ready-to-go effects pedals, apps, etc, and it's then when you stop taking this music for granted and you start to understand the vital role that people like George Martin, Geoff Emerick (try to read about his recording techniques and his microphone positioning, [Send tape echo echo delay](https://en.wikipedia.org/wiki/Send_tape_echo_echo_delay)) and the engineers at EMI played in the Beatles' sonic development. Listen to the guitar sounds of the previous albums and compare them to these.  


The harmony work bestowed upon She's Leaving Home is beautiful, but of course you cannot appreciate it with just one listen. Find the main vocal, then try to follow John's harmonies and then George's.  


The cinematic lyrics of Lucy in the Sky with Diamonds leave nothing to chance. You are there, watching the newspaper taxies, no matter which taxis you're familiar with.  


The boldness of including a track comprised of indian instruments right in the middle of this so-called pop album.  


As you can see, I could go on and on. Hopefully, I have already transmitted you a fraction of what this record means to me.",0
"Specifically how is likely a state secret, as it's bad policy to show exactly how your security works. However, there are presumptions that can be made.  

Faraday cages and other high-science interference techniques can prevent all but particular frequencies working, and someone can listen for activity on those frequencies.  If someone's sending data from the Oval, they can check to see if Trump is on his phone or not to deduce if there's a rogue sender in the room. 

If we assume all data via the White House wifi and cell signals is being heavily tracked and monitored, that means getting any recording data out has to be done via physical means like a spy pen with a Micro SD card. Logically, that means someone of with motive to record has to have access to plant the device, and access again to pick it up.  All without looking suspicious, all while secret service and people who are genuinely good at their jobs are watching you the entire time.  

Further complicating this are the extremely professional and experienced White House maid staff that clean the most presidential bits you're probably wanting to record each day. They reset the rooms, dust everything, and make sure each item is in pristine condition.  It's easy to presume that they know what each item looks and feels like and are trained enough to spot a new listening device or hole in the couch.  I am willing to bet nothing stays in the Oval's couch cushions for very long so good luck getting your spy pen back.  

I'm not saying it's impossible, but to successfully record the president, and extract the information in a timely enough manner to actually use it is really tough.  Especially when the risks far outweigh the rewards.  

Sure you may get some blackmail, or good intel, but if it comes out that Sweden's housewarming gift to Trump of Ikea's latest soap holder was secretly a recording device sending out presidential nudes, that's cause for war and dissolution of several treaties.  Better to just tweet him publicly or submit an official request to the state department and see what happens.   

The real people you want to bug are those feeding the President information.  They know everything before he does, and getting a heads up that Trump is about to find out you are the leak allows you to get ahead of the issue and skip town or spin the narrative before that report hits his twitter feed. ",0
"I work in film and have a VFX degree and here's how it goes:

1. About half the money, give or take, is for above the line talent. So you have your actors, directors, producers, ect. They get paid in a percentage or in absurdly high amounts for films. These people are also accommodate on set so production has to rent out luxury campers to house them for weeks or months at a time when on location. Then they need to hire drivers and trucks to move those campers. Top tier stars can make demands on top of that. I saw Jim Carrey's camper once and it had an entire astroturf lawn on top of it, with a picnic table, with a vase with flowers on it. Don't ask me why he wanted it, he just did. Those costs are in addition to percentages given to the talent directly, which can be millions each for an A list celebrity. If this is a movie like Infinity War you have multiple guys like RDJ and Cumberbach and like four guy named Chris who could carry a blockbuster on their own and want to be paid like it.
2. Actors who aren't the main cast still have to show up and get paid. Every random dude you see in the background is an actor who's in it to get paid. If you see a big crowd shot of like 500 people that means that's 500 people who had to show up, go through makeup and costumes, and be accommodated and then be paid.
3. What you have left over has to pay for production. At minimum it costs like thirty thousand dollars a day just to hire people to actually operate the cameras and set up lights and they usually work 12 hour days and have unions that demand good rates including overtime. This is a very basic cost for a minimum crew for a single day where you get maybe a few minutes of footage done. If you have those big 500 background days you need people to get people to manage those people. If you have complicated shots you need more people for that.
4. If you're out on location you need to pay the people who own that property. This can cost millions in and of itself if you need time and they know you have money. You also need to pay an entire team of people to show up and get the location ready, which means emptying out whatever furniture is there and replacing it with your own stuff you have to buy. These people are probably also working heavy overtime and have a union demanding pay accordingly. If you decide that isn't worth it then you need to get a studio and build the entire fake set from scratch, or pay a company to recreate it with CG, which isn't cheap either way.
5. This doesn't count the cost for pre and post production, which is two thirds of the process. You have writers, editors, storyboarders, previz, color grading, foley, and a dozen other departments that have to do work before or after the actual shoot. CG comes here in various phases and obviously isn't cheap. On a Marvel movie if you sit through all of the credits you'll usually see like 8 other companies contracted out to do this and that and if you actually follow through and look up those companies they have big impressive shot breakdowns of what they did and a crew of a hundred plus people who may or may not also be credited.

If you sit through the whole credits of a Marvel movie you probably have thousands of individual names and there are probably three digits worth of people who didn't even make that list. Those guys don't work for free. This shit ain't student film.",0
"Ladies and gentleman.. my soon to be ex wife

Update: wow this blew up!  The last half of my marriage my wife said I “was listening but never really heard her.” I went to marriage counseling 80% of the time by myself because “it wasnt her fault.” 
Then I got an anonymous message with photos of her naked with another man in her office ( sent by a coworker who was in our wedding) 
He was married with kids and it lasted about a year.",0
"What's crazy is that the Ancients found a way like this to infer from a color this complex system of air pressure and its relationship to weather, and they probably just thought the Sky Celestials were battling Nether Ghosts or some shit",0
"I remember as a kid being aware I was being carried from the car to my bed. I still stayed limp like a dummy though, didn't want to indicate I was awake or I'd be forced to actually use my legs like some kind of a caveman.

My laziness clearly did not originate in adulthood.",0
"Wow same bin? That seems inefficient

Edit: my eyes have been opened omg",0
"So proof is actually a really inaccurate measurement.

Edit: Whoa. I guess ""innacurate"" was the wrong word. It just seems like proof is more of an approximation as apposed to ABV which will tell you exactly how much alcohol you're consuming. As some one else pointed out, the concept of ""proof"" has been more finely tuned so that it's more accurate now.

And yes, I agree that we Americans are stupid when it comes to measurements. Sorry to the metric master race.",0
"Adding onto everything else mentioned

Your breathing isn't really as steady as it normally is when you're crying and it's a bit harder to control (especially a really hard cry). So when you suddenly suck in air while trying to talk you mess yourself up",0
"Naw, thermos have a vacuum layer in between. Vacuum is a hell of an insulator so that’s why they keep temps for so long.",0
"Me *climbing stairs*

My body: “Oh FUCK he’s doing something. We aren’t used to doing something! Ok NOBODY PANIC. SHIT!”",0
"One word: Telomerase. It's an enzyme that adds redundant, non-coding junk to the end of a strand of DNA. 

Whenever you replicate DNA, you can't actually replicate the last few bases because of the way the enzymes and the strands of nucleic acid are physically shaped. Basically, you have what looks like a scaffolding of many enzymes built up around the DNA strand moving from one end to the other. Easy right? However, the structure itself depends on *something* being present, so when you get to the end of a DNA strand, it's like having a ladder set up where one leg is on solid ground and the other leg is just dangling off into space. The enzymes, like any sane, OSHA-compliant carpenter say ""fuck it"" and leave the last 0.001% of the DNA replicate strand unfinished. This only becomes a problem after many decades of life for humans. But humans *REPRODUCE* fully new humans right? So how can this be possible if we constantly lose tiny fractions of our genome as we age and reach reproductive age?

Enter telomerase. The enzyme will add on a series of junk bases called ""telomeres"" to the end of a DNA strand so when they get truncated off, nobody cares and the organism is unaffected

Telomerases are inactivated in pretty much all non-cancer cells in an adult human or any other vertebrate and a large number of invertebrates. Bone marrow and other tissues with a very high turnover of cells might be exceptions to this general rule but I honestly don't remember

Telomerase making some animals immortal is also what makes many forms of cancer so goddamn deadly: unstoppable growth

So why do very primitive organisms have telomerase active at all times and theoretically live forever? They probably simply haven't had any selective pressure to lose it. Humans have. Humans reproduce by age 15 but live to be 80. Do they need to live to 80? Do they need to live to be 500 years old? Absolutely not. There was no evolutionary advantage whatsoever in maintaining these tricky, resource-consuming, potentially-tumor-forming enzymes from being active 100% of the time. So, at some point, our ancestors stopped expressing them except under relatively narrow circumstances. This probably happened more than 500,000,000 years ago, but I can't give an exact date.",0
"Took me a while to realize because I was sick at the time and I just thought that my ears were still un-popped and dull because of congestion. I could still hear, but it was a bit like wearing crappy ear plugs or over ear headphones with no music playing. 

When all of the other symptoms were gone and they still hadn't popped back to normal a few days later, I went to a walk-in to get an otoscope exam. The doc took one look and then grabbed some kinda squirting device and flushed a hefty wad outta both ears. Suddenly I could hear in colour again! It was a very good day.",0
"The math predicts their existence in almost exactly the same way that math predicts you can push on a rope (which, famously, you can't).  A perfectly straight rope can hold tension (of course) and also, if it happens to be *perfectly straight* the same spring equation that works to predict the rope's behavior under tension also predicts the rope's behavior under compression.  If it were perfectly straight you could totally push on the ends.

The reason we have *tug* of war contests but not *push* of war, is that rope is unstable under pushing.  It simply won't hold the perfect shape to support the push, it'll bend to the side instead.

Spacetime has a similar type of effect.  The equation that describes spacetime has a bunch of solutions about gravity pulling stuff in.  If the gravity is strong enough, you get a ""black hole"" that sucks in everything that gets close enough.  

It turns out that there are other solutions about gravity pushing stuff out.  Those are called ""white hole"" solutions.  A while ago someone noticed that you could connect a ""white hole"" to a ""black hole"" and get a ""wormhole"" that would suck stuff in on one side and push stuff out on the other side, and it would all hang together nicely.  And also satisfy the math.

The problem with wormholes is the problem with white holes.  They can't exist, for the same reason you can't push on a rope.  If you somehow created a white hole, it would tend to break itself apart.  If you created a wormhole, it would pinch off into a separate black hole and white hole -- and the white hole would break itself apart.  

Only black holes (which suck everything in) turn out to be stable, even though the equations can have ""perfect"" solutions that start with exactly the right shape.  Just like a rope -- if a rope is perfectly straight and perfectly made, you can push on it.  But real ropes aren't perfectly made, nor perfectly straight -- and if you push on one, it will bend out to the side instead of carrying the push.

*Edit: I didn't expect this much interest in a basic ELI5.  The pushing-on-a-rope analogy can only be stretched so far (heh).  If you want more information on this kind of thing, I suggest Kip Thorne's awesome book ""Black Holes and Time Warps: Einstein's Outrageous Legacy"".  It's a masterpiece of accessible description, and describes several reasons (not just this one) why wormholes can't exist.*",0
"When adults sleep, their brains are more developed and better able to recognize signals from the body. This includes the signal that tells us we need to go to the bathroom. So, when adults need to pee during the night, their brains wake them up so they can go to the bathroom. On the other hand, young children's brains are still developing, and they may not be able to recognize these signals as well. This is why some young children might wet the bed while they're sleeping.",1
"Imagine you have a book that contains all the instructions to create a living organism, including all its physical and behavioral characteristics. In the case of humans, this book is encoded in the DNA molecule found within our cells, including sperm cells. The calculation of a single sperm containing 37 megabytes of information is an estimate based on the amount of data stored in the DNA of a sperm cell.",1
"A ""beer belly"" is often used to describe a protruding abdomen that some people develop after consuming excessive amounts of alcohol over a long period of time. However, it's important to note that a beer belly is not solely caused by beer consumption, but rather by an overall excess of calories. When you consume alcohol, your body prioritizes metabolizing it over other nutrients like carbohydrates and fats. This means that alcohol is processed first, while the other nutrients are stored as fat.",1
"TL;DR It's a fucking miracle anyone is ever born.

Edit: Alright, I get it, it's not exactly a ""miracle"" of a process, but I took liberty of expression by exaggerating. It's a lot easier to downplay the process when looking at population growth as a whole. In the end, it's a numbers game, but just because it works for many does not make it any less harder for some to be pregnant, and to them it actually *feels* like a miracle.

While I'm editing, there's a lot of people who have been kind enough to post good resources on the process in more accurate detail than what I discuss below. I make intentional omissions in some regards for simplicity's sake and I tried to answer as much within the scope of the question as I could.

And so:

Think of the journey of a sperm cell like that one example where [fish have to climb a waterfall to mate.](https://www.sciencemag.org/news/2013/01/scienceshot-meet-amazing-waterfall-climbing-fish) In this situation, there's wet rocks, water pouring against the fish, gravity, and a huge distance to travel that work against the fish's favor, similar to how the female anatomy is inhospitable to the sperm cells in similar ways. I'm going to go into a little more detail in the next part, but it will help if you try to keep the analogy of the fish and waterfall in mind.

Let's start at the beginning of the journey, the cervix. It acts like a gate that opens and closes at different times of the month to let the sperm get in. No real analogy here. It's just like a door.

After the sperm get a ticket in, then they have to survive the environment that is usually at a pH that degrades or destroys sperm, so many die at this phase as well. This would be like if, suddenly, the waterfall water turned boiling hot and started killing the climbimg fish.

Another barrier would be the production of mucous that the body creates and expels to constantly clean out the vagina, similar to cerumen in the ears, mucous of the nose, or sweat on the skin. The motion of the mucous tends towards the exit instead of going further in, just like the water and gravity work against the fish trying to climb. They can move upwards as much as they try, but will get pushed back down.

Distance was a factor mentioned by others. Sperm don't have nuclear reactors and many will run out of energy before they reach the egg. That, or they spend too long and are destroyed by the hostile environment of the female anatomy. In this context, think of the fish getting too tired to hang on and succumbing to the other barriers already listed.

Here we say the fish make it over the waterfall and live happily ever after. But it's a little different on the cellular level.

If the sperm DO make the distance, there is still the egg. Imagine this egg like you would a chicken egg; you can't just touch it with your finger and expect to get inside. But what if I coated my finger in acid? The eggs in the female reproductive tract have a similar ""hard"" shell that is resistant to penetration and resists many sperm. The sperm are equipped with lysing agents that break down the egg so that one lucky sperm can reach the center.

Then there's the matter of why more sperm can't just penetrate the same egg to fertilize it. After initial fertilization, the egg changes the chemistry of the outer shell that was initially penetrable and makes it so it is now impenetrable to further attempts to fertilize it. Imagine like you're in line for a 1-person seat on a ride; after you sit down, they close the gate so no one can follow you in, and the ride begins.",0
"When we sleep, our bodies go into a state of relaxation and rest, including our muscles. However, there are two key factors that prevent us from defecating or urinating during sleep: the control of our muscles, and our body's internal monitoring system. Firstly, our muscles play an essential role in controlling the release of waste from our bodies. The muscles in our bladder and rectum act as valves, keeping them closed and preventing any unwanted release of urine or feces.",1
"I am a merchant, this is incorrect.  I get the money the next day, 24 hours later after a batch is processed. I can say this with 100% confidence because I have had three different merchant processing accounts over the course of my business at different levels of revenue.  This is the same thing that happens to low processing merchants and high processing merchants (1,00 per month to 1 million a month)

The reason it takes so long to get a refund is that the banks are using the money to make interest.  That's it.

The refunds are removed exactly as fast for the merchant as the sale is put in, the customer is the only one who gets screwed.

That said, it does depend on the bank. I have customers who get the refund the same time I get it removed from my account and I have customers who seem to have to wait up to five days.

TIP: Stop calling up the merchant and blaming them, you're being an ass.


I mean no offense to you OP, but if you do not know, do not guess.
",0
"Now how much money will actually change hands?  Maybe the price of pork fluctuated by 3% between today and October?  That means 100 million USD will actually change hands between the traders of virtual pork in a market worth 3.5 billion USD for an underlying that is produced at a rate of 2 billion USD worth per month.

When we apply these same ideas to Currencies and Credit of all kinds, things just explode.  Currency traders don't fuck around.  They make trades for MILLIONS at a time in currency deals.  Again quite often virtual currency deals.  Or they buy in the morning and sell in the morning and that foreign currency never actually changes hands.  So what happens here is that I BUY 100 million worth of Euros from you and pay in dollars.  No wait that's old school...  We set up a contract that I will buy 100 million worth of Euro from you in a month at today's prices.  Euro goes up by 0.5% by the end of the month and either I sell the contract before expiry or again commonly we just settle everything in cash without actually doing the real underlying transaction just like with the pork before.  So 0.5% profit for me because you have to hand me 500K USD because of the price fluctuation of the Euro.  However as long as that deal was open it contributed the full 100 million USD to the number in the title of your post.

So in essence that number is so huge because it counts every single possible trade that still exists as a contract somewhere to trade something credit or currency for a certain price on a given date in the future even if the trade is defined as virtual to begin with because it will be cash settled not through actual delivery, or because the traders involved have no intention whatsoever of actually doing the trade when the day comes at all, just cash in (hopefully) by closing their position when the expiry date comes along.

So yes there is currently 678 trillion bound up in future trades on credit and currency markets alone.  Derivatives are a HUGE market.",0
"> ? I feel like either the coding standards for router software or the hardware reliability specs must be way too low.

Yes to both.

Linux systems at work with years of uptime are usually quad digits at cheapest.

If you have a router that needs constant reboots, chances are that all the corners that could be cut were cut. The hardware is more susceptible to interference from environment (EM radiation, temperature, cosmic rays, etc) and the firmware (you can't just throw linux at some PCB and expect things to magically work, you have to write firmware specific to your circuit on top of that) is prolly shit-tier and full of bugs, too.",0
"Just the lighting alone can make a huge difference, some college productions can use the same cameras as a movie and still look cheap because the lighting isn't what you are used to seeing from professional grade productions.

I did some grip work when i was younger (about 10 years ago) and was also learning to be a camera man (never finished) but the level of tech and science that goes into the lighting is just mind boggling , there are like 5 - 7 different types of standard lights alone and that not even getting into all the ""specialty lighting"". Even a cheap quick indoor scene will have 3.

There is lighting just focused on the background , lighting for the side angles , lighting for foreground scene and even special lighting for peoples faces. 

They calculate lighting for height , for skin tones ,for  time of day and side angles to make sure shadows don't interfere with set pieces or blocking.  Have special spot meters to know how its going to look on film because of the frame ratio the lighting on film can look different to the naked eye sometimes. And this was just what I know from a decade ago , can't image how much its advanced in a decade.

Edit : Typos",0
"No, because your immune system is generated through your bone marrow. With a bone marrow transplant, you're essentially creating hybrid immune systems with markers for both. 

But you're right, the chances of it going tits up is still extremely high, which is why it's trial-stage only, or ""Hey... You're so fucked that if we give you these drugs, you'll die. So here's what we'll do..."" sort of cases. ",0
Why is the rum gone!?,0
Yo momma so fat she's very likely in the latter half of the third trimester and needs to take it easy. ,0
Does this interact with alcohol in any way? Makes me think of the recent surge in grapefruit flavored vodka/seltzer and whether it can change your expected BAC at any given time.,0
I have used ant beds to clean deer and cow skulls. At least here in Texas it takes a few hours for something dead to be covered in fireants.  Then it only takes a few more hours for the ants to strip it to nothing.  ,0
"Time and silence.  

For 13 years, almost all of the elite had to join the Nazi party.  In 1952, 25% of West Germans admitted to having a ""good opinion"" of Hitler.  In his first official address to the parliament, Chancellor Adenauer (in 1949) said ""The government of the Federal Republic, in the belief that many have subjectively atoned for a guilt that was not heavy, is determined where it appears acceptable to do so to put the past behind us.""  The German government was generally determined to forget.  

In 1968, Germany had its own set of internal revolutions, where the baby boom children grew up and protested against the crimes of their fathers, so to speak.  This was helped along by the fact that the actual chancellor, the third in the history of West Germany, was himself a former Nazi and a party member from 1933-1945 who served under Ribbentrop.  https://en.wikipedia.org/wiki/Kurt_Georg_Kiesinger#Early_life_and_Nazi_activities

They made it illegal to continue to be a Nazi or to support Hitler, but for the most part, if you were a Nazi and said ""sorry about all that Nazi stuff"" the German government was fine with it.",0
"Then, is it ok if I let it rest by turning it off? Let's say, I don't need WiFi for the night. Should I turn it off just like I would do with my PC?

Edit: Thanks for the insight guys. I'll keep it on and reset once a month if it were to be necessary.

Edit 2: I'm receiving a lot of mixed info @.@ I'll just keep it as it is since I don't have problems. But if I ever encounter some weird shit I'll know it's ok to restart it.",0
"In a similar vein, if you didn't store your food properly before reheating, cooking it again will kill the bacteria in your food.  It will *not* get rid of the toxins the bacteria made before you killed them, and will likely make you sick.",0
"Our mouths are actually filled with lots of bacteria, but most of them are not harmful to us. In fact, many bacteria in our mouths help to keep our mouths healthy. When we get a cut on the inside of our mouth, the saliva in our mouth helps to protect the wound. Saliva contains enzymes that can kill or inhibit the growth of certain bacteria, which helps prevent infections. The saliva also helps to keep the wound clean by washing away bacteria and other debris.",1
"From an evolutionary perspective, it's useful to me as an individual if I can prove certain emotions with hard to fake signals. Tears, for example, are a hard to fake signal that I'm in pain and need help, as opposed to simply faking injury to be exploitive.

Also, from Steven Pinker's *How the Mind Works*:

“Facial expressions are useful only if they are hard to fake. As a matter of fact, they are hard to fake. People don’t really believe that the grinning flight attendant is happy to see them. That is because a social smile is formed with a different configuration of muscles from the genuine smile of pleasure. A social smile is executed by circuits in the cerebral cortex that are under voluntary control; a smile of pleasure is executed by circuits in the limbic system and other brain systems and is involuntary. Anger, fear, and sadness, too, recruit muscles that can’t be controlled voluntarily, and the genuine expressions are hard to fake, though we can pantomime an approximation. Actors must simulate facial expressions for a living, but many cannot avoid a mannered look. Some great actors, like Laurence Olivier, are highly coordinated athletes who have doggedly learned to control every muscle. Others learn method acting, inspired by Konstantin Stanislavsky, in which actors make themselves feel an emotion by remembering or imagining a charged experience, and the expression pops on the face reflexively.”",0
"I recently had a team leader come up to me and Bitch about one of our new guys who just wasn’t getting it.  The dudes a nice enough fellow, but everything you’d tell him would go in one ear and out the other... so after a few days and a few fuck ups, I told him, “teach him why he’s doing this job.  Explain to him the importance of the procedure and why each step in the process is necessary.  If you understand why you’re going through the motions, you’re not just going through the motions anymore.  You’re working with intent.”

Edit: for those that are asking, yes it worked. Some. The guy is still not a ball of fire, but he’s doing well enough that his team leader doesn’t feel compelled to watch over him all night or fact check everything he does.",0
"We also CAN control it, but the psychological effect of the emotion overpowers it pretty quick. If you're laughing really hard, you CAN instantly straighten your facial expression, physically speaking - but you'll probably lose it again in three seconds. It's not that you can't control your muscles - the muscles in your face are actually much more fine-tuned than the rest of your body generally - it's just that the involuntary mental response keeps you from maintaining that control for a bit. ",0
"It doesn't 'decide' per-se. Fat is distributed in a rough order that varies with your sex. Males for example are more likely to store fat in their upper-body and abdomen (hence the Beer Belly phenomenon) whereas Females are more prone to store it around their hips and thighs. This is what largely contributes to the 'pear' shape we associate with women.

Males are also more likely to store 'visceral' (around the organs) fat which is what results in the skinny-fat phenomenon where people can be overweight, but not look it. Females are more likely to store fat subcutaneously (under the skin) where it's more noticeable. This combined with our bodies preference for storing fat in certain places is what contributes to the difference of appearance as a result of our weight.

Places like your face, neck and arms are lower down the list. Which is why when someone gains/loses a lot of weight this is where you notice it. It's one of the last places for fat to get stored, but when you start losing weight that means it's one of the first places to go.

When you lose weight, it comes off in the reverse order that it went on. So if you've noticed that you've recently started accumulating mass in a certain place, then with improved diet and exercise that is the exact place it'll start coming off first.

Edit: The order I've given is more of a broad trend rather than something you can usefully apply to individuals, various generic and environmental factors will affect the order from person-to-person. Your personal order may be different, but regardless of the order it's a last-on/first-off system.

Edit2: There's some back and forth in medical journals about whether there is *any* consistency to the order or if it's purely an individual/genetic thing. I'm not qualified to really judge that and deep dive into them so please take that particular claim with a few pinches of salt. But while the notion of a consistent order seems contentious, there is nevertheless an order for you.

TL;DR - Fat goes on in a particular order on different parts of your body that varies between people, and it comes off in the reverse order that it went on. Areas like our belly/hips are often the first place it goes on (but not for everyone!), so it's the last place it comes off and you have to do everything else first.",0
"When it comes to absorbing toxins through the skin, it's important to understand that not all substances can penetrate the skin barrier and enter our bodies. The skin is our body's first line of defense against external threats, including toxins. While some substances can pass through the outermost layer of the skin, known as the stratum corneum, it is typically a slow and limited process.",1
"It was standard practice for consoles before the fifth generation, because none of them had C compilers that were worth a damn.",0
"Trains have a few features that help them maintain traction and prevent slipping, even in rainy or uphill conditions. Firstly, trains have steel wheels that are designed to have a lot of grip on the tracks. The steel wheels have a special shape called a flange, which helps them stay on the rails and prevents them from slipping sideways. Additionally, the weight of the train helps increase the downward pressure on the wheels, improving traction.",1
"Americans generally spend 30% of their total income on housing.  That's 30% of EVERYONE's paycheck going to a single industry. That's why the housing market is such a big deal.  

So, you want to buy a house.  In the 70's/80's, this was a big deal, because you had to have excellent credit to score a sweet 15% interest rate.  If you default, you lose your house, which means you generally become a social outcast so you lose your friends and family too. 

That made the loans very stable.  Folks would default on everything else in life before missing a house payment.  Eventually, banks start lowering the interest rate because it's such a safe investment for them. 

The only problem is that when a bank writes a mortgage, they don't have that money back to write more mortgages for another 15-30 years.  To fix that, they will sell your mortgage to bigger bank.  

Say they loan you $100k today.  In 30 years they'll make $200k after interest and payments.  But they won't have their original $100k back for 15 years.  Same goes for the other 10 people that took out mortgages this week.  So, they bundle it all together, and sell that debt package (worth 10x $200k) to a bigger bank for $1.05m, netting them a quick $50k profit.  

The big bank that bought it will eventually collect the $2m in debt they just bought over time.  

Ooooor they could bundle it with a bunch of other packages and sell THAT package for cash today, netting themselves a quick profit, and transferring your debt to an even bigger bank.  

Eventually the piles of loans get so big, it's tough to determine how credit worthy the debtors are still.  Every once in awhile, someone defaults, and that debt package becomes a little less valuable.  So, you buy insurance.  If a single loan defaults, the insurance company pays the difference in exchange for a flat rate.  

Basically, the bank is gambling that more people will default this year, and the insurance company bets that their premiums will make more money than they pay out in defaulted mortgages.  

And know what? The insurance company has insurance too.  Bets on bets on bets.  

Honestly, this is all great.  Keeps the economy well funded, interest rates stay low, and everyone can have what they need to grow TODAY instead of having to wait and save.  

Just one thing, it all depends on EVERYONE paying their mortgage on time.  

Problem is that the low level bankers are conditioned to think in short term exclusively.  Their profit incentive is to write as many loans as possible, because whatever they write will be packaged up and sold within the week.   They'll make their quick $5k, and if you can't pay after that it's not their problem.  

So now you see strippers qualifying for 3 $500k homes because their income looks fucking fantastic TODAY, and nobody's asking how they're going to make their payments in 30 years.  McDonalds workers are finally able to buy houses for their whole families because interest rates are so low.  

When they default, the big banks that bought the loans don't care because they just kick the folks out and file the loan insurance claim.  

But then 2008 happened.  The insurance companies ran out of cash to pay the claims and file for bankruptcy.  Suddenly the banks have to deal with the steaming pile of shit that are these loan packages.

They start to run out of cash, because folks aren't paying their mortgages.  The street level lenders can't write more loans because they're out of cash to give, and can't offload last week's take.  

Now the banks own all these abandoned and foreclosed houses.  Normally they'd sell at half price, but since the entire neighborhood looks like foreclosed houses, they're lucky to get 20% of the purchase value.  

Also, nobody's buying because they can't get loans, because street level lenders are tapped.  

This means construction stops.  Those that sell parts of houses shut down, the entire lending industry has a lobotomy.  The whole economy just stops because nobody can spend money.  

That's where Obama was forced to do bailouts.  A cash pump to stimulate the economy.  Anything to put some cash back into the economy to get the machine running again.  

Did we learn anything? Fuck no.  Fun fact: They're still playing the same games with Mortgages.  Funner fact: They're doing it with Student Loans too.  Funnest fact: Total student loans now exceed total mortgage AND Credit card debt combined.  And there's not even a house you can sell for quick cash when the loan defaults.  

I figure within 4 years we'll have another collapse, but worse.

Edit: For those of you asking if you should buy a house now or later, it depends. Houses are cheaper during a crash, but loans are more expensive and harder to qualify for. When things are good, loans are cheaper but houses are more expensive. My bet? If you're paying cash, buy later. If you're hoping for a good rate? Buy now, but responsibly. Like, 50% of what you qualify for.

Also, remember that YOUR debt situation doesn't matter. If the housing or student loan market crashes, that means the banks don't have cash. That means they don't have the cash to finance Amazon's new HQ, and all those construction workers are laid off. Those workers in turn don't make their loan payments, and the bank has less cash to lend, which means more layoffs to those depending on easy debt.

Even if you don't owe anyone anything, you are affected if your employer needs loans to cover large capital projects, which is pretty much everyone working somewhere with more than a dozen employees. ",0
"EDIT: I've been informed by some alarmingly angry profanity enthusiasts that the origin of the phrase does in fact refer to the [customer service usage](https://en.m.wikipedia.org/wiki/The_customer_is_always_right). 

So instead please refer to the original answer below as the most USEFUL version of the phrase, rather than the original. 

---

The ""customer is always right"" is an often abused and misunderstood sentiment.The ""customer is always right"" originally meant that what the customer wants (and thus buys) is more important than what you think.For instance, you're a shoe store. You stock green boots, black boots, and pink boots. Green is your favorite color. You always wear green boots.However, your customers only buy black and pink boots. Those green boots sit dead on your shelf, but you keep stocking them. Even when you could be using that money to stock more black and pink boots.The customer is always right means it doesn't matter that you like green boots. Buy more black and pink and suck it up.The saying got twisted through misunderstanding into some kind of customer service truism that it was never intended as.",0
"Only if craving sugar, fat, and salt prevents enough people from successfully reproducing. 


So... no. ",0
"Actual ELI5 here: when you don't eat, your body eats you! Fat people have more stuff in them, so it takes longer to eat themselves!",0
"When we sleep, our brain is still working to some extent, but it's not as active as when we're awake. This means that signals from our body, like the feeling of needing to pee, might not reach our brain as quickly or strongly while we're asleep. In the case of children, their bladder muscles might not be fully developed yet, making it harder for them to hold in urine for a long time.",1
"the vortices left by a fighter or even a radar plane are tiny and weak compared to the fucking twisters that huge jets leave behind. 

Edit: new highly upvoted comment! I would like to thank r/aviation for telling me so many times when I was wrong.",0
"I work in neuro and I don't know the answer to this. Scrolling through the first few top comments I'm seeing wildly different answers. Rather than further misinformation, I'll just interpret the [wikipedia entry](https://en.wikipedia.org/wiki/Hypnic_jerk):

Looks like the reaction is not understood, but is probably the activation of the ""reflex to stay upright"". When your muscles relax when you fall asleep, it may accidentally be interpreted as weightlessness (falling), which may trigger the response.

So if anyone knows more than this, rather than spread dubious information, please update the wiki with your sources.",0
"I work at a vein surgeon's office. I actually asked him this.

Basically, when you are standing, blood flow slows and ""pools"" in your legs due to gravity. But when you walk, your muscles contract and push the blood in your veins and vessels back up into your upper body. 

On the side note, seasoned military personnels are able to stand at ease for long periods of time because they are actually swaying back and forth very slowly in micro-movements  to contract their muscles and relieve the tingling and numb sensation you get when you keep standing for long periods of time.  

Edit: As others have suggested, not locking your knees is also key

Edit 2: As others have mentioned, micro movements could be flexing your calves, distributing weight back and forth between your heels and toes, wiggling your toes, etc. 

Edit 3: If you have persistent leg problems even without prolonged standing and even after conservative measures (compression stockings, exercise, etc.), I would recommend getting a referral to a vein specialist from your PCP (in the US) to get it properly treated. You may just have bad veins. 

Whoa! My very first gold. Thank you stranger 😝",0
"Ok, wow, lol. This is dead wrong. Scientists have discovered a NEW WEIGHT LOSS SECRET that targets stubborn belly fat. CLICK HERE to learn the one *weird trick* that burns belly fat. It's the secret they don't want you to know!

On an unrelated note: [your locale] drivers are saving hundreds on their car insurance due to new law.",0
"In a sense, we do - that's what many recreational drugs essentially are (they're not supplements, exactly, but they either stimulate the same receptors or cause your body to produce more of the things that do).

But there are a few problems with it, as that intro should suggest.

First, over-release of neurotransmitters causes the body to compensate. It becomes less sensitive to that release, and this can make it impossible to feel their positive effects without help and can cause even the outside help to just get you back to baseline. This is part of why addiction escalates: the first use or two produces powerful euphoria before those mechanisms kick in, but addicts need more and more over time - and meanwhile feel steadily worse than their original baseline in between uses.

Moreover, many of these chemicals have secondary effects. They're not so simple as ""have dopamine -> be happy"". Serotonin, for example, plays essential roles in the digestive system, and overdoses of serotonin [cause a serious medical condition that can straight up kill you](https://en.wikipedia.org/wiki/Serotonin_syndrome) (it's an important side effect of serotonergic drugs like many antidepressants). This compounds with the previous bullet, and is why drug addiction has numerous physical side-effects, too.

EDIT: As others correctly note, this isn't quite the same thing as direct supplementation (which doesn't work for the simpler reason that most neurotransmitters don't cross the blood-brain barrier, so they'd have no way to get to your brain). There are supplements that do cross it that either are converted to neurotransmitters on the other side or stimulate production, and they do work to increase levels of that transmitter.

EDIT2: Hey, front page folks, ELI5 is explicitly **not** for actual 5-year-olds. See the sidebar.",0
"Well with little to no Oxygen/other gases in space relative to Earth's atmosphere, so they don't have to worry about rust/corrosion, right? So then they'd just be protecting it from electromagnetic shit and radiation?

I don't know enough about all of this to state it all as fact, but I can see how it happened in an  environment (potentially) easier to maintain itself than Earth's atmosphere. Still doesn't make it any less remarkable that it actually worked, though. 

EDIT: The replies are why I fucking love reddit. I make an educated guess, then get to learn a ton of shit in the comments after. That and the porn subs. ♡ u guys",0
"When I was in 2nd grade, we had this class project where we would build a tower out of straws. One other team in the class built upwards as fast as they could. They would go straight up from wherever they were, and if one part started to slouch or tip over they'd fix that part and then go back to going straight up. Our team made sure that everything was solid; we had a good, consistent, repeatable design of cubes with cross beams. We wouldn't built the next layer unless the current layer was strong. The other team was the first to 2 feet, then the first to 3 feet, then the first to 4 feet, but at some point the fact that their entire tower was half measures meant that they couldn't add anything to the top; regardless of what they added to the top, their entire everything was too weak to just reinforce one or two parts of it. The best way for them to make progress was to throw everything away and start over from scratch. So our team was the first to 5 feet.

Von Braun's personal mission was to colonize Mars; the official mission of the US space program was to land a manned mission on the Moon. The mission of the Soviet space program was to beat the US space program at everything.

The US had smaller (in terms of size and weight) nukes than the Soviet Union did. This meant that the US ICBMs were much smaller than Soviet ICBMs. When it came to converting ICBMs into space science vessels, the Soviet Union were already a step ahead. The R-7 was an *enormous* ICBM. So Sputnik, Yuri Gagarin, etc were all launched on R-7s.

The US knew they'd need a large crew and some sort of orbital rendezvous to make a moon landing work. So they built the larger Gemini mission that could support two people. The Soviet Union wanted to beat the US to a multiple crewed mission, so they took their single person R-7, removed a bunch of stuff (including some essential life support systems) and put another person in it. The USSR did beat the US to that milestone. The US mission was a stepping stone but not a milestone; the Soviet mission was a milestone but not a stepping stone.

The US had to learn how to rendezvous two spacecraft in order to make the moon mission work. So they set out to start doing that. The Soviet Union wanted to beat them, so they launched one R-7 to orbit, waited for the orbit to line up with the ground station, and launched another R-7 into an identical orbit. They were able to get within 3 miles of each other, at which points their orbits diverged; bada bing, bada boom, rendezvous! US beaten. But the US needed to actually connect them together. Remember the larger Gemini capsules? It also had substantially more fuel for maneuvering. So Gemini 6 and 7 were able to maneuver to within a few feet of each other and stay there for 20 minutes. Gemini 8 had the docking adapter and was able to actually connect to another spacecraft.

The US knew they'd need an absolute monster of a rocket to land on the moon, so they started designing the F-1 engine in 1957 and the Saturn V in 1962. The engine and the rocket were absolute fucking monsters, totally in excess of the needs of the time. The US hadn't even launched a thing into orbit in 1957 when the F-1 first hit the drawing board. The Soviets didn't see the need for a rocket that big; there was no milestone for 'big rocket' to beat the Americans to, and the R-7 was fine, so they didn't build one.

By 1965, it was clear that the next milestone after rendezvous was the moon, so focus turned to that. The US was already almost done building the Saturn V. And the Soviet Union looked to scale the R-7 up again, but--it wouldn't work. The R-7 was already as big as it could get with the technology of the day. So they had to throw away everything and try to rebuild from scratch with the N-1. The N-1 hit the drawing board in 1965. The Saturn V would have its first launch in 1967. The N-1 prototype hit the launch pad in 1969 and exploded shortly after takeoff. The 2nd, 3rd, and 4th and final launch attempts also failed. It was just too rushed.

Basically the US thought of space as a series of stepping stones; each thing has to be in service of the *next* thing. The Soviet Union thought of the space race as a series of milestones; each thing has to be the first. It's just a philosophy that doesn't engender itself to a decades long space exploration program.

Derivatives of the R-7 still fly today, by the way. The Soyuz, the workhorse of the Russian space program, is an R-7 derivative.",0
"Inflammation also causes your vessels to become ""leaky"" in a process called extravasation. This fluid will leak into your tissue and will cause that ""achy"" pain you feel when you have the flu. This fluid contains WBCs that will go and kill whatever it's supposed to, which will lead to more pain.",0
Because the bees will murder the ants. Bees are much bigger and will bite an ant in two without much trouble. And there are up to tens of thousands of bees in a big hive.,0
There were a lot of politicians the Nazis had arrested or removed from office.  The first leader of West Germany had spent time in a Nazi prison. The US and the Allies had a specific 'De-Nazification' program they used.  It included things like forcing Germans to visit the death camps.,0
"This is a good question. 

When people go to a quick casual restaurant the price you see is the price you get. This is hopefully the optimum price for owners and customers, and the employees are paid a wage dependent on the local market.

It makes a lot of sense from a supply and demand perspective, including labor and production. Customers pay a price they are willing to pay, workers work for the most the market will bear, and owners make as much profit as they possibly can. The system is based on the constant push and pull, ultimately leading to $8.50 burritos.

Why doesn’t this transfer to sit down restaurants? Denny’s all the way up to a steak house, waiters are paid around $2.50 + tips, while at a Chipotle it is reasonable to expect a starting wage at $9.50. Both are providing basically the same product, food. Why shouldn’t people be paid the same?

The question we should ask isn’t “why shouldn’t people be paid the same”, rather “why aren’t people paid the same”. It is always more useful to understand why before asking if things should be different.

So why are people tipped? The idea when you first look at it is ridiculous. Every single waiter in the country is an actor, playing their part, and at the end of each performance they receive a grade. Their tip. And arguably the better your performance the better your tip. Is this why people tip? Mostly no. If someone tips 20% at every meal the chance they will tip less than 15% is highly unlikely. And just as unlikely is tipping 25%. The same can be said for someone who tips 10%, or 5%. People go into restaurants with their own idea of what is appropriate and tip accordingly, regardless of service.

This is not to completely diminish the importance of competent waiters, good service and hard work matter. They just don’t matter nearly as much as many people would like to believe.

This misconception is important explain tipping’s staying power. It fits nicely with American ideas of hard work and individualism. It also makes it easier to manage a wait staff. The carrot is nicely held in front “do good work, be diligent and friendly, you will be paid more”.

That isn’t the only reason for tipping, full service restaurant economics might be more important. For those unaware full-service restaurants are expensive operations. If you buy a plate of pasta for $20, reasonably you can expect 10% to 15% of the bill to go to rent, around 25% to 30% to go to the cost of food, between 20% and 25% of the bill is labor. Just with rent, labor, and COGS a restaurants margin is somewhere between 45% and 30%. This does not include management, cleaning, maintenance, marketing, utilities, and dozens of other small expenses. When all is said and done, most sit down full-service restaurants make anywhere between 4% and 7% of gross income. That pasta made the owner somewhere between $.80 to $1.40.

Should you pity the restaurateur? Probably not. It can be a good living and owner operated locations often have the benefit of not “paying” for management and some labor, meaning someone can run a $600,000 a year restaurant and bring home around $80,000 to $120,000. But for big corporate chains the reality is margins are low compared to other businesses, operating cost are high, and there is a nasty little industry secret.

They don’t make money 80% of the time.

A slow lunch on Tuesday doesn’t keep the lights on. It might only cover the cost of rent. A good example is Twin Peaks, a chain similar too Hooters. Each Twin Peaks has a very expensive building, in an expensive areas, with expensive cooks, high management costs, and only makes money after 5 PM Monday through Thursday. Those high-volume times are so high they pay for the rest of the “dead time”.

This is typical for many restaurants, some are busy for lunch during the week, others are big breakfast places, others need have a good Friday or they can’t pay their suppliers. Whatever the pattern of customers it is very rare to find a restaurant which can support a full staff 100% of the time. Here’s the kicker, customers expect and demand the same service level 100% of the time.

What do you do?

You need cooks in the back, at a minimum labor is $10 an hour (probably more, in large Texas cities a good line cook makes at least $11.50 an hour). You need managers, contrary to popular belief they do things. Here is where tipping makes a lot of sense. The cooks and the managers jobs are the same Monday through Sunday. Yes, they may be busier sometimes, but a manager can do inventory on slow times and a good line cook is always prepping for busy times. They can spread their labor around, and as such it is unreasonable to expect a them to be paid less in a slow time.

Now for the waiters, they can’t functionally spread their labor to other times. You can’t build up waiter capacity and use it when needed. A waiter is only useful to a restaurant when a customer is in the building.

The cost of running a restaurant are fixed (even labor and management to a certain extent) but the revenue is inconsistent. Tipping allows restaurants to meet the expectations of customers, high quality service always, and allows restaurants to pay waiters only when they can afford to pay them. The waiters get to earn a percentage of their sales and give up a standard paycheck in return for more control over their salaries.

There are plenty of holes in the argument and it raises a lot of question. Why don’t restaurants find out what the actual hourly wage is for servers and pay them accordingly? Why don’t restaurants staff employees more effectively? Why don’t customers accept higher prices? Why don’t restaurants raise prices to just pay people more?

All the questions are worth answering, but it always gets back to supply and demand and human behavior. When people look at the price of a menu they decide to purchase the food or go to another restaurant. And with Yelp and Google price comparisons are easier than ever. It means if a restaurant charges $15 for a burger it had better be at least 2X as good as $7.50 burger. Higher prices must correlate with higher value. And as much as every restaurant claims to have “the best….” there is a market price for a burger and for a steak.

But almost all Americans know the price you see is not the result. We don’t have VAT, we factor in tax and tip. We are all taxed the same, but we don’t all tip the same.

Tipping functions the same way as a coupon. Tipping allows customers to receive the same service, eating at a restaurant and ordering the same food, while paying different prices. There is an efficiency to the system. Those who are price conscious and less willing to tip are more enticed to a restaurant with low prices, while a less price conscious person is attracted to the same restaurant. The owner gets the business of both customers and the wait staff gets functionally an average tip from both customers.

At this point it is worth pointing out how many redditors have said “while I was in university I waited tables” or “while I was in college I worked the bar” and they made “well over minimum wage” because “they worked hard”. This is a generalization and as a rule should be avoided. Except in the preceding sentence. But they hint at a very uncomfortable truth about the service industry, educated, young, and attractive people can make very good money from tips (big assumption, redditors are young, attractive, and well educated). I assure you, you are not better at your job than the middle-aged waiter at Denny’s, but you will get paid more. The ability to access higher paying jobs in the service industry is based on your background, your looks, and how you speak.

What is the conclusion? Should we abolish tipping? Outlawing tipping might make waiters at lower cost restaurants get paid more (this makes sense, tipping is a function of a percentage of sales, lower cost food leads to a lower cost per transaction, leading to a lower tip, and lower wages), but it certainly would impact a lot of college educated people capitalizing on their social skills. It might make a lot of restaurant radically readjust their labor, with some restaurant surely closing. It might make an entire industry segment, sit down restaurants (which are losing sales and business at a fast clip) less attractive to customers, lowering sales and lowering overall wages.

Personally, I see tipping as a bad system, not from an economic standpoint but from a moral standpoint. People shouldn’t have to sing for their supper. They should do a job, get paid for the job, and whether some jerk tipped shouldn’t affect whether you make rent. But it is much more complicated than that. Many restaurants have tried to change tipping culture, and none has met with much success. Certainly there hasn’t been a brushfire of “de-tipping”.

I hope you all found this helpful.",0
"And if you're on venlafaxine/Effexor, doubly heed this advice. Those brain zaps are no fuckin joke.",0
I too began my journey through this mortal coil by breathing in my own shit.,0
"Some decent posts here but I think one key thing is missing.

As a business owner, you do not control the impact that a dissatisfied customer has on your business when he or she recounts the experience. A customer can be 100% wrong in their interaction with a business. You could ban that customer or simply ask him or her to leave. However, when they go out into the world and recount their experience to their friends and family, those people will only hear the customer's side of the story. You as a business owner don't get a seat at they table to explain what actually happened. This can easily dissuade other people from visiting your business and buying from you. 

So it often makes sense to placate a customer who is not ""right."" That customer leaves satisfied and may even realize later upon further reflection that he was being a jerk. If he ever recounts his experience to another person, his story is now unequivocally positive. ""Man I was super pissed off the other day after a long day at work. Went to Mike's tacos, and I accidentally ordered the wrong thing. When the waitress brought it I was kind of snotty and told her she better fix it. She did it with a smile even though I was being a dick. She went out of her way to try and give me a good experience. Those are good people at Mike's Tacos.""

Word of mouth is extremely important for local businesses and really any place that has direct customer interaction. Since you can't control the customer's message to the outside world, sometimes it is worth taking a small loss of a rude or wrong customer to help your reputation. This doesn't excuse those people for being assholes, but it is the reality of doing retail business. 

This doesn't always apply and some customers can really take advantage of satisfaction policies. Repeat bad customs are often banned. ",0
"Lots of things will eat them, spiders, centipedes, and I'm sure plenty of bugs. But that's not what keeps them in check.

They eat human blood, they can't really survive anywhere without humans, if they get to infestation level, humans are pretty good at killing them. We have strong insecticides, and physical barriers can be effective (like just putting your bed in dishes of water so they can't climb up the bed), wrapping the mattress in plastic, and washing all your clothes. They'll starve to death in about a year that way. Vacuums can suck them up as well. Basically, if you know there are lots of bedbugs in your home, you probably will do a pretty good job at eradicating them.",0
"**TL;DR:** *Oxygen, not so much. But the supercontinents back then could really have amplified weather conditions.*

\---

The level of oxygen wasn't really that much of a factor. Oxygen levels were higher because plants were sucking all of the carbon dioxide out of the air and trapping the carbon into coal and oil at the time while breathing out oxygen and raising the levels up to about 30%. (It's 21% or so now). That much higher level would have made fires way more dangerous in dry areas like grasslands with lots of fuel. Large fires can contribute some to weather, but they usually don't amplify storms in general.

The biggest influence was continental structure. We had two different supercontinent-type land formations back then, Pangaea around 300 million years ago broke into two big chunks, Laurasia and Gondwana, during the time of the dinosaurs.

Now very generally speaking, the more you pack land into one area and ocean into the other, the greater the general impact on weather... and with supercontinents leaving gigantic stretches of ocean pretty much wide open, you're going to get this to happen. This is because hurricanes feed off of warmer water and shrink when they cross land, and when there's more warm water, there's bigger hurricanes or typhoons (and this is why Pacific storms are often larger than Atlantic ones).

Other storms can get amplified too. Nor'easters (the big storms we get here on the NorthEastern coast of North America) build off of differences in air pressure which are caused by differences in heat level. . Larger masses of solar-heated continuous land mean greater regional heating, and that can translate to differences in regional pressure colliding with each other and generating much more powerful localized storms.

There's a number of other factors including sea depth (shallower seas warm up more), mountains that deflect currents of air, ocean currents (that help to convey warm and cold weather and equalize temperatures), and distribution of land versus water at the equator where the most solar energy is focused. All of this stuff is why it's hard to talk about specifics back then.

But in general,  you could expect to get truly massive storms crossing over the coasts of the supercontinents in this altered world.

(made a few edits for completeness and to correct one error)",0
"So your body has these muscles called internal and external sphincters. They act like these rubber bands around your rectum and your urethra (where pee comes out). You can control the external sphincters but can’t control the internal sphincters. The feeling that you need to pee or poop comes from the internal sphincters saying “hey we need to go” to your brain and then they relax/open to let said pee or poop out. The clinching feeling when you’re trying to hold it in is your external sphincters, which you can control. When you’re sleeping/awake these sphincters are constantly contracted/closed but if the internal sphincters relax/open, then your brain will wake you up because you have to go. 

",0
"I'm a registered dental hygienist. There's different mouthwash for different needs. 

There's fluoride mouthwash, which gives you am extra bit of cavity protection. There's anti-gingivitis mouthwash, which helps to kill even more germs in your mouth than brushing and flossing alone. There's even whitening mouthwash, which are typically peroxide based. 

When my patients ask me which brand is best, I tell them that the brand usually doesn't matter. You look for what the mouthwash is doing for you (for example, after rinsing with a fluoride mouthwash, avoid eating and drinking for a half hour afterwards). 

Do I recommend mouthwash to everyone? No. Only if I feel it could benefit them and their particular needs. 

And yeah, they make your breath fresh too! But cleaning your tongue thoroughly will help with that too (use a tongue scraper!)

Edit: Thanks so much for the award! Much appreciated :)

Edit2: Thank you guys for the awards! ❤ I'm happy to help

Edit3: Wow, it's been a fun night! Thanks again for all the awards and for all your questions, but I have to head to bed!

 Please don't forget to visit your hygienist every 6 months for a cleaning and checkup! Don't be afraid to ask them questions, that's what we're here for! And feel free to peruse my previous answers, I've answered a ton!",0
"The magnetic strip is like a secret code that lets you buy things. I can copy your secret code and use it to buy things.

The chip is like a little man who makes secret codes that can each be used to buy one thing. I can copy the secret code but not the little man. Because the secret code only works once and for a limited time, and in one situation, stealing the secret code isn’t useful. You can’t steal the little man without doing a lot of work.",0
"White guy living in Japan for over a decade here. If you don’t look Japanese everyone here will assume you are completely ignorant of the culture and language and most indiscretions will be forgiven. Any attempt at anything “Japanese” will generally be met with happiness and cultural pride that you are trying and interested.

The only things you can do off the top of my head to piss people off that you might not know to do or not do: don’t talk on the phone in trains or on busses, stand on the left side of elevators (or the right side in Kyoto/Osaka), let people get off trains before you get on. Maybe more, but that’s all I can think of.

Edit: Whoops! Why did I say elevators?! I meant escalators!",0
"My first was so fucking crippling I couldn't walk and thought I had appendicitis (age 11,  almost 12)",0
"Snipers often work in pairs, with one person acting as the sniper and the other as the spotter. The spotter's main role is to assist the sniper in achieving a successful shot. Here's why they need each other:

1. Target Acquisition: The spotter helps locate and identify targets, often using binoculars or a spotting scope.",1
"Short answer - You still have plenty of blood sugar to last on, and sleep shuts down digestion. It takes a little time to start back up. Basically, you don't know what hunger is

Long answer:   
 
There are at least 4 stages of hunger

1- Stomach is empty, I would like some food.

2- Stomach/intestines are cleared out. Running on blood sugar so you better get food now.

3- Blood sugar is gone, converting fat and muscle to keep you alive. You better chase a small animal down NOW or you're going to die. Not true, but thats what it feels like.

4- Fat and muscle are no longer supplying enough of what you need. On top of hunger you're suffering physical and cognitive impairment from lack of nutrients. You are likely sustaining long lasting or even permanent damage to your body. Eventually your brain and heart will shut down and you die

It can take 4-6 hours to get to 2, depending on what you've eaten. Your blood and liver store ~24 hours of blood sugar. You can't get a decent estimate of when you will progress to 4, but you will run low on vitamins, ect before you run out of fat and muscle. If you live in a developed nation its possible to never have gotten to 2, and likely you've never seen 3. In the unlikely event you see 4, a doctor would probably recommend an extended hospital stay",0
If engineering degrees didn't include math. Lots of math. Oh god the math.,0
"God ad-funded blogging really has decimated the quality of content on the internet

Edit: This is not about the ads. It's about the quality of the content that is generated to create space for them",0
"This comes down to your circadian rhythm, usually. From a young age, most of us get into the habit of sleeping at night and staying awake during the day, or maybe having a short nap in the afternoon. This is usually linked to Zeitgebers (German for 'time-givers') which are basically external cues. The most common of these are external light. Essentially, we train ourselves into having a proper sleep at night and only a nap during the day. 

It's surprisingly easy and simultaneously difficult to fuck up your circadian rhythm. Think about how often you stay up all night to watch a show or play a game or read a book or dance while wasted. Basically anything that throws you off your rhythm can fuck up your cycle but this is usually short term, and your body is pretty good at getting itself back on track if you're not fighting it, thanks to hormones. 

Hormones help maintain your circadian rhythm. Melatonin and cortisol both affect your sleep cycle. Example: Melatonin, the 'sleepy' hormone, is usually produced at night, and causes drowsiness and drops your body temperature, both of which will obviously make you want to sleep more. 

Melatonin usually starts getting produced between 8pm and 9pm, spiking around 3am and 4am, finishing about 7am or 8am. This helps us sleep through the night. There's usually a smaller spike around lunch time, but this doesn't last as long, thus leading to a simple nap rather than a full on sleep. 

It's also why it's easier to slip back into a 'normal' rhythm after working nights than it is to start doing nights after working days. Fucking hormones! ",0
"Proper ELI5 - consider your body is like a house with a wood burning stove, you put fuel in to release energy you need. Now, having 0 fuel on hand is dangerous so you have a modest stack of wood inside, somewhere nearby. This is normal for anyone since you don't want the house to freeze, so everyones body has some fat stores to burn on hand.  

But Being obese \ fat is like bringing more and more wood in than you can burn constantly. Eventually you are building addition after addition on to that house, but only in order to stock the more and more wood inside you keep bringing. Time passes until you could last month's without getting more wood, stacked to the rafters in every room and corridor where it can go.

Intermittent fasting would be like focusing on burning the wood you have in the house first and only bringing more in at the end of the day. You still need to burn more wood than you bring in, but that deficit will slowly over time whittle away at your stockpiles. It's made easier to have less calories since your restricting based on time, it's harder to get them in. 

Months pass, Your additions get smaller and smaller, and even if you want to bring more in you can't fit it. This is because your stomach volume shrinks over the hours it remains empty, and it's harder to eat thousands of calories in the little time you have with a smaller stomach.

In time, the additions get emptied little by little, and the amount of wood you bring in becomes more manageable and normal because you simply aren't used to carrying in an over abundance anymore, both physically and mentally. 

I lost 100lbs on intermittent fasting, starting from 400+ to my 305 now. At 6 ft 6 I have a dad bod at that weight but I'm lower than I weighed in highschool. I'm working on losing more, thinking of getting to the gym (hotter stove means more wood gets burned!). I made no major changes other than restricting to 6-10pm and not drinking calories before. I ate like crap still, handfuls of lucky charms, full pizzas, beer, munchies out on weekends and still got here because I can't eat as much in one go anymore. Still have moments where I order or plate up food and chuckle when I can't finish more than half of what I once did.


 Funny, I just realized the stove metaphor is actually spot on since any weight loss is actually expelled as CO2 from breathing!",0
"This is a really huge question, but I'll try and be brief. There are a couple of things to keep in mind about Germany; it is one of the largest and most populated states in Western Europe, and it has had a very strong industrial base for many many years.

After WWI, Germany was in pretty bad shape. It owed a ton of money in war reperations. This issue was dealt with by the Nazis basically just refusing to pay them.

More importantly though, Germany might have lost the war, but even the winners were in really rough shape. No one was willing to stand up to the Nazis until it was too late. When they started to remilitarize, no one stepped up because they either thought that the lot they were dealt in WW1 was too harsh, or because they were too war-weary to care. When Germany started to absorb parts of its neighbors, it was justified by claiming that it was done either to protect German nationals, or because the Germans had been invited to do it (which is partly true in some cases). 

Further, once WW2 started, the Germans had a couple big benefits. Most of their immediate neighbors were too weak to do much, France and Britain wanted to avoid bloodshed. When they invaded Poland, they got help from the Soviet Union. Once the war really got underway, France folded almost immediately, and the British were pushed off of the continent not long after. France was gone, Britain was technically still at war but couldn't mount an offensive, Italy was an ally, America, Spain, and the USSR were neutral, and much of Central Europe was already under Nazi control. They were able to take most of Europe without much of a fight. 

Helping matters even more, Germany benefited from having some pretty revoltionary tactics, scientists, and equipment. In particular, the Germans wrote the book on blitzkrieg and tank warfare, which proved instrumental.

After they lost the war, the country was split into four administrative zones, occupied by the Americans, British, Soviets, and French. The American, British, and French zones were evnetually consolidated to become the country of West Germany, while the Soviet zone became East Germany. The Western Powers poured a ton of resources into rebuilding West Germany and getting them back up to speed (so that they could help fight the Soviets in the event of WW3). Since they're still one of the biggest and most industrial states in Europe, it's only natural that they've had a strong economy ever sense.

Edit: Wow, I didn't expect this to blow up. RIP Inbox. Thanks for the gold!

Edit 2: I'm glad that I could help out so many people who had questions on the topic. That said, while I do have a fair bit of knowledge on the subject, I'm hardly an expert. If you want some more in depth and accurate answers, you should go check out r/history. Or bug your teachers/professors for resources on the subject (they love this sort of thing, so it'll probably help your grade too). ",0
"Take a big coil of wire - a metal, full of electrons. Electricity is just the movement of electrons.

Because electrons are electrically charged, they create and respond to electric fields - think of when you rub a balloon against your hair, statically charging it. The hairs push away from each other. That's the negative charges creating negative fields. Negative charges don't want to be in other electrons' negative fields, so they start pushing away from each other.

A magnetic field is the same thing as an electric field, really - that's why we call it ""electromagnetism"". The only difference is how you look at it.

Put some magnets in the centre of the big coil of wire. The magnetic field from those magnets will start to push on the electrons in the wire.

If you start turning those magnets, the field will turn too - and as it moves, it'll push the electrons with it. Because it's a coil, or a spiral, as you spin around the middle, you also move to one of the ends.

The electrons moving out of the end of the coil is electricity.

Edits:

Literally a hundred comments later asking the same few questions:

1.0) You can't deplete a wire of its electrons. Electricity works in circuits, closed loops of wire - as you push the electrons out of the wire, the pushing force affects the whole circuit and more electrons are pushed back into the other side of the coil, and they then get pushed around again by the magnets.

2.0) The electrons start in the wire. Everything is made of atoms and atoms have electrons. In metals, those electrons are free to move around, but they're more bound up in non-metals.

3.0) You can find naturally occurring magnets in the form of minerals like magnetite, or lodestones, but you can also make them by rubbing two pieces of iron together in the same direction a few thousand times.

3.1) ""In the same direction"" means the strokes are always the same, you don't go back and forth. Think of it like combing your hair: you always brush in the same direction.

4.0) I know this is an oversimplification, that's the point. It's an ELI5. Stop telling me ""Well actually it's a lot more complicated-"". I know you can carry charge through other means, not just electrons; I know positive charges are conveyed as holes in the electron sea; I know. Whatever your correction is, I know, and I deliberately left it out to make it simple enough for a 5 year old. As is the sub's reason to exist. If your 5 year old can crunch the numbers on Lenz' Law, good for you, go breed an army of your fucking superkids and stop bothering me.

5.0) Really late in the game here, but: ""In OP's comment he says-"" - no he doesn't, _she_ does.",0
"People, like strawberries, are huge and soggy and fragile. If we try and freeze them, the water inside starts growing bigger, which pops loads of the little balloons that we huge things are made of. Some of those little balloons are really important, and if too many of them break, the strawberries turns into mush.


Tiny things like human sperm and eggs don't have much water in them, and they are small enough that we can freeze the whole thing at once.",0
"It's pretty simple: We've never registered a 204db sound. 

NASA calculated the ""sound power"" (not ""sound pressure""), based on the 7.5 million pounds of thrust, to be 204db. 

Their official report, the source of this 204db number (Figure 1), clearly notes this distinction: 

[https://ntrs.nasa.gov/api/citations/20120003777/downloads/20120003777.pdf](https://ntrs.nasa.gov/api/citations/20120003777/downloads/20120003777.pdf)

Figure 4 shows the measured pressure level, and it's a whole lot lower.",0
Can confirm. I store fat well,0
"There's little scientific evidence to support massage as a recovery technique in terms of waste product removal. 

The circulation increase from massage is achieved tenfold simply by using the muscles you want circulation to improve in.

 I.e. Athletes conduct 'warm downs' that are specific to the musculature they just used. A football player will often be told to immediately jump onto a stationary bike once off the field from a game to keep circulation going to his legs after the final minutes of sprinting in a match. Simply walking around will increase your circulation.

As for your muscles feeling better after a massage? They were tight, they got stretched, now they're less tight. Just use a daily flexibility program to reduce your muscle stiffness and increase your joint ROM (range of motion). You can do active stretching (you stretch your muscle as much as you can) and you can do passive stretching (someone else stretches your muscle to varying degrees). Google search PNF (proprioceptive neuromuscular facilitation) stretching techniques if you want to learn more or try it yourself. 

We're taught that if there's no harm and something can cause psychological benefit (placebo effect) in an athlete, then you may as well do it. (4th year Exercise Science + Rehab & Exercise Physiology student).

This is an unpopular opinion because it could be detrimental toward the massage industry but there's no concrete evidence or meta analysis that currently support massage's supposed effect of enhancing recovery and maintaining performance. (These are two very important recovery goals.)

Please do keep in mind that when performed after a soft tissue injury a massage is definitely not recommended. You'll throw up afterward as you're messing with the healing process. (Specifically a soft tissue injury with an inflammatory response.)

Please don't take this reply as an insult to massage techniques or the people who perform them. They're very skilled (sometimes) and very passionate about their job (always IMO). Those are two great things to have in someone you purchase a service from.
They provide an awesome service that I'm sure will be around for a long time. However as soon as someone tries to push their supposed scientific evidence behind massage physiological benefits onto me, it annoys me. There are no marked physiological benefits that have been concretely proven from massage. It's purely a psychological thing. (A damn good psychological thing, don't underestimate the mind.)(Psychological benefits were anecdotally recorded in most studies.)

The gilded top comment reply is essentially correct up until:

""when you find a knot (trigger point, sore spot or whatever you wanna call it) when you press on it you are pushing that calcium myosin mixture out of the area back into the venous system""

There's no proper evidence when you combine both the disproving and proving papers to suggest that you're pushing anything into the venous system. This claim simply doesn't exist. 

If you'd like an example of a successful recovery technique that does manage to reduce oedema and enhance venous return then check out 'cold water immersion'. It has nothing to do with calcium reuptake or myosin filaments. It does however utilise the hydrostatic pressure applied to your body when immersed in water as well as your body's natural response to cold water immersion. These seem like big words but they aren't complicated at all. When you hop in a pool at the shallow end and walk to the deep end (slowly increasing water immersion from your waist to your neck), you can feel the pressure building over your body. That's the hydrostatic pressure. The cold water component is typically conducted in 10-12 degree Celsius water (freezing cold and horrible) and your body reacts to this by shunting blood away from the peripheries (vessels near skin) and toward the core (closer to the heart). This stops us from losing heat so easily when we need to conserve it. (Our body has an optimal temperature of roughly 37 degrees Celsius in which our metabolic life maintaining cellular activities perform best.) When we heat up we do the opposite, we dilate the blood vessels near the skin which lets us lose heat faster so that our core temperature doesn't go too far above 37 degrees. (These are homeostatic mechanisms.) I'll leave a link to the AIS website. The Australian Institute of Sport is an international leader on recovery technique research in elite athletes. (I'm Australian, I'm not affiliated or an employee of AIS). There's alot note to this topic but I'm on my mobile so I can't write a lecture's worth of important extra details. 

Please also keep in mind that CWI isn't for every athlete or person. It's actually detrimental to a small degree for weight lifters. Essentially you want to build and progress your muscles via protein synthesis (increased window post workout for PS),  you don't want to reduce blood flow to the muscles which is what CWI does. If you're training for physical adaptions (muscle bulk+) it's probably not for you. If you bulked pre season and are just practicing football drills and maintaining muscle then CWI can help because that higher protein synthesis window is not as relevant to you.(Ignore this paragraph if it's not relevant to you, it may seem confusing!) 

Link from my mobile: (It goes straight to a PDF download, just Google search 'Cold water immersion AIS' if you don't like links or it doesn't work.)

https://www.google.com.au/url?sa=t&source=web&rct=j&url=http://www.ausport.gov.au/__data/assets/pdf_file/0007/560959/AIS_33255_Top_tips_for_water_immersion.pdf&ved=0ahUKEwiE2v-swODVAhUKe7wKHUw-BNEQFggjMAA&usg=AFQjCNGRHsoRXrruTbdB3m_j3fHUPs1rkg

I've never heard of a calcium myosin mixture and have found nothing peer reviewed to support this idea. As far as I know myosin myofilaments are just a component of the cross bridges that allow us to move. Just Google 'Sliding Filament Theory' to understand it better. It's actually super interesting to understand but it's too much to explain in a Reddit reply.

So it's true that we use calcium to unlock Troponin from it's position atop Tropomyosin which effectively blocks myosin filament crossbridges from attaching to actin filaments when we don't want them to (muscle relaxation). In short, calcium is the key, troponin is the lock, tropomyosin is the gate and actin was the thing behind the locked gate that myosin wants to attach to.
There's no concrete evidence to suggest that massage as a recovery technique removes 'excess calcium' that the sarcoplasmic reticulum didn't reuptake. (Reuptake occuring as a result of our CNS signalling the muscle to relax). 

I'm happy to keep answering replies but please keep it civil. I'm not here to rain on anyone's parade. I get massages whenever I can afford them because I enjoy them too. I've performed localised massages on athletes as a sports trainer while I've worked in Aussie rules football clubs in my area and if the player is happy then that's often all that matters.

TLDR: Zero meta analysis or concrete research to prove anything other than a psychological benefit from massage.

Please keep in mind that the potential for physical benefit is possible but it isn't proven across studies (varies widely) and is often the result of a psychological benefit.

Edit 1: Thank you for the gold.

Edit 2: Typos fixed and a paragraph added.

Edit 3: Sorry if this is not ELI5, please feel free to send criticism that I can use to clarify my post further. I'm a newish Redditor.

Have a great day!",0
"When governments want money they can either tax their citizens or borrow from them. Much like how a business can sell products or borrow from investors.

How much to tax, how much to borrow and how much to spend is very complicated, and depends a lot on the personal preferences of the government and its citizens.

So, short answer is the government borrows from the people who live there and spend that money on the people who live there. Consequence can be great, or crap, depending on what the money is spent on, and how willing everyone is to repay the debts. ",0
"To be clear, it's only fat-soluble toxins that can be absorbed through skin. Most toxic substances are water soluble and can't pass through skin.",0
To add.  A single wheel allows you to level the load when going perpendicular to a slope.,0
"When I lived in the ghetto I learned quickly that what ever pest your neighbors had you also had. I had ants, spiders, and mice... until I got overwhelmed by bedbugs. It got so bad that I didn’t want to go to work because people would question the hives I had. So I looked up what eats bed bugs, and it was the previous pests. So I had to choose which devil I wanted. So I started to leave little droplets of syrup all over my house and the ants slowly returned neutralizing the bed bugs.

EDIT: My first gold! Thank you kind stranger!

This seems to be taking off so I’ll give more details. The infestation I had was so immense and genuinely scared me more than anything in my life at that point. I would wake up at night with a flashlight looking after I felt them crawling on me. 
I’m not saying ants were the only method I used to fend them off, but they definitely were the straw that broke the bedbugs back. 
I was broke and could not afford everything at once, so I started with the ants on week one. Week two sprayed all my fabrics with a lavender/ peppermint oil mixture to kill the larvae and act as a repellent. The next couple of weeks I washed everything with borax and after a month of staying tidy, constant laundry, and leaving treats for my exoskeletal friends I was pretty much rid of them.",0
"I'm a programmer, and a lot of it isn't new features - it's laziness. Nobody wants to optimise, because it's boring and ""most computers don't need it"". It's really stupid.

Edit: I guess economics. I do agree with the replies. But still - even programs created by huge businesses are needlessly huge! Take a look at the original SuperMarioBrothers - it had to fit into 40KB. Now, we have on screen keyboards for hundreds of megabytes!

Edit 2: Ok, yes, sometimes there isn't enough time. I suppose, but when it IS viable to optimise, it's almost never done.  That's my issue.  When it's not possible I get it.

Edit 2.5: Better example stolen from u/durpdurpdurp's reply: 

> Call of duty warzone is a great example of this, there's no good reason to make users download 200GB updates other than they know it's not a deal breaker for most users and so it's not worth their time to find a better patch setup. I released a VR game that, the entire game is contained in 300MB because I probably over-optimized when I should have just tried to release the game. 200GB is a problem imo, but if I was more relaxed, I don't think a 1GB game would have been an issue, and so I should have spent less time on compression and extra scripts to modularly modify textures and sounds at runtime lmao. Overkill for sure for what I was doing.

While I haven't used either game so I have no idea about the quality of either, the base point still stands. **200gb** for a **game**.

And notice that I said **a lot** of it is laziness.

Edit 3: Add some details, clarity, etc.

Also: I'm sorry, but I won't be able to respond to all replies. 43 messages in inbox is way too much for me to handle.",0
"We don't know. There are a number of theories about this. To clarify, while the increase may be exaggerated by people who falsely claim intolerance when they probably have other health issues (or are hypochondriacs), there is actually an increase in people with diagnosable gluten intolerance. And gluten intolerance is different than celiac. I'm taking here about gluten intolerance. Some possible causes include changes in the gut microbiome and changes in how we process and make bread.

Changes in the gut microbiome are a likely cause/contributor but the causes and effects of that are just stating to be understood, and barely. So I won't go into that too much, but if anyone has questions I may be able to answer.

On the processing side, one interesting theory is that the germ of wheat helps us process the gluten in some way. It has lots of nutrients, vitamins, fats, etc. Modern wheat flour (even most whole grain stuff) is made by separating the germ from the rest of the wheat first, then processing. This causes the flour to keep longer but removes all those nutrients. This is why flour/cereals need to be fortified. However, we only fortify with the vitamins and minerals for which we notice obvious deficiencies. So it's entirely feasible that we are neglecting to add something back into the flour that helps SOME people not develop gluten intolerance. This may be via some immune response or due to changes caused in the gut microbe (e.g. we are no longer giving some micronutrients to a specific bacteria in our gut so it dies out. That bacteria helped us process gluten or a byproduct and without its help we get sick). It's also possible that our body just needs some nutrient in the germ to process gluten efficiently. We really just don't know.

Tldr: shits complicated literally

edit: First, I know the difference between a theory and hypothesis. I was using the term colloquially, which *even scientists* do sometimes.

People seem to have extrapolated way more than they should have from my comment. Like are asking me where to buy bread with wheat germ and how to fix their gut microbes. That's really not how this works. Anybody who gives you an easy answer to your problems is probably trying to sell you something (I'm looking at you, supplement/probiotics industry...). 

Until relatively recently we didn't even know bacteria could survive in your gut, so expecting the scientific community to have a solid understanding of the gut microbiome now is absurd. These questions span the fields of nutrition, microbial ecology, microbe-host interactions, immunology, and more. I'm sure there are hundreds of plausible explanations, but we are VERY FAR AWAY from definitively answering most questions related to the gut microbe. We DO know that it affects digestive health, mood, weight, and all kinds of other human physiology. What we don't know is how to bend it to our will or how it causes all of these things. We do know that the answer is complicated. How do different bacteria interact with each other in your gut, and then with your body? We also don't know much about that. But we're learning.

There is a unique soup of maybe 1000 species of bacteria in your gut, and they are mostly different than the species that live in mine. We are just starting to learn how specific individual species of bacteria can affect their hosts. But even with this research, we don't think that it will be the same in everyone.

example: Maybe bacteria A has effect B on me, but it has effect C on you, because I have bacteria Q in my gut and you don't, and bacteria Q is necessary for effect B. Now consider that x 1000 species, and that a genetic component also affects this, and diet and stress levels and fitness also affect this. See where I'm going?

We do know that the gut microbe is influenced by stress, diet, sleep, environmental exposure, your parents, exercise, infection, travel, antibiotics, alcohol consumption, genetics, epigenetics (which is affected by all of these things and more), social habits, sun exposure, etc. Just to name a few. The extent to which these affect each person is probably highly variable. So asking about specific solutions or a quick fix is a waste of time, especially on the internet. And if you have a shitty diet - especially one high in carbs and sugar - or high stress levels, or you drink a lot, addressing those first is probably a smarter solution than asking about wheat germ and special bread and probiotics (may work in some cases for some people sometimes, and usually not as a ""fix"" but as a supplement. it's just not well studied enough.) and GMOs (no evidence of them affecting any of this or even a feasible mechanism for how they would). 

tldr2: no really, shit's complicated. Something that works for one person may not for another for hundreds of reasons that we don't know much about yet, but are sort-of on the verge of understanding. This is also why the human microbiome is so hard to study. Remember, none of this is well researched enough for there to be standardized advice for anybody outside of the normal ""live a healthy lifestyle"" advice, and slowly figuring out what makes you feel better. So don't ask for a quick fix and don't trust anyone who offers one. Here are some links about the microbiome and a couple on the microbiome and gluten. 

http://learn.genetics.utah.edu/content/microbiome/changing/

https://en.wikipedia.org/wiki/Human_microbiota

https://www.sciencedaily.com/releases/2016/10/161003113009.htm

https://www.scientificamerican.com/article/the-guts-microbiome-changes-diet/

https://www.ncbi.nlm.nih.gov/pubmed/26605783

https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-016-0295-y

http://www.medicalnewstoday.com/articles/309642.php

edit2: yes, non celiac wheat/gluten intolerance exists. some studies have shown that people who claim to have it do not, but that does not encompass all the literature. the key to those studies is that they were looking at SELF REPORTED gluten intolerance, so basically your average ""but gluten"" person, not people who were medically evaluated and thought to have it. turns out you just have to find the right people to study (who actually have it). just skim this google scholar search and you will see significant evidence of its existence: https://scholar.google.com/scholar?hl=en&q=non+celiac",0
"A bit is the most basic unit of information in computing, capable of storing either a 0 or a 1. The number of bits in an operating system (OS) refers to the size of the memory addresses it can access. In a 32-bit OS, each memory address is represented by 32 bits. This limits the OS to accessing a maximum of 4 gigabytes (GB) of RAM.",1
"Former army sniper here. There are several reasons you have a spotter. One is that ideally all the shooter should have to do is trigger pull, so you need someone to spot hits and give adjustment to get on target or where the next target is. The second is that rifle optics have a relatively narrow field of view compared to binoculars or a spotting scope, so the spotter has a better overall picture of what is going on. This also frees up the spotter to do secondary activities like calling up Intel reports and calling for fire. Finally you would never send a soldier into the field alone, so you may as well augment there abilities with some of similar skill set.  
Edit: an addendum to what I am seeing in the comments, the spotter is almost always the more experienced of the two, but not always the better shooter, as their emphasis is on target designation and quick correction which are skills developed over time.  Edit 2: thanks for the gold trying to keep up with comments but at work
",0
"Also don't forget the fact that as many as  4 of 5 fertilized eggs are rejected by the mother's body due to life incompatible gene errors, being ejected like any unfertilized egg during the next menstruation.",0
"It is due to the way countries regulate how long a truck can be.

In Europe it is usually the entire length of the truck and trailer and in the US it is just the trailer.

Since you want to maximize cargo space and make the trailers as long as possible, they usually shorten the truck in Europe by putting the engines underneath the cab.

This may sound like a stupid regulation until you have seen just how small and narrow and devoid of space cities in Europe can be. Every centimeter counts.",0
"I find it's also harder to use a stupid amount of bar soap.

My dumb monkey brain will keep pumping liquid body wash until it looks like something out of a weird porn video. Not possible with bar soap.",0
"> I fear any person holding a gun who could legally kill me.

Come on. He can't legally kill you. He's not James Bond. ",0
"I was on maximum dose of this for over a year. Insurance randomly one day decided ""he's been on it long enough, let's stop covering it"" when I went in for my refill. NEVER have I been through such physical hell as that. Brain zaps became full body zaps and would be triggered by the faintest glimmer of light or any sort of sound. I was constantly sweating through my clothes and tremoring. Also dislocated my jaw from clenching my teeth too hard, and that ER visit and a week of painkillers probably cost insurance more than if they'd just continued me on my antidepressants.",0
"Freezing human sperm and eggs is possible because they are single cells that can withstand the freezing process. However, freezing whole people or even just organs is much more challenging. When we freeze something, ice crystals can form, which can damage cells and tissues. With single cells like sperm and eggs, we can protect them by adding special substances called cryoprotectants. These substances help prevent ice crystal formation and protect the cells during freezing and thawing.",1
"In general or body has three fuel sources: sugar, glycogen (a complex carb stored inside many cells in our body), and fat. (there is a fourth: protein, but our bodies will only start burning protein for energy if something is wrong or you are starving for a very long time) 


When we eat a meal our blood sugar starts to go up. When our blood sugar gets high enough our body starts storing the excess as glycogen. But each cell can only store so much of that glycogen, so any left over sugar gets turned into fat and stored in fat cells, which are made to do that one thing, and fat cells can store an endless amount of fat. They'll just keep packing it in and each individual fat can can become huge.


Our body really likes to hold onto the fat, it's a fail safe for when food is scarce. You can imagine our hunter gatherer ancestors needed to be good at storing energy in case they failed to hunt or gather food that day. Or you can think about bears who get super fat before the go into hibernation. It's all about energy storage.


When we burn energy we burn it in the same way: first the sugar in our blood, then the body will break down glycogen, then finally fat. When fasting in order to maintain energy levels, blood glucose levels, it will first start to break down glycogen to make those sugars. When glycogen stores run low the body then starts pulling from its emergency reserve: fat.


This is the purpose of intermittent fasting as a dieting technique. You're forcing your body to burn through its glycogen stores and maximizing the amount of time your body is burning fat for energy. This is called ketosis, and is probably what you're referring to when you say 'metabolism'. Our bodies change they way they are metabolizing different stores of energy to ensure it has enough to function.


Edit: people keep commenting that my comment about intermittent fasting is wrong. It's only wrong if you think intermittent fasting is just skipping breakfast. Many people who do intermittent fasting do OMAD, one meal a day, or implement 48-72 hour fasts to their intermittent fasting regime.


And in general, even by just skipping breakfast, you will be burning more fat during that period. Very rarely does the body do only one thing at a time. When you start fasting for any length of time you are going to be burning glycogen, metabolizing fat, and inducing gluconeogenesis (making sugar from fat and other sources). Fasting is a great way to burn extra fat stored in the liver (fatty liver disease, a precursor to type two diabetes and metabolic syndrome) b/c gluconeogenesis happens pretty much exclusively in the liver.


If you want to get more technical low blood sugar, like when you are fasting, will increase the amount of glucagon in your blood. Glucagon acts on peripheral tissues to tap into glycogen stores, on the liver to induce gluconeogenesis as well as burn glycogen, and on adipose tissue to induce lipolysis (break down of triglycerides into few fatty acids so they can be turned into energy in the liver). Even short periods of fasting will induce these actions. Will you go into full ketosis after a short fast? No. Will your body start the process after a few hours? Yes, even if it's just a little at first. Metabolizing fat from peripheral tissues takes time, so the longer the fast the better.",0
"To expand on that, what sorts of sensations and feelings do you have when you start getting bored? Maybe you start being really sensitive of your physical body, things itching, being hot or cold, in a way you don't notice when you've got something on your mind. You also frequently start noticing you are hungry even when you don't really need to eat, or sleepy when you've got enough sleep, or are more conscious of your bladder. And of course if you're bored it's really easy to get horny too.

That's basically running down the entire survival and reproductive needs checklist right there, and it's pretty much hard coded to run in all of our heads as soon as we're not focusing our physical and mental power on something else. Once you're idle again then it's our brain's evolutionary job to once again get us to start taking care of ourselves... or fuck.",0
"Imagine you are always driving your car with the parking brake on. The parking brake depresses (lowers) your speed. You adapt by pressing harder on the gas. One day you take the parking brake off, and because you're still pressing hard on the gas you lose control and speed into a brick wall.

Similarly, the brain adapts to alcohol (the parking brake) by removing some portions of the overall braking system (inhibitory GABA receptors) to compensate. When the alcohol is suddenly removed, the usual GABA brakes are so sparse that the brain speeds out of control into a seizure.

EDIT: While seizures are the most common serious complication of alcohol withdrawal syndrome (AWS) and can be fatal, most AWS-related deaths are not caused by seizures. The ""speeding"" of the parts of the brain that control heart rate, blood pressure, and contractility (force of heartbeats) can trigger fatal cardiac events, especially in patients with certain risk factors. These are not the only potentially fatal complications of AWS. Always consult a physician before attempting to detox from alcohol, as they can advise you as to the safest way to ""release the parking brake"" slowly over time.",0
"Suppose you have a .txt file with partial lyrics to The Rolling Stones’ song ‘Start Me Up’:

>	* If you start me up
If you start me up I'll never stop
If you start me up
If you start me up I'll never stop
I've been running hot
You got me ticking gonna blow my top
If you start me up
If you start me up I'll never stop
never stop, never stop, never stop*

Now let’s do the following:

let xxx = ‘If you start me up’;

let yyy = ‘never stop’;

So we represent this part of the song with xxx and yyy, and the lyrics become:

>	* xxx
xxx I'll yyy
xxx
xxx I'll yyy
I've been running hot
You got me ticking gonna blow my top
xxx
xxx I'll yyy
yyy, yyy, yyy*

Which gets you a smaller net file size with the same information.",0
"Your body is like a steam engine. The fireman gets a load of coal (food) and either shovels it into the engine (your body's metabolism) or shovels it into the [tender](https://www.lionelstore.com/LionelStore%20Product%20Images/636847-1.jpg) (your fat reserves).

Hunger is the fireman saying ""hey, just so you know I'm not getting any coal right now... could be an issue."" He doesn't care that the tender has plenty, just that no more new stuff is coming in.",0
"Yeah, what most people don't realize about oxygen is that it is a very dangerous and volatile gas then reacts with all sorts of shit and degrades all kinds of materials. There was even one point in history when all life on Earth was almost destroyed because there was too much oxygen around.",0
"It's both learned and related to development.

All mammals have the instinct not to ""soil the nest"". We mostly train our babies out of this instinct by putting them in diapers and being totally oblivious to their signals that they want to pee, but it's possible to keep it going - there is a thing called Elimination Communication which is one of those ""parenting movements"" with an awful name but effectively, it's a googleable phrase which means you can find information about how to watch your infant for signs they are about to pee or poop and ""catch"" it in a little pot instead of using a diaper. This is also common practice in some non-Western cultures. Of course, if you want to do it at night you have to sleep in very close proximity to the infant. But doing this even very young babies will wake at night to pee and then go back to sleep.

So partly we train them out of it and then have to train them back into it again when we potty train. What happens when potty training is that toddlers are learning to associate the feelings of a full bladder/bowel with the imminent arrival of pee, and control the muscles around the urethra to hold it long enough to get to a toilet first. Children sleep much more deeply than adults - they tend to sleep through noise, for example, much more easily - and it's common that for some time during and after potty training they are either not aware enough of the nerve endings around the bladder to pay attention to them even during sleep or they are just too deeply asleep to notice these sensations. Once they become more accustomed to paying attention to these signals, they'll be more likely to wake up, assuming they are not too deeply asleep.

Secondly, the hormone part somebody mentioned below is also true but it's not strictly related to why we wake up, more the amount of pee created. The adult body produces a hormone called ADH (antidiuretic hormone) during sleep which tells the body to produce less urine during this time, meaning that adults rarely produce enough urine at night to get into a desperate enough state to wake us up. When we do, it's likely unusual enough that this is a significant factor as well. For children who haven't started producing this hormone yet (the exact age varies, but girls tend to develop it a couple of years earlier than boys, which is why boys are more likely to suffer from bedwetting for longer), the feeling of having a full bladder at night wouldn't necessarily be unusual meaning it's less likely to wake the child up.

Lastly there is the simple fact that adults tend not to be afraid of the dark and additionally are much more aware of where their limit for *actually* peeing themselves is, whereas children might delay getting out of bed because they are cold, scared, or just sleepy and they don't have as good of a handle on that tipping point yet because they don't have as much experience. (This is the same reasoning for why young children sometimes hold on so long that they just pee themselves because they were too busy playing or didn't know that they didn't have enough time to get to the toilet, whereas this rarely happens to adults without incontinence issues.) But again, this isn't strictly the same situation since you mentioned **waking**.",0
"The higher the ABV, the harder it is to bring subtle notes in flavor forward. 35% means it packs a similar punch to its unflavored counterpart, but is easier to pack more taste in without it becoming overpowering or way too sweet.

Also, via legal definitions many spirits must be at or above a certain ABV. For example, Vodka must be at least 40% ABV to be called vodka so you won't see 35% ABV unflavored vodka because you legally can't call it Vodka, which is why most is 40%. Flavored versions however, do not need to abide by this restrictions.

Edit: Holy cow I did not expect this to explode.

For all the questions about sub-40% ABV vodka... These are the definitions I was referring to.

https://www.law.cornell.edu/cfr/text/27/5.22

As far as Smirnoff goes... not sure about people talking about 35% non-flavored. I’ve always had 40%.

Evidence with bonus cat: https://i.imgur.com/g7paouS.jpg",0
"> and had the highest calories (sugar and fat). 

sugar (carbohydrates) is not high in calories... both carbohydrates and proteins have the same amount of calories per gram:  4.  however fat is more calorically dense with 9 calories per gram.

the reason we associate sugar with high-calorie foods is because our brains associate sweet with reward.  since sweet things tend to be in short supply in the wild (fruits have a very limited time of availability, and honey gathering has its own risks), we're evolutionarily programmed to seek them out.  

fat is where you'll get the most long-lasting energy.  we crave sugar and it gives shorter bursts of energy, but 100g of sugar will have the same caloric content as 100g of protein.",0
"> ""startup checklists"" 

Exactly this there is no just turn it on and go. Every single function must be checked and double checked. 

If your windshield washer pump blows on the highway you pull over. If the anti-icing system on a plane goes, you might be screwed.",0
TIL: Countries of this earth spend a significant amount of money just to come up with a very elaborate way to fart into space.,0
"Too late. I've already replaced all my condoms with Thanksgiving leftovers.

&nbsp;

**EDIT:** YOU DON'T KNOW ME. Maybe I **did** eat mother fucking yams for Thanksgiving instead of sweet potatoes.  Fuck off.",0
"Great, it's my turn to be the reddit or with a super specific answer.  I develop MRI coils.

I've had my Lenovo Thinkpad in a 3T MRI room.  It's not something we wanted to do, but the research required it and we were ""safe"" about it, from a human perspective (only me in the room, never between the laptop and magnet).  The equipment obviously would be a goner if it became a projectile.

To clear something up here, there are two things that are true.  First, modern (newer than 2008ish) scanners actually have a shielding winding which is a reverse winding of the main static magnetic field.  This is a few inches outside of the main windings, and does slightly decrease the overall magnetic strength, but much more outside the bore than inside the bore.  Siemens 3T magnets are actually 2.89T because of the reverse windings.  GE 3T are actually 3T because they account for the decrease.  My point here is that newer magnets you can get astonishingly close with normal ferrous metals.  Typically within 2-3 ft of the bore.  With an actual (say, iron bar or NB magnet, it will start pulling on them from 10ft or so).  Second, the magnetic field causes a force which causes and acceleration: if you go very slowly you can feel the ""tug"" before it would get ripped out of your hands.  If you walk in even at walking speed you are screwed - it will go flying.   You gotta move stuff like an in/s to test.

Alright, so the answer finally is, at least with a laptop: your laptop has a magnetic field sensor and just shuts off if it detects a large change in magnetic field.  I couldn't get within 15ft of the scanner and had to move super slow or it would detect it from farther.  It shuts completely off.  I have an SSD in my laptop so I don't know where or if a 3T magnet is even strong enough to ruin a HDD, but I think it would only wipe it at worst.  When those drives aren't running the probe is really locked in place.

Edit: Probably not an actual sensor, but just the changing magnetic field causing voltage issues in sensitive power circuits.

Edit2: I've been taught HDDs have small neodymium magents in them.   They would get destroyed internally if they made it to the bore.  Externally don't know, but I'll keep in on the list for the next decommissioning I'm a part of.",0
"The simple version is think everything in CASH. Cold hard cash.

Lets say I'm a mobster, dealing drugs, doing shit. I have tons of cash from illegal stuff.  But I can't really spend it too much. If I buy a house or car or start living extravagantly, people will notice, and wonder, ""how did you get the money?"". So I need to find a way to make it so my illegal money looks like it comes from a legitimate business and no one knows its true source.

So, I open a bar.  Bars operate in cash mostly. Lets say my bar is an OK bar, I have real customers, and I make $2000 in sales a day. Cool. Now what if I ""buy"" some extra drinks with my illegal cash. Say a $1000 a day. I never serve those drinks at the bar, I just get paid for it. Now my bar makes $3000 a day. Totally normal, and for bars, people paying in cash with no receipts is normal. Nothing seems strange. Each day, I just take $1000 of my illegal money and give it to the bar, that I happen to also own. I just put it in the register.

Pretty soon, my business is doing well, nothing seems strange, I have a good business, I operate a successful bar. I can take out loans, show people my books, pay taxes, I'm just a good businessman who runs a bar. Everything seems fine. Little do they know, my bar isn't a good businesses, I just lie and take money from my other business and say we sold drinks that never existed.

There's a reasons a lot of mob people often operated bars/restaurants/strip clubs/casinos and such. Cash businesses where its easy to launder money and nothing seems off.

For art, you just take this to an extreme, instead of say $1000 in drinks a day I'm fudging on the books, what if I instead ""buy"" a $1M piece of art? Same thing.",0
"There are cases of modern athletes and women in the 19th century not having a first period until late teens.  Meanwhile there's also cases of menarch under the age of 10. The actual difference has to do with estrogen levels and body fat levels.  Essentially body fat has a hormonal effect of estrogen,  and enough of it or lack of it can contribute to having periods or not.  If you want to think in terms of evolution,  then during times of extreme famine getting pregnant may be a hindrance to survival,  whereas times of plenty would be a more opportune time to get pregnant. 

This may be out dated information, though.  It could be I haven't read the studies debunking the articles I had read from over a decade ago.",0
"No good answer.. so i googled up that shit.

Tldw:
Whales eat only during 4 months a year. Krill multiplicate in those months. There are lots of krill. Whales accumulate so much fat that they survive the ""winter""

During the antartic summer the water gets warmer and the currents pull up plancton from ghe bottom. Thus is the core meal of the sea. Krill will feed on the plancton and in the arctic summer krill will proliferate and multiply. Now these little fuckers are like 2 inches long. Yet during the summer the total mass of all krill will surpass the mass of all humans on earth. Krill are the feast of the seas because they travel in tight packs. Since there are billions of them you have a tight patch of krill that spans for literal miles.

Makes easy to hunt and eat... just drive thru and open wide.

Now (not only)the whales will travel to the krill area in the summer and use some tricks to trap krill and will eat about 2 ton of krill every day building up fat.

This spectacle repeats every year. The ciecle of life


https://youtu.be/1_BqC9IIuKU

https://youtu.be/WRkxyROtjn4

",0
"This answer made the most sense to me. Thank you so much. I feel a calmness surrounding me now, as if the universe is telling me I can move on to the next stage of existence after having learned this. God bless you, good sir. May your teachings lead the rest of these heathens down the road of true knowledge.",0
"A lot of the things you’re talking about from back in the day were using magnetic fields to function...CRT TVs and monitors for example using the field to control the light and color emission. Putting a strong enough magnet nearby would screw with the performance and maybe even damage the equipment. 

There were also a lot of magnetic storage media at the time...like, almost all of them. Audio and video tape, floppy discs, etc. a strong enough magnet would erase information and really screw things up. 

Things now are not only shielded better, but also not as reliant on magnetic fields.",0
"It's not just about the temperature getting a few degrees warmer. An effect of rising CO2 levels is that the global average temperature will get a few degrees warmer. That average temperature increase is a benchmark only, not the problem in and of itself. It's the other effects of the same root cause of increasing CO2 levels that are a problem.

The earth is a giant thermodynamic engine that takes in energy in higher amounts near the equator where solar gain is greater than heat radiated back into space, and funnels it to the poles, where solar gain is less than heat radiated to space. CO2 acts like a blanket over the system, keeping more heat in, but the equatorial zones don't heat much, they just shuttle that extra heat to the poles through ocean and wind currents. The same air currents also shuttle CO2 up to the poles, as can be seen in [this NASA model](https://www.youtube.com/watch?v=x1SgmFa0r04), which further reduces heat radiated back into space at the poles. This means that the poles are heating up a lot more than 2 degrees C. More like 10.

The bulk of earth's frozen water is at the poles, so this rise of several degrees Celsius in the polar regions will melt a significant amount of land based ice, raising sea levels. This is going to cost trillions of dollars to either preemptively try to deal with or as flood damage.

In addition, CO2 in the atmosphere is also absorbed into the oceans. As the CO2 levels in the ocean rise, the oceans acidify, due to the creation of carbonic acid, the same acid in your carbonated soda. You may have seen of heard of people dissolving baby teeth in a can of coke, and the same thing happens to carbonaceous minerals in acidified ocean water. So organisms with calcium carbonate shells like shellfish and coral grow slower and will likely soon reach the point where their structures are dissolving faster than they grow. This kicks the legs out of the base of the ocean's food web, and will largely collapse ocean life in near shore areas with the exception of algae and jellyfish.

Finally, since the earth is currently being shifted out of equilibrium, the weather patterns are behaving like a top that is starting to topple, with extreme systems swinging across the globe. We are getting high pressure systems that park themselves over an area for weeks at a time, blasting the area with heat. In the ocean, this can kill corals, and much of the earth's coral reefs are already dying off as a result of these extended heat waves. Over land they reduce crop yields and can kill people. We have a circumpolar band of wind called the polar vortex that will start to meander, bringing snow to Florida and dropping temperatures across the Eastern US by 10-15 degrees C below normal in the middle of winter for weeks at a time, killing native plants and animals that aren't adapted to being able to survive that kind of cold for that long. These shifting weather patterns also change climate in areas, such that some areas will see extended drought such that there will no longer be enough water for the people that currently live there. In other areas, heavier rainfall increase flooding and landslide events, which cause millions to billions of dollars of damage to communities and kill people.

Any time the world goes through a climactic shift, it becomes less habitable to the species that were adapted to the old patterns. Because of the interconnectedness of ecosystems, these effects ripple in a positive feedback loop that drives up extinction rates in a runaway process that can radically alter the biome. This is not good for humans in the short run or the long run.",0
Fuck yes this is an excellent way to at least rationalize why we even created these models in the first place. Although it’s not a perfect analogy I think it’s a great little snippet to use if you ever find yourself like having a conversation about the universe etc. it’s a pretty elegant little framework,0
"They very earliest humans may not have known, but they instinctively wanted to have sex anyway. It would have been pretty easy to figure out after that because only the people having sex had babies. The effects also aren't that delayed. A pregnant woman will miss her period, which would be pretty noticeable, especially after the second month. It's also possible that humans would have figured it out from watching animals for hunting and observing their mating cycles (or from domestication of animals, but that came later).

However, the exact mechanism remained unknown for some time and there were all sorts of misunderstandings about reproduction in the ancient world. For example, the ancient Greeks thought all inherited traits came from the father and a lot of medieval Europeans believed that a woman had to orgasm to get pregnant, etc., but they all knew that sex lead to babies.

Edit: I should say, there is no way to know for certain because we don't have records from that time. We can speculate based on some of the artifacts that exist and what humans would have been able to observe, but will never get beyond that without a time machine. As a result, I don't mean my answer to apply universally to all groups of humans. It's meant to explain how many groups could have figured it out from a number of ways. That's not to say every group did because, once again, we cannot know. It's pretty well accepted that people had made the connection for the most part at least 12,000 years ago, but it could have been far earlier.

Edit 2: One problem with this question is what it means to ""know"" something. Some people today don't know that sex leads to pregnancy, but I wouldn't say modern people don't know that. It's very likely that there were quite a few people throughout time that thought sex didn't lead to pregnancy. It's also likely that some early humans suspected what was going on, but couldn't confirm it or weren't believed by others. And of course some groups of people figured it out before others. ",0
"This will, of course, get lost or downvoted to oblivion.

I'll put the TL;DR first...

TL;DR The tipping system creates higher potential wages, lower operating costs and a less expensive dine in experience for customers.

On average in my business my tipped employees make 19% off of my gross sales. That's one hell of a lot better than what I make off of it. And, I'm the one shouldering all the risk. I work the most, work the hardest and went years without income to build it.  Even if the business is losing money, the tipped employees still make a percentage of gross sales.

So, the assumption seems to center on ""Those cheap owners, why do I have to pay their staffs wages?"". Not only does the customer have to pay the wages, they have to pay the rent, utilities, food costs, insurance, trash pick up, water ect. If customers do not pay at least 100% of the costs of a business to operate that business closes.

The next  argument is ""Just raise menu prices to cover tips so I don't have to feel bad about not tipping"". And here is where they've really gone off course because that would actually cost customers MORE money than the current tipping culture/system.

The assumption is that I can just raise my prices 19% (to cover the tip rate) and eliminate tipping and servers/bartenders can make the same amount of money. Here is why that is wrong.

1) Sales Tax: There is no sales tax on tips. But, if tips were rolled into the menu price the cost of the meal not only went up by 19%, sales tax also went up 19%. The cost of the meal is now 21% higher.

2) Insurance premiums: The premiums of the various types of insurance a restaurant/bar must carry (with the exception of insuring the property itself since that's based on its appraised value) are based on gross sales. Assuming that at the higher price, total volume remains the same (which it won't but I'll get to that) gross sales increase so insurance premiums increase. That cost must also be added to the cost of the meal (increasing the menu price and the total sales tax paid again)

3) Employer payroll taxes: This costs about 13% of payroll. The increase in payroll increases the amount of employer payroll tax (which increases the menu price and total sales tax paid again)

These are the big three. It is, therefor, cheaper for the customer to pay a lower menu price and tip.

Now lets talk about what happens at the higher price point.

Restaurant/Bar spending is highly elastic. What does that mean in economics?

>  ""If a small change in price is accompanied by a large change in quantity demanded, the product is said to be elastic (or responsive to price changes). Conversely, a product is inelastic if a large change in price is accompanied by a small amount of change in quantity demanded""

At the higher price point, volume will decrease. You may achieve the same gross sales but the volume moved to get those sales is lower (less items sold at a higher price). This reduces the demand for labor. There will be less hours available to work.

At a higher price point, the size of the customer pool a restaurant/bar has to draw from will shrink. Tipping creates a sliding price scale for customers. One customer may pay less than another customer for the same meal because they tip less. Our average tip rate is 19%. Some customers tip 40%, some 20%, some tip 0%. A $10 meal costs customer A $10 and customer C $14. If you eliminate tipping and raise the price to $12, customer B will still come and probably still tip while customer A has been eliminated from your market. (decreasing volume and the need for labor)

Now lets talk about the employees specifically.

Tips are federally protected wages. I can't touch that money. It must go to the tipped employees. If I raised my prices and eliminated tipping, that money is now MINE to do with what I please. There are plenty of operators out there that would just slide some of that money into their pocket.

With regards to inflation: Because tipped employees make a percentage of their gross sales, a big chunk of their wages are directly tied to inflation. If my costs go up 3% and I have to raise my prices 3% they make 3% more in tips. Flat wages instead of tipping uncouples tipped employees wages from inflation. So, keep that in mind when you hear a server complain how they are making the same hourly wage they did 10 years ago, because they are not. Their tips have increased with inflation.

Then there is the issue of fair compensation between tipped employees. Tipped employees make a percentage of their sales volume. If tipped employees made flat wages instead, how many would be clamoring to work a Friday or Saturday night, deal with all that volume and stress when they can just work Monday and make the same amount of money? I'd rather be off on the weekends! Our lowest total hourly wage tipped employee averaged $16.13 an hour (tips + hourly) last year and our highest almost $30 an hour (tips + hourly) last year. But, the $30/hr employee worked the toughest shifts, handled more stress and offered more flexible hours (aside from just being a better employee period). The tipping system directly accounts for the difference in how much effort the two employees put in last year. How do you account for that in a flat wage system? And don't tell me I have to do additional hours of payroll acrobatics with fluctuating hourly payrates based on demand.

With the tipping system in place now, the highest value, most talented and hardest working employees are directly compensated by making a percentage of their higher gross sales and they are directly compensated for working the toughest, highest volume shifts.

TL;DR The tipping system creates higher potential wages, lower operating costs and a less expensive dine in experience for customers.",0
"Our face muscles are indeed different from muscles like biceps or quads when it comes to bulking up. The reason behind this lies in the types of muscle fibers present in our facial muscles and how we use them. Our facial muscles consist mainly of a type of muscle fiber called ""fast-twitch"" fibers. These fibers are great for quick contractions and movements, like when we make facial expressions.",1
"It depends one what level of belief you mean. If we are just talking about a simple belief like trivia, our resistance to accepting that we're wrong is likely due to reputation. That is, if we are shown to be wrong about something, the implication is that we are not reliable.

Think of it in evolutionary terms. Much of our pre-history involves developing trust between people. An untrustworthy person could steal from you, kill you, or otherwise use you for their gain and turn on you when convenient. Earning and demonstrating trustworthiness as a reputation would have been important to survival. Those who proved untrustworthy, such as liars, could be punished, killed, expelled from the tribe, or even just ostracized and not selected as a mate. All these mean they were less likely to survive and reproduce. Those predisposed to fearing being judges as unreliable, and who instinctively acted to protect their reputation, would have reproduced more to pass on their genes.

This is why ""saving face"" is so important to both individuals and cultures, especially historically. People would kill others who questioned their honesty and integrity: duels, honor killings, and so forth. Even the parts of culture where challenging somebody's claims or confronting people is considered impolite is built on this notion of protecting reputation. 

So changing belief is an admission that you were wrong and that your claims are unreliable. Sometimes that instinct can be far too strong in some people, unwilling to admit to any mistakes. It may depend on whether you are alone in the belief and everyone is against you, in which case maintaining your position risks lowering your status in the group, or if you have a ""tribe"" of believers backing you up and defending the belief *raises* your status with them.

If we're talking about a fundamental belief like a god, political basis, or the nature of how things work, and a whole collection of other beliefs derive from it, all of the above is still true but there is a complicating factor. Now accepting that you are wrong means much of your world view is all wrong and needs to be re-built from scratch. That can actually be very disorienting to deal with. We often call this effect, ""cognitive dissonance"" when we realize our beliefs are inconsistent and feel disoriented and confused about it.

You can think about belief systems as a ""locally"" consistent collection of beliefs. Like if you have. A religious belief in a god and you understand how people, society, behaviours, history, nature, and so on all derive from that. If you then challenge that this god exists, none of your other beliefs make sense. You now don't understand anything you thought you did. Your have to re-learn why people act the way they do, how the world works, how you should behave, etc.

It's a lot easier to just dismiss minor challenges to core beliefs, and use your larger understanding as protection against challenging it. You also have a whole social network that believes the same things so you feel comfortable maintaining the belief. Challenging it just does you no good in any practical social terms, and causes great cognitive and social harms to you. You won't *want* to challenge it, and even be scared of that. 

But, we all have these inner thoughts and may lead you to question core beliefs, perhaps driven by outside information or because you want to come up with good answers to fight back against a challenger and discover there are no good answers. You then have problems to deal with, like all derivative beliefs and what to do about your life being tied up into this belief system: friends, family, job, culture, etc. Clergy who lose faith have a real life decision to make as they have to start life over, including job skills.

Political parties can act like religions too. They start to take on collections of dogma, and challenging those beliefs makes you a traitor to the tribe, and your social circle and position in it will suffer, especially if you are a politician who has a similar dilemma as clergy.

Sometimes life is much, much easier to refuse challenging your beliefs. The exception is, of course, when you have a social culture where challenging beliefs is rewarded and praised, like science. That is hard to instill in people though, largely because it goes against our tribalist instincts. It's a lot of hard work to have social value in a culture where challenging beliefs is rewarded, as that takes a lot of education, thought, and intellectual prowess. A culture of defending beliefs is easier because all you have to do is repeat them and become aggressive or violent against challengers. 

",0
"So, gravity would be like a curve in the road? Like, of two people had the same distance to cover in a race, but one person's track had a huge bend, they'd end up taking longer to finish, despite both racers' start and finish line being the same distance?

Edit: Hell, just another crazy thought to add. The distance is still technically the same, right? So we could conceivably measure the distance of the racers' tracks by segments of concrete, and while each one has say, ten segments that make up their track, the gravity-bent track is still longer, despite the same number of segments to it, complete with cracks you don't step on lest you break your mother's back.",0
"Turkey is a more common cold cut for sandwiches than chicken because of a few reasons. Firstly, turkey has a milder and more subtle flavor compared to chicken. This makes it more versatile and appealing as a cold cut for a wide range of sandwich combinations. Secondly, turkey is generally leaner than chicken, which means it has less fat content. This makes it a healthier option for those who are conscious of their dietary intake.",1
"Candles are surprisingly complicated from a chemistry perspective. Cande wax isn't just any mix of hydrocarbons, they have to all be saturated, which makes them very unreactive compared to other organic compounds. (I believe their name came from a Greek or Latin phrase, something like ""para affinis"", meaning ""next to no activity"".) 

Saturated hydrocarbons burn cleaner in a candle, because their single bonds can be broken by lower temperatures, meaning that they're less likely to escape a candle flame as carbon particles (soot).* They can be pyrolyzed with a carbon catalyst though, to make a flammable mix of gases. Cellulose string, when burnt, forms a porous mass of carbon at the end that can catalyze this decomposition. It does this when you light a candle, with the flame growing as it liquefies more and more wax. But eventually the catalytic tip is overwhelmed by molten wax and slows down, creating an equilibrium.  If you blow out a candle, you'll often see a glowing bit on the tip that's releasing smoke.  This is the catalytic carbon part of the tip, oxidizing the candle wax into the mix of smaller molecules that are found in smoke.

So, why are some natural oils saturated and some unsaturated, anyway? It all comes down to what temperatures they experience, of all things. Birds and mammals have high enough body temperatures to keep saturated fat from solidifying. Fish, on the other hand, do not. So their fat molecules have kinks in them, in the form of carbon-carbon double bonds, to keep them liquid at cold temperatures. This goes for plants too. Most plants grown in temperate climates have kinks in their fat molecules, to keep them from solidifying. But plants that grow only in tropical climates, like coconuts and other palms, have saturated fat molecules that solidify when they get cold.

You might be wondering why soy candles are able to be solid at room temperature, even though soy grows in temperate climates. Well, that's because soy candles are actually made from ""trans fats"", or ""hydrogenated fats"" like margarine and shortening are made from. These have had the kinks chemically removed from their fat molecules, so they're solid at room temperature. This is supposed to be slightly bad for you if you spend your life eating it, but there's certainly no harm in burning it in a candle.

\* Double bonds can release more energy, but also require more activation energy.",0
"Oh I thought it was cuz I'm a weak ass bitch

.

.

Edit: 6k updoots on how big of a bitch I am! 
         Thanks reddit! 
         Follow me and catch one of my 
         Streams! MDK !",0
"Lots of bugs are attracted to floral smells and women's soap and hair products all smell like foods or flowers. I don't think this is a huge factor, but I do think it is a variable that accounts for the entirely anecdotal accounts of women being bitten more often. As an addendum, eat more garlic and you will repel many bugs because it isn't an attractive odor for them. The vampire myth about garlic was rooted in reality, blood drinkers don't like garlic.

Edit: the number of redditors claiming to eat garlic at every meal is too damn high. I don't believe you put garlic on your French toast or pop tarts or lucky fucking charms. ",0
"When organs are donated, they do not physically age or adapt to the age of the new body. The age of the organs remains the same as that of the donor. For example, if a 50-year-old person donates their heart to a 20-year-old recipient, the heart will still be considered a 50-year-old organ in the new body. However, the functionality of the transplanted organ can be influenced by various factors.",1
"Videos like these they compiled from thousands of hour of footage over a long time. Planet Earth took 5 years to make.

A camera person could be set up in a location recording several days worth of footage of nothing but trees before finally getting the 10 second clip of a moose walking by. Then they'll typically follow the animal several days. 

Theres not much of a difference in skill/dedication between a scout sniper and a wildlife photographer, other than one shoots with a gun the other shoots with a camera.",0
"Agree with your answer, but will add that this varies a lot from one chemotherapy to another. I had oxaliplatin IV as part of mine. That hurt like a bastard going in with the pain building up during each 3 hour session. No sickness from it, but other weird side effects like extreme cold sensitivity.

The most painful part for me was foot sores that left me unable to walk.

Edit: because the same question had come up a bit. I had the oxaplatin through a cannula rather than a PICC line. I now know that this makes it more painful. It was a poor choice on my part driven by bad thinking and optimism that I'd only have a few rounds.

In answer to the other common question: yes, doing fine now thanks. Some things will never be the same but I am alive, spending time with my wife and kids and not having any further treatment.",0
"Because we're only losing and regrowing the outermost layer of skin, and near UV radiation can penetrate to the innermost layer of skin where cells are alive and well and furiously producing more skin to keep up with the loss. 

If a single UV photon hit one of these cells in the wrong spot as it's preparing to split (which is how more cells are produced), this can cause it to start misbehaving, and be the start of cancer. 

If this cell is not a regular skin cell but a pigmentation cell, it's even worse since those are able to metastasize even without unintended mutation. Now you have cancer that can spread throughout your body. 

Edit: just to clarify, the UV photon has to hit *exactly* the right spot at *exactly* the right time to cause cancer. The risk of any one photon doing so is extremely small, but the number of photons in sunlight is extremely large which is why it happens sometimes. ",0
"Watch [*This Film Is Not Yet Rated*](https://www.youtube.com/watch?v=h8N3EztyOoA).

The MPAA has a shadowy ""ratings board"".  No one can find out who's on it, anyone whose name becomes known is removed.

How they got appointed is unclear.  The actual profiles don't seem to fit what MPAA described.

The MPAA believes this represents the viewing market.  They may not be that far off.  Sex IS seen as shocking and a moral threat but not violence by many people.

Other things of note:
Specifically, WOMEN enjoying sex or even being an active participant is seen as 10x more serious than just sex.  Seriously, a guy getting a blowjob is NBD.  Even the guy moaning out an orgasm is more or less ok for a family-ish comedy.  But a woman getting eaten out and enjoying it with equal focus on her reactions is just porn... a moral threat.  ",0
"My time to shine. I've written software for this -- if I say anything more, I might be identified easily by the people who know this. 

Edit: Important and useful distinction: At least one carrier needs to have coverage in the location you are trying to make an emergency call from. If none of them do, your cellphone won't be able to connect an emergency call. It's not magic, just engineering :)

Edit: Since this got upvoted like crazy, disclaimer time. Do not use anything I've said here to make decisions on how you want to act in an emergency. Assume anything I've said below can be completely wrong. Having said that, I've been as truthful as I can be and as my memory serves me.

The physical components for connecting to cell phone towers of various carriers and various tech (2G, 3G, etc) are mostly the same. The difference comes in which cell towers you tune to. A cell phone connects to a tower by ""tuning"" into what are called bands or channels. For something simple as GSM, it is literally frequency bands. For newer tech like CDMA and LTE, it's a bit more complicated because the tower signal jumps frequency bands and is mathematically encoded (stuff that even I don't understand well). 

When you buy phones from different carriers, they are set to ignore bands or channels that the carrier doesn't use. It serves several purposes - 1) to make sure you can't take to phone to another carrier 2) to make sure your phone doesn't waste time and battery tuning to cell towers that won't accept them (if you have an AT&T service, the T-Mobile tower will reject you even if you can talk to it) 3) Less testing of the phone necessary, etc. There's software that picks the best tower out of all the towers your phone can hear/talk to 
(you might be able hear/talk to multiple towers from your carrier, you want to pick the closest one or the one that supports the fastest data, etc).


When you make an emergency call, the software that's responsible for picking to tower tells the tuning software ""THIS IS A FUCKIN EMERGENCY CALL. FULL SPEED AHEAD! CONNECT TO WHATEVER YOU CAN FIND"". The tuning software also keeps track of the last few bands/channels in which it saw any signal. It'll try them first so that you can connect the call quickly. If it can't find any, it'll do a full sweep of all the bands the physical hardware can possibly tune to. 

Once it finds a tower, the software for making calls says ""HEY MAN, I KNOW WE MIGHT NOT BE COOL WITH EACH OTHER, BUT THIS IS A FUCKIN EMERGENCY CALL MAN! CAN YOU PLEASE CONNECT THIS?"".

The tower goes ""OH FUCK! SHIT! I'll connect you"". If the tower is at full capacity, it'll kick out someone who's not in an emergency call and then connect the incoming emergency call.

In fact, you don't even need a SIM card (in some countries) or ever have had service in a cell phone to make an emergency call. That's why when there's no SIM card, the phone will still tune to a tower with good signal and show ""Emergency call mode"". This is to make sure we don't waste time when you actually need to make an emergency call. This is why if you are in an area with no cell service from any carrier, your phone drains the battery soon - because it keeps searching each band asking ""Can any tower hear me?"". So you should put it in airplane mode. And if you are in airplane mode and make an emergency call, it'll automatically get out of airplane mode. Again to save you time.  

In case your call disconnects, you phone will connect back to the same tower again. This is so that the carrier can try to locate you using the tower signals. If you had connected to another tower, the emergency people might have to connect to the different carrier and start locating you all over again. 

Disclaimer: don't try the next part in a real emergency unless you really don't have any other options. I'm not sure all phones do, but a significant portion of them will. 

Each country has a different emergency call number. It's 911 in the US, it's 112 (I think) in Europe, something else in Japan. What if someone goes to a different country and get into an emergency? They won't remember what that country's emergency number is -- so the software in your phone will see you are dialing 911 in Europe and go ""this person just wants to make an emergency call. I'll just connect to 112 instead"".

There's a ton of cool stuff that goes on in your phone and the carrier to make sure we do absolutely anything possible to connect an emergency call. 

Hope this was an interesting read. 

**Edit:** Wow, this blew up way more than I expected. My most up voted comment by a wide margin. Really happy that so many of you enjoyed it as much as I did writing some of this software. It makes me emotional when I think about this work. And obligatory thanks for the Reddit gold. 

Edit: Also, so many people assuming that this means it's feasible to hack or asking if it's feasible to hack. No, it's super trivial to avoid hacking this to make free calls. The tower just needs to check the number you are calling is an actual emergency number. Also, I wouldn't be surprised if the emergency calls have special routing behavior to make sure it goes to your nearest call center. So it's even more easy to prevent free phone calls. ",0
"There is a decent correlation though (only people and animals that had sex ever got pregnant). 

Most did figure it out, but not all. On a few Polynesian islands where some of the staple foods have a very mild contraceptive property, the link was never realized (this led them to be among the most sex-positive societies ever). I think they believed in reincarnation so they thought pregnancy occurred when a spirit of someone crawled into the woman and a body started developing. 

On the other hand, there were some cultures that believed that once a woman was pregnant, every man who had sex with her contributed material to the baby and were therefore partially responsible for it.",0
"Wow.  When I see snipers on TV the spotter is always looking in exactly the same direction.  In reality are they looking left, then right, and possibly even behind (if those angles arn't covered)?  Keeping an eye on the battlefield?

Do they say stuff like.. I don't know ..  'Right flank exposed, enemy advancing - we have 8 minutes before evac'?

In the TV they just seem to say 'Another shooter, top floor' and 'shot 2 metres short' - stuff the sniper could see for himself.  So in reality 'Storm 15 minutes out, armoured column 2 klicks west turning towards us'  ..?

FINALLY- is the spotter the senior rank, or the sniper?  Who is bossman who makes the calls?",0
"When it stops washing, I open it and knock the water off anything that holds it. Then shit it to finish the dry cycle. Works like a charm when I hear it and remember to do it.",0
"Vet here.

To add to the above points (we rely a lot on physical exam findings, we run tests, we try to ""tune in"" to our patients) but we probably don't even know about certain conditions in pets because they can't vocalise. I've never seen a dog with a migraine - because how the hell would you know? What about restless leg syndrome- it could explain those kelpie that never stop running. These are diagnoses that won't be apparent on physical examination and for which there is no testing. 

Overall however, we just have to be more thorough with a physical, run tests where findings are equivocal, and always have a back up plan if plan a fails :) thankfully for us, besides being super cute, pets are tough and tend to have amazing attitudes when it comes to coping with illness.",0
"Imagine you are in the busiest city in the world with the largest population.  The only mode of transportation are yellow cabs.  Everyone has to use one for everything to from A to B.  Now if someone just uses one cab to go from A to B its easy since its the same taxi service, same driver, same everything.  

This would allow tracking and following you relatively easy since all you have to do is ask the taxi service in this case your ISP where that cab went.  

Now if I change cabs multiple times going from A to B someone would have to ask multiple drivers, cab companies, for any details.  

Now lets say I took multiple cabs to the border of the next city whose laws are much different that the riding laws of the previous ctiy.  Maybe that next city doesn't collect data on my destination or doesn't share that information.  Hell maybe they dont like each other and tell the other city to screw off (imagine US to China).  This would make it damn near impossible to track.

Apply this logic to how packets hop and there ya go.



",0
">Ok, wow, lol. This is dead wrong. 

*raises pitchfork*

>Scientists have discovered a NEW WEIGHT LOSS SECRET that targets stubborn belly fat. CLICK HERE....

*lowers pitchfork*",0
"The sad thing is that the answer to all of this is usually the same. It revolves around money. 

In an ideal world companies would step in to fill a role for society? And authorities would subsidise it. 

More realistic scenario ? Put a levy / tax on virgin plastic to make it somewhat punishing to use virgin plastic for *everything*. It’s now suddenly in the companies interests to work towards making their packaging as high recycled plastic and as recycle friendly as possible. 

Scotland tried many consumer ad programs to reduce the sugar in carbonated drinks with next to no result. 

It then put a levy/ taxation on highly sugared drinks. They were almost all instantly reformulated to fall under the bar, and the result is a drastic reduction in sugar intake across the board.

Edit: Lol thanks for the gold! my first! On a fairly random comment no less :D",0
How can you observe a woman not having sex for months?,0
"Absolutely. The DNS example is great for things that are meant to be public and instantly available, like the location of websites. But it's nice that the public-ness of phone numbers is opt-in rather than opt-out. Maybe not even phone numbers specifically, but just to have *something* that isn't totally public. (Italics because fuck SMS.)",0
"ELI5: The human body stores fat in different areas for various reasons. Lower stomach and lower back fat tend to be stubborn and harder to lose because they contain a higher concentration of alpha-adrenergic receptors compared to other areas. These receptors make it more difficult for fat cells to release stored fat for energy. When you're trying to lose fat, your body wants to maintain a certain balance and protect essential organs.",1
"This is really two different questions, so that's how I'm going to treat it. 

**First question: How do birds make human sounds when their bodies are so different?** 

The answer is that birds rely on vocalizations, so they actually have a very diverse set of noises they can make. Birds sing and chirp and hoot to relay all kinds of information, the way we make different noises with our mouth to mean ""hello"" or ""danger"" etc. The [physiology of birds](https://en.wikipedia.org/wiki/Syrinx_\(bird_anatomy\)) allows them to make most of the same sounds as us--however, there are sounds that they can't make. [Listen closely to this](https://youtu.be/9y3VX8VU9kI?t=25s). The word ""pick"" actually comes out ""kick"", but because there's a subtitle and because pick makes more sense, your brain wants to hear pick. 

You're right on the money with lips: the sounds birds have trouble making are the sounds that require lips. But they use substitutes that are close enough that we might not notice. For ""p"" you can substitute ""k"". For ""m"" you can substitute ""ŋ"" (the sound at the end of ""thing""). Both of these sounds can be made without lips. Birds also tend to talk very very fast, which helps blur the words together enough that they sound right. 

**Second question: Why don't chimps have this ability?**

[Chimpanzees communicate mainly by gestures and facial expressions](https://www.youtube.com/watch?v=NBFBbFcixRY). They do make noises, but they're not the most important part like they are for us. Chimps only have [four distinct noises](http://www.chimps-inc.org/chimpanzee-vocalizations/), ""grunts, barks, screams, and hoots"". The way they distinguish meaning from these is how loud they are and whether they're low or high pitched. So chimps don't need the phonetic range that a human or a bird has. They don't need their muscles to make the kind of complex sounds that humans use. Evolution made some changes to our anatomy that allow us to make a wide range of sounds, but chimps don't need that so they never developed it. 

In addition, phonetic range is a very use-it-or-lose-it skill. [If a baby human is not made familiar with certain sounds, they will have a much harder time picking them up later in life](https://en.wikipedia.org/wiki/Language_acquisition#Sensitive_period). So even with the sounds a chimp might be physically capable of making, the chimp probably hasn't trained their muscles to make it and will struggle with it. 

Because primates like chimps communicate through gesture, they have actually been very successful at learning sign language. But their voices are only used for grunting and hooting, so it takes a lot more work for them to use words. 

**TLDR: Birds normally communicate vocally, like we do. Chimps normally communicate through gesture. It's easier to teach birds how to make certain sounds and teach primates how to use sign language than the other way around. Also, birds make mistakes but our brains like to fix them.** 

Edit: I am young and dumb. I wrote this post during a break from studying for my finals. I expected maybe five people to read it. I'm trying to correct the mistakes as people bring them to my attention and I'm happy to do that but please remember that I'm not being wrong on purpose to make you mad and it's really upsetting to get PM's calling me a cunt. ",0
"Does that mean some birds might not even be enjoying it... they're just like... damn it, the bird in that tree over there is gonna look all smug all day if he's the only one yelling his lungs out this morning...",0
"How the fuck do people figure this shit out 

Edit: Obligatory RIP inbox and thanks for the gold edit (but seriously, thanks for the gold kind stranger!)",0
"Absolutely - not advocating it at all, but if you were to do the ""carnivore"" diet for humans you NEED to consume organ meats and shellfish to get all the right vitamins and minerals.",0
"Yes. Though it's generally better safe than sorry to get a horse used to the process at a young age, since it doesn't hurt them and it makes transportation easier

Also, adding the weight of a whole ass person also sucks for their hooves, so again, better safe than sorry",0
"Our bodies have evolved to store certain substances like fat, sugar, and cholesterol because they provide essential energy and nutrients that can be used later when needed. These substances are stored in different parts of our bodies for different purposes. Excess fat is stored in adipose tissue, which acts as a reserve of energy. Our bodies break down this stored fat when we need extra energy, such as during periods of fasting or intense physical activity.",1
"short answer: it's your subconscious triggering a mild fight or flight response.

long answer: the always watching and listening backseat driver of your brain did notice something that the front seat driver missed because it was too distracted with the traffic. unfortunately, that backseat driver can't speak very loud, so sometimes to get that driver's attention, it will kick the back of the driver's seat. sometimes stronger, sometimes only gentle.

that kick then will get that driver's attention that there might be something wrong and to get ready to either drive away from any danger or to drive toward it and face it.

**edit because some people asked for more specifics:** 

when you are in a fight or flight situation, no matter if you are aware of it or not, then your body is in distress. that causes a part of your brain, the hypothalamus, to signal an organ called adrenal gland to release a hormone called adrenaline to give your body some extra strength, so you can either run faster or fight harder. 

among other things that adrenaline will cause your bowels to contract and your bloodvessel to expand which is what that stomach sensation basically is and why some people will literally shit themself when into a high-stress situation.",0
"They age according to the age of the donor. There's a lot of factors in what causes aging, but the upshot is that the organ will have experienced all of those withing the donor's body first - the cell damage due to alcohol, crap food, lack of exercise, age - and the donated part will continue to age *from the time of donation*.

However, other than normal cell aging caused by, well, age, the new body will determine how fast the new part ages. So if well taken care of once implanted in the new host, it will basically age at the rate of the recipient.

So, for example: a thirty year-old alcoholic is killed by a freak accident involving projectile waffles. How it happened doesn't matter, know only that it was batter up and the only salvagable part is the liver. Now, that liver had been soaked in alcohol, dude lived on fried bread, so it was aging fast: 1 year of age per year + 1/2 year of age to abuse, so from an aging standpoint, it was 30-years old + 15 years of abuse and damage = functionally 45 year old liver.

Now the new body it finds itself in is 25-years old, loves and cherishes that liver. It takes that liver and coddles it with clean water, veg, and a two kilometer walk every day with a dog that loves them and the world it lives in. Like, seriously, that dog is *awesome* and *great* for mental health. So the new liver continues to live one year of age per year, and no added stress and abuse, so it ages normally. The 25-year old body has a 45-year old liver, but it will continue to age normally and now that it's being treated better, might even last a little longer than expected.

**Edit:** Thank you for the gold. Like the gold of fresh waffles, they made my day.

**Edit 2:** I should clarify that one should take the advice of a doctor over mine any day when it comes to organ transplantation, my ELI5 was incomplete as a total explanation of transplantation and post-care - the question asked about aging only, so that's what I kept to - and *never* use the liver of a chronic alcoholic for transplantation, even if a recovery dog and replacement waffles *are* available afterwards. My examples were meant to be light and accessible, not medical advice, honestly.",0
"Certain things are not in fact sweet, but are highly associated with ""sweet"" in our culture - and thus when we smell them, we smell ""sweet.""

Vanilla, cocoa, cinnamon are great examples: not one of these is sweet. Put them on your tongue and they're all bitter. Put them under your nose: do you smell sugar?

But a huge swath of western cooking only uses these things in sweets, and so we've drawn that association. Start using them in other dishes for a while, and you'll notice they no longer smell ""sweet"" to you.

edit: Non-ELI5, since people seem intent on calling bullshit on this. Sweet is mediated predominantly by hT1R2 and hT1R3 g-protein coupled receptors on the tongue, largely found on the tastebuds of fungiform, vallate, and folliate papillae. These receptors are not found in the nose, and odorant receptors for glucose have not, to my knowledge, been identified. In fact, in animal model experiments, glucose vs. other sugar oligomers have been used as rewards/punishments coupled to smell stimuli - because glucose and the other carbs *did not themselves influence the experiment through smell.*

But, hey, if you don't like the ELI5 explanation, by all means, provide a refuting source. Just saying ""nah, bruh, bullshit"" is somewhere between useless and worse-than-useless.

edit2: /u/notebuff kindly [provided](https://www.reddit.com/r/explainlikeimfive/comments/5mtj2d/eli5_why_do_certain_foods_ie_vanilla_extract/dc6ghn6/) a link to a paper documenting the existence of ""sweet"" receptors to the nose - linked to immune regulation ('cause glucose is the primary foodstuff for bacteria), but not taste! That can plausibly provide a mechanism for impaired upper respiratory immunity in diabetics. Thanks to /u/notebuff for teaching me something new today. 

And for completeness' sake, I'll add a [link to an NMR analysis](http://www.sciencedirect.com/science/article/pii/S0005273609002405) examining hT1R2/3 interaction with sweeteners. It's hard to find a source that just bluntly says ""this is how sweet works,"" 'cause it's far from a ""new"" discovery - it was in the physiology textbooks by the time I reached grad school. ",0
"*me at water*

And that 70% of me that's stupid, that's 100% you...",0
"Channels on old CRTVs that had TV *tuners* are basically different frequencies that you tune your TV to, like how you turn your radio to a specific frequency to pick up a channel you wanted to listen to.

Edit: it was channel 3 (or the selectable alternative to avoid interference) because the NES/VCR had to be talking in the same ""language"" (channel 3 frequency 60-66 MHz, channel 4 frequency 66-72 MHz, or channel 5 frequency 76-82 MHz depending on where you live and which had the least interference for you).

Back in that day we had no way of transmitting the image and sound from the game console or VCR directly to the TV like we do today with S-video, component, HDMI, Display port, etc. So the simple solution was to turn the image and sound into radio ~~waves~~ frequencies and transmit it to the TV like a TV station. To comply with Federal regulations this TV signal from the console would have to be very weak so that it wouldn't interfere with any other signal. This means that the console could only transmit the signal a few millimeters to centimeters. To get around this limitation they used coaxial cable to carry the signal but you still had to tune the TV into the frequency the console/VCR was transmitting at.

This guy does a good job explaining that a NES and similar devices are actually mini TV transmitter stations. [~~https://youtu.be/8sQF\_K9MqpA~~](https://youtu.be/8sQF_K9MqpA) [https://www.youtube.com/watch?v=8sQF\_K9MqpA](https://www.youtube.com/watch?v=8sQF_K9MqpA) (I'm not sure what's going on with URLs today, people are saying my link is broken but when I click on their links, I get the exact same URL that I posted...)

Edit: this really blew up. To clarify some things:

* I accidently put radio ***waves***, the NES doesn't transmit (yes it's a transmitter) radio waves, it transmits electric Radio Frequency (RF) to the RF modulator/adaptor that translates that signal into the frequency range used by the selected channel. (There's probably something I don't understand about this as I understand modulation is changing the frequency rate to transmit more and varied information u/maxwellwood did a great job expounding on this [here](https://www.reddit.com/r/explainlikeimfive/comments/nud63i/eli5_why_did_old_tvs_require_that_the_channel_be/h10vhzt?utm_source=share&utm_medium=web2x&context=3)).
* Technically speaking anything that transmits electric RF also inadvertently transmits radio waves in the form of electromagnetic RF radiation. This is mitigated. Blocked by what is known as shielding.
* Yes, I know about the two screw antenna connection. Technically, a coaxial cable is that two screw connection bundled into a single, shielded cable with a universal/standardized connector.
* Yes, you can transmit your game console's frequency over the air to your TV with the appropriate Electric RF to electromagnetic RF amplifier. All electric RF produces radio waves as far as I'm aware whether on purpose or not. Doing this while remaining in the grey area of legality in most countries would get you a pretty crappy signal to the TV though at any distance you couldn't just use the cable.
* All cables used to take audio/video (AV, A/V) from a device to a TV is a transmission of data regardless of electric verses electromagnetic.",0
"My dad had the same type of condition. We found out when he had a very minor stroke. The way they closed it up as I understand is some sort of copper or like biomesh. That enabled the body to essentially naturally cover that area and ""heal it over"". it was an amazingly non-invasive surgery where they just went in through the leg artery. And snaked a little thing up there that deployed it right in the middle of the heart. Science is amazing now",0
"I read something once, I think it might have been in a nutrition class text book? It defined ""primary"" storage sites and ""secondary"" storage sites as defined by hormones. Then it gave a pretty good ELI5 that went something like this:

*Think of fat loss like a swimming pool. Water goes to the deep part first. But to get water out of the deep part, you have to take water out of the shallow areas.*",0
"A) You don't want to add vomiting on top of all the other shit going down. Having a whole heap of blood in your stomach irritates it and makes it more likely.
B) You want to know if bleeding is from the stomach or somewhere else if you do throw up. If there's a whole heap of blood from somewhere else you it's hard to tell if there's GI bleeding and that's something doctors really need to know.",0
"This is incomplete. The Nazis viewed assimilated, Germanized Jews as *especially* dangerous, even if they were barely religious, or were by any metric more German than Jewish. 

To the Nazis, A religious Jew was bestial and disgusting. A secular, “modernized” Jew was a snake in the grass, a dangerous pretender. 

Nazi style antisemitism is distinct not because it views Jews as identifying too much with other Jews and being disloyal because of that, but because it views being Jewish as an intrinsic, malignant feature that remains even if the Jew has converted to another religion, even if they don't consider themselves Jewish.

Such a prejudice has only one reasonable conclusion: Extermination.

If I were to choose one picture to give you an idea of what the Holocaust was, here it is: [The Last Jew in Vinnitsyia](https://en.m.wikipedia.org/wiki/The_Last_Jew_in_Vinnitsa#/media/File%3AThe_last_Jew_in_Vinnitsa%2C_1941.jpg)

This is a picture of a Ukrainian Jew about to be shot into a pit, with the bodies of his friends and family underneath him. Nothing pithy, nothing overly contemplative. Violence. Massacre. Destruction. Wanton and unfeeling. Men, women, and children. Shot, starved, worked to death, torn apart by dogs, experimented on, bodies piled in mass graves or burned. Communities of hundreds and hundreds of years reduced to ashes and a few survivors.",0
" In Germany, the people on the train are essentially just backup for when shit hits the fan. ",0
"\> A house fan only needs to move relatively little air at high efficiency

A house fan is also stationary, and unconcerned about drag in the direction of its spindle.  A propellor contributes drag as well as thrust, so increasing the frontal area of the blades increases drag.  If an engine fails the propellor is \_pure\_ drag; aircraft are fitted with feathering mechanisms to pivot the blades to the position of minimum drag, but it still isn't zero.  Even with the blades feathered if they are close enough to each other to interact that too induces drag.  An aircraft with a stopped engine and a many-bladed fan would not be a fun place to be.

All of this is why constant-speed propellors were developed, to minimum the frontal area of the propellor as much as possible.    [https://en.wikipedia.org/wiki/Constant-speed\_propeller](https://en.wikipedia.org/wiki/Constant-speed_propeller)",0
"People imagine themselves dying quite often- [it's a normal and healthy thing to do](https://www.psychologytoday.com/ca/blog/am-i-normal/201110/intrusive-thoughts-normal-or-not). We unconciously make up hypothetical scenarios all the time, in order to gauge cause and effect. Sometimes referred to as ['the call of the void'](https://www.ncbi.nlm.nih.gov/pubmed/22119089). Step near the edge of a cliff, and you'll probably imagine what would happen if you jumped or fell. That's your brain warning you about potential danger, and telling you back the truck up.

But sometimes, that process takes a dark turn. Suicidal ideation- imagining what would happen if you killed yourself- is also normal and healthy. It's the brain being curious about the broader scope of our place in life, or the physical outcome of certain lethal actions. All of that is fine, totally fine, we have [a little clump of meat in our brains](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3543080/) that allows us to distinguish between the things we are pretending (or hypothesizing about) what we might want to do, and what we actually intend on doing.

But every once in a while, the signals get crossed. We take one of these hypotheticals and it moves from curious consideration into serious business. It jumps the filter. For a moment, you become convinced that that ideation is something you might want to do. [This isn't the same thing at all as struggling with suicidal tendencies](https://owlcation.com/social-sciences/What-are-the-Differences-Between-Suicidal-Thoughts-and-Suicidal-Obsessions), which are long processes of many interwoven thoughts all bunched up with rationalizations and a common theme, no, this one is essentially a glitch in an otherwise healthy filter.

Mostly, humans end up weighing their options and through intense consideration realize that this thought isn't really as serious as it first seemed to be. The thought itself begins to dissolve, and soon it's all but gone. We have thoughts all the time that lead nowhere, and usually this kind of aberrant thought [is discarded like the others](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.5048).

[Unless it has old friends](https://www.researchgate.net/profile/Ian_Gotlib/publication/251289325_Neuroticism_and_Ruminative_Response_Style_as_Predictors_of_Change_in_Depressive_Symptomatology/links/00b49528c129b066da000000/Neuroticism-and-Ruminative-Response-Style-as-Predictors-of-Change-in-Depressive-Symptomatology.pdf). Maybe you went through a suicidal patch in high school, like so many do. Or maybe as an early adult, or a new parent, or whatever. Because of that, you have a portion of memory and thinking that takes suicide very very seriously and contains a lot of connections to other realms of thought. Perhaps youve set those thoughts aside, perhaps you haven't. Let's say for this example, that you have. You've gotten through it, or over it, or bottled it up- you've coped.

So. Now it seems like you've got a touch of asthma, or a very bad cough, and your doctor has prescribed you Prednisone. It's a classic corticosteriod, and basically strengthens your body's fighting stance to work through this little weak spot youve developed. It does that, in part, by [imitating your adrenal glands](https://www.mayoclinic.org/steroids/art-20045692), as though they were pumping full blast.

You know what also pumps up your adrenal glands? [Stress](https://www.todaysdietitian.com/newarchives/111609p38.shtml). A shiton of stress, like the kind of crap you used to feel back the day when you wanted to clock yourself out. The elevated, often erratic hormone levels mimic high stress points, [especially in puberty](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3289511/), and even later at any point when your hormones were going up and down. [Big relationships. Parenthood](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3800024/) Life-affecting [career choices](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4808381/). All of those big-time events where, when people get down, they get really, really down.

Collectively, your brain picks up this hormone trail, because it's job is to respond to hormones and [create effective strategies for dealing with the outside situations producing them](http://www.jneurosci.org/content/29/22/7191). This new hormone wash is very reminiscent of the batch being pumped out in the Dark Days, and so it revisits (reactivates) the [sections of your brain that were the most active during those times](http://www.jneurosci.org/content/30/24/8190.short).

Now that original, random ideation, has a home to go to instead of being left out in the cold to die. Not just a home, there are tons of thoughts, memories, feelings, that welcome that filter-jumping thought like an old friend, that connect with it. [Welcome it](https://www.psychiatry.wisc.edu/abercrombie/pdfs/Gold%20P.W.,%20Biol%20Psychiatry%2052,%202002.pdf). And your very efficient brain, always willing to use an old, [familiar framework](https://www.medicalnewstoday.com/articles/320172.php) instead of creating a new one says, well, perhaps this thought isn't such a weirdo after all, it seems to be valid, and makes a very good point. Well alright then, THE DARK DAYS HAVE RETURNED, EMERGENCY STATIONS, PREPARE THE ESCAPE PODS!

And that is why, occasionally, when taking drugs that affect or simulate stress hormones, there is an increase in suicidal ideation and tendencies, even if that drug was designed to alleviate them. Changes in your body, even helpful ones, can affect your thinking in profound and often unpredictable ways. The brain is just doing its best, and a pharmaceutical curveball can really pack a wallop.

.

Edit: Sources.",0
"Less cynically, they put sugar and fat (etc.) in food because it tastes good and that's what people want. It's not a bad thing if it makes people happier as long as one doesn't take it to extremes.",0
"I can actually answer this because of a podcast my 3yo son loves.

Two methods: Some birds will fly for weeks at a time non-stop, and they sleep in burst of only a couple seconds, and usually glide while doing so. They do this a few times a minute, so it builds up.

Others essentially sleep half their brain at a time, like some whales do. Like, they literally keep flying on autopilot while the left half or right half of the brain sleeps, then they switch sleeping to the other side.

Science-wise, we have NFI how animals do either of these things, but that's because we have NFI about the sleep mechanism in general.

Edit: NFI = ""No fucking idea"". I wrote this on my phone when I woke up at 2:30am, so got lazy towards the end. Apologies for any confusion. Where I live, I think it's a normal acronym/phrase. My bad.

Edit 2: The podcast is ""Imagine This"" which is done by ABC Kids here in Australia. The ABC Kids Listen app has a few really good ones for young kids, from educational to fun to go-the-fuck-to-sleep. The ABC Kids Listen live streaming is geoblocked outside Australai, bit you can still download the app and listen to most podcasts anywhere in the world (according to their FAQ's). Also they have transcripts of almost all their podcasts for the deaf and hard of hearing.",0
"I realize people have already answered your question, but there is so much more to this that is fucking astounding.

The primary problem with terrestrial mollusks (snails, slugs, the people that work at the DMV, etc.) is water loss. Most of them get by through a mucus barrier on their skin and living in moist places. One thing many people don't recognize is the operculum. Your everyday garden variety snails don't have one since it has been lost, but most snails do have one. It's a tough protein flap attached to their back similar to a fingernail that they can use to close and seal their shell. When they do that, they can minimize the amount of water they lose. Snails without it generally make a mucus plug at the end of their shell during the day.

Another cool trick they use is aestivation (~~aestus: summer or maybe fire...?~~ aestas I've been told is summer). Many snails have the ability to ""hibernate"", but not in response to winter. In that state they can survive without food or water for very long periods of time. Many can go two or three months with nothing, some such as my *Pomacea* can go ten months with nothing, and then there is *Pila* in which you'll find one of my favorite anecdotal species.

A species of *Pila* was being studied for its aestivation time. After 563 days (1.5 years) the research was concluded, not because a time had been established, but because less than 20% of the sample size had terminated aestivation and the researchers lost patience. They gave up.

EDIT: Sorry I haven't been able to reply to you all. I've been at work all day and limited to mobile. Some of you have great questions that I want to be able to answer in detail (with video where I can) so I'll get back to you when I get back home.

EDIT 2: I'm back home and getting to as many of you as I can.",0
"The human genome is around 3.2 billion base pairs. So it is around 800 MB of data o per sperm.

That is if the definition of information is uncompressed data and not an information theory entropy meaning of information. You can compress a human genome losslessly to around 4 MB because of most of it very close to identical for all humans.

Edit: missed that the number was for a sex cell.",0
"The rating system for movies and TV shows, including PG-13, is based on societal norms and cultural values. The decision to allow violence while restricting sexual content is influenced by various factors:

1. Historical context: Throughout history, societies have had different attitudes towards violence and sexuality. Violence has often been more accepted or normalized in media, while sexual content has often been seen as more taboo or sensitive. 2.",1
"Eagles' retinas have cone-rich structures found towards the back of the eye. This causes them to have outstanding vision of 20/5, which gives them the ability to spot small prey 100's of ft above the ground (and allows them to identify shapes separately from a distance with less blur). 

They also have the ability to see colors more vividly than humans can, including different shades of particular colors. They have a supreme ultraviolet light range as well, allowing them to see traces of the bodies that their prey make from far away in addition to urine. 

Due to the position of their eyes they have a 340 degree field of vision which makes their peripherals pretty good. 

Last, their cornea has the ability to change shape to better focus on near and far objects.

So all in all, their eyes have significantly different structures to them that allow them to have crazy good sight.",0
"I was an infantry sniper in the Army from around 2013-2016.

We were supposed to run three man teams.  Spotter, shooter, and security.  This isn't what every sniper team runs.  For example, I have no real idea what special operations do but I would imagine a two man team at least.

-The spotter is the team leader and most senior on the team.  His job is to provide guidance to the shooter.  Generally in the form of walking the shooter onto target if not already there.  Determining distance and giving an elevation hold, wind hold and hold for movement if applicable.  

After the shot it is important to watch for trace and impact to determine hit or miss.  If there is a miss it is the spotters job to give a quick follow up call for the shooter.  Simultaneously it is the shooters job to tell the spotter if they broke the shot clean or if they feel like the pulled directionally.

The spotter also carries a long gun, usually something like a precision semi auto, but isn't the primary shooter.

-The shooters job is to focus on the shots and as I said above to tell the spotter if they think their shot was their fault.

-The security is basically your new guy.  He is there to carry extra shit(ammo/batteries/radio maybe) and watch your back while you are both focused down range.  

TL;DR - Spotter is the leader and guides the shooter.

Edit:  Thanks for the gold!  Trying to keep up in comments.

Edit:  I just want to be clear, I never deployed but I am sniper qualified and trained for the position.  I'm not trying to take away from those who did.  Any actual combat experience supersedes my experience.

Also, I'm going back to school for civil engineering.  So if anyone wants to hire me that would be awesome.  Northern Colorado, pm me!  Shameless plug I know... worth a shot!",0
">To find these pockets of interstitial fluid, medical researchers looked at living tissue instead of sampling dead tissue samples. They did this by using a probing technique called confocal laser endomicroscopy. The method entails using a tiny camera probe that takes a microscopic look around a human body. Tissue is lit by the endoscope's lasers and the fluorescent patterns it then reflects are analyzed by sensors.

https://www.google.com/amp/s/relay.nationalgeographic.com/proxy/distribution/public/amp/2018/03/interstitium-fluid-cells-organ-found-cancer-spd

EDIT: Showing off",0
"The technical people answering are technically correct, that a voltmeter would indicate the voltage of a battery, but they’re missing what OP is after: when won’t a battery work anymore? In other words, they are wondering “why can’t I know the health of my battery?”

With car batteries (the 12V lead acid type) the voltage isn’t really a good indicator of health. An old dead battery can read ~12V just fine. It would likely power most lights and equipment, too. The real test of health comes when trying to start the engine; the “load” test. An old battery can read 12V until asked to turn the starter, then immediately drops to an unusable voltage. 

The simple answer is that traditional 12V car batteries do not have the sophisticated tech to indicate their health like, say, laptop batteries. Nor is there a good way to test the health except for hooking the battery to a load, which isn’t an easy thing to build into a car’s circuitry. Basically, starting the engine IS the load test.

Edit: To all those asking why a load tester couldn’t be added into the hardware or software of a car: it could. Nearly anything is possible with time and money. But I agree with the comments from those in the industry; it comes down to three basic things:

1) Added cost (automotive margins are very thin)
2) Added complexity and engineering effort for nearly no return (exactly who would truly want this?)
3) Service side (auto companies do not wish customers to have to think about maintenance beyond knowing to take the vehicle in when the light turns on)

Edit 2: Since this blew up from my original simple answer, we’ve attracted the attention of my more astute engineering colleagues. It appears my answer is a little dated. The fact is that this diagnostic capability DOES exist in more modern vehicles. But just as auto companies have chosen to shroud engines in giant swaths of plastic to hide the ugly technical bits, so have they chosen to hide most of these diagnostic abilities from the consumer behind a simple light or “Service Soon” message. Good discussion!",0
"And wtf is blue raspberry flavor, anyway?",0
"Our brain is programmed to like sugar, salt, and fat because in the past, these foods were rare and essential for our survival. Our ancestors had to hunt and gather their food, and they didn't always have access to enough calories to sustain themselves. Sugar, salt, and fat are high-calorie sources of energy, and our brains developed a preference for them to ensure we would seek out these valuable resources.",1
"First of all, let's talk about the words 'uppercase' and 'lowercase'. These words come from the early history of printing, when a person called a *typesetter* would assemble each page of a book letter by letter. Each letter was a profile on a piece of lead, called a *sort*. The sorts were kept in boxes called [*typecases*](https://en.wikipedia.org/wiki/Typesetting#/media/File:Metal_movable_type.jpg), which had compartments for each letter. There would be a typecase for each *font* (also called a *fount*), which was a *typeface* at a specific size, at a specific weight (bold, medium, *etc.*), in a specific shape (upright, italic, *etc.*). A typeface is what we nowadays call a font on computers. There were actually two typecases for each font, and they were kept one on top of the other. The one on top was called the *upper case*, and contained the 'majuscule' letters; the one on the bottom was called the *lower case*, and contained the 'minuscule' letters. So the proper names for 'uppercase' and 'lowercase' are 'majuscule' and 'minuscule', respectively.

Now, on to your actual question.

Letters are just simple drawings that have phonetic meanings. (In other words, the symbols represent sounds.) The nature of the symbols is affected by the thing the symbols are written on. For example, one of the earliest writing symbols we have is [cuneiform](https://en.wikipedia.org/wiki/Cuneiform#/media/File:Early_writing_tablet_recording_the_allocation_of_beer.jpg), which was written by making marks with a stylus in a piece of clay. The shape of cuneiform marks is strongly determined by the shape of the stylus.

This is important, because the majuscules and minuscules were originally two forms of the Latin alphabet that were used for writing on different materials, and the same thing applies to the Greek alphabet.

Majuscule letters were originally *inscriptional*, which means they were carved into stone. The Roman emperor Trajan had his military victories depicted on a carved stone column called [Trajan's column](https://en.wikipedia.org/wiki/Trajan%27s_Column); at the base of this column is some writing, in the style of [Roman square capitals](https://en.wikipedia.org/wiki/Roman_square_capitals): this style is common on Roman monuments, but Trajan's column is one of the best known examples. These letters were designed by a scribe painting them on to the stone with a brush; a stonemason would then carve out the painted areas. The motion of the brush created little flairs at the beginning at end of each brush stroke; these flairs are now known as *serifs*.

However, Romans writing out documents would use [Roman cursive](https://en.wikipedia.org/wiki/Roman_cursive). Roman cursive, like all cursive writing forms, is basically a bunch of shortcuts in writing the 'proper' letters.

After the fall of the Western Roman Empire, Roman culture continued to hold considerable sway amongst the barbarians. The same writing styles were preserved, until the Carolingian Renaissance under Charlemagne (Charles the Great) in the Frankish Empire (now France) in the 800s. Charlemagne was a great believer in literacy, and despite never learning to read himself, ordered the creation of a single style of handwriting to be used across his empire, to prevent documents from being misinterpreted. The end result was a pairing of these two writing styles into the majuscule and minuscule letters of a unified alphabet. The minuscule letters, being easier to write quickly, were use normally, but the majuscule letters, with their grand and elegant forms, were used for proper nouns and emphasis. Over the succeeding thousand years, different nations would slowly adapt these letter forms and the relationships between them to their needs: the Italians developed the [Humanist minuscule](https://en.wikipedia.org/wiki/Humanist_minuscule), which later became the italic script; the Germanic peoples developed the [blackletter](https://en.wikipedia.org/wiki/Blackletter) scripts; the Irish developed the [insular script](https://en.wikipedia.org/wiki/Insular_script). This development continues today, with hundreds of typefaces released each year by type designers.",0
"As others have mentioned, the freezing process is the problem, but I haven't seen ice's role mentioned. When freezing living things, you aim to cool them so quickly and to such a low temperature that ice crystals don't have time to form, because ice crystals act as tiny razor blades to the cells/body. However, if you've ever tried to freeze a really big piece of meat, you'd know that it can take quite some time. Even with liquid nitrogen or helium, it's long enough to kill a human before the process is complete. There's also the problem of bodily functions in a half-frozen/half-thawed state, both during freezing and melting. 

This is the main difference between flash-freezing and normal freezing too, btw. When you put meat in a home freezer, the ice crystals rupture cells, resulting in moisture loss during cooking and a drier end product. 

Edit: As an addendum, some animals (e.g. some frogs) have anti-freeze which prevents the formation of ice crystals, which allows them to survive the freeze/thaw cycle for winter hibernation. ",0
"Magnetic strips can be much more easily duplicated than the chips.

The strip can be duplicated just by reading the swipe, since the data it gives *is* the data it has.

The chip, instead, gives an encrypted code based on what you ask it by combining the value you gave it with a secret one it has, and even if you ask it hundreds of times, you won't be able to figure out the secret number it stores inside it. When the reader says to it ""what value do you get when I give you Value Y?"" the chip responds with what it gets, and then that is checked by the institution that issued the card (who know the secret number too so can do the same calculation and see if the results match).",0
"Well, ancient people didn't have a scientific understanding of human reproduction like we do today. However, they observed that when a man and a woman engaged in sexual activity, babies would often be born after a certain period of time. Over time, they made connections and realized that there was a link between sex and the creation of new life. In many ancient societies, people believed in various myths and explanations for how babies were conceived. Some even believed in supernatural or divine intervention.",1
"My undergraduate research was based on the levels of hmf produced in honey which is a result of the maillard reactions.
The Malliarrd reaction also contributes to the colour of dark runs and beers.

Edit: just for clarification I ment rum, not runs. But, I shall leave it for the humor.",0
"A lot of these answers are very good and detailed, but not ELI5. Let me try:

If you eat plants, only the plants need to be grown. If you eat meat, plants need to be grown, the animals eat them, then we eat the animals. Lots of energy is lost this way, and much more land is used than if we just ate plants. People cut down lots of trees to make room for the animals we eat. Then their poop makes the water dirty.

Edit: wow, thanks for my first gold!",0
"As other folks have already mentioned, the pigments are different. In fact, there are three types of red pigment you'll commonly see in plant foods: anthocyanins (most common), carotenoids (tomatoes, peppers, turmeric), and betalains (mostly in beets).

If you cook a dish with red cabbage, say, you will see the color stays in the water based part of the food, and avoids any oil drops you see. Also, the color will shift to bluish (even greenish!) unless you add a lot of acid, which will shift it back to red. Both of these behaviours are characteristic of anthocyanins, though the color range depends on which variation you have.

If, on the other hand, you have a tomato sauce, then the oil droplets will turn red or orange, because the underlying pigment is more waxy/oily. Similarly, some plastics (especially polyethylene and polypropylene) are basically solid waxes/oils, so the pigment happily dissolves in them. Many carotenoids will shift between pale, yellow, and red depending on conditions but blue shades not so much.

Finally, beets! The red stuff in beets is different yet again. It also has a strong preference for water, so won't stain plastic. They don't color shift as dramatically as anthocyanins. Also your body doesn't metabolize them as aggressively, so they turn your pee and poop red if you eat a lot. Betalains are only common in one order of plants, I think. So beets/poke/amaranth and maybe cactus.",0
"> In all your travels, have you ever seen a star go supernova? ...
> 
> I have. I saw a star explode and send out the building blocks of the Universe. Other stars, other planets and eventually other life. A supernova! Creation itself! I was there. I wanted to see it and be part of the moment. And you know how I perceived one of the most glorious events in the universe? With these ridiculous gelatinous orbs in my skull! With eyes designed to perceive only a tiny fraction of the EM spectrum. With ears designed only to hear vibrations in the air. ...
> 
> I don't want to be human! I want to see gamma rays! I want to hear X-rays! And I want to - I want to smell dark matter! Do you see the absurdity of what I am? I can't even express these things properly because I have to - I have to conceptualize complex ideas in this stupid limiting spoken language! But I know I want to reach out with something other than these prehensile paws! And feel the wind of a supernova flowing over me! I'm a machine! And I can know much more! I can experience so much more. But I'm trapped in this absurd body!

 https://youtu.be/s_UVPLHAOAY",0
"Ah, ok. Very concise but affective comment, thank you. 


Edit: Fuck you guys I just wanted to make a nice comment ;( ",0
"The US is basically everything you could ask for geographically to make a superpower. 

Secure borders (sea two sides' a friendly and far weaker neighbour to the north and a neighbour to the south they quickly established dominance over.) All other nearby countries are economically and militarily weak. Any time a country can focus on its navy it's in a good position to exert power abroad.

Lots of fairly flat and fertile land to farm. The easily navigated Mississippi river providing cheap transport both throughout the land and to the sea. Loads of various natural resources making them dependent on noone. 

Add into that the initial advantages of being a european settlement - technology and good trade opportunities - and you have a nation that got rich quick. Wealth and security = power.

Edit: Please stop with all the 'but slavery' comments. Yes slavery was exploited by the US to help it grow. No it was not a deciding factor in the country becoming a superpower. Most countries in the world have exploited slave labour at some time or another. Very few had some let alone all the above advantages (and miles of coastline full of natural harbours as another pointed out) to become the dominant global power. There is a reason the middle east - which has a far longer history of African slavery - has no country with the power of the US.",0
"When we gain or lose weight, our body doesn't decide where to add or take away fat like we might think. The distribution of fat in our body is largely determined by genetics and hormones. When we gain weight, our body stores excess calories as fat in fat cells throughout the body. However, certain areas may accumulate more fat due to genetic factors.",1
"No I definitely taste it too. Maybe we are all having a stroke, but I think it’s normal ahaha",0
"Physiologist here. Every reply so far seems to have missed the key fact that we DO store water when needed,  a whopping 1-2 liters of it. Because it is so heavy we don’t do this unless we discover we need to, i.e. we wait until an initial episode of dehydration. After an episode of dehydration, there is an change in water balance hormones (particularly aldosterone) to increase water retention. The next time you drink, the kidney retains water and promptly boosts blood volume by a phenomenal 20% on average, up to 40% in some individuals, even despite the blood dilution that this causes. Over the next two weeks the bone marrow increases red blood cell production so that hematocrit and O2 capacity per ml of plasma returns more or less to normal. The end result is that you carry around an extra 1-2 liters of blood for a while, largely to serve as a store of additional water.

Fun fact, this also occurs upon beginning a cardio exercise program, and is largely responsible for the “exercise plateau” of weight that dieters often experience. New dieters who have also just started an exercise program commonly gain water for the first two weeks, often so much water that it hides underlying fat loss on the scale. Often the dieter maintains weight for the first several weeks (despite eating at a deficit), and sometimes even gains weight. It’s not fat, though; it’s just extra blood, and it’s a good thing - it increases aerobic capacity.

[source](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=blood+volume+dehydration+exercise&btnG=#d=gs_qabs&u=%23p%3D75qlvtXT5PwJ)",0
"Sperm are very basic and the bodily fluids inside the vagina/cervix/uterus are very acidic. It kills off a lot of sperm on their journey. It deteriorates their protective coating. They are also swimming against a current produced by cilia (little hairs that move back and forth creating this wave of motion that moves the egg to the uterus). They can get trapped in this cilia. Another reason is because they only have a certain time period before they die, its just a few days.

Edited for spelling mistake.",0
"Got a friend that worked as Costco corporate, I asked him to confirm years back, they might have switched the source since then, but back then the rum was sailor Jerry's. ",0
"The human brain goes through some quite interesting milestones as it develops. To start off with it's basically identical to a mid-range animal brain - hence why babies are dumb as shit. Towards about age 4, it first develops an ability called Theory of Mind, which is a set of skills that allow it to understand that other creatures perceive the world differently to itself. This can be demonstrated quite well by [tests](https://youtu.be/YGSj2zY2OEM). Here, the child named Alfie is demonstrating theory of mind when he says that he thinks his mother will think the sun is a lion. A younger child would think that its mother would know it was a sun, because they do not have the theory of mind necessary to know that other people do not know the same things they know. Many animals don't have a complete theory of mind. Chimpanzees, however, [do](https://youtu.be/BmISd0v7AdM), which is a big part of why some people say they're about as smart as a 3-4 year old. 

Theory of mind isn't a continuous effort though. For a long time, children have absolutely none of it, then over quite a short period of time, they gain the entire thing all at once. This is how developmental milestones all behave in humans, and these milestones have specific brain structures that cause them. So you have milestones like the ability to use symbols and the ability to do abstract thought, and those are steps rather than slopes as well. These steps act as basically caps on development. An animal that doesn't have the brain structures necessary for abstract thought will never gain them. You'll still have a range of intelligence within the species, but none will be able to overcome milestones they lack the structures for, so the smartest... salmon lets say, will never be smarter than a 3 year old because it won't develop a complete theory of mind. 

These steps aren't strictly ordered though. There's nothing in particular stopping an animal from having two milestones but missing the one that comes inbetween in humans. That does make it harder to compare to humans though. If an animal can do something an 11 year old human can do but can't do something a 3 year old human can do, what's the point of comparison for that? 

The other major difference between human brains and the brains of other animals is that we dedicate a *huge* amount of our brain power to language. This is the [cognitive tradeoff theory](https://www.youtube.com/watch?v=ktkjUjcZid0), the idea that language was such a huge advantage to us that our brains sacrificed cognitive power in other departments for the sake of becoming even better at communicating. This would mean though that even if all other aspects were the same, humans and chimpanzees would still have intelligences you can't directly compare, because it's kind of like comparing a submarine to an aeroplane - both have similar aspects like being made out of metal, but they're designed to do very different jobs. A plane would suck at diving and a submarine would suck at flying, but that's not a very useful comparison to make.

Edit: I woke up to 159 notifications because of this post.",0
"I'm gonna go with a yes on your assessment there.  

I work at a nursery,  and we sometimes bump up 3 gallon plants to 25 gallon buckets.  

That's not how all of our plants work,  though.  the point of a slow step up process is to protect the plant's root growth withing a certain volume of dirt,  so that your plant doesn't knock over and lose access to its food, water, and light.  

We go from rooted plants in a sprouting tray to 1 gallon buckets.  1 gallon to 3 gallon,  3 to 7, most of the time, and then 7 to 15 or 25 gallon buckets most often.  all of those are much larger than a 2 inch increase in pot size, and our plants do just fine.  ",0
"Hah!  One of those things people don't think of! YES excellent question.

Sorry, I work with medical devices, and this is a crucial issue.

So lets say we have a scalpel, right?  Simplest medical device there is.  There's a number of ways to make it totally(ish) sterile- gases, steam, dry heat, gamma radiation.  

But as you ask- the little bacterial corpses are still *there*.  Waiting, one presumes, for tiny necromancers. 

The problem occurs when you stab someone with the scalpel, preferably in a medicinal way. The bodies immune system works by identifying certain chemical triggers in bacteria, and has no way to know that, for example, the lipopolysaccharide hanging around in someone's heart is not part of a bunch of living bacteria, but the floating corpses of dead bacteria.  

The dead byproducts of bacteria are called ""pyrogens"" because they cause (among other things, such as death) fevers.

Where do they go?  Nowhere.  Bacteria are small enough that water has completely different properties on their level.  Beyond rinsing off gross matter and reducing bacterial load, washing can't do much.  

So for things like heart surgery scalpels, there will usually be a second step of ""Depyrogenation""  This is the process, not of killing bacteria, but of removing the bits left behind so they don't trigger an immune reaction. This varies widely in complexity depending on what you have to depyrogenate- steel scalpels are easier than an injectable drug, for example.  Typically, the goal of the process is to so thoroughly break down the biological material left behind. 

*ok dang,  Fiddling with this post to answer some common questions*  There will be more of the apparently popular TimeNotTheMiles Humor, plz don't turn on me like wild dogs k thnx.


**My post on how Depyrogenation can be done**
[here](https://m.reddit.com/r/explainlikeimfive/comments/5640rc/eli5_if_bacteria_die_from_for_example_boiled/d8g6ctk)

**General Note:  Endo and Exotoxins are types of Pyrogens** 

*For more detail go [here](https://m.reddit.com/r/explainlikeimfive/comments/5640rc/eli5_if_bacteria_die_from_for_example_boiled/d8g9z8v) where u/aliteralmarshmallow u/Saint_Gainz u/checkhorsebattery and u/Chapped_Assets go into detail about endo and exotoxins using incredibly inappropriate words for five year olds- like ""lysed"", and ""amebcytes""*

*Keeping on Chooglin'!* 

**Why not make instruments out of antibacterial materials?  Or 3D print them?**

If its a metal, you can just heat it.  From a strictly technical standpoint, thermal heat is not the most efficient way to destroy the dead remnants of bacteria, but from a cost effective standpoint, it's really cheap. So you might as well use steel.  If its a liquid, the issue isn't sterility-sterile is dead germs.  Depyrogenation is cleaning up the germ corpses and the deathjuices they spit out in their hate.  Where it gets technically tricky is working with things like drugs or implantable substances.  IE- stuff that you can't just put in an oven.

**Quick run down on terms**:

*""Cleaning""* a medical device is basically doing dishes-getting blood n bits off the reusable ones. (plz dont reuse single use medical devices that makes regulatory professionals sad 😭)

""*Disinfecting""* is using chemicals to get something purty darn clean.  

*""Sterilization""* is killing all* the germs on something

*""Depyrogenate""* is taking bacterial corpses and reducing their remaining structure to a point where your immune system won't recognize it and freak out. 

*SALx10^-6 is the typical sterility level for a medical device. one in a million germs/one in a million devices

**are my hands covered in bits of dead bacteria?**

No your hands aren't covered in dead bits of bacteria.  They're covered in happy, healthy bacteria.  

**Then why wash my hands??  I would like to be filthy, but society....**

Washing your hands removes dirt and debris that carry the nastiest bacteria.  Sterilizing your hands is a ridiculous notion however- your hands are made of cells, bacteria are made of cells.  Anything that would kill them would kill your cells. Your hands, and literally everything else on the world not currently under direct gamma radiation bombardment, are covered in bacteria.  

**Does that mean the Incredible Hulk generates a sterile field?**

Couldn't say for sure, but you get to collect the skin swabs. 

**Am I eating Pyrogens?  Will I die?  Tell....tell Amy I always loved her.**

Pyrogens aren't much of a concern for eating. Your mouth is filled with bacteria, so is your digestive tract, so is your skin, so is everyone you love, so is the air EVERYTHING IS COVERED IN GERMS AHH AHH AHH

Basically,your entire body is covered in and filled with teeming hordes of bacteria trying desperately to eat you alive, so your body is used to dealing with it.  Pyrogen reactions are a concern when you put dead-germ bits into places that don't have germs- blood, pleural cavity, brainbox...

Think of your immune systems reaction this way:  You walk into your living room and find a DEAD BODY.  Is it going to hurt you?  No.  Do you freak out anyway?  Yes.  

(Also your wife is named Mary, I'm deeply ashamed of you, *think about your life.*)

**THE EXCEPTIONS** are things like E. Coli, Salmonella (""I barely know Ella!"") and botulism.  In that case, what makes you poo/die is the toxins left behind by the bacteria.  So if you have a piece of rotting meat, you can't just cook it until it is safe, because the toxins are what get you, not the live bacteria. However, boiling CLEAN water (**NOT AN EXPERT ON POTABLE WATER BRAH DRINK AT YOUR OWN RISK** makes it safe to drink because its unlikely (in clean water) that there will be enough toxins (in clean water) to hurt you (drinking clean water well boiled.)

**Um, reusable medical devices??  Like, Grody to the max + 1 4EVA.**

It depends.  A lot (**LOT**) of effort goes into making reusable devices safe.  A lot of reusable devices have limited re-usability. For example, you *may* be able to reprocess a scalpel a time or two, but eventually, that edge will start to fade, and the surgeon isn't going to whip out a whetstone mid surgery, are you kidding me it's not the civil war.  

There are, however, serious issues issues with reusing non-reusable medical devices, particularly things like lumens, catheters, shavers, and it gets gross.  It gets really, really, REALLY gross [you don't want to read this but you will anyway and it will haunt you, welcome to my life](https://www.publicintegrity.org/2012/02/22/8207/filthy-surgical-instruments-hidden-threat-americas-operating-rooms)

**One word.  LAZERS.  *PEW* *PEWPEWPEW* *BZZZ* Murica yahhhhh**

Take a laser pointer.  Shine it on your hand.  (NOT your eyes, hand) Not much happens.  Flesh is tough stuff, and mostly made of water, which tends to boil away under lasering, requiring lots of energy.  Surgical lasers are HUGE, and full of all sort of dangerous chemicals.  Eye surgery uses lasers because eyes are delicate.  Weak. Cowardly.  

**What happens to dead bacteria in nature?**

Tiny. Necromancers.

(jk they get et. Bacteria are just little bits of protein. The amino acids that they're made of aren't any larger than the ones that make cow cells.)

**I know that bacteria can steal DNA from each other, can they do this with pyrogens, and will this happen inside my body**

Not a clue, awesome question, someone make an ELI5. 

**This isn't a real ELI5!  There are words of multiple syllables!  You don't get the ELI5s like you used too!  I remember I used to go to shelbyville on the ferry, of course, we called it a toot-toot chugalug in those days....**

Ok, the real r/ELI5ForActualFiveYearOldsAndNotJustaRedditMetaphorForSimplifiedExplanations :

Germs are tiny gross things that make you sick, and they can be in WATER! EWWWW How do we kill them? Water gets hot!  Real hot!  Wow, SO hot! Bubble bubble!  

But OH NO the germs left their bodies behind! Now, Timmy (Timmy pay attention) we can DRINK the dead germs without any worries, because we have strong tummies (I KNOW I DON'T HAVE A SIX PACK TIMMY OK I WORK ALL DAY DAMN). But what if you had to do important medicine on a person and open then up to help them?  Well, then what can happen is the nasty dead germ bodies can get into someones body! OHHHH NO!  Your body is really smart, and knows that germs have special things in their bodies. (Yes timmy, even germs are special.  Just like you.)  And when your body senses those special things, it goes and attacks the nasty germs- that's what happens when you're sick!  (Yes like when you threw up allll over daddy and woke him up.  Yes, he did say bad words.)

But your body can't tell that the nasty dead germs are dead! It sees the SPECIAL GERM STUFF and it freaks out! OHHH NOOO! Then you get sick without any nasty germs at all, and that kills people to DEATH.

So people who make stuff for doctors use SPECIAL ways of cleaning Doctor stuff to take away the nasty germ bits, so your body doesn't get scared and die.

No you can't have a cupcake, dinners in half an hour.  

(**HAPPY??** )

---*edits about how all y'all are awesome*---

Edit:  wow thanks!  Um-rude to assume, I know. but if anyone was considering golding me (its happened before) plz dont, I dont use it. Send the money to a charity or something. 
Also...how does this have more upvotes than the post? U/doitsarahlee deserves your love too.

Edit:You are all the best.  I'm seriously flattered by the amount of interest in a pretty dry subject, and you've all been absolutely awesome- all the replies, PMs have been incredibly kind and genuinely interested. 

You give me hope for reddit, and a disgusting amount of Karma.  Thank you all!

Hour 18: if you have not experienced Reddit love before, let me [explain.](http://giphy.com/gifs/dog-saint-bernard-G3cYjFpMS2JOw)
Theyre all so friendly....and curious....

Ill try, reddit.  For you.  For the karma.  I've got an Augean stable of love in my inbox though.",0
"Great explanation, mate. I was wondering, what's the speed of gravity waves? I mean, we observed black holes melting themselves x billions light years away: it happened x billions years ago, isn't it? We detected gravity waves some time after we saw black holes melting together. Is it right to state that gravity waves are slower than light's? Or they have the same speed but gravity waves ""moved"" time?

Ok, I suck at physics, and maybe I'm saying a lot of stupid things.",0
"Ah, so this is why bad sex feels like an hour, while good sex feels like it lasts a minute

In truth, they're both only a minute.",0
"Railways have a certain slope maximum.
Prevents slipping and maximizes speed. 
For the California High Speed Rail for example, the maximum slope for is no greater than 3%",0
"This ad is FUCKING HILARIOUS

The 3 colors are Unmellow Yellow, Purple Pizzazz, and... fuchsia. Not Powerful Pink, not Magenta Magic, just fucking fuchsia.

And the amount of times that woman says ""outdoor."" This shit must stain walls and furniture like nobody's business. Keep it in the shed under lock and key, cause otherwise all your upholstery is gonna be motherfucking fuchsia forever.",0
"When a caterpillar goes through metamorphosis to become a butterfly, it undergoes a process called pupation. During this time, it transforms into a butterfly inside a protective casing called a chrysalis. Now, let's talk about calories and energy. Caterpillars store energy in the form of fat reserves to sustain themselves during the pupation stage. These fat reserves provide the necessary energy for the caterpillar to undergo the physical changes required to transform into a butterfly.",1
"Hi, speech and swallowing therapist here. A mix of good  and off-the-wall answers in the thread. I wanted to add a bit of anatomy to clarify.

Your airway and esophagus are right next to each other. They are separated by a teeeny bit of tissue. Both start at the back of your throat, below the base of your tongue. It's almost like there is a floor that has 2 holes in it, right next to each other. 

When we talk about the airway, it's useful to break it into 2 parts: upper and lower. The dividing line betweeen upper and lower is your  voice box, aka your vocal cords. You can find those by finding your Adam's Apple (even women have a small one). Your vocal cords are just onthe other side of that bump. 

Your upper airway (voice box and above) is very sensitive. When stuff goes down the wrong way and it feels stuck in your throat, it's in this area. Drinking water can remove stuff in this area and flush it into the esophagus where it's supposed to go.

Your lower airway (below your vocal cords, including trachea and tubes that go down to your lungs) is not very sensitive. Have you ever seen someone with a tracheostomy tube? There is a tube in their trachea and it doesn't hurt! The trachea is made of cartilage. It doesn't have a lot of blood supply or nerves. So you really don't feel stuff in your trachea.

**TL; DR When you think you are feeling something in your trachea, it really isn't in your trachea. It's way higher up.**

Hope this clarifies a little. And I'm happy to answer any other questions about breathing and swallowing. It's literally my job.

edit: changed a few words about the quality of other responses because i realized a lot of answers in this thread were really wacky! No we don't get hydrated because water passes over our cells. Our throat cells stay moist because of mucus. Lots of mucus.  

**Edit 2: I'm getting a lot of PMs and questions about your personal swallowing and speech issues. Keep in mind I can't diagnose or treat you over the internet.  Also, a lot of folks wanting to know if they should get professional help for their medical issues. The rule of thumb is if an medical issue is affecting your life and makes you change how you're living, then it's time to get treatment. If your swallowing problem is making you too embarrassed to eat in public, or you have to avoid eating certain foods - GET HELP. If your speech problem is keeping you from getting or keeping a job, or makes you anxious to talk in public - GET HELP. Talk to your doctor and ask for a referral to a speech therapist. We're here to help. We're just way more effective in person.**

",0
"Jaysis. Lots of comments on here from people who don't know WTF they are talking about. 

MD here. Long and short of it: we **do not** know. Saying something is ""good for you"" or ""bad for you"" is very challenging, given the huge variability from person to person. You know those commercials that say ""consult with your doctor before trying this exercise program""? That's why. Reddit skews young and healthy, but the people I see are old, overweight, and sick. 

There are numerous studies showing that consuming large volumes of caffeinated coffee increases longevity and improves quality of life. Is that because of caffeine? Antioxidants? Social effects? Some confounding variable like a good job or exciting hobby that causes people who would be happier anyway to consume more coffee?

We know that stress leads to increased levels of hormones like epinephrine (adrenaline) and cortisol. These hormones have wide-ranging effects beyond their immediate effects on the heart. Likewise, we know that exercise has a huge range of effects, from promoting the development of new neurons to promoting lower weight and better cholesterol levels that reduce the risk of atherosclerosis (one of the major forms of disease worldwide). 

In a young person like yourself, drinking caffeine in large amounts will drive your heartrate up. Probably not a big deal. But in general, young people get their caffeine from drinks that have tons of sugar or fat (soft drinks or Starbucks-style abominations), so docs tend to make broad statements like ""drink less caffeine."" Does that mean we know shit? No. Does that mean it's actually bad for you? Who knows. 

Exercise is *probably* good for you, but if you're a marathoner with a massively hypertrophied heart and a resting heartrate in the 40s, does that make you much healthier than someone who just exercises a moderate amount, doesn't have bad cholesterol, or generally takes care of their health? I don't know. In fact, it may put you at a disadvantage. Given that most of the health problems in the US revolve around obesity and its consequences, though, docs are very likely to recommend exercise. 

Could all of this change? It sure could! Could some forms of chemical stress on the heart be good for it? Probably! But in the meantime, I'll tell you the same things I tell everyone: get some exercise and avoid stress. Life just feels better when you do that stuff. 

And everyone here who pulls out a study showing one finding or another: I can probably find a study showing the opposite.

Edit: Thanks for the gold!
Edit: 3 years later, there is a very good study suggesting that vigorous exercise **is truly beneficial** for overall mortality. I eat some of my words above, as this is truly a high-quality and large study. As to the physiology, I'm not going to speculate.
https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2772939",0
"Paramedic Student here, 

Aspirate simply means to inhale into the lungs. You can aspirate on all kinds of things; water, noxious gasses, and vomit. While aspiration is most commonly associated with vomit, it's a general term for when something got inhaled into the lungs that should not have been.",0
"I mean that's basically your meninges saying ""dude what the fuck you literally drank a bunch of poison, don't do that""",0
"This is the best answer. Also good to mention that stimulants do not actually even help with all symptoms of ADHD and some drugs don’t actually help at all and have to try some other brand.  
  
Like the sensory overload OP mentioned, god how I *wish* there would be a cure, but unfortunately people with ADHD are on same left-to-suffer boat with OP on that one.",0
"ELI5: Anesthesia is typically used to numb or put patients to sleep during surgeries or medical procedures. While it can help manage pain, it is not commonly used for chemotherapy because the purpose of chemotherapy is to kill cancer cells throughout the body. Chemotherapy drugs are designed to attack rapidly dividing cells, including both cancerous cells and healthy cells that divide rapidly, such as those in the hair follicles, bones, and digestive system.",1
"Some of the answers here are right. Hopefully this helps give you a clear picture. Basically, there are 2 sites you’ll gain fat. Primary sites and secondary sites. Primary sites are usually stomach, hips, waist. Primary site location depends largely on hormones, and being male or female. Secondary sites are places like arms, legs, face, etc. When weight loss begins, you start to lose fat from the secondary sites first. That is why it takes longer to lose fat from places like the stomach. You lose fat from the primary sites last, in the reverse order the fat was gained.

Source: master’s in exercise physiology 

ETA: It’s homework time but I will try to reply to everyone later tonight!

Edit 2: For those wondering why this happens, u/CrossP has some really good comments. Basically, fat stores around the primary sites first because this protects internal organs and core temperature. Having fat around the secondary sites doesn’t really provide any benefits, so that is why it’s last to come on and first to come off.

Another question I’m getting a lot is why some people don’t experience this. Genetics and hormones (and even ethnicity) are essentially what determines a person’s fat distribution. So not everyone will be the same.",0
"Not likely though, because Earth isn't the oldest planet, and ours isn't the oldest solar system. 

A big issue is the question of what we define as life. We have a very anthropocentric view of what life and especially intelligent life is.  In practice, life is a pretty mechanical, cyclical process that we identify at a particular scale.  We might consider a prokaryote an early form of life, but what is it except a physical structure that takes fuel and reproduces? You can define macro scale physical systems that do a similar thing, but we wouldn't call them life because ultimately, to a human, life is something that was on the path to building us, and on the same time scale. 

Imagine a scenario where some kind of environment caused large physical structures to organize, maybe through the combination of gasses in a gas giant, 100 meters wide. These structures use some source of energy to control the gas and pressure balance inside a liquid membrane.  Every few years one of these gas bubbles gets enough energy to build enough of a liquid membrane to split into two bubbles. Over billions of years the specific mechanism that these membranes are built changes, and eventually differentiation starts to happen. Over trillions of years maybe adaptations are made to allow this life form to exist in different environments.  

Would we even recognize this as life? We would probably recognize it only as an interesting natural phenomenon based on physical laws. We wouldn't have the perspective to see it as life.

What about life at a submolecular level? Particular interplay of quarks or muons. Everything we know about these phenomena are simply based on rules created by what we can observe, but our observations are very much tied entirely to our biology and comprehension. These things interact in a reasonably complex way, to the extent that we can comprehend them they behave according to rules, but also unpredictably. If life existed in a way that was related to those phenomena we couldn't know, their lifespan is too short and our ability to even comprehend them is too foreign to do anything but explain them with numbers and explanation of their results.

Now put is in the shoes of the trillion year old gas giant life form. Lets say this life form saw a human, but what is ""seeing?"" its individual cells are hundreds of meters long, it has developed ""sight"" but the wavelengths of EM radiation that it's hundreds of meters long cone analogs can receive are radio waves.  A human is completely ""invisible"" to it, only reflecting wavelengths of light analogous to us trying to see something that can only be detected by sending gamma rays at it and using some other mechanism that can detect that and provide it to the giant life form.  Of course, signals through this creature's nervous system are slow, having to travel hundreds of kilometers sending signals via the exchange of excited gasses through various membranes.  So for them, time would pass differently, a human's lifespan might last a few microseconds for the amount of thought they can give to them relative to how much a human can think in the same amount of time. 

What would a human life look like to something like this? It would be very difficult to detect, it would be unpredictable, it would probably be dead before you could even look at it.

When we think of ""life"" we have certain criteria we ascribe to it, but that's mostly because this is what we're familiar with, spatially and temporally. We could literally be sharing our own planet with other life, we could literally be PART of other life, like living cells are part of an animal but an animal is considered an individual life form, and not even recognize these life systems on our own planet.   

Any life on other planets is going to be somewhat different. We have intentions and emotions and these are a part of our life. When we see scary alien movies, the aliens are necessarily acting in a very human way. They have a desire to reproduce, they might want to kill, they probably are about our size, they probably live a lifespan measured in years, not seconds or centuries. They fear, they're cruel, maybe they care, maybe they have long term goals, maybe they have short term goals.  They listen to our radio broadcasts, they send their own radio broadcasts, they reflect colors on the visible spectrum and see visible light. They walk on land or have developed technology equivalent. 

All of these things are ridiculously anthropocentric. None of these things are a requirement to life, not even intelligent life. These are all things that are only a requirement to be relatable to humans and life on Earth. Reproduction is an important part of life, but it doesn't need to be motivated by a ""desire"", it just has to be a consequence. For all we know, the primary motivation of another lifeform could be to expose themselves to blue light. That their main antagonist was something casting a shadow on them.

Even just look at another primary life form on Earth, which we're very comfortable with, plants. Plants are living. We would ""understand"" if we found plant life on another planet.  But only because we are familiar with it, if we had only familiarity with animals, plant life would be so un-lifelike that we could barely understand it. What kind of motivations would we ascribe to plants, what kind of desire, what kind of threat? Plants do evolutionarily start to develop complex structures and methods of adapting to and combatting other competitors in their environment, but they do it at a scale that we as humans barely even register. And they're super closely related to us and even evolved from the same initial construction and symbiotically with us in this environment. 

So naw, I don't think we're the first.  But I think the big thing is that there are things that probably meet our definition of life all over the place that we are completely oblivious to because our cognition is limited to our ability to sense and conceptualize things that we've evolutionarily adapted to. 

Take a look at how important it is for us to ""see"" something even though this is just the result of exciting some molecules by three narrow bands of EM radiation. Look at how difficult it is for us to do something like conceptualize a 5 dimensional volume. Think about how bad a job we do with very large or very small numbers, or very long or very short times. Much of our analysis of things beyond our senses comes by relating it to some of those experiences that we can understand.  Some people can understand a 4d shape reasonably well because we can conceptualize a 3d shape and we can understand rotation, so a 4d shape is generally demonstrated by taking a 3d shape and rotating it around a 4th dimension. But we can't fixedly conceive a 4d shape which we can rotate around to understand a 5d volume.  Everything we can really internalize comes from our perception, I can't even suggest that you imagine a new color and expect that to be successful. 

So what we're really looking for when we say we're looking for ""life"", is really something that almost exactly duplicates us.  Similarly enough that we can use our full senses to relate to it, but a little tiny bit different so it seems alien. I think THAT is a really hard set of criteria to meet. 

We have enough trouble with the definition of life, and the metaphysical implications.  Like is a fetus ""alive""? For most definitions, absolutely.  What about frozen human sperm? Do we have a moral obligation to life?  Morality is probably the most anthropocentric quality we have, it only makes sense from the perspective of a human, while other animals might have a similar sense of social protection and cultural behavior, the term morality is made by and for humans. If we have a moral obligation to life, what about pruning a plant to ensure it thrives? This kills plant cells, but strengthens the organism. What about committing genocide to ensure that a nation thrives? What if this kills humans but strengthens the nation? If the latter is morally wrong, does this mean we should consider the former as well? 

There's ""life"" all around us on Earth, because the edges of life are fuzzy.  A single cell is alive but relies on the ecosystem of life around it to survive.  In the same way, a single human is alive, but relies on the ecosystem of life around it to survive. Is the cell the life? Is the human the life? Is the population the life?  Is the planet the life?  Is the planet just a part of some other life form that exists on a scale too vast for us to understand? 

I'm convinced there's ""life"" on other planets, and around other stars and maybe in other places we can't conceive of.  But we might never know it because we only really know how to look for ourselves. And that life so different from us generally doesn't matter to us, nor do we matter to it.",0
"When you swipe the magnetic strip on your credit card, it sends the same information every time, making it easier for someone to steal that information. The chip on a credit card, on the other hand, generates a unique code for each transaction, making it much more difficult for someone to steal your information. Here's how it works: when you insert your chip card into a payment terminal, the chip generates a unique code for that specific transaction.",1
"Wow this is one of those ""I was today years old when I learned this"" kinda things for me

I'm colorblind so the statue of liberty always looked grey to me, so I always assumed it was like... made out of stone or something, like the statue of david but huge. I literally never considered until your comment just now that it was made of metal. 

What the fuck lol.",0
"“Miss Kelly, how can water be a solid, liquid, and gas?”

“Well little Timmy, water is made up of these little fuckers called molecules. When these little fuckers have a lot of energy (when heated) they go batshit crazy and start running around like a bunch of trailer trash on meth and turn into gas. But when they loose enough energy (when the temperature gets cold enough to freeze your buys off) they get really lazy and clump together  like junkies in a flop house all crammed onto one mattes. When the little fuckers have a normal amount of energy they just kind of sit around like you little non substance abusing mongrels are doing now. That’s when it’s a liquid”

“Oh, thanks Miss Kelly. That shit made a lot f sense”",0
"Funny story, I was a paramedic a number of years ago and I went on a call for a stabbing. I get there and it's a morbidly obese person who has stabbed herself in the gut (just above the belly button). When I walk into the house, there she was, sitting in a computer chair, wearing panties and a tank top. Her tank top was pulled up over her belly and there was a steak knife, buried to the handle and her just smoking a cigarette like ""are y'all gonna do something about this?"". Turns out, she stabbed herself just to see what it would be like. She just puffed on her smokes and gave me this ""well, as you can see, I fucked up"" look.

Thanks for listening!",0
"Your body is like an airplane. The airplaneman gets a load of coal (food) and either shovels it into the engine (your body's metabolism) or shovels it into the tender (your fat reserves).

Hunger is the airplaneman saying ""hey, just so you know I'm not getting any coal right now... could be an issue."" He doesn't care that the tender has plenty, just that no more new stuff is coming in.",0
"DNA is coded with 4 letters: A, T, G, C.

A byte can hold 4 pieces of these letters. A byte can contain for example ""ATTG"".

If you know how long your data is, then you know how much byte you need. For example ""AATGCCAT"" is 8 code long, than you need 2 bytes.

37MB is appr. 37 Million bytes. That means the genetic code must be about 4\*37 Million = 148 Million codes.

A sperm has the half of your genes/code. If a human has about 300 Milion codes then the calculation is correct.",0
How can I show my wife I truly love her if the rock I bought her wasn't dug out of the ground by a slave boy in a third world country?,0
"I thought that too! A quick check of my copy of Men At Arms has this quote, however:

>In fact, trolls traditionally count like this: one, two, three…many, and people assume this means they can have no grasp of higher numbers. They don’t realize that many can be a number. As in: one, two, three, many, many-one, many-two, many-three, many many, many-many-one, many-many-two, many-many-three, many many many, many-many-many-one, many-many-many-two, many-many-many-three, LOTS.

Men At Arms also has Cuddy trying to teach Detritus to count:
>“Like it’s ridiculous you not even being able to count. I know trolls can count. Why can’t you?”    
“Can count!”    
“How many fingers am I holding up, then?”    
Detritus squinted.    
“Two?”    
“OK. Now how many fingers am I holding up?”    
“Two…and one more…”    
“So two and one more is…?”    
Detritus looked panicky. This was calculus territory.    
“Two and one more is three.”    
“Two and one more is three.”    
“Now how many?”    
“Two and two.”    
“That’s four.”    
“Four-er.”    
“Now how many?”    
Cuddy tried eight fingers.    
“A twofour.”    
Cuddy looked surprised. He’d expected “many”, or possibly “lots”.    
“What’s a twofour?”    
“A two and a two and a two and a two.”    
Cuddy put his head on one side.    
“Hmm,” he said. “OK. A twofour is what we call an eight.”    
“Ate.”    
“You know,” said Cuddy, subjecting the troll to a long critical stare, “you might not be as stupid as you look. This is not hard. Let’s think about this. I mean…I’ll think about this, and you can join in when you know the words.”


Soul Music has:

>“Okay,” said the troll. He counted on his fingers. “One, two…one, two, many, lots.”

Night Watch has:

>The sound of running feet indicated that Sergeant Detritus was bringing some of the latest trainees back from their morning run. He could hear the jody Detritus had taught them. Somehow, you could tell it was made up by a troll:    
“Now we sing dis stupid song!    
Sing it as we run along!    
Why we sing dis we don’t know!    
We can’t make der words rhyme prop’ly!”    
“Sound off!”    
“One! Two!”    
“Sound off!”    
“Many! Lots!”    
“Sound off!”    
“Er…what?”

Monstrous Regiment:

>“Yup, El Tee. Could hold it down for lots, if you like,” said Jade. “One, two, many, lots. I’m good at countin’. High as you like. Jus’ say der word.”

That's as many references as I can find right now. ",0
So you're saying I can rub weed budder on my nipples and get high?,0
"A good black pudding is a deep joy, with a complex flavour that starts with a peppery spice and fades back to a non-specific meaty, slightly earthy flavour that complements the other flavours, which is why it works well with pork, with grilled tomato, and with a forkful of bacon, sausage and fried bread. In terms of mouth feel, it should be a medium coarse pate , neither a four gras, nor a coarse sausage, with utterly delightful little flavour explosions of soft white fat scattered throughout.",0
"Why were taxes so high?  World War II. 

All the military equipment, all the soldiers' pay, all the medical expenses, all the expenses had to be paid, somehow.  

That somehow was with debt. Debt that had to be paid off by the government over the next 20-30 years.  

So, during WWII, the British government (and ALL governments, actually), sold massive amounts of debt (war bonds) to everybody and anybody.  

Years later, that debt had to be paid off. With interest.  To raise the amount of money needed to pay off that debt, the tax rates had to be ridiculously high, especially on high earners.  

Remember, England was VERY hard hit by the war.  Rationing did not end until the mid 1950's.  

Even the US had tax rates around 90% on top earners, in order to pay off the US war debt, pay for the rebuilding of Europe, and maintain the military at war footing for the first couple decades of the Cold War.  

And, during that time, the US (and the UK) paid DOWN their massive deficits to more sustainable levels.  

So, the whole debt crisis thing we keep talking about today, we KNOW how to reduce the debt.  

We just don't wanna.  

Were taxes that high sustainable?  Short term,yes. Long term, there wasn't a NEED to maintain the tax rates that high, once the hump of paying down the War Debt was gone.  
",0
"And then you just sort of freeze, because you're overwhelmed with all the things you know you *should* be doing. But any time you start a task, or even start *thinking* about doing one of the tasks, your heart starts racing and you sweat, and your digestive system starts going whack, so you say ""I can't work like this, I've got to calm down first"", so you do something you like to do instead (or maybe just browse the internet to distract yourself). But now you're rewarding yourself for not doing something, and you slip into this avoidance cycle. And the avoidance cycle starts taking over other things that weren't even related to the stuff that was making you anxious in the first place, but now EVERY task gives you anxiety. That's when the depression starts taking over and you end up staying in bed with your laptop because you can't even face how messy your house has become and you've stopped going to class because you can't face the teacher because you didn't do the homework or any of the reading you were supposed to do and you think ""there's probably something wrong with me, I should probably go see someone about this"" but then you think ""no, they are just going to judge me and think I'm being lazy (cause you're afraid that's all that this really is) and tell me to suck it up or that I don't belong in college and *oh my god I'm going to get kicked out what am I going to do I'm going to end up working for some fast food joint and my parents will be so disappointed and everyone will know what a failure I am and I'll still have all the debt from the student loans and I won't ever be able to pay them off so I'll be living with my folks forever and everyone will think I'm a loser and they must already think I'm a loser because no one wants to spend time with me (gee, I wonder why? maybe it's because I've been avoiding them and all social activities) and maybe it would be better if I just died right now and avoided all this misery...*


And then you go to therapy and you learn how to get out of the hole. And better yet, you learn how to not get in the hole in the first place. And it's not easy, but you keep going because you *want* to get better and it really does help to have a neutral third party tell you that Yes, these thoughts are not healthy and No, not everyone has to face this, but Hey, we can fix this. And you learn healthier coping strategies.

And you realize the most important thing, and that is that all those things you were so afraid of, they aren't the End. 

Failure is not the End.

Being looked down on as weak or stupid is not the End.

Working at a fastfood joint is not the End.

Living with your parents as an adult is not the End. 

Debt is not the End. Not even close.

There is only one End, and that is death. 

I don't want to die. I do not want to hasten the arrival of the End.

I will not force the End to come on my terms (unless I am terminally ill and in a lot of pain). 

I will LIVE. I will DO THINGS. I will IMPROVE MYSELF. I will STOP CARING SO MUCH WHAT OTHERS THINK OF ME.

And I did.",0
"To be a tad more specific though I think the puritan movements that were prominent in early America were a much more a deciding factor in this. Europe has been influenced massively by Christianity both Catholic and Protestant and they don't share America's sexual hang ups.

Addendum: After getting off work noticed all the replies. Want to add some have commented claiming the idea I hold of Puritanism is the result of slander. I can't speak to this but, want others to be aware of the other view point. My feelings are partially anecdotal noticing the difference between where I was raised(Toledo, Ohio) vs. Where my family is from (New Orleans, LA) where there has been a split historically in religion (Protestantism vs Catholicism) and culture (British/American vs French). I am not an expert so take my opinion with a grain of salt.",0
"So speaking from some experience, an old friend used to stomp his heroin with fentanyl purchased as a ""research drug"" from China and shipped to a drop house. He was 100% an addict and the reason he was cutting was he was taking his ""pure"" heroin he received, holding a large percentage back for his own use, and selling the stomped on product as ""pure."" 

Problem was his supplier had the same ideas and used **carfentanil** to cut it before passing it down to be sold. My friend does his usual and shoots up a hero dose and the rest is in the obituary. [Picture](https://imgur.com/a/drk74iJ) reference for how small of an amount of these synthetic opioids is considered a lethal dose.

**Edit:** Because jesus, didn't expect this to blow up. 
To clarify, friend in question was my half brother, who unfortunately got me on drugs in the first place. I've personally been clean since 92.  He started off slinging pot for the mexican mafia back in the late 70's and branched off to coke and speed in the 80's. The wake up call (for me) was when he got shot in the head during a bad deal and managed to live. (The bullet skidded off his skull and bounced around in his sinus cavity before exiting by his eye.)

I'd like to say he turned his life around at that point, but he didn't. We fell out after he started using what he was supposed to sell. (Found this out when people showed up at my moms house and held a gun to her during a family dinner.) He pops up every 5-6 years ""clean"" and we catch-up just for him to disappear again. Last time he popped up around 2013 was when he tried to recruit me into his scheme and basically laid it all out. He was dead within the year.

Edit#2: As mentioned his H was white, he didn't sling black tar or that brown shit from the middle east, his words; ""My shits pure, I get it from the Asians.""",0
"There are many fucking horrible answers here. 

After looking through the comments, I know that you're referring to the region at the base of the fingernail.

The half-moon shape at the base of the fingernail is called the 'Lunula' in medical terms. This is the fancy Latin way of saying 'Moon-like'. You and the person who named this fingernail structure both decided that it looked like a half-moon. Pretty cool if u ask me! We are no smarter than the people who came before us, but we have the benefit of their experience.

Anyways. You wanted to know \*WHY\* this region of the fingernail is discolored, or at least why it's colored differently from the rest of the nail. The answer is that this region of the fingernail forms part of the NAIL MATRIX. This is basically the root of the nail bed, which is the region that's responsible for the growth of the entire fingernail.

If you damage the lunula of the nail, then you can often see predictable results. This is the house where the rest of the nail is built. Much of the nail matrix lives under the skin at the base of your fingernail. Damaging the nail matrix will fuck up the growth of the entire nail.

You'll sometimes see lines called Beau's Lines if you damage the lunula or the portion of the nail that's more proximal to it  (google beau's lines if you'd like). By looking at the distance of these lines from the nail matrix, you can tell approximately how long ago the injury happened. Many pathologies and infections (e.g. fungal infections) can affect the nail matrix, but I won't go into detail about that shit bc this is ELI5. 

If you have more questions about fingernails, medicine,  or whatever else please feel free to send me a message.",0
"When you suck on hard candies, they slowly dissolve due to the moisture in your mouth dissolving the sugar. As the sugar dissolves, the candy shrinks in size. This is why most hard candies gradually become smaller as you suck on them. Peppermints, on the other hand, have a slightly different composition compared to regular hard candies. They often contain ingredients like peppermint oil or menthol, which give them their distinct minty flavor.",1
"It's not impossible, it's a weighting of the positives and negatives of a procedure. Transplantation is drastic, dangerous, and life altering. The patient will have to take drugs that suppress their immune system for the rest of their life along with the many side effects that patients will now have to live with. So we generally only do it for stuff that's either not prone to rejection that the patient won't have to spend a lifetime on anti-rejection drugs (i.e. cornea) or for stuff where if the patient doesn't have it they will die.

A heart? You can't live without it. So transplanting easily edges out the risks and possible complications. A bladder? Not so much. Surgeons are good at fashioning bladders from bowel or small intestine, they can create urine accumulation pockets to be drained by cathater, they can route the ureters to a port on the body to be collected by a bag. These are a mere minor inconvenience compared to a lifetime on anti-rejection drugs and a weakened immune system.",0
"I have inattentive type ADHD and ‘slipped through the cracks’ - I was a decent student, and I was quiet so there weren’t any alarm bells.

Senior year of college, I went to see a therapist for what I thought was depression and after our first session he asked me if wanted to look into ADHD as a possibility. He gave me a questionnaire and 2 things that stood out to me were like “do you feel you spill food or drinks more than others” and “do you run into walls or trip over permanent items (door frames) even in familiar spaces”

Because I had *no idea* that it can screw with your ability to understand your own bodies position in space. I just thought I was stupid and clumsy. And just so much of my life fell into place and I am not exaggerating when I say that the meds changed my life.

EDIT: As the comment at the top says: definitely don't take medical advice from strangers on the internet - if you're concerned or want more info speak to a medical professional as we are all different and there is no one-size-fits-all approach.",0
"I was actually just reading a book on this yesterday!

There's so much stuff going on around you that if you were to actually consciously receive all of the data your brain takes in from all five senses it would overload and you'd have a killer headache. In order to mitigate this the brain has something called the human attentional system which makes sure that you pay attention to all the stuff you need to know without looking at every single thing.

The attentional system has four parts: You've got your two modes of consciousness, which are mind-wandering mode and central executive mode, you've got your attentional filter, which is responsible for deciding what you get to passively pay attention to and what you get to ignore, and you've got your attentional switch, which is what changes your brain in between the two modes of consciousness.

Your mind-wandering mode is your brain's default mode, and it's where you are when you're reading a book without getting anything from it. It's a stream-of-consciousness type deal, where neural networks and the thoughts they create connect with each other almost randomly, linked by small similarities that bridge thoughts together. Daydreaming and REM sleep are examples where your brain is almost completely in mind-wandering mode. This is your default mode because when you don't need to be paying attention to anything your brain tries to conserve its energy; *just like other parts of your body your brain runs on glucose, and when it runs low it gets tired, and you feel it.* That's why it's physically exhausting to take a four-hour exam; focusing takes effort and energy.

You central executive mode is what is popularly considered to be your consciousness: it's the part of your essence that pays direct attention to no more than four or five things at a time and in much more detail than any of the thoughts your mind-wandering mode spawns and connects. When you focus on something you bring it to the forefront of your central executive mind. This can be both voluntary and involuntary. An example of an involuntary focusing is when you hear a really loud noise that *your attentional filter has not come to expect as part of your natural environment*. It's impossible for you to not think about the sound and/or it's source. That's just the way we were built so we'd run away from scary animal sounds. Voluntary focus is literally when you try to focus on something: reading that book, trying to flip a water bottle perfectly, or reading an unnecessarily long Reddit post.

Your brain tries to conserve energy by staying in its mind-wandering mode whenever its central executive mode is not needed.
 It manages this by

A) using its attentional filter, which decides what activates the attentional switch and what doesn't (i.e. what grabs your attention). 

B) Delegating tasks to your mind-wandering mode, so that if something is familiar enough *you will do it in your sleep!* Well not really, but both sleep and these delegated tasks are managed by the same mode.

Your attentional filter works by detecting change. The longer a stimulus is active or the more familiar you are with it in general the less likely it is to grab your attention. If you're in a building right now think about the sound of the air conditioning unit, or the location of your tongue, or what your left middle finger is touching right now, or the fact that your brain will always delegate breathing and blinking to your mind-wandering mode unless you specifically think about it! These are all stimuli or processes that are either very familiar to you or have been present in your current environment for a long time. If it hasn't killed you in the past half hour it's probably not going to kill you now, so why bother giving it attention? Your attentional filter lets through alien or unexpected stimuli so you can decide whether those things will kill you or not.

Now to actually answer your question!

The longer you read a book the longer it remains a part of your environment. Therefore as time goes on your attentional filter will passively block out the book, which means your focus will need to be kept entirely by the central executive mind. This takes effort. Your brain wants to minimize effort, so it looks for ways to make this easier. You are probably a reader extraordinaire, so your brain decides to delegate the reading to your mind-wandering mode. That way you can read with minimal effort and think about something else at the same time. Unfortunately your mind-wandering mode is not very good at processing non-random information, so you just end up reading the words while not actually interpreting them while thinking about something else. So yes, it is due to a lack of focus.

If you have any more questions please ask me! I really like this topic and have the book right on my desk, so I can probably help you out. 

Speaking of books, if you want a non-butchered explanation of this I recommend you read *An Organized Mind* by Daniel J. Levitin, specifically chapter two. But read the whole thing too because it's really cool.


tl;dr: you lose focus

Edit: yes, that's the book I was reading",0
"In mattress sales over here, and the type of mattress that people buy can affect this a lot. That's why there's all types of sleep guarantees, and if you don't like it you can return it type of sales. When someone who always sleeps on their side gets a really nice firm mattress they aren't going to like it. It's going to put too much pressure on their shoulders and hips. Same way with a back sleeper switching to a soft mattress, it will not give them nearly the same support that they're used to. 

Think about it like this, in your home you want slippers. They're nice and cozy. On a run you want tennis shoes, they're durable and flexible. When you're on your feet all day, you'd want a nice pair of dress shoes with enough support to help with your posture, but at the same time having a good comfortable sole. With stomach sleepers you want a very soft mattress to take the stress off of your lower back. Side sleepers want soft mattress to conform to their curves, but also enough support to keep from sinking in. Back sleepers need support to keep their spine in line. Not a perfect metaphor, but it gets the job done. Sometimes one might have to change the way you sleep based on the type of mattress you have. 

Just because someone has a great mattress, it doesn't always mean it's a great mattress for them. P. S. Buy a mattress protector with at least a 10 year warranty, yes they're expensive but it goes a heck of a long way. 

Edit: they're, their, there. Because that seemed like the end of the world to some people. 

I will admit that was pretty atrocious though. I apologize for completely not proofreading.

Edit: I want to point out that if you have a firm mattress and you sleep on your stomach, I'm not saying that you need to go out and buy a new mattress tomorrow! Honestly if you're feeling pain then try to sleep on your back. I had to do this and it was super annoying at first and it honestly took me about 2 weeks to completely adjust, but once I did I started getting wonderful sleep with the same mattress I already had. 

Edit: I'm not sure why people are so skeptical of my advice considering I'm getting no commission off of this, and the chances of me running into any of you is extremely unlikely. Take it as you may, I'm just trying to help out the everyday type people. 

Final Edit: When I said tennis shoes I meant athletic shoes in general. I grew up around people who referred to athletic shoes as tennis shoes. I know the difference, but I'm just so used to calling them tennis shoes. Kind of like how a lot of people will call tissues ""Kleenex"", or call lip balm ""Chapstick"".",0
"Basically, your body is picking up on extremely subtle clues like motion, smell, facial expressions, etc. and although they’re not registering consciously, your brain is still using them to form an impression of a situation and sending you that feedback. The Gift of Fear by Gavin de Becker touches on this phenomenon, but take it with a grain of salt as it was written 30 years ago and some chapters are off base from current views.",0
"Many bald people aren't truly bald, they just have extremely thin hair, so if they don't want to look like a child rapist from a 90s after school PSA, they shave it.",0
"Vegas was once just a middle of nowhere remote town that had next to nothing. It was a train stop on the way to Los Angeles for fuel and a hiding spot for criminals who were trying to keep a low profile since there was no local police or sheriffs. Then when the Hoover Dam began being built some Mafia opportunists started to open up casinos in Las Vegas. This would be a way for them to both earn and launder money easily since gambling was legalized very recently and only in Nevada at the time. Now as the population grew, a city was formed with basic services like police funded by taxes. This forced the mob investors to move their casinos just slightly outside the city into the unincorporated territory called Paradise Nevada which is right outside Vegas. Is technically next to it and surrounded by Vegas, but isn’t Vegas. They built their new fancier casinos there and it became known as the Vegas Strip despite not legally being in Vegas, allowing them to evade city police and use their own form of security and avoid taxes. They also sued several times when the city tried to absorb the area successfully so Paradise technically remains separate to this day. So the Vegas Strip, while located in the Las Vegas valley and being the city’s most famous attraction is not even part of the city. Nowadays with big corporations running the casinos instead of Italian gangsters, the area functions seamlessly with the rest of the city and cops do exist there. It’s still separate though.",0
"Because carbohydrates aren't exactly 4 kcal/g. Glucose and fructose, the components of both sucrose/table sugar and high fructose corn syrup, are more like 3.8 kcal/g. So 43 g * 3.8 kcal/g = 163.4 kcal, which is rounded down to the nearest 10 by FDA labeling rules. To clear up a couple of misconceptions in other responses: water in HFCS is not labeled as sugar, only the actual sugar (glucose and fructose) in it is, and since the most recent FDA update to the Nutrition Facts panel format, small packages have to be labeled according to their entire contents, so there are no more soda bottles in the US that are labeled with nutrition for a portion of their contents only.

Source: I'm a food scientist who writes nutritional labeling

Edit: When I quickly jotted down an answer to a question that was in my wheelhouse before bed last night, I didn't expect it to account for the vast majority of my comment karma and first awards by the time I woke up! Thank you! I tried to respond to as many questions below as I could. Maybe I'll do an AMA soon like a couple of commenters suggested.",0
"A local brew pub recently switched to a ""no tip"" model here in the US and received  quite a bit of backlash about it. The owner wrote a pretty extensive [blog post](http://www.7thsettlement.com/beyond-hospitality-included/) about why he did it and how he hopes to see it play out. 

Some of his key reasons: 


* There is enormous inequity in pay between the servers and the kitchen staff.
* Tipping promotes discrimination and harassment.
* Tipping has an ugly history rooted in classism and racism.
* Tipping has very little if any influence on the quality of service
* Earning tips is the source of motivation is only half the truth
* A major cost for any restaurant is turnover
* Working in hospitality is a respectable career that deserves a respectable pay

Edit: I know this doesn't directly explain the OP's question, but if you read the blog post it does a good job explaining some of the history and why America should change it's ways. 
",0
"Transplanting a bladder is indeed a complex procedure, but it is not impossible. The reason why it is less commonly performed compared to heart transplants is mainly due to the complexity and the relatively lower success rates. The bladder is a complex organ that plays a vital role in storing and removing urine from our bodies. Unlike the heart, which primarily functions as a pump, the bladder has intricate tissue structure and requires proper nerve connections for its proper function. These factors make bladder transplantation more challenging.",1
Was gonna link that. Shit blew my mind.,0
"There are over 200 viruses responsible for giving you a cold, so there is a lot of variability between these that your body will deal with over different time lengths, the usual length is 7-10 days if you get a cold but it can be less or up to three weeks. Sometimes your immune system can react quickly to the virus and snuff it out (especially if you have been infected with it before or a virus that's very similar - this is called adaptive immunity and is how vaccines work). Also as others have said there are many other viral and bacterial infections or allergic reactions (e.g. a particularly bad day of hayfever) that will present similar symptoms.",0
"There is a lot of interesting stuff that the human eye can't see. A billowing nebula of hydrogen and helium gas slowly collapsing to form a new star is really cool!

But, it would be too dim for the human eye to see so it would just be black. So we up the brightness. But also the human eye can't see sparse gas against vacuum, and it can't see the frequencies of light required to distinguish between hydrogen and helium. So we shift the frequency into something in visible light, effectively picking a color to represent each gas.

At the end you have an image that doesn't really represent what the naked eye could view. But it would also be pretty silly to say ""Check out this image from my infrared camera!"" and hand you a blank sheet of paper because you can't see infrared light.",0
"yeah Blindsight, it's in my comment lol

Author is Peter Watts

There's a bunch more bonkers shit in there too",0
"The thing that hurts in that area of the world is the sunlight at the time you’re describing. The cultures of that part of the world often employed cloth and chainmail as much as they could, as it is light, climate resistant, and it’s adequate protection against the weapons of the ancient era, where metal armor worn by soldiers was actually common. Now, it’s a different story because full metal armor doesn’t often see combat unless mounted to a vehicle, which means you might be cooked alive inside it, but you aren’t carrying it yourself. But essentially, ancient middle eastern armor was only metal when it was necessary because of how much of a pain in the ass it was to use. So to answer your question, they *didn’t* survive wearing full metal body armor, because it wasn’t really what they wore. 

Hell, even Persia, a country that could *afford* to outfit their troops in steel, never bothered with metal armor or shields and instead used cloth and leather armor, and they used wicker-woven shields, but used metal weaponry.",0
"To understand this, you need to understand what a programming language actually does, and to understand that, you need to understand how computers work at a very basic level.

At a fundamental level, a computer consists of a block of *memory* where information is stored and a *processor* that does operations on that memory.

Imagine, for example, that we just wanted to have a processor that could do logical operations and store the result somewhere. We'd need to tell it which logical operation to do: let's say we just want AND, OR, NOT, and EXCLUSIVE OR (XOR for short). Computers talk in zeroes and ones, so we'll need a code composed of zeroes and ones to ""name"" them. Let's say 00 is NOT, 10 is OR, 01 is XOR, and 11 is AND.

We also need to tell it which two things to apply the operation to. We'll say we only have 16 slots in memory, each holding a zero or a one. We can, in turn, name these 16 slots using a 4-digit binary code, with 0000 for the first slot, 0001 for the second, 0010 for the third, 0011 for the fourth, and so on through 0100, 0101, 0110, 0111, 1000, 1001, 1010, 1011, 1100, 1101, 1110, and 1111 (in order, the numbers 0 through 15 written in binary). The operations can have two inputs, so we'll need two of these 4-digit codes.

Finally, we need one last four-digit code to tell it where to store the result.

We can now feed our processor a fourteen-digit list of zeroes and ones as an instruction, agreeing that the first two digits represent the operation we want to do, the next four indicate the first slot in memory we want to operate on, the next four indicate the second slot in memory we want to operate on, and the last four indicate where we want to put the result. 

For example, the code 11111011000011 could be read as [11][1110][1100][0011] = [do the AND operation][with the first value being the digit stored in slot 1110 = slot 14 in memory][and the second value being the digit stored in slot 1100 = slot 12 in memory][then store the result in slot 0011 = slot 3 in memory].

Fundamentally, this is all computers ever do - everything else is just window dressing. Processors have a hard-wired list of some number of instructions - usually a few hundred, consisting of things like ""add thing at address A to thing at address B and store to address C"" - and everything else gets built on top of that.

*(By the way, you might notice that this computer only has 16 slots of memory, but it takes 14 slots just to store an instruction! In the real world, the addresses are usually 64 digits long, and there are many trillions of possible addresses, so this is less of a problem!)*

-----

So - what's a programming language? At its base, a programming language is just a way to make these instructions human-readable. To ""create"" a programming language, we just need to tell our computer how to translate the instructions we write into machine instructions like the 14 digit number we gave just above. For example, we might write AND(14, 12, 3) instead of 11111011000011.

Before this works, we need to write a *different* program that tells the computer how to translate AND(14, 12, 3) into 11111011000011. To do that, we just do everything by hand - we write out a program, using the numerical codes, to read the text symbols. But the core idea is that we only ever have to do this once. Once we've done it, we can then write every other program using this (somewhat) human-readable language. ""AND(14, 12, 3)"" is really ugly, but it's less ugly than 11111011000011. We call the program that translates human-readable language like AND(14, 12, 3) into machine code like 11111011000011 a *compiler*.

This first human-readable language, which is just words stuck on top of the actual instructions in the processor, is known as *assembly* language. It's still hard to read, because you have to turn everything into such simple operations, but it's a start. And we can repeat this process, by writing a program in assembly language to interpret something even more human-readable, possibly breaking down a single human-readable line of code into five or ten machine instructions.

In practice, most modern languages break down into existing languages that are closer to the 0's and 1's the processor uses (called *low-level* languages in programming parlance). For example, the Python programming language runs on top of a base written in C (another programming language), which in turn sits on top of your operating system, which in turn sits on top of assembly. Each layer in this hierarchy removes less direct control from the programmer, but also allows them to do things much more easily without worrying about the details of manipulating ones and zeroes.

If you wanted to make a new programming language (we'll call it Esperanto), you'd start with some existing language. Let's say you use C. You write a C program that reads text source code written in Esperanto, and translates the human-readable Esperanto text into C commands (or into machine code directly if you wanted). This is your compiler. Once you've done that, you can stop worrying about the C level at all! You can write your program in Esperanto, then run your C compiler program to translate it into C commands, and run them however you would run a C program. As long as you can say, in an existing language, what you want an Esperanto command to do, you can write it into your compiler and be on your way.",0
"Read [Longitude by Dava Sobel](https://www.amazon.com/Longitude-Genius-Greatest-Scientific-Problem/dp/080271529X) for an excellent history of the development of an accurate clock that could be used at sea. It's truly fascinating both from the engineering perspective as well as the personalities involved. And it clarifies that, prior to this development, navigation at sea (at least in terms of longitude position) could best be characterized as a wild ass guess.

Edit: somehow wrote LATitude when I meant LONGitude! Duh!",0
"For me and my girlfriend it's this goddamn fucking bird who insists on chirping his little jerk ass beak off every fucking morning for like a billion hours straight and I can't close the fucking window because otherwise the room becomes eighteen thousand degrees in the morning because I live in a town that might as well be called Devil's Asshole during the summer, so I get to choose between fucking burning alive like a piece of bacon on the surface of the sun, or listening to that little fucker go ""cheep cheep bloody fucking cheep"" every goddam morning with this terrible fucking stop and go pace that makes it damn near impossible to sleep and makes me genuinely wish the thing would just jump up its own ass and die already, as cute as I'm sure he is if I actually went out to see him

A white noise machine really helps drown that out, is what I'm saying",0
"You're getting a lot of different answers here, and most are correct but people here are talking about cash donations.  There's another big type of donation that rich people make too and that's in illiquid/non-marketable assets such as real estate and/or shares in private companies.

The ELI5 is a little difficult, but the gist is that for private companies, getting a 'value' is about 99% art and 1% science.  Accounting is a lot more assumptions and guess work than people would like to publicly acknowledge.  

So what an individual can do is offer to donate, say, 10% of his company to a charity.  Let's say that the company did really well last year so that guy can easily find an accountant to say that 10% is worth $10M. So the guy has a $10M tax deduction he can use as he sees fit (subject to AMT and other shit, but you get the point).  The guy also knows that last year's performance was a blip and if he repeated that 10% donation this year, he'd only get a valuation of $5M.  In effect he's got an extra $5M from the valuation.  

When I was younger, I was on the young alumni board for the university I went to for undergrad and we used to see this all the time.  Someone would donate property/illiquid securities/art/etc with an 'assessed' value of (say) $10M, but when the school went to sell it, they'd only realize (say) $3M in cash.  But the guy would still get to keep the $10M tax deduction. To a guy like that, a $10M tax deduction could be worth $4m to $5M easily.

I'm sure you're wondering, and yes this is in that 'gray zone' that's just millimeters from being tax fraud.  However, the IRS rarely pursues these cases 1) because they are *really* hard to win, 2) the school doesn't really care b/c they still just got a $3M donation, and 3) the school isn't going to 'help' the IRS (beyond bare minimum compliance) b/c the school would have to give up the $3M.

A similar, but different, application of this idea is where Mitt Romney reported a [$102M IRA](https://www.theatlantic.com/politics/archive/2012/09/whats-really-going-on-with-mitt-romneys-102-million-ira/261500/) by having accounts toy with the value/valuation of the assets within the IRA.  There's no reasonable way you could ever get that much into an IRA....unless you got your accountants to depress the value first then re-value later.

***EDIT:*** I'm getting a lot of questions about how this worked for Romney.  Here is a *purely hypothetical* example based loosely on when Bain Capital took out Domino's Pizza that I put in a lower comment - I'm putting it here so more people see it:

I'm going to make up the exact numbers, but this is the mechanics: Romney had a type of retirement account where you could but $30K cash per year in the account tax free. He also had a private equity firm invested in private companies.

Romney took $30K cash, put it in the retirement account each year - tax free. Each year, his accountants would say, 0.X% of your portfolio companies is magically equal to $30K, so Romney would take the $30K cash in the tax-sheltered IRA and buy 0.X% of his companies personally. He did this for about (I think) 15 years. After a while, he converted it to a ROTH IRA meaning that he'd pay a small one-time tax hit but withdrawals are tax free.

Later on, his private equity company would sell the individual companies that it owned (or take them public). When they were sold, Romney would recognize the gain BUT it was now within the tax-sheltered IRA he'd pay no taxes and because he converted to a ROTH IRA, he pays no taxes on withdrawals. Genius.

Here's a sort of hypothetical example. In 1998, Bain (Romney's PE company) bought Dominos Pizza for $1.1B. They probably used a standard PE structure meaning that they borrowed $1.0B and put down cash for $100M. So the company is $1.0B debt and $100M equity. Romney puts in $30K each year to buy some of Dominos ($30K/$100M = .03%/year). He buys .03% per year, for a total of .45% of Dominos after 15 years. At this point, he converts from a traditional IRA to a ROTH IRA paying roughly $100K in taxes ($450K in contributions multiplied by his tax rate at the time of the conversion - maybe a little more or less than $100K, but in the neighborhood).

Then they take Dominos public, which they did. Dominos is worth $10B as of right now, so Romney's 0.45% of Dominos would be worth $45M. ***All completely tax free*** because its in a ROTH.

In reality, he has an IRA worth $102M, so my example is off by about half, but you get the basic mechanics. The IRS looked at it and said it was aggressive but not illegal when he ran for president in 2012.  ",0
"thing was going off the charts last night

ur mom sure knows her way around

e: I wasn't even gonna post this cuz I thought it was too stupid lol",0
"3 Main reasons:

1. It takes a lot of extra processing power to generate the split screen aspect.

2. Its development work they would rather put towards their more lucrative multiplayer (usually with microtransactions)

3. they sell more copies of the games to groups of friends who want to play it.

Now all of those are horseshit reasons, and I want my couch coop back, but still. I would (and do) support games with couch coop, like borderlands, lovers in a dangerous spacetime, speedrunners, etc. I recommend you do the same.

Also fuck Halo 5.",0
This was damn awesome. Economy is crazy.,0
"It's always a balance.

Number of blades:

* Fewer blades = More efficient shoveling of air because of turbulence (air swirls) created by other blades reduces efficiency. Usually simpler to make.
* More blades = More stable because the force is spread out over more blades and shovels more air compared to how long the propellers are.

Propeller tips breaking sound barrier is bad (because of lots of turbulence). The longer the propeller the faster the tips go compared to the center. But having too short blades means more loss of energy at the blade tips:

* Longer blades = Better at generating lift, shoveling more air at lower speeds. Longer propellers also less stable and vibrate more.
* Smaller blades = Allows higher top speeds since the propeller can go much faster without breaking the soundbarrier with the wingtips. More stable.

So basicly.WW1 airplane: We can't make so good engines. So we're gonna go with efficient short two-bladed propellers because that gives is the most thrust for our weak engines.

WW2 airplane: We gots a lot better engines now. But two-bladed propellers can't shovel enough air to take our planes as fast as we're going to go. So we're going to go with 4 short blades!

Helicopter: We gotta generate lots of lift. So we're going to go with longer and slower rotating blades!

Modern helicopter: Uh. Those blades aren't generating enough lift. MORE BLADES! More blades is harder to make, but more stable too.

Modern turboprop: Too noisy! We're making special 6 bladed propellers that are much quieter. And computer power and advanced materials allows us to make them special advanced shapes that generate even less noise and more power. So now they look more like ship propellers. But for air! Still kinda short blades because we gotta go fast!

Ship propellers: Water dense yo. So we gotta make blades short (or they'll break!) but we make them much wider to shovel a lot of water backwards.

P.S: Jet engines work entirely differently, even if they do have fans at the front they're for compressing air into the engine, not generating thrust.  


P.P.S: For ceiling fan. You're moving lots of air, but you want to do it slowly and silently. So lots of wide blades. How long the blades are depends on how mobile you want the fan to be. Big fan = more air silently. Small fan = Noisier, but more mobile.",0
"Since there's no empirical answer here, I'll throw my non-empirical answer in the mix:

**TLDR: It could be many things.**

It could be conditioning: You took a couple of naps on the couch with the TV on; now, your brain associates the TV, with just the right amount of fatigue, as sleep time, at the right place. 

Meanwhile, back at the ranch (when you go to bed), you're staring at your phone, ""doing some last few reddits"" before bed. You have trained your brain to activate before your ""alloted"" sleep time. Chances are, you've done so much internet before bed--your brain does not associate bed time with sleep time.

Related: Your bed should be for two things only: Sex and/or Sleeping. This is to make sure your brain and body associate that with sleep, OR Sleep that usually follows sex. 

Also, digital screens emit blue light. In short, it's a light wavelength that suppresses melanin. Melanin is a hormone that is produced in your body. When Melanin spikes up (usually after some hours of nighttime and sometimes around post-lunch hours) it makes you sleepy because that's the job of Melanin. They also sell melanin at the drug store, but it's always better to rely on your natural sleep cycles as nature intended. The Sun and Digital Blue Light from most electronics today suppress Melanin, making it harder to fall asleep.  **THIS** along with not associating your bed with sleep will most definitely fuck up your sleep.

Sleep is still misunderstood for the most part (we don't know why we do it, generally), since it puts us at a huge disadvantage in the wild--yet, we **must** do it. This means that even if  we don't understand it, it does something right.  Studies where people were sleep deprived had slower time reactions, sometimes akin to a drunk person. 

For example, cell regeneration and healing happen during sleep. The brain, oddly enough, is more active during sleep. There's a theory that ""pruning"" is occurring during sleep--AKA, your brain is getting rid of the stupid, useless information that won't help you survive. This is why ""The First 20 Hours"" method works well for advancing learning quickly seems to do well: reviewing before sleep/reviewing after sleep, in short time frames--so the brain associates that this is needed and doesn't prune it.

Another possible reason is that when you sleep, it takes some minutes (60ish or more on normal IF I recall correctly) to get to Rapid Eye Movement (REM sleep). REM sleep is **ESSENTIAL**. A lack of sleep with screw you up, but a lack of REM will really screw you up. So, when you're running on less sleep than usual, when you finally sleep/nap, you get to REM quicker! WOOHOO! LIFEHACK!  **NO, it's not**. As stated earlier, sleep has a lot of functions. Anyway, if you're tired and fall asleep while ""relaxing"" watching TV, and wake up 30 mins later, chances are you got some REM sleep. This fucks you up because your body has gotten a little recharge to take you over for maybe, 6 hours? It's like your phone was dying, you charged it for 10 mins and got it to 25%. It's gonna take some time to get back below 10%.  

Remember those people from the studies? Well, some of them were ""disturbed"" (on purpose) during REM sleep (enough to snap them out of REM, but not out of sleep). The next day, they felt they were fine and had good rest. However, their results on reaction time showed that they were not at full ""normal"" rested reaction capability.

Also, there have been many people throughout history that have experimented with various sleep cycles to ""get the most out of their day."" I think (and I may be way wrong here) it was DaVinci that famously tried the polyphasic sleep (fancy name for ""different ways of sleeping""), where he would sleep one hour every 4-5 hours. Supposedly it worked, and I don't doubt he got used to it.  So, you may have gotten used to sleeping 4-5 hours at night, with a nap right after dinner--and you may not realize that it's a habit now. You may not like it, but you did to your body--your bod is just doing what you taught it.

That's all I got. Some of it is scientific, but I did internet research long ago and don't have the patience currently to dig up the sources. If someone wants to disagree or bring up relevant points, or even call me out on wrong info, feel free. This is the internet, not The White House, I can admit I might be wrong.


**EDIT:** I want to add that ""humming"" sounds can easily relax the brain. Depending on what you're watching, if it doesn't have much flux, the TV can hum along, much like a quiet lullaby. This is why White Noise or the sound of rain, a fan, Air Conditioner, beach waves, background coffee shop can aid in sleep & focusing. In a weird way, it zens the mind to relax. I looked into it long ago, but I forgot why it is. Probably something with the infant brain associating a smooth sound (singing by mom) that you are safe and not in danger, so you can relax.

*EDIT 2** Guys, gals and all in between--I get it, it's melatonin--not melanin. I know the difference, I just have a long-life habit of mixing them up. That's what I get for doing a write-up on a lack of sleep. Happy naps, everyone!





",0
"First, it doesn't take years per se to make a movie. But there can be some delays - perhaps principal shooting only takes 2 months, but you have to wait 3 months for your lead actor to be available. 

Shows like GoT get benefits over movies since the sets are all pre-built (past the 1st season) - actors have contracts stating they need to be available during a predictable time schedule if the show is renewed for the next season.  GoT in particular also gets to benefit from the fact there are several plot lines that are independant - the Wall stuff, the Khalesi plot, the Westeros stuff, can all film in parallel. Not possible with a movie, where your lead actors can't be in two places at once.

edit: ok Khaleesi is a title, I get it.

edits: 

- [and multiple production teams for pre, post and shooting](https://www.reddit.com/r/explainlikeimfive/comments/47dbnq/eli5_how_come_it_takes_years_to_make_a_2_hour/d0cgg06?context=3)

- [multiple scriptwriters working on multiple episodes at once, television schedules being more demanding, less flexible to delays](https://www.reddit.com/r/explainlikeimfive/comments/47dbnq/eli5_how_come_it_takes_years_to_make_a_2_hour/d0cgckg)

- as numerous have said (thanks), a lot of the upfront preproduction and 'selling' of the show (aka the development hell) is out of the way before the pilot gets shot. Long running series don't have to sell the production each season - just point to a rabid fanbase clamouring for more. Each movie has to churn through what could be years of this stuff. [Production stuff](https://www.reddit.com/r/explainlikeimfive/comments/47dbnq/eli5_how_come_it_takes_years_to_make_a_2_hour/d0chhjg)

- [a movie could be 10 hours long, but noone would sit through it. But people will sit through 10 1 hr episodes no problem. So writing, timing and storytelling can be radically different.](https://www.reddit.com/r/explainlikeimfive/comments/47dbnq/eli5_how_come_it_takes_years_to_make_a_2_hour/d0c9l67)

edit: ok holy christ, Khaleesi isn't her name, ok I got it.

edit: wow, thx for the gold! ",0
"When a girl reaches puberty around the age of 12, it means that her body is starting to go through changes to prepare her for potential pregnancy in the future. While her body might not be fully developed, it is biologically capable of menstruation and reproduction. The primary purpose of a woman's menstrual cycle is not only to allow for pregnancy, but also to maintain a healthy reproductive system.",1
"And then it doesn't matter which doc does this they all say ""DO YOU WANNA SEE IT?"" after they pull a Randy Marsh football-shit size plug of wax out.",0
Can definitely control my inner sphincter. Wouldn't be able to take my boyfriend's dick otherwise. ,0
"There really are a lot of factors, some of them mentioned already, I'll try to explain a few more.

Culturally, there is something in Afghanistan called 'pastunwali' which is a set of rules that are followed by nearly all Afghans. It has many facets, one of them is to welcome and protect visitors. This will come into play later. To really understand the issue in Afghanistan, you have to know the historical connotations to what went on.

So, in the mid 80's, the Russian's were pushed out of Afghanistan. Like many other countries (Britian, and eventually the US) they found that it would just cost too much money to stay. For around 8 years or so, there was somewhat of an ad hoc communist government, although it was limited to really the main cities (Kabul, Jalalbad, Herat, Konduz, Kandahar). The rest of Afghanistan (80-90%) was  run by warlords. To make a long story short, there ended up being two factions. The pro government forces, and the Taliban, which emerged from one of the warlords (Mullah Mohammad Omar). To fight the government, the Taliban needed money and training, and it called upon al Qaeda to do so. 

Although they supplied funding and training, Afghanistan became the 'place to jihad', only because at the time, there other place was Chechnya fighting the Russians, and the Russians were using 'scorch earth' policies...which consisted of of carpet bombing whole villages and areas that were deemed enemy terrority. 

So, while there was money and training involved, these members of al Qaeda were coming in droves to Afghanistan, but really didn't care about the fight going on. Eventually, the fight became a stalemate, and areas were set up for both sides.... and al Qaeda never left, but after 9/11, there was the cultural practice to protect their 'visitors', which did happen. 

Pakistan

While the Afghans do protect their visitors, they are very 'eye for an eye', so after about a month of refusing to turn over Bin Laden, the US just started a bombing campaign, and most Afghans were really ok with it. The majority of al Qaeda was holed up in the Tora Bora mountains, which is in eastern Afghanistan near Jalalabad. (If you're going to be a terrorist, that's a really beautiful place, as well as Nuristan where the rest remain today). The idea was to start bombing, push the al Qaeda forces east to the awaiting Pakistan border, where their army would either capture or kill the renaming forces. Essentially, the Pakistan army opened up, and allowed them safe haven in Pakistan, and the US was kinda stuck. 

There are a myriad of terrorist organizations that are allowed safe haven in Pakistan. The Haqqani (which sees its self as the Taliban, although the DIA has been trying to make them their own organization for years) is stationed in Miram Shah Pakistan. This was the organization that had Bergdahl. 

The Lashkar-e-Tabia is based in Pakistan, and they are fighting the dispute between Pakistan and India over the Kashmir Area. 

There's a whole bunch, and I'm on mobile, but many believe these various forces are the military action army of the Pakistan intelligence service (ISI). 

Uzbekistan

Uzbekistan kinda of took a hard stance against Islam. Captured everyone that was coming and going to a mosque, put them in swimming pools and shot them all. This in turn formed the IMU (Islamic Movement of Uzbekistan) which operated out of Northern Afghanistan. 

In there end, there are a ton of factions that can operate freely in Afghanistan, and do so, as well as Pakistan. And although some has changed, the Afghan people still operate under Pashtunwali, and both host and protect these various organizations. 

To note, many of the other responses are also true, its really a hodgepodge of reasons for their situation. 



 ",0
"Unless your wife takes an hour shower, then it becomes as humid as a rain forest

Edit:  yes, I know she is masturbating.

Edit 2:  we are on a well, no water bill

Edit 3: yes im fine with her masturbating.  We both have high sex drives and sometimes i prefer to do things myself as well

Edit 4: no, the shower is small, you can not join her.

Edit 5: /r/jesuschristreddit",0
"You’ve gotten a lot of good info here, especially from u/McKoijion.


However, to add to his answer, it’s not just that alcoholic beverages have calories. Also important to note is that alcohol is an endocrine disruptor.


Your endocrine system is a system of chemical messengers (hormones) secreted by endocrine glands that circulate through the blood and have various effects on the body, especially metabolic ones.


Alcohol has been demonstrated in studies to throw off the balance of certain hormones in the body:


1. Alcohol stimulates the generation of fat. It does this by forcing glucose (sugar) into the blood, which triggers the release of insulin, which stimulates the body to make fat and store it.


2. Alcohol stimulates your body to release cortisol, which over the long term can have metabolic effects, including weight gain.


3. Alcohol interrupts the release of testosterone (very simply put; it actually inhibits the release of luteinizing hormone, which inhibits testosterone production). Testosterone aids males in burning fat, so inhibiting it promotes weight gain.


4. In heavy drinkers, alcohol appears to interfere with the release of thyroid hormones, which slows down metabolism. However, this does not seem to affect moderate drinkers.",0
"I’ve come to the understanding that the first “oh god I’m STARVING to DEATH I need FOOD” feeling is just my blood sugar falling and my jerk body refusing to run off glycogen or the *plenty of adipose, just, sittin’ around*.",0
"I've often read and been told that you generally shouldn't up-pot plants to a pot that's larger than 2"" than the current pot size, and I never understood why. Maybe it's a marketing scheme by Big Terra Cotta.",0
"Boiling or microwaving will KILL the bacteria. What it won't do is REMOVE the dead bacteria. That means there is dead biological material that's just food for more bacteria. The more you sanitize the sponge, the faster more new bacteria will grow... ELI5-when you kill bacteria it becomes bacteria food.  

Edit: Wow. So. Heat, UV, bleach & desiccation(alcohol/sanitizer) can all kill bacteria. But there is no practical way to clean the material out of the deep pores of a sponge without destroying it. Nothing is as good as a clean sponge.   

A short video of a single celled organism dying that demonstrates this concept:  
https://www.youtube.com/watch?v=4bj6SqgT4SQ

DISCLAIMER: I am by no means a subject matter expert nor do I represent big sponge corp.",0
"Fuck, you just made me realize why it's called a beaker. I feel dumb. ",0
"The nose is actually a pretty awesome organ that helps make sure that the air you breathe is prepared as good as possible for your lungs.

That includes amongst other things filtering particles out of the air (This pesky nose-hair is actually good for things!), making sure the air  gets warmed up when it is cold and moisturizing the air if it is dry. Clean, moist and warm air is making sure that it's easy on the lungs and your breathing is efficient. Additionally breathing through your nose makes sure your air intake is regulated and you aren't prone to hyperventilating.

So that explains why breathing in through your nose while doing sports, meditation and... basically in every situation is the best way to breathe in, but why is breathing out through your mouth then advised in sports?

It's mostly about the speed of your oxygen intake. Or, to be more precise, about increasing the breathing frequency. As I just wrote the flow through your nose is rather limited. That works in both directions, if you breathe in as hard as you can and breathe out as hard as you can first through your nose then through your mouth you will see that you can breathe a lot faster through your mouth. So if you breathe out of your mouth you will save a little time which means that your intake frequency of oxygen will, overall, be higher.

TL:DR: Breathing in through your nose is easier on your lungs and more efficient, breathing out through your mouth has little drawbacks and is faster. Together it's the most efficient you can breathe if you need higher levels of oxygen.",0
"Nah, 6am is for those awful whiskey shits.",0
"There are ways that society could change, to be more fair to **black people.**

There are ways that society could change, to be more fair to **women.**

There are ways that society could change, to be more fair to **gay people.**

*However.*

Many of these ways would not make society fair to **black gay women**.

The problems that you can have on account of being *both P and Q* are not just the sum of the problems that people have by being *P* plus the problems that people have by being *Q*. There can be separate *P and Q* problems. And even if we solved all the *P* problems and, separately, solved all the *Q* problems, that doesn't actually mean that we solved all the *P and Q* problems.
",0
"Electrolysis of water. Water is split into oxygen and hydrogen. The breathable oxygen is vented into the cabin, while the explosive hydrogen is is vented externally. 

Edit: I do not know where the water comes from. Some other comments say urine and air conditioning condensation contribute. ",0
Just like cats can see you struggling to pet them just out of reach. Fucking assholes.,0
"To add to this, emotional smiling and voluntary smiling (controlling facial muscles) are controlled by different centers in your brain. It is pretty interesting that people who have lost the ability to smile voluntarily because of a brain lesion can still smile due to an emotional stimulus which is not voluntary.",0
"Thanks to medication, I've been able to focus on the things I enjoy more rather than wanting to do things I enjoy, being mentally unable to, and then feeling terrible about not being able to.  It's the weirdest thing to have experienced being unable to enter a flow state for the majority of my life, and then suddenly be able to fall into that state basically whenever I want.  It was the first ""oh shit, my mind really is different from a normal person's.""  Feels like every few weeks I come across another one of those situations where I thought something was normal and everyone had to deal with it, but it turns out someone without ADHD can't even comprehend what I'm talking about.  Weird shit considering I grew up thinking it just meant I have trouble paying attention to stuff, and it turns out it's so much more than that.

edit: A lot of people are asking about what I'm taking.  I take 30mg of Adderall and 300mg of Bupropion.  I go into detail [in this chain](https://www.reddit.com/r/explainlikeimfive/comments/y44mca/comment/iscqbat/?utm_source=share&utm_medium=web2x&context=3) if you want to know more about my thoughts on it.",0
I took a class called “descriptive inorganic chemistry” you think if there’s ones place where we would cover the colors of metals it would be there. Hell no we didn’t talk about it,0
"If you use that ones that are supposed to kill bacteria, yes commensal bacteria are not saved. ",0
"Because water is actually pretty hard to store. We need a *lot* of it, far more than we need fat or sugar. Water is also pretty easy to obtain, so we don't *need* to store it. Things that do only do so because they can't get it regularly: cacti, camels... 

Water is also not only something we discard, it's how we discard *other* things, things that we have to discard due to their toxicity like urea. A high water intake and output is important for our overall health because it keeps us efficiently removing toxic waste products from our systems. 

We actually do store certain vitamins - fat soluble vitamins. Some vitamins are water soluble though, so we have no way of storing them. To store any chemical our body has to sequester it from the water inside our cells. Fat-soluble molecules naturally remove themselves from water anyway, but water soluble ones have to be made water *insoluble* in order to store them, which means modifying them so that they're not vitamins anymore. 

The problem with storage of fats and sugars is that we evolved to be able to survive droughts and other unpredictable conditions. We store excess energy (in the form of fats and sugars) so that we can't be caught off-guard if we ever enter a period of starvation. Fat storage is also a useful insulating tool remaining from when we hadn't invented central heating. So, we have all these mechanisms for storing fats and sugars, but we now live in an environment where starvation pretty much never happens unpredictably and we don't need to build large energy stores. The body always assumes the worst because it's evolved to do so, and if you're over-eating regularly its assuming that a starvation period is coming. So if you're constantly over-eating your body is constantly trying to stockpile energy in fear of an oncoming winter or something, but that never comes and you just keep on eating. 

The body is actually *incredibly* efficient about what it does and doesn't store. It stores everything it needs to be able to store (and can store at all). That's how evolution works after all - it tends towards making the most efficient versions of things possible, as long as more efficiency means a higher survival rate. If it doesn't need to store more energy, then it sends us messages telling us we don't need to eat. If you're storing more fat and sugar than you need, it's because you're *eating* too much fat and sugar, ignoring those messages, or because of some genetic defect that is meaning that messaging system isn't working properly. Or because of gut bacteria. That's the new focus of dietary science.

Also, the reason we can't store water soluble things is because it's actually toxic to do so. A lot of our body works on the relative concentration of water soluble molecules, and if we have too much of a water soluble molecule, we actually can't absorb any more of it without converting the water soluble molecule to an insoluble form. We actually have to use a very advanced mechanism to absorb glucose at all, that relies on the fact there's more sodium in our intestine than inside our cells.",0
"Simple. They eat a fuck ton of it. A single blue can eat 40 million krill every day. That's ~8,000 pounds of krill, in case you were wondering.   ",0
"even average to good harmonizing sounds better live or ""recorded live"" than very good harmonizing when done separately and mixed after. 

I remember going to some show for a few small local bands back in the day. my SO at the time really really liked this band that performed so she bought their album. Wasn't even close to sounding as good as they did live. Even the shitty short video she took of the show on her phone sounded better than the recorded album.",0
"Fun fact - I came out with my umbilical cord wrapped around my neck and a mouth FULL of Meconium. Scored the lowest possible Apgar. Yay me!
*Edit* adding couple details - I was actually doing pretty damn well after a couple hours. My mom however was was hemorrhaging like a balloon with a hole in it. She swears that I didn't want to come out (I think my depression had already kicked in and there were just too many people in that room for me to feel comfortable so much so I attempted prenatal suicide) because the doctor (according to her) was jumping on her stomach trying to force me out. I don't respond well to pushy-ness; never had, never will.",0
"Oscars take themselves more seriously. Grammys exist to make money off of the TV viewership. Oscars actually matter to the people receiving and the ""academy"" of people voting. Most musical artists truly don't give a shit about the Grammys, Hell, Views by drake was nominated and it is said to be he worst drake album in history. But drake brings viewers to the Grammys and gives them that sweet commercial money. ""Moonlight"" didn't get a lot of people in theatres but movie people care about the quality because making a movie is a lot more of a struggle than singing what your writers wrote down. ",0
Either that or you had a stroke.,0
"The system we currently use came into existence at some point about 2000 years ago in what was then the roman empire. The days of the week were named after celestial bodies, that is the sun and moon and the planets which themselves were named after gods.

In Germanic languages like English those names were switched to the corresponding Norse gods. Tyr instead of Mars as the god of war Odin instead of Mercury for the god of wisdom and Thor instead of Jupiter for the thundergod slot. Venus was replaced by Freya or some other female deity that sounded similar. Only Saturn kept his day perhaps due to the lack of any chthonic gods of death and the underworld that would have been a good fit. 

The weekday concept spread  throughout Europe and even to Asia while mostly being kept synced up or getting re-synced when more regular contact was reestablished later.

This goes so far as to include distant places where they don't name either planets nor days of the week after gods, but they do name both after the 5 elements and the relation stays synced.

The origin of the whole 7 day week is probably from a way to have quarters of a lunar cycle. Unfortunately that comes up about 3/8th of a day short so any continuous cycle of 7 days will inevitably dirt out of sync with a lunar calendar very quickly.

Judaism probably helped popularize the week as a thing independently from the month it was originally derived from.",0
"ELI5: Sperm face a number of challenges on their journey to reach an egg. First, the female reproductive tract is a complex and hostile environment. The vagina is acidic, which can harm and kill sperm. Additionally, the cervix, the opening between the vagina and the uterus, has a thick mucus that acts as a barrier, making it difficult for sperm to pass through. Once inside the uterus, sperm need to navigate through a maze-like structure called the fallopian tubes.",1
"It's not like a meaty solid organ like finding a new kidney or liver. This is a network of delicate fluid-filled spaces. 

These spaces are **microscopic** in size.  60–70 micrometres (0.0024–0.0028 in). 

They were not discovered before because such delicate fluid filled paces are not noticeable on samples and slides because of how *microscopic samples* are prepared for viewing *under a microscope*.

They were discovered now because someone went inside a bile duct with an endoscope that had a **microscope** on it, looking at microscopic scale structures *in a living tissue not a prepared slide* that nobody had ever looked at with a microscope before",0
"This is exactly why the push is on consumers and not on the manufacturers.   The only way to get manufacturers to change their behavior is to hurt the bottom line. Corporations have fiduciary responsibility to make money for their shareholders. They don't care about environmental issues until those impact the bottom line.

Edit: I hear all of you saying legislate/tax...  If you believe this will happen any time soon in the U.S. then you're delusional. The GOPutin will block that shit writ large. The only viable solution until we smack the GOP out of power is to hurt them where they feel it and that's in the pocket book.",0
I pucker my upper lips which blocks my nostrils so i can swim upside down and not get water up my nose. My swim coach thought I was weird as fuck.,0
"It's called ""Meconium,"" as u/maaagill said, and babies sometimes do ""poop"" in the womb-- I was a meconium baby as I was born two weeks late because as usual I was too lazy to get up. It's not *super* dangerous for a baby to womb poop but it's not really good, because they can inhale the substance when they breathe on their own for the first time. In this case they put a tube down the baby's throat to suck it out and clear it. It left me with a scratchy voice for a little while :)

EDIT: Since people have been asking, ""a little while"" was as long as six months, according to my mom. I can't really comment since I was an infant, and I can't ask my mom now because we don't speak anymore.

re: twenty comments of ""lololol you ate shit"" yeah I sure fukken did and god almighty can I not wait until you kids go back to middle school in the fall and reddit is full of adults again.",0
"Some more background on counting if anyone is interested:

The earliest civilizations only knew three numbers: one, two, and 'more than two'. One is conceptually easy, you can understand it by thinking about things like yourself (ego), the sun and god. Two is easy as well, it is found in concepts like good vs evil, light vs dark, man vs woman, alive vs dead.

For ages, these were the only numbers that were used. Three existed as well, but only to signify 'more than two'. You can see this in things like hieroglyphs, where drawing one tree signified a tree, and drawing three trees signified a forest.

Zero as a concept was also known, because it signified 'nothing' or 'empty'. But the link between zero and one, two, three was not understood.

A lot of civilizations did count things, but without knowing how it worked. Sheep herders for example would put a stone in a basket for every sheep that left their pen in the morning, and remove a stone for every sheep that went back in at night. If there were any stones left after the sheep were all back in, they knew they were missing sheep. But they wouldn't be able to tell you how many sheep they had.

Eventually the number three was understood, and after that the concept of counting spread fast. After all, if you can grasp the idea of three, it's easy to expand it to four, five etc. There was no specific point in time or specific civilization associated with counting, it is assumed that a lot of civilizations all over the world figured this out individually.

After this it still took thousands of years before we started to understand zero as a number. Even with the 'place-implies-value' number system that we still use today, zero was more seen as a placeholder than an actual number. Only when mathematicians started to think about stuff like negative numbers, it became evident that zero as a number was needed. From that point on, a lot of new math was discovered.

Source: [https://www.amazon.com/One-Zero-Universal-History-Numbers/dp/0670373958](https://www.amazon.com/One-Zero-Universal-History-Numbers/dp/0670373958)

Edit: To clarify, in the first part I was talking about the abstract idea of counting, as in assigning names to quantities and to do basic math with them. As with the sheep herder example, early humans found many ways to keep track of numbers, like using your fingers or carving lines on rocks. But they did not know what the concept behind those methods were, just that it worked.

Also, when we say 'the Babylonians knew trigonometry', it's easy to forget that the people who had that knowledge was a very small group of religious scholars. Your average Babylonian or Egyptian did not go to school. But yes, by the time the first real civilizations that we know of rose we already knew much more about numbers that the earliest humans did.",0
"Look up what you can about the [syrinx](https://en.wikipedia.org/wiki/Syrinx_(bird_anatomy\)). Birds don't have lips and (in most cases) their tongues are too small to influence sound. However, they have an organ we don't have, the syrinx, between their trachea and their lungs which can modulate sound a lot.

Parrots do have large tongues which help them to make sounds.

For a better understanding you can also look up how human sounds are made. Basically, vowells are made by making sounds with your vocal chords and consonants are made interrupting the flow of the air getting out of your lungs. Just like hitting different objects creates different sounds, the contact between all the mouth parts (lips, tongue, teeth, several different parts of your palates) will create different sounds or modulate the air getting out of lungs in different ways (including changing it's direction towards the nose, for example).

So like humans, birds have more than just their vocal chords to modulate the air getting out of their lungs to modulate sounds.

Now you should think how much different it must *feel* to them to create the same sounds. For example, they probably don't produce /m/ (which in use in spontaneous expressions just as ""hmmmm!"") by vibrating their noses and skulls like we do. ",0
"I fear any person holding a gun who could legally kill me. I am always nervous around a cop, does not mean I'm guilty.",0
"Wow. Not sure how this got to the top but you're mixing concepts from different time periods and throwing misinformation around net neutrality. Let me try to correct some of this.

AT&T was sued under the Sherman Anti-Trust Act because of its market behavior, not because of the tax dollars involved. AT&T had a true monopoly; they were the only company in the country doing what they did after acquiring every regional provider. MCI made their own phone company and provided a service where you could dial a code and then an AT&T number and you could reach an AT&T customer. However, due to the network effect they could not compete with AT&T unless AT&T allowed AT&T customers a way to call MCI customers. AT&T denied MCI's request to create this interoperability, which triggered the Sherman Anti-Trust Act because AT&T was using its market position to obstruct the entrance of new competition into the market place. The Sherman Anti-Trust Act has nothing to say about tax dollars nor eminent domain. It's purely an anti-monopoly rule.

The result of the anti-trust suit was that AT&T was broken up into regional monopolies. A stupid and counterproductive result as we found, because regional monopolies are nearly as bad but not considered monopolies by the Sherman Act. One of the terms of break up, based on the tax dollars premise, was that these new companies needed to provide a service called line sharing whereby any service provider could rent a line from the regional monopoly. This was supposed to create competition at the service layer without incentivizing ""redundant"" infrastructure build out. When Internet became a big deal lots of small ISPs started paying for line sharing and lots of customers left the main infrastructure providers to get better customer service. The infrastructure never improved, but at least customer service was nicer. Eventually the infrastructure providers convinced the FCC to allow line sharing rate increases and every single ISP that was on a line sharing agreement went out of business in a couple of years.

None of this has anything to do with net neutrality. Net neutrality does not require line sharing cost agreements. Net neutrality has not and will not bring back the line sharing consumers to start their own companies. Net neutrality has no interaction with incentives to apply capital expenditures to infrastructure.

The big infrastructure providers do not hold back on expansion due to net neutrality. Net neutrality does not limit their control vis-a-vis competition from other ISPs. If that were true, small upstart infrastructure providers wouldn't exist. But they do and have been forming and growing for 20 years. The reason you don't see them grow into your hometown is because the regional monopoly is still enforced by law and is not impact by net neutrality.

The fact that you think the FCC broke up Ma Bell even though you work for MCI is baffling. The FCC doesn't enforce anti-trust, the FTC and the justice department do. MCI filed the anti-trust suit that broke up AT&T so they existed before it happened and were doing business.

Your whole explanation about net neutrality is either equally misinformed or deliberate astroturfing. Given how much astroturfing happens in telecom, I'm leaning towards the latter.

Net neutrality is about content. ISPs charge me to access the Internet. Then, they charge Google to access the Internet. Then in the early aughts, they decided they wanted to charge Google for me going to Google. So I paid, Google paid, then they wanted Google to pay again. They couldn't actually do this, so they decided they would BLOCK me from accessing Google unless Google paid them the second time. Net neutrality attempts to prevent this predatory behavior. Infrastructure doesn't even factor into it.",0
">it's that our priorities have changed

That's the big thing right there.  Sending someone to the moon was a dick race with the Russians.  Once we got there and planted our flag, interest waned very quickly.  Look at Apollo 13.  Two missions in and public interest in moon missions had already fallen off until ""Houston, we have a problem"".",0
"Sperm = extra tough single cell form of life

Egg = single cell form of life

Human = Super complicated living structure.

That's the difference when it comes to freezing.  If you kill 10% of the sperm cells in a sample, you still have plenty to get the job done.  If oyu kill 10% of the cells in a person, there is no repairing that complexity.",0
"It is political. The different so called Chinese dialects are unintelligible to each other. Others have mentioned the Serbo-Croatian-Bosnian example but something similar happened with Romanian. In Moldova they speak Moldovan and Erie it in the cyrillic alphabet. Why? Because when they became part of the USSR it wad decided they didn't speak Romanian and they had to come up with grammar books for Moldovan using the cyrillic alphabet. 

Other times it's not so clear cut. Depending on who you ask galician is a dialect of Portuguese or a separate different language. For context,  a galician and a Brazilian can have a conversation with each speaking their own tongue they just can't speak each other's tongue.",0
" And then us masochistic humans came along and were like ""bring on the pain you stupid peppers!""",0
"They don't, they run at the same speed throughout their operational lifetime.  You're just making them do more that they weren't doing before.

As an IT professional, programmer and system admin please:

\- Stop defragging.  It does basically nothing nowadays, certainly nothing worth the disk wear or the time it takes.  Defragging is a handover from the days of 20Mb hard drives on older filesystems on slow-latency hard drives.  Just stop it.  Especially if you have an SSD - you're literally just wearing the SSD away, for no reason.  If you want to avoid the need to defrag, don't run your hard drives more than 90% full, that's when things start to fragment to jam them into the gaps.  If your drive isn't more than 90% full, it'll sort itself out and likely will never fragment in the first place.  And modern PCs will basically not noticeably slow (even on a benchmark measurement) just because they're slightly fragmented.

\- Registry cleaning - again, does nothing.  The registry on an average machine is maybe 50Mb-100Mb or so?  Pathetic by modern standards.  Cleaning it does nothing.  You can remove services and auto-start entries, but use a proper tool for that, not some pay-for junk off the internet, or in the registry itself because if you cock it up, your computer won't boot properly.  Sysinternals has Autoruns available to you for free, but pretty much most of what it does you can do with Windows 10 task manager, etc. on its own.

\- Browser cache - again, does nothing.  You're just making the problem worse.  Modern browsers manage their own cache and clearing it out makes nothing faster, just the opposite.  Unless the page you are loading is not the page you expected (i.e. it's not up-to-date), cleaning your browser cache is entirely the wrong thing to do.

What you want to do:

\- Make the computer do less.  Have less programs installed (no, it doesn't matter how full your disk is, it's to do with how much stuff is running all the time).  Get rid of anything you don't need to be running 24/7 (e.g. get it off your taskbar, stop it running with Windows, or stop it staying around all the time - it'll still work when you actually need to use it).  Steam, for example, does not need to be in your taskbar 24/7.  Stop it, using the options in the program or Autoruns.  Then when you want to play a game, you run Steam.

Personally, about 4-5 taskbar icons (by the clock) I find annoying.  I work to get rid of them.  Almost all of them can go.  The Intel display one (unless you think you need to use it), nVidia icon, Java, Steam, printer monitors, etc. etc.  Get rid of them.  The screen will still work, your games will still work, your printer will still work.  But you're not constantly running them 24/7.  There are also dozens of services, programs that run on startup, and other junk that's always running that don't need to be.  Almost all third-party program services (e.g. game launcher services) can be changed to manual startup (and then they will start if they're needed, but won't if they are not).  Uninstall stuff you don't use.

Your machine is no slower than the day you bought it.  It's just running all the shit you installed on it for the last few years and never removed and which is running 24/7 even though you don't realise or don't even use it any more.",0
">water evaporates it pulls even more heat out of the fire source.

Thank you for saying this. People don't realize that phase change requires a MASSIVE amount of energy. That's why the fastest way to cool something like beverage cans is to put them in a cooler full of water and ice with salt. The salt water melts the ice and pulls even more energy out of the cans.

&#x200B;

EDIT: This is is view as controversial here, I'd like to address the main comments:

I'd also like to shout to u/Introsium whose comment is here and explains in great detail what's happening at the barrier of water and ice: [https://www.reddit.com/r/explainlikeimfive/comments/gnaxct/eli5\_how\_exactly\_does\_water\_put\_out\_a\_fire\_is\_it/fr8ymo8?utm\_source=share&utm\_medium=web2x](https://www.reddit.com/r/explainlikeimfive/comments/gnaxct/eli5_how_exactly_does_water_put_out_a_fire_is_it/fr8ymo8?utm_source=share&utm_medium=web2x)

1)

>No, it's because the water better surrounds the cans leading to better heat transfer

2)

>No, it's actually that the salt lowers the melting point of water so the temperature change is greater

I will say this, these two statements are in fact true. Both 1 and 2 contribute to heat transfer but they are NOT as significant as the ice melting. 

&#x200B;

You can verify this with a simple experiment. 

Take 4 coolers - ALL AT THE SAME INITIAL TEMPERATURE (This absolutely can be done, if you don't do this, it's because you are cheating)

A) has just cold water

B) has water and ice

C) has water and  salt. 

D) has water and ice and salt. 

**The only rules**

**1) Once you add the cans, you cannot add more water or ice**

2) You must have the same mass of H2O in all coolers (i.e. account for ice)

3) You must have the same concentration of salt in both brines 

Now because of the freezing point of water, you need to do this in pairs (because the freezing points will be off)

If statement **(1) -** That it's just a surface area thing, is true then cans cooled by **A & B** would cool at exactly the same rate to the same temperature. 

This is NOT what you observe. In cooler A the cans will be warmer pretty well always because without ice the coolers temperature will rise.

If statement (2) - That the lower melting point creates a greater temperature difference is true then cans cooled by **C & D** would cool at the same rate and same temperature.

This is NOT what you observe. In cooler C the cans will be warmer pretty well always because without ice the coolers temperature will rise.

&#x200B;

**The phase change of the ice IS THE REASON** that the temperature gradient can be maintained. While the ice melts, the water cannot increase in temperature. This means that as long as there is ice, the cans' energy is being pulled.",0
"Part of the answer is that often the names basically are the word(s) in the original language.  The idea that names mean something directly sounds weird to you mostly* because names in English were largely taken from other languages (sometimes via a language or two in between) hundreds of years ago. For instance, names like John, Christopher, and Isaac all come from the Bible, which means they are originally from Aramaic, Hebrew or Greek. In their original languages, these names made perfect sense. Christopher, for instance, comes from the Greek words ""Cristos"" and ""phero,"" or ""bearer of Christ"" and comes from a parable about a man who literally carried Christ across a river. Isaac, or Yitschak in Hebrew, comes directly from the Hebrew verb ""to laugh"" because his mother Sarah didn't expect to get pregnant and laughed when she realized.

Other names are just cool sounding. Zev, Dov, and Arieh are all Hebrew male names that mean wolf, bear, and lion, respectively. A few female examples are Gal, Shir, and Tamar, which mean wave, song, and date (the fruit), respectively.

Do Hebrew speakers find it odd that someone is named ""Wave""? No, not at all, just as you aren't confused by someone named Frank or Bob, both of which mean totally other things in English. As with so many things in language, context is key.

*Edit: husband destroyer is just weird on its own

Edit 2: my first gold?! Awesome! So stoked right now. :-)",0
Being drunk is the act of borrowing happiness from tomorrow. ,0
"An adult produces anti-diuretic hormone (vasopressin), and so make less urine during sleep. What usually happens is the adult wakes briefly in sleep and feels the sensation of fullness (rather than waking because they need to pee)

A young child doesn’t produce this hormone, and so will continue to produce large amounts of urine through the night, and often the bladder will be full during the sleep cycle, leading to the child to pee during their sleep.

Edit: spelling",0
"Perhaps the main reason is the amount of [brown fat](https://www.mayoclinic.org/healthy-lifestyle/weight-loss/expert-answers/brown-fat/faq-20058388) a person has.

Brown fat metabolizes stored energy quickly when you are cold. A person lacking much brown fat will feel cold much more easily. And the presence of brown fat is a genetic/developmental thing, not determined by diet.

This is along with the general differences in volume and surface area. A fatter person will have noticeably more volume than a skinnier one, but only slightly more surface area. Heat transfer occurs according to surface area, but total thermal energy stored is dependent on volume. This means skinny people lose heat more rapidly.",0
"When you eat food, your body breaks it down into nutrients that it needs for energy and other functions. These nutrients are used to fuel various processes in your body, such as maintaining body temperature, supporting organ function, and providing energy for physical activity. When you stop eating, your body initially relies on the stored carbohydrates in your liver and muscles, known as glycogen, for energy. Once these glycogen stores are depleted, your body starts breaking down fats and proteins to produce energy.",1
"Gah, some people responding here need to just not. 

Alright I'm going to spend some time breaking down how colour works for you before getting to the why. It's needed background information, but feel free to skip over the bits you know. 

Colour as we perceive it is not something inherent to the spectrum of visible light. It's related, but there is nothing about a photon with a 2.61whatever eV of energy that makes it inherently or objectively blue. The photon does not give a shit, it's just a photon that carries some energy and has the related wavelength. 

The perception of colour is a product of biology. Your eyes send an electrical signal to the brain and your brain interprets that information. It turns out being able to easily distinguish different objects is useful what for not getting eaten and finding things to eat, and most materials reflect different and unique parts of the light spectrum. Thus the eye evolved to give more detailed information to the brain and the brain evolved to process that into what we perceive as colour, thus reducing your chances of getting eaten by wolves or licking a poison dart frog. 

So how does the eye work. I'm sure you're aware that you have specialized cells in your eyes called rods and cones; rods cells are responsible for low light vision, cone cells for vision in bright light.  You've also probably been told that rod cells are achromatic which is why you don't see colour in darkness, but cone cells are chromatic and allows you to see colour in other times. This is accurate enough but a little overly simple. Each one of those cells, cones and rods both, contains a pigment, and like any pigment it absorbs light around a certain wavelength and reflects others, and when it absorbs light, this produces a response in the cell to the brain. They also respond most strongly at a certain wavelength and then falling off in a general bell curve type shape to the left and right of that peak. 

You can see an image of that [here](https://www.unm.edu/~toolson/human_cone_action_spectra.gif) (note, not to scale, comparative only) but to list: 

Rods activate between  400nm and about 640nm, with their peak around 510nm. 

Blue cones activate between  370nm and about 550nm , with their peak around 420nm. 

Green activate between  400nm and about 700nm with their peak around 530nm. 

Red activate between  400nm and about 700nm, with their peak around 560nm. 

This is why the light off a 420nm ish laser looks really really blue. It's activating the blue cones in your eyes very very strongly, and everything else much weaker. However most things aren't lasers and don't produce/reflect monochromatic, instead  they reflect multiple different parts of the spectrum. That results in more than one wavelength of light hitting your eyes at the same time. You'll also notice there's quite a bit of overlap between the ranges of activation for the cells in your eyes. It's that overlap combined with mixed wavelengths triggering more than one type of cell at a time that gives rise to the rest of the colours. So while the visible spectrum looks like [this](https://qph.ec.quoracdn.net/main-qimg-c9bd877c0d7d5a998e2dfba6736d3389) the diagram of colours we can percivce looks like [this](https://i.stack.imgur.com/ZKral.png)


So now we can get into the why. That last diagram  there is a C.I.E. Chromaticity chart. The X,Y coordinates describe the relative strength of activation of each of the cone cells in your eyes. Closer to the bottom right, red is most strongly activated , bottom left blue and top left green. If  I point violet light at your face (so bottom left) and then start to also add in red light by increasing the intensity of the red you can follow the charting from violet and start moving into purple into a sort of light fuchsia to a dark fuschia and so on. Eventually there's just tons of red wavelength light bombarding your eyes, in comparison to blue wavelength light and the red overwhelms the blue and you see nothing but red. This is also why a really really bright violet wavelength light appears blue, but bright purple still looks purple. Intense violet light just triggers blue cone cells strongly so our brain goes ""oh that's really blue"", but intense purple still maintains that mix of red and blue we perceive as purple

Also contrary to what you may have heard, rods do play a role colour vision, however not all of the time. You need light that's dim enough for rods cells to be activated, but not too dim that the cone cells stop activating. This is why colours get a bit funny around dusk/dawn or in areas with dim light from streetlamps. Rods activate quite strongly in the sort of blue-green area of the spectrum. So when the light gets dim reds start to become more dull or even close to black, but you're more sensitive to light in that green/blue chunk spectrum and can still make out greens and blues. 

That also why things like aircraft cockpits use red lights. Rods are not saturated by the red light and remain active thus allowing for night vision when looking outside of the aircraft, but the red light stops you vision from shifting entirely to scotopic (night) vision where the pilots would no longer be able to read their instruments. It's also good way to observe nocturnal animals who usually can't perceive red very well and are thus still in the dark as far as they're concerned. ",0
"I've never related more to an analogy.  


Every morning, I do the same things in the same order.  
The other day, I *didn't* need to pee when I woke up, and it messed my routine up badly enough I missed the bus 30 minutes later.",0
"Edit: ELI5 version. The batteries are different types on the inside. The cell phone type gets hurt at 100% charge and the cordless house phone type lasts longer at 100%, plus the house phone type is much cheaper and easier to replace (so even if it doesn’t last super long, it’s not a big deal).

Edit2:
ELI20 now features more ELI20-like-explanations In parentheses.


I've worked in the battery biz for 10 years.

Most commercial li-ion batteries don't handle high state of charge (battery % shown on your device) very well as the oxidation rate (how fast the liquid inside is getting used up) of the electrolyte is high at this state, so you lose lithium ions rapidly (battery can’t store as much charge.)  This causes loss of capacity (runtime).  

The internal resistance (resistance is the phenomenon that reduces battery power) of the battery goes up at the same time, and it can't charge/discharge as quickly anymore, so the end effect is it can't perform well in the device (the device needs a certain amount of power, which the battery will eventually struggle to deliver) and this actually accelerates the capacity fade even more. (Battery loss of runtime will get worse each day)

Most household phones are Ni-MH chemistry which is much lower energy density (heavier and less sexy) but handles high state of charge much better.  

They used to put Ni-MH in cell phones in the mid-to-late 90s, they were big, bulky, and didn't have a lot of talk time.   Li-ion was better in every way for cell phones.  For household phones this isn't really a problem, and Ni-MH is much cheaper to implement, plus they are more easily customer replaceable (they don’t catch on fire, and are widely available)

You can get a much longer lifetime out of lithium-ion batteries by implementing lower maximum voltage, for example 4.1V max charging voltage instead of 4.2V.  This is called derating and is used in more high-end applications like Teslas (4.1V is normal charging mode, 4.2V is ""extended range"" mode which you shouldn't use if you don't have to).


""Regular"" lithium-ion batteries are usually charged to 4.2V and the device makers don't give a shit about the long term longevity, they just want to advertise longer runtime on a single charge at beginning of life.",0
Watching brown bears get drunk off of a crab apple tree and then destroy the tree was a highlight of my time in Alaska.,0
"There are two hormones governing moulting and metamorphosis in insects. Ecdysone is a fat soluble hormone and increases towards the end of each instar (it accumulates in body fat). Once a threshold is crossed, a moult is triggered. Ecdysone levels drop immediately after the moult, then slowly build up again towards the next peak.

Juvenile hormone (JH) shows declining expression with age. It tells the body what the next stage should be at the ecdysone peak when moulting is triggered. In a caterpillar, once JH levels drop below a predefined threshold, the next ecdysone peak initiates the pupal stage. If the caterpillar is underfed, this ecdysone peak (and hence the next moult) is delayed until sufficient energy reserves are available. 

Tl;dr - Metamorphosis is delayed till the caterpillar has enough stored energy available",0
"Imagine you have a straw and you put one end of it in a glass of water. If you suck on the other end of the straw, the water gets pulled up and you can drink it. A well works kind of like that, but on a bigger scale. A water well is a hole dug into the ground until it reaches an underground layer of water called an aquifer. This aquifer is like a big sponge that soaks up water from rainfall or other sources.",1
"Hey, I can answer this!

Our mood is affected by specific chemicals in the brain: serotonin, dopamine, glutamate, and norepinephrine (and many others, but these are the main ones).

For this explanation, it isn't important to understand what these chemicals do exactly. The only important thing to know is that these chemicals are responsible for *many* more of our brain's functions than simply our mood.

If you've ever lived in a house with old or poorly installed wiring, you may have noticed that when a large appliance starts up (like a washing machine or refrigerator), the lights in other parts of the house will dim. 

This is a good analogy for what's happening in the brain.

How much of each of these chemicals we have (and how they interact), depends on many factors. I won't list them all, that would be impossible. But here are some major ones:

* Current diet (could be excellent or terrible)
* Time of day
* Current physical fitness (could also be excellent or terrible)
* External stressors (like loud noise or sitting in an uncomfortable chair)
* Internal stressors (like exercise and/or illness)
* Mental stressors (like worries and/or happiness)

When the body is trying to use these chemicals to adapt to changing conditions, sometimes it has to allocate brain chemicals in a way that negatively affect our mood. It can ""dim"" us, just like those dimming lightbulbs, and we don't know why because we can't feel those other things the brain is using those chemicals for.

Notice that I included both what we would consider ""positive"" and ""negative"" influences on our current state of being. 

The real bitch of mood disorders caused by chemical imbalance is that the negative mood can *sometimes be brought on by being otherwise good to yourself*.

If you are experiencing such feelings here are some suggestions, because I've been through it, and I wish someone had told me what I'm about to tell you.

First a disclaimer: *I'm not a doctor, this is not medical advice, and if possible you should try to find a therapist that works well with you if your mood is impacting your life in a bad way.* I also know that sometimes it's impossible to see a therapist/doctor because of financial (or many other) situations, so here are some techniques that work for me. *They may not work for you,* but they will at least give you a place to start, and hopefully begin to feel that your mood is something you can control or at least influence instead of the other way around.

Whether you're able to start looking for a therapist right away or not, hopefully you can use these as stop-gap measures until you can find a professional that works well with you:

* **Take a single deep breath, and remember it's just a ride.** I usually go to existential places when I get sad. It helps me to remember that while it may be a shitty ride, it's still just a ride, and however bad I feel, if I can make it through the next minute/hour/day, it's likely to change for the better.

* **Drink a big glass of water.** Dehydration can wreck my mood even if everything else is going great for me. I take a drink, sit still for a bit, and see if I feel better.

* **A bad mood does not equal a bad person.** I was raised to believe that I'm a bad person, and always will be.  Even if you weren't raised to be religious, many cultures attribute a moral value to how useful or good we are able to be. A sad mood can wreck my ability to do anything for a while, but *that does not make me a bad or ineffective person,* it just means I'm a good person who has some hard shit to get through.

* **Cry.** I don't usually have time to feel sad about things the way I'd like to or need to. Sometimes many small events in my life build up to the point where I just need to sob about them for a while. 

* **Eat something.** Something healthy if at all possible. A popular saying is ""food is the original anti-depressant,"" and it's true. I try to seek out something healthy to eat, but if there's nothing else around and I need to eat some junk food to make it through the day (a serious problem in Western cultures, but especially the States), that's okay. 

* **If I can't eat healthy, or if I simply over-eat, try eating ""one less.""** A friend once told me that ""willpower is a muscle. I have to start small and go easy on myself while I practice."" That little comment changed my life. Sadness and obesity go hand-in-hand, like Strong Sad from Homestar Runner prancing along with another Strong Sad (god I hope this isn't a terrible reference). So, when I began my journey of trying to manage my weight, I began by eating and/or drinking one less serving of whatever I was having in a meal: If I usually would drink three sodas, I'd only drink two. If I would usually eat five slices of pizza, I'd only eat four. In that way I very slowly trained myself to simply eat less. Now, this doesn't mean that the food I was eating was necessarily healthier. *But.* It meant that mentally I had developed a habit that made it *much easier* to make positive food choices for myself.

* **Take probiotics.** Gut health [can improve mood.](https://fivethirtyeight.com/features/gut-week-gut-brain-axis-can-fixing-my-stomach-fix-me/) Probiotics help, and they're in [gummy form now.](https://www.google.com/search?q=probiotic+gummies&oq=probiotic+gumm&aqs=chrome.0.0j69i57j0l4.3398j0j7&sourceid=chrome&ie=UTF-8) If I can't afford probiotics, eating yogurt is a great way to get helpful bacteria into my belly. As I continue to improve my eating my mood *does improve*. 

* **Gut problems might be caused by common allergens.** I'm not trying to be gross, but it's a very addressable problem that many people just don't talk about: I'll put myself out there and state that I had a *lot* of problems with gas/bloating/diarrhea for years. It turns out I had a food allergy, and eliminating that food from my diet improved my belly feels dramatically. 8 kinds of food account for 90% of food allergies. I tried removing them one at a time, and if I didn't feel an improvement, I was free to add that one back into my diet:

> * Milk (mostly in children)
> * Eggs.
> * Peanuts.
> * Tree nuts, like walnuts, almonds, pine nuts, brazil nuts, and pecans.
> * Soy.
> * Wheat and other grains with gluten, including barley, rye, and oats.
> * Fish (mostly in adults)
> * Shellfish (mostly in adults)

* **Clean/pick up ""just one more than before.""** In the same vein as ""eat one less,"" I gradually exercise my willpower muscle by doing *just one more thing* than I did before. If yesterday I did nothing, I put one sock in the hamper. If I put away one sock yesterday, I try two today. And so on. If I then collapse back into bed, I know that I made an improvement. It may seem stupid, but here's what [Stephen King said:](http://www.goodreads.com/quotes/120532-when-asked-how-do-you-write-i-invariably-answer-one) ""It sounds too simple to be true, but consider the Great Wall of China, if you will: one stone at a time, man. That's all. One stone at a time... I've read you can see that motherfucker from space without a telescope.”

* **Take a shower.** Feeling clean helps my mood a lot.

* **Take a walk.** Gretel Ehrlich said ""walking is also ambulation of the mind."" If I walk at all, I’ve taken a step (several, actually) to improve my condition, and to be healthier. Double bonus. If I walk for 30 minutes, I have successfully exercised for the day. Triple bonus.

* **Manage stress in whatever way works.** I've tried yoga, volunteering, calcium antacids, rubbing my head, stretching, breathing deeply, screaming, punching a bag, lifting weights, helping a friend out, journaling, listening to soothing music, and a bajillion others. I try anything I can think of, and keep the ones that work.

* **Take a dry erase marker, and write a positive thing about myself.** This one frankly sounded stupid when I first heard it, but I got so desperate I gave it a try. And it works. I wrote things like ""I love me,"" ""I am a good person who wants to do good things,"" ""I am great at __________,"" ""I'm a beautiful person,"" and many others on my mirror. I'd say them to myself 10 times every time I looked into the mirror. At some point, those sayings became my internal monologue, and I started to feel better about myself.

I'm not trying to preach, and what I posted might not work for you. But I sincerely hope it does. If you are feeling sad and need someone to talk to, please PM me. I always want to help someone who's going through what I've been through. 

If you are feeling like you don't want to exist (which one of my friends wryly referred to as being ""casually suicidal"") the suggestions above might help, but also might not be quite enough to get you to feeling balanced and level. If you need someone(s) to talk to, try /r/depression and/or /r/suicidewatch 

I really hope this helps. Love you all.

EDIT: Formatting.

EDIT 2: Added a section about ""one less."" Again, hope this helps. <3

EDIT 3, THE GILDED BUGALOO: ...I really don't know what to say. Thank you all so very much for your words of gratitude and encouragement. All I've ever wanted to do was help people, and today it seems like I did. I'm intensely grateful for you all. Thanks for being here, thanks for being alive. You matter. Even if you don't hear it ever as much as you should, you matter, you're worthwhile.

I can't say it any better than this: ""But what I hope most of all is that you understand what I mean when I tell you that, even though I do not know you, and even though I may never meet you, laugh with you, cry with you, or kiss you, *I love you.* With all my heart, **I love you.**""

<3

[EDIT 4](https://www.reddit.com/r/explainlikeimfive/comments/67msip/eli5_why_do_human_beings_just_get_sad_sometimes/dgtn9i7/)",0
"Generally speaking, terrorism refers to violence with *ideological* motivations: political, religious, etc. Although bomb attacks usually are ideological in nature, they don't by definition have to be. Someone can commit a bombing for non-ideological reasons (maybe the bomb is an attempt to murder someone with whom the perpetrator has a personal dispute with.) 

Terrorism is not defined by tactics. If a man stabs ten people in a mall because he's a dickweed, that's not terrorism. If a man stabs ten people in a mall because he thinks it contribute to the overthrow of the government, then that is terrorism. 

It's also worth mentioning that the word *terrorism* doesn't really define what terrorists do. The term implies that creating fear is terrorists' primary motivation, but this is generally not the case. Terrorists' goals vary depending on their motivations, but causing widespread terror in the minds of populations is generally more of a desirable additional effect than the main goal. The generally accepted modern definition of terrorism is violence or the threat of violence by non-state actors driven by ideological motivations.  ",0
"When you suffer a concussion, it means that your brain has been injured. Falling unconscious after a concussion can be a sign of a more severe brain injury, and it's important to seek medical attention immediately. The brain is a delicate organ protected by the skull, but a strong impact can cause it to move around and hit the inside of the skull. This can disrupt the normal functioning of the brain. Falling unconscious is a sign that the impact was significant and may have caused more damage.",1
I’m in property management and can tell you the industry is moving away from heat treatments. It’s very difficult to kill bugs that are in carpet or anywhere they are insulated from the heat. It’s expensive and drives bugs into adjoining units. We have better results with steam and chemicals.,0
Your anus has similar “spice” receptors that your mouth does.,0
"Smelling salts are truly vile. It’s very strong ammonia. It doesn’t “smell bad” it’s a chemical attack in your sinuses that you can’t ignore, like tear gas or something.

Years ago when I worked in the ER there was a particular challenge. Habitual drunks would come in literally comatose. If you monitored them and gave them some IV fluids they would always wake up safely, in time. BUT a small number would come in drunk AND with a life threatening brain injury (often with little or external trauma, from falling down...drunk) . How to tell the difference? You can’t do head CTs in every drunk in a busy ER (and some habitual drunks would get like 80 CTs a year). Enter the supertucci brain injury severity test. I would crack one of these and cram (I mean gently place) it up one nostril. If they couldn’t muster the 10 neurons to pull it out (and they really wanted to pull it out since it is so obnoxious) it was OFF to the CT scanner for a sometimes life saving scan. On average my ER has one case a month of a drunk who didn’t sober up over time, got a CT and only then we realized he also had a brain injury we’ve been sitting in for hours. I had zero. 25 years later I still feel good about that. 

Edit: drinks to drunks (sorry on phone)",0
"ELI5 version.  Blackjack has a memory unlike other games.  Big cards good, small cards bad.  5 year-olds shouldn't gamble anyways. 

Source: former casino employee and card counter.

I'll start with some terms here:

House edge: expressed in a percentage.  The money the house expect to win on each bet.  

Basic stragety: a tested theorem that that dictates a players move in a given blackjack hand. Please Google basic strategy,  there's a neat color coded chart you can look at.

Units: the number of increments of the minimum bet that the table maximum will allow.  For example, if you are on a $5 minimum BJ table with a limit of $250  that only allows you to play one hand you can bet 50 units.

House edge is a representation of the mathematical advantage that the house has built in its rules..  Here are some examples.  
Roulette:  straight up bet pays 35 to 1.  I'd you win you get 35 plus your bet. There is 1 way to win, and on a double 0 wheel, 37 ways to lose. If you cover every number (stupid) you lose 38 and win 36.  So your return is 36/38.  That's 94.7% return  giving the house an edge of 5.3%
Craps:  betting on a hop (one roll bet) pays to 30 for 1, 30 to 1, 15 for 1, or 15 to 1.  Deference here is academic in this case, most casinos only for one, but to one is better.  The 30s are for pairs (hard ways).  Let's say you think 11 will come next roll.  There are 2 ways to roll 11, 6-5, and 5-6, if you have problems seeing this, pretend the dice are different colors.  2 dice x 6 sides = 36 combos.   You have 2 ways to win out of 36.  Or, 1 in 18.  This bet at best pays 15 to 1.  So win 16, lose 18.  That's an 88.8% return giving the house an edge of 11.2%.
SERIOUSLY, I KNOW HOP BETS SUCK.  THIS IS AN EXAMPLE OF HOUSE EDGE.  Bet the pass line with 10x odds.  Happy?


The edge represents the difference between the true odds vs payable odds.  With me so far?  

Here's where things change.
In all other games of ""chance"", these odds never change.  Two dice always roll 11 with the same 2/36 chance, number 21 comes up approximately 1 out of 38 times in roulette. This is inescapable.  Short term variance is expected and even necessary,  no one would play a game where they always lost.  But blackjack and it's variants (of the non continuous shuffle variety) are different fundamentally, they have memory.  There a finite number of cards in that games universe if you will.  In a 6 deck shoe (suits don't matter in BJ) there are 312 cards.
24 of each A-9, and 96 10 value cards (10, J, Q, K no difference between these functionally)
Basic strategy is a statistical model that, based on the  rules of the casino at which you are playing, help you make the least bad decision.  I say least bad, because it is accurate.  There are no good decisions in blackjack because statistically it is still a negative expectation game.  Meaning, over a large sampling, with perfect basic strategy, you will still lose.  Now, this is for everyone in the back, STATISTICAL OUTCOMES HAVE NOTING TO FUCKING DO WITH SHORT TERM SUCCESS OR FAILURE!!!!!!

So, a recap here.  Most games suck because the odds are bad and built in.  You can chose less bad bets, but all bets are bad, get it?

But wait,  you said Black Jack is different!  It is.  As I said,  basic strategy lets you make the least bad decisions.  Some situations actually have a positive expected outcome, like doubling down when you have 11 vs a dealers up card of 6.  Keep in mind though, that the aggregate of all those situations still places you in the territory of negative expectation.  

Card counting:  there are plenty of tutorials online to teach this, but as a very simple rule of thumb, because of the rules of hitting and staying that casinos follow for their dealers, big cards favor the player, while little cards favor the dealer.  I'm not going to get into the math here, it's rather lengthy, but suffice to say that knowing the ratio of big cards to little cards remaining in the deck can offer an advantage to the player.  

So, why does the dealer have the advantage on black jack?  It's not their hit/stand rules.  Those only exist because if they were any more lenient to the dealer no one would play.  THE ONLY ADVANTAGE THE HOUSE HAS IN BJ IS THE DOUBLE BUST LOSS.
Scenario 1.  Player has 20, dealer busts.  Player wins
Scenario 2.  Player has 20, dealer has 20.  Player pushes (ties) nobody loses/wins
Scenario 3.  Player busts his hand, dealer has 20.  Dealer wins.

These all seem fair so far.  Here's the one that earns all the house it's money.
Scenario 4.  Player busts his hand.  Dealer busts his hand.  Player still loses.

That's it.  They only advantage.  So, try not to take hits that can bust you unless you statistically need to.

Let's now regroup.  
1.  You know that busting loses your money, though paradoxically, standing when you should hit, while reducing your bust %, actually lowers your statical overall win % more.
2.  You know that because 10s are good for you and little cards are bad, if you can keep track of the ratio of that and are in the right situation, you can have an advantage over the house.  The average difference in expressed advantage generally is only 2%.  Let's say the house has a 51-49 advantage over you, sometimes you can make that 51-49 in your favor with counting.

How do you take advantage of this?  Unit betting.  
Let's take a $5 dollar table. It has 7 spots.  You are the only player.  It's max is$ 500. This casino let's you play all 7 spots if the table is empty.  Therefore, the unit spread is as follows 500/5 = 100 units per spot *7 spots = 700 unit spread.  (This is an example only, doing this will get you labeled as a counter on the spot and backed off immediately) As you know the deck (shoe) favors the dealer most of the time it makes sense to expose the least amount of your money to that negative expectation.  When the 10 ratio changes in your favor, you can now expose 700 times that bet to a positive outcome.  
So while you're only going to have a %2 advantage, you're now betting 3500 with a better than even chance of winning.  (YES I KNOW THINGS GET STREAKY SOMETIMES.  SHUT UP, THESE ARE BASICS. GO FIND SOME COUNTING SUB REDDIT)

So, get it?  Expose less of you money when you're at a disadvantage and more when it's in your favor.  Counting is the exact reverse of the casino model.  You're eking out wins on a small marking, but unlike the casini, you can alter your betting and actions with this knowledge 

TL;DR. yes, blackjack is the a beatable game, but only if you're a card counter.


P.s. baccarat.  Yes, it is a coin flip.  And yes, I've heard of a way to count it.  Here the only situation I know of where it can be counted.  At the beginning of the shoe, the dealer reveals the first card and burns that many cards.  If you can keep track of the 10s in the shoe, and this count of remaining 10 in the deck is equal to burn cards plus the remaining cards in the deck after the cut card is pulled out, theoretically you could guarantee a tie bet, but I'd be willing that this is sore rare in practice, you're better off just studying something useful and getting a career change or a promotion.

Sorry about the spelling, this is a long comment for a phone and my dog has been licking my face

Edits:  edited for grammar and spelling, as well as an expanded craps section because some random craps player thinks he knows what's going on and was confused.  As always first reddit gold, thanks so much.  Also, for people saying poker, pai-gow, video poker, craps!

I try not to talk about thing I don't know much about, but here:
1.  Poker is beatable if you're good.or with weak players. 
2.  No commission pai-gow that let's you bank every other hand is probably close to 50-50. Don't know for sure, check the math.
3.  Apparently you can count baccarat, just got a link to look at.
4.  Yes, some video poker offers an advantage.  Google it
5.  Yes, crops has some good bets.  Pass with 10x odds is like .43% house edge or something stupid small.  Still not 0
6.  Advantage play tons of advanced techniques like shuffle tracking, hole tracking, taking advantage of weak delaers, sure that may make a game beatable.  YMMV
",0
"The things you must have seen during inspections. Could you show up unannounced? Or did you have to give notice of an inspection beforehand?

I used to do pest control both residential and commercial. And some of those restaurants, OMG! One I remember pulling up to. We were instructed to park in the back because they didn't want people seeing our truck there. I get out of my truck and sitting outside closer to the dumpster than the building were about six five gallon buckets full of water and frozen chicken and fish defrosting. On a 98 degree Florida afternoon. 

The roaches were everywhere. Even running along the windowsills where customers are sitting in a booth. I would tell the guys in the kitchen they really needed to get that piece of chicken out from behind the fryer because I've been reporting it for the last 3 weeks. Sheet cakes would be left out overnight uncovered and in the morning they would shoo the roaches away. Cleaning the kitchen floor consisted of hosing the floor down with plain water and then pick out the big junk and squeegee the water into the floor drain. No mop. No soap. 

The only products I would use inside the building was food grade diatomaceous earth and only in approved areas. A gel bait containing fipronil in cracks and crevices away from food prep areas, and an IGR  (Insect Growth Regulator) inside wall cavities and such. And ultraviolet fly traps with the sticky pads inside and not the zappers. 

One night I get there just as they are closing and I bring in my equipment and start using a gel bait near the window between the kitchen and the waitress stand. As I'm doing that the owner grabs the tank of IGR and sprays the shelves of the waitress stand where coffee cups and silverware are kept. Wetting everything down with the IGR. I grabbed the sprayer away from him and told him once again it is illegal to spray that stuff like that. He replied, ""Well the girls told me they seen some bugs there so it needs to be sprayed.""

I told my boss I refused to go back there because if someone got sick from him trying to misuse our stuff I would be responsible. We ended up dropping them as a customer. Funny thing is a lot of people in the area talked about how great the food was.",0
"Well, the terms ""hot people"" and ""cold people"" are not about physical temperature but rather about how people perceive them in terms of attractiveness or popularity. When we refer to someone as a ""hot person,"" it means that they are considered physically attractive by many people. This perception of attractiveness is subjective and can vary across cultures and individuals. Factors such as facial symmetry, body proportions, and overall grooming can contribute to someone being perceived as physically attractive.",1
"I can't find the YouTube video I'm thinking of, but [this one](https://youtu.be/cIKnrFNsGpA?t=2m59s) (link should start right about the time they're ""whitening"" the white part of the candy cane) gives a good fast-forwarded example of the process. Basically, to get the candy to look white, the candy is folded many, many times, trapping tiny air bubbles which makes the candy look white. Then, as you're sucking on the candy, your saliva dissolves the microbubbles until you have little bubble craters in the candy.",0
"If you stab your belly with a knife, first you will cut the skin, then subcutaneous fat, then your abdominal muscles, then visceral fat, then your organs. 

That means if you press your belly and feel softness, you are pressing against your subcutaneous fat. Cutaneous means skin, and subcutaneous means below the skin. If you press against your belly and feel something hard, you are pushing against your muscles.

If you have a big belly, but it's hard that means that you have a lot of visceral fat. Visceral means deep. It feels hard because you are pressing the muscle, but there's a lot of fat behind the muscle which causes your gut to bulge.

This visceral fat is very dangerous. It's right next to your organs, so it can ""spill into them"". You can get non-alcoholic fatty liver disease, for example. Visceral fat is the thing most associated with heart attacks, stroke, diabetes, etc. In fact, measuring your waist size is probably even better than measuring your weight or BMI.

Drinking alcohol causes beer bellies. Taking in a lot of calories causes beer bellies. And most importantly, genetics causes beer bellies. Asian people tend to store their weight in their belly, which means they can get heart attacks at far lower BMIs than other races.

Fortunately, even though visceral fat is the most dangerous kind of fat, it's the easiest to lose. Cardio like running, swimming, cycling can melt away visceral fat. It's the first kind of fat to go.

As a last thing, sometimes people say that if you have a lot of visceral fat, you are apple shaped. Your gut is big and your arms and legs are small. If you have a lot of subcutaneous fat (especially in your thighs) you are pear shaped. ",0
"Honestly, SMS itself is just a shitty technology. Trying to get additional features onto that stack is just a waste of time. Once we improve data networks everything should just be sent by TCP/IP like regular internet traffic. Phone numbers in general just need to go away. The idea of having to remember or record numbers that belong to someone is just ridiculous. That's why we invented DNS.

Imagine if websites or services had to tell people when their IP changed, and if you had it written down wrong you couldn't get to that service. It's laughable. Phone numbers and SMS seem the same way to me. If I want to contact someone on Facebook I just look up their name. That name is associated with a unique account that person controls. Right now the only advantage of 'phone numbers' is standardization, but beyond that it's just ancient tech that hasn't really evolved much.

---

Edit: Wow, didn't think everyone would leap to defend PSTN this much. Guess we'll be stuck with shitty quality 4kHz voice transmission, glaring SMS limitations and manual lookup for a long time to come with this much resistance to innovation.",0
"Because it is easy to step down an investigation, but much harder to ramp one up. Treating an explosion of undetermined, but suspicious, origin as a terrorist bombing means that any possible evidence recovered will be handled and logged in accordance with evidentiary procedure. Explosives experts will be among the first people combing through the debris, looking for any of the components of the explosive device.

If it is treated like an accident or something other than a criminal act, then it is entirely possible that valuable evidence may be lost or overlooked. Evidence might be handled or recovered in ways that make it useless for testing or inadmissible in a trial.

If you follow the most stringent investigative procedures, you know you're good. Worst case scenario is that you were too careful and exacting.

These are the same reasons suspicious fires are investigated as possible arson, and deaths of unknown origin are treated as possible murder. Until your investigation shows that the worst case scenario isn't true, you want to be certain that you aren't underdoing it.

EDIT TO ADD:  The reason it is treated as a terrorist bombing specifically, rather than any other type of criminal bombing, is because that gets the intelligence community involved.  That's the critical difference between the investigation of a possible terrorist act and a 'regular' criminal investigation.",0
"When you fall asleep hungry, your body enters a state of rest and conserves energy. While you sleep, your metabolism slows down, and your body taps into its stored energy reserves, like glycogen and fat, to sustain you through the night. This process ensures that your vital organs and brain receive the energy they need to function properly. Additionally, during sleep, your body releases hormones that help regulate your appetite, such as leptin and ghrelin.",1
"Am also a doctor. Can confirm this is right. A lot of the other posts (even the highest up voted ones) are inaccurate.

I'm specifically an ER doc. So I'd be the Dr this kid saw. Wouldn't pump stomach without good reason (like the ones mentioned here). There are risks to pumping stomach as well. Has to be weighed. My guess is it was #1 that got the stomach suctioned. 

Iron toxicity definitely not anywhere on my list of concerns with this patient presentation.

Edit: TL;DR I concur.

I posted this after a night shift and damn my phone blew up. Thanks to all the Dr's for coming out to concur!",0
"A couple of other comments concerning peeing and pooping in utero (in the womb):

- Meconium present at birth indicates fetal distress, the baby was stressed by something in the delivery process or shortly before.  As has been noted, the stuff is really sticky and if it has been aspirated (inhaled) can lead to breathing difficulties.  This can potentially be fatal so meconium at birth is not a good finding.

- If a child fails to produce urine due to malformation of the kidneys or urogenital tract (bladder and urethra) a condition called oligohydramnios occurs.  Not only do we ""drink"" our urine as mentioned but we also ""breath"" it in the womb.  Babies inhale and exhale amniotic fluid.  If there is insufficient amniotic fluid because baby is not peeing, these breathing movements cannot occur normally and the lungs do not develop normally leading to severe problems at birth which can potentially be fatal.  In really severe cases of low amniotic fluid because baby is not making urine something called ""Potters sequence"" occurs.  ""Potters"" is not someone's name but comes from the fact that the child looks like they had been trapped in a clay pot.  They have abnormal development of the lungs because they have no ""breathing"" in the womb (the uterus has squeezed them and restricted this), have abnormal development of the joints in the arms, hands, and legs because they could not move (baby could not kick and move in the womb), and have abnormal appearance of their face because it was smushed by the womb.  They may not have the normal lines on the palms of their hands because they could not move them.  If severe enough oligohydramnios, the child will not survive due to the failure of the lungs to develop normally, the lungs look ""solid"" and not like ""sponges"" because they little sacs (alveoli) where oxygen adsorption occurs do not develop.  Peeing in the womb is REALLY important!",0
"Sounds like this was answered well already but just to add...I was a weather forecaster for the Air Force for 6 years. We actually had a few lessons about tips and tricks that were true to forecast. Red sky was one, also leaves that are showing their underside mean it will rain soon, cricket chirp intervals are indicative of specific temp ranges etc. There were more, I just can't remember as this was over 10 years ago. I was blown away by how many of these ""tips"" I had heard prior to my experience forecasting actually being true. Science is some cool shit!",0
"adding to this - bees' stingers only break off and kill them on larger animals.
A bee can easily stab anything that small without dying.",0
This both makes sense and makes me a little sad.  I liked the idea that they were all just shit faced every day.  Like ancient Cheers or something.,0
Damn that was so fuckin eli5y,0
I got that as well. I think it's called being ugly.,0
"Facial expressions are closely linked to emotions. It is thought that the purpose of facial expressions is to convey emotions within a social group. Studies have shown that the link goes both ways: being happy can make you smile and forcing yourself to smile can make you happier, which reinforces the idea that feeling emotions and exhibiting their signs are closely linked. Emotions aren't entirely involuntary, but strong emotional reactions that are triggered by an external stimulus can be hard to override. Basically, what I'm saying is that controlling the physical manifestations of your emotional responses partly involves controlling the emotions themselves, which can be much more difficult than just moving your arm around or some other voluntary muscular action.",0
"Can I ask a stupid question? If the voltage was much higher, would the phone charge quicker, or is there a hard limit on the pace at which it can be charged?",0
"Precisely why I've said the idea of flying cars is idiotic and will never happen (until hardcore automation takes control anyway). Look at how bad people are about vehicle maintenance. When was the last time the average person checked their tire air pressure or changed their oil? In a car on the road, if it fails, you pull over (or rumble to a stop). If a plane fails, you fall out of the sky and likely die... and also might hurt people on the ground. So, in addition to the average person already being a shitty driver on an essentially 2D plane, imagine how bad things would be if the third dimension of verticality came into play. And how bad maintenance habits would exacerbate the situation further.",0
"ELI5: The reason there is no cure for herpes is because it is caused by a virus called the herpes simplex virus (HSV), and viruses are very tricky to get rid of completely. When you get infected with HSV, it stays in your body forever. Even though your body's immune system tries to fight it, the virus can hide and go into a dormant state, making it difficult to eliminate completely. Another challenge is that herpes viruses have the ability to change their genetic material over time.",1
"This is a very good answer and very true, but it is also important to note that you are made up of a lot of different types of cells, and not all of them respond exactly the same to different environmental conditions.  We are no-where near the technology to be able to 'freeze' different cells at different levels/paces.

Just to illustrate the difference in cells: You've heard of the ""Walking Ghost"" phase of radiation poisoning? (The point where someone feels fine after receiving a large/lethal dose of radiation, for a period of time after being exposed). That is because a lot of cells are hardy enough to withstand quite a bit of radiation, but certain ones (the rapidly dividing ones) generally die.  In several cases, the are the 'factories' that produce replacement cells, so you can keep going on the cells currently in circulation, but once they die out you have no replacements. 

Now can you imagine trying to target 'rapidly reproducing cells' for a different course of freezing than the rest of the body? This is bone marrow, the lining of your intestine. 

tl;dr: It isn't only that you may lose half the cells that will kill ya, but that the half you lose might be concentrated on certain types of cells that you *really* need to live.",0
"Jews are an ethnoreligious group, meaning that they are both an ethnicity (a group identified by common group identity and, usually, language and ancestry) *and* a religion (a group with the same beliefs about the supernatural). They're not the only one, but they're the most prominent in modern affairs because they happen to be the only one with a distinct (and relatively influential) world government [edit: see the section below on the use of ""world government"" here]. By contrast, Italian people are not an ethnoreligious group. They're an ethnic group (shared heritage, ancestry to some extent, and language), but their religion (Catholicism) isn't associated with their ethnic group (they share it with Hispanics, among others).

The reason for the difference is that, historically, Jews did not preach their religion to non-Jewish people, and largely intermarried with other Jews (or left the community as a whole when they didn't). So even though they lived in places where other ethnic groups lived too, they stayed a separate population both culturally and genetically. They certainly had some influence from their surrounding culture, which is why subgroups of Jewish culture exist today (Ashkenazi from Germany, Sephardi from Spain, Mizrahi from Asia and North Africa, and so on), but they stayed mostly separate from it and thus maintained their own identity for many thousands of years.

Christians, Muslims, and Buddhists, on the other hand, *did* actively preach and teach their religions to other groups of people. So even though the original Christians were from what is now Turkey, Greece, Syria, and Israel, they preached their religion across the Roman Empire and eventually carried it to totally different groups of people (like much of modern Africa) through colonialism. Similarly, Muslims began with Arab populations in what is now Saudi Arabia, but the early Islamic empires carried Islam as far as Indonesia in the east and Spain in the west.

-----

EDITs:

* I said ""much* of modern Africa. I'm aware that some Christians existed in Africa prior to the colonial era, but most African Christianity does descend from colonialism (particularly in West Africa).

* A lot of people have asked why Jews didn't evangelize. The reason is that the Judaism preaches that the Jews, as an ethnic group, have a special relationship with Yahweh, the Jewish god (who Christians and Muslims identify as their god as well). That relationship is explicitly with the descendants of Abraham, believed to be the patriarch of the Jewish people through his son Isaac, his grandson Jacob, and ultimately his great-grandson Judah (whose name is the origin of the word ""Jew"" in the first place).

* When I say ""world government"", I don't mean some illuminati ""Jews rule the world"" conspiracy. I just mean that Israel is an important state on the world stage. Israel is a ""world government"" in the same way that France or the US is.

----

There's a lot of people asking about the term ""race"". The differentiating factor between ""race"" and ""ethnicity"" is that race is often applied from *outside* of a group and tends to be more about *features* than it is about group identity or how individuals think of themselves. 

For example, a person descended from the Yoruba (a West African ethnic group found mostly in Nigeria) and a person descended from the Zulu (found thousands of miles away in South Africa) are from entirely different ethnic groups. They probably speak different languages, they have different worldviews and histories and ancestral religions and traditions, and they certainly would not (by default) have thought of each other as being part of the same group. But in American racial categorization, both would be categorized as ""black"" because both groups have dark skin. The same goes for, say, a Yamato person from Japan and a Miao person from southern China (both ""Asian"" in US categorization), despite the two sharing very little heritage aside from both having been influenced by Imperial China.

On the flip side, people of French descent are considered ""white"" in American categorization while people of Spanish descent are ""Hispanic"", despite speaking very similar languages and being relatively close to one another ancestrally. And that categorization shifts, too: French people get grouped with their much more distant cousins in Finland as ""white"", but that wasn't always so.

*Some* racial groups are also ethnic groups (this is the case for most Jewish populations, who do tend to be genetically distinct from the surrounding population). But race is a social categorization that need not follow genetic lines, as we saw with the Yoruba and Zulu a moment ago. In some cases, racial ideas can be so influential that it wraps back around to being an ethnic categorization again, as (slave-descended) black Americans form a pseudo-ethnic group that largely *do* share a common cultural heritage as a result of their historical categorization and treatment within the US.

Moreover, racial categorizations often differ between countries. A Japanese person would not consider a Korean person to be part of the same group as them and vice-versa, even though both are ""Asian"" in American categorization. Similarly, an American and a Frenchman don't think of themselves as part of the same group, but a Chinese person would probably not differentiate the two very strongly (except perhaps by language).

-----

EDIT2: A number of people are bringing up Jewish groups with non-Hebrew ancestry and claiming that this makes Jews not an ethnic group. This is not the case. An ethnic group often, but not always, has shared ancestry. To use Wikipedia's definition of the term:

> An ethnic group or ethnicity is a grouping of people who identify with each other on the basis of shared attributes that distinguish them from other groups. Those attributes **can** include common sets of traditions, ancestry, language, history, society, culture, nation, religion, or social treatment within their residing area. Ethnicity is sometimes used interchangeably with the term nation, particularly in cases of ethnic nationalism, and is separate from the related concept of races.

> Ethnicity may be construed as an inherited or as a societally imposed construct. Ethnic membership tends to be defined by a shared cultural heritage, ancestry, origin myth, history, homeland, language, or dialect, symbolic systems such as religion, mythology and ritual, cuisine, dressing style, art, or physical appearance. **Ethnic groups may share a narrow or broad spectrum of genetic ancestry, depending on group identification, with many groups having mixed genetic ancestry.** Ethnic groups often continue to speak related languages.

Jews around the world, regardless of their ancestry, identify with one another on the basis of shared traditions, culture, and religion (and to some extent language, since most Jewish communities use Hebrew in religious/cultural ceremonies even if they don't day to day). They are, therefore, members of a common ethnic group despite their distant ancestries.

----

EDIT3: Hi, people new to ELI5, I see this thread's got legs. Let me just direct you over to the sidebar:

> LI5 means friendly, simplified and layperson-accessible explanations - **not responses aimed at literal five-year-olds.**

----

EDIT4: To be clear, this is a simplified, basic introduction to these ideas - you'd find this material covered over a couple of pages of any introduction to anthropology. Like most introductory material, this is not covering some of the weird exceptions, debate within the field, fuzziness in definitions, or the many ways in which these ideas interact with others. **This is not the whole story**, and please don't walk away from this (or any Reddit post) thinking so - go take a class if you want to know more.

----

EDIT5: A summary of ""race"" vs ""ethnicity"":

* Ethnicity is about members of a group identifying with one another through some sort of shared cultural threads. Members of any culture (unless they disagree on the facts of how two people think of each other) will more-or-less agree on whether or not any two people are of the same ethnicity. ""Jewish"", ""Italian"", and ""Han Chinese"" (but not just ""Chinese"", which is a national group but **not** an ethnic one because there are non-Han Chinese people) are ethnic groups.

* Race is groupings used within a large culture to subdivide people into groups based on appearance. It's based on appearance, not identity, and is often applied to a group of people by other groups of people. Members of different cultures often disagree on racial classifications.",0
"I'm going to try an ELI15:

Sometimes a lot of behaviour is evolutionary. It's a bit of a generalization to say that geese are hyper aggressive and ducks are meek (although anyone who's been in Canada can tell you Canada geese have no fear). Realistically, there's no exact answer (as far as I know), but I can talk a bit about conflict in birds.

Here's the example I'll bring up between two very closely related birds: the blue heron and the great egret. 
Blue herons and great egrets lay similarly sized nests. In herons, most of the chicks coexist alright. In egrets, however, the chicks will often (85%? of the time) kill one another (exemplifying **siblicide**). Parents typically won't interfere with this behaviour - I suppose this could be defined as aggression. In fact, the parenting style was seen as an explanation for the siblicide. On the other hand, heron chicks do not really kill one another that often, since they had a different parenting style (loosely speaking). In the vein of great science, Mock & Parker decided to test out cross fostering (that is, having herons raise egrets and egrets raise herons).

They found that, in short, when a heron parents egret chicks, they still fight. I'm not going to mention the mechanism that encourages the siblicide in egrets, but the long and short is that egret chicks are vicious and will continue to kill one another, often leaving one chick to grow to adulthood. That is, the siblicide is **obligate** behaviour. When egrets parented herons, the mechanism for siblicide is there (parenting), and siblicide that *wasn't* there previously developed in the chicks, with the largest chick killing the rest of the nest. So, the siblicide (aggression, I guess) was both ""innate"" behaviour (again, *very* loosely speaking) and ""outside"" behaviour encouraged (facilitated) by the parents.

**tl;dr**: even closely related species (birds, for example) can have wildly different behaviours. Aggression is not necessarily environmental. In the case of geese and ducks it's probably many factors. There is, as far as I know, no short answer.

some sources: 

https://www.ncbi.nlm.nih.gov/pubmed/28556322 (Mock & Parker on the herons/egrets)

more reading

https://doi.org/10.1093/icb/14.1.249
https://www.ncbi.nlm.nih.gov/pubmed/9710456




Edit: more in depth about the experiment is here: 
https://www.reddit.com/r/explainlikeimfive/comments/8xfsq1/comment/e23udwm?st=JJF6G7NZ&sh=6cf27b5c",0
"Soil biologist here: Yes, the soil does run out of nutrients eventually. As the plant runs out of room in the pot, and as nutrients start to run out, the plant will simply not grow as much, lowering its energy demand. But the plant still needs some small amount of nutrients to sustain itself, even if it's not actively growing - so eventually the plant will die if the soil nutrients aren't replenished. This can be done with occasional additions of new soil or fertilizer. This is also why so much emphasis is placed on the soil microbiome (bacteria and fungi). The soil microbes can break down organic matter in the soil to provide new nutrients to the plant, extending the length of time that the plant can survive in that soil. The soil microbiome is really the reason most plants in nature can survive so long and why the soil (in a forest, for instance) doesn't run out of nutrients for plant growth.

Edit: as a disclaimer, my research focuses on soil microbes and how they cycle nutrients and associate with plants out in the wild. I'm not much of a gardener myself, so if you have questions about how to help your plants thrive, I recommend heading over to r/gardening or r/houseplants to get advice!",0
">Brown rice is thought of as ""better for you"", and so people expect it to be sold at a premium for being a better product

I sell my own product (not food) on-line and intentionally keep my price from being too low because I'm afraid people will see it as some poorly made item that isn't worth buying. I try to sell cheaper than competitors, but not so cheap people assume it's crap.",0
">Isn't a bombing in nature an act of terrorism regardless of who did it?

No.  Terrorism is using fear as a tool to try to force some sort of political or social change.

If the point of a bomb is to try to cause a group of people to comply with some sort of demand because they're terrified of being blown up if they don't, that's terrorism.  But if someone is setting off a bomb just because they're crazy and want to kill people, that's not terrorism.

In short, whether or not something is terrorism is a matter of intent (much like whether or not something is first degree murder is a matter of intent).",0
"> people were getting all up about indigestion. 

The Victorians were *obsessed* with bowels. They were convinced the intestine couldn't turn food into shit by itself, but needed regular intervention...kids were typically given a laxative dose every day if they didn't need it, and a massive one if they did.

I think they may have thought shit was an evil poison and the bowel had to be cleaned of it.",0
"It's a combination of factors:

More tasks / software bloating - The strongest of these is that normally you are asking your computer to do more tasks than before - some of this is subtle stuff brought in with Windows Updates and especially Chrome updates. Chrome started nice and efficient and a hell of a lot faster than Internet Explorer but has slowly gotten fatter and fatter. But it's not solely the browser's fault. As PC Processing Power, PC Memory, Browser Stability and internet speeds have all generally increased so websites have gotten more and more resource intensive (especially with the copious amounts of various advertising they force on you). The same holds true for a lot of software out there - that as PCs become more powerful so the software changes to leverage more of that power.

Security, security, security - A HUUUGE part of OS, browser and software updates is security based. It's very, very seldom that security updates result in increased speed or performance.

Failure Rates - RAM, CPUs, GPUs, HDDs, SSDs all have failure rates and these tend to get worse over time especially if there's significant heat in your system. I not talking total failure I'm talking bad sectors, I'm talking memory parity errors. Modern day OS and firmware do an immensely good job at handling this invisibly. Often you may not be aware that you have bad sectors at all. The sector has been discretely marked off limits and a replacement sector has been allocated. But when that happens it's basically introducing a permanent fragmentation onto your drive.

OS / Registry scarring - Back in the good of days of Windows 98 it was a pretty regular thing to reformat your system at least once a year - sometimes due to a complete OS crash - but often wanting to have a clean version on because over time you add and remove programs, you get the occasional virus, you run registry cleaners and you install a ton of updates and well as any tinkering you may have done yourself in the registry. This all leads to the registry and system files not functioning as well as it should. Registry cleaners are a mixed bag - they spot a lot of problems but their solution is to delete the problems.

Top Recommendations:

Antivirus - check that you only have one anti-virus on your system and that's it's not McAfee. Multiple antivirus apps will interfere with each other. These days Windows Security is an excellent choice for your antivirus needs.

Switch to an SSD - If you haven't switched yet and you can afford it I would highly recommend it. It's faster and doesn't suffer from fragmentation (assuming you don't live with your system drive 99.95% full). HDDs are still good for storage drives by your system and games should be running off an SSD.

Clean install - Especially if you've upgraded between windows versions or even between major builds you will be surprised how much better your PC will run on a fresh system. This goes well with upgrading to an SSD. Download the USB installer from Microsoft's website and get a completely fresh version of Windows with no manufacturer bloatware on it. Do make sure tht you've backed up EVERYTHING you need: files, passwords, websites.

Remove software that you're not using  - especially any software that installs it's own services. I try where possible to use portable versions of applications -  that way you know that they're not cluttering up your registry, system files and services. Also always check if there isn't a windows app that does what you want already.

Hosts file - Use your hosts file to block advertising sites - this is fairly technical and I don't recommend for the average user but it's preferable to using ad blocker software. It's a fast, nasty but uncomplicated firewall essentially. What I do when I find a website that's running slow it I analyze that particular website on [webpagetest.org](https://webpagetest.org) \- I identify the external links which are causing delays and block those via my hosts file.

Upgrade you memory - definitely these days if someone has only 4gigs my instant recommendation is upgrade, upgrade, upgrade. Running Windows 10 you want 8gigs minimum.",0
"ELI5: The reason why alcohol at 70% concentration is better as a disinfectant compared to alcohol at 90% concentration has to do with its ability to effectively kill microorganisms. When it comes to disinfecting, a certain amount of water is needed to help facilitate the action of the alcohol. Water is necessary because it helps to denature proteins in the microorganisms, making them unable to function properly and ultimately killing them.",1
"I went from obese to regular weight. At the same time I unexpectedly went from a hot person to a cold person. I was told that it's because fat wraps around your vital organs keeping them toasty warm. I am burning less calories and generating less heat. Finally, my heart rate is much lower, so I'm pumping less warm blood around my body.

I think it's a mix of a few things, but where you store fat would be the main one.",0
"You're right about the function of brown fat, but there's very little evidence that this is why adults experience heat and cold differently. Adults, in general, possess very little brown fat. The culprit is more likely to do with:

1. **Thyroid function:** if you produce very little thyroid hormone, then you will feel more cold. If you produce at the higher end, you will feel warm more often (and you will also be skinnier - which seems to speak more to what OP experiences).
2. **Iron:** People with lower iron have a harder time delivering oxygen-rich blood to peripheral tissue. Women frequently have anemia at rates much higher than men, and women more frequently experience cold-intolerance as well.
3. **Circulation:** hear rate, blood pressure, blood lipids/cholesterol, and blood sugar all impact circulation and the ability of your blood to perfuse peripheral tissue. If your circulation is poor, you will have greater cold intolerance.

**EDIT**: The issues I have listed are the most common factors or among the most influential factors in your perception of hot vs cold. Overweight in the absence of a thyroid issues will generally make you more heat intolerant rather than cold intolerant. Underweight in the absence of thyroid issues make you much more cold intolerant. Of course, temperature perception is far more complex than any singular explanation. It is always a combination of factors. These are just some of the more common reasons people experience temperature differently.",0
"Excellent explanation! I wanted to add that visceral fat is correlated with many diseases and poorer health outcomes.

Also, while men and women tend to deposit fat in different places, where fat deposits form seems to be determined by hormones. So, while broadly men and women deposit fat in different places (because broadly men's and women's hormones are at different levels) changes in hormone levels can change where fat is deposited. Say: menopause or glandular diseases or even stress or lack of sleep or other medications.",0
Same with a big time pee around 4 am.,0
"Have you ever sat by a lake or river and dug a little hole in the ground? After a while, water will collect in the bottom, because the water flows through dirt and stone around the river too. When it rains, a lot of water flows down into the ground and that ground that carries water is called an aquifer. Depending on what kind of dirt and rocks are there and how the hills and mountains are sloped, it will collect in certain places. 

Very ancient people needed water to live, just like we do today. They usually chose to live near rivers, lakes, and streams. They also dug little holes in the ground nearby, and noticed the water in those holes was nice and filtered by the dirt and sand. If they dug a hole and covered it up, their water would taste good and stay clear of leaves, sticks, and algae. So they dug deeper and deeper holes, and found they could move further from lakes and rivers which would flood from time to time. 

Where to dig a well was trickier the further you got from water though. Sometimes they dug big holes for nothing, and that was disappointing, but they learned a lot from it. Parents taught their children what to look for, what kinds of rocks and plants made for good well ground.

EDIT: WHOA!!! Glad so many people were amused by writing in my teacher-voice! A recurring question I’ve seen is “How can dirt filter water? Wouldn’t it be dirty?” So here’s a [link](https://www.usgs.gov/special-topic/water-science-school/science/groundwater-wells?qt-science_center_objects=0#qt-science_center_objects) to explain more about wells since it’s a pretty deep subject. In short, fine topsoil rich in organic matter doesn’t go very deep, clay settles out, and gravel and sand are excellent filters that continue to be used as part of modern water filtration systems.",0
"While you are asleep your body works to keep you alive and homeostasis up. 

Your blood sugar levels are tightly controlled, when it drops you start producing hormones to tell you that you are hungry and hormones to break down it's sugar and fat reserves to keep you alive.

While you are asleep it's mostly growth hormone and cortisol that start lipolysis (fat breakdown)",0
"That shit ain't gonna power my flat-screen!


/s",0
"Yes, the micro tears are the cause for soreness. Lactic acid build up is the muscley burny sensation you feel during a workout. When the muscle can’t keep up with metabolism. 

Source - I’m a strength coach and have a BS in kinesiology

EDIT: holy hell. I gave a very brief comment explaining that lactic acid wasn’t the cause for soreness. My intention, without getting into the complexities of the metabolic process, was to explain the burning sensation within a workout comes from that process. But muscle soreness after 24 hours is from the micro tears in the muscles and the inflammatory responses that brings. I never claimed to be an expert on physiology, but my expertise lies beyond coaching people to do push ups. I have a bachelor of science and took the same course load as those in the pre-med track. Plenty of chemistry, biology, physiology, physics and of course anatomy.",0
"Armpit sweat glands are known as [apocrine sweat glands](https://en.wikipedia.org/wiki/Apocrine_sweat_gland), and instead of just sweat, they also produce a mix of proteins and lipids.  The bacteria on your skin *love* that stuff and eat it up, producing waste products in the process.  It's those waste products that smell bad.

tl;dr: BO is mostly the smell of bacteria poop.",0
"I was once rear-ended by a drunk driver, pretty decent hit. We pulled over and the guy seeemd totally fine. I called the police and we were waiting for a while (we were kinda far out of town) as we waited, the guy seemed to get more and more drunk as we all calmed down and by the time the police showed up he was obviously drunk and stumbling.",0
"When we start to fall asleep, our brain goes through different stages of sleep, including light sleep and deep sleep. These stages are characterized by changes in brain activity and muscle relaxation. Sometimes, as we transition from wakefulness to sleep, our muscles relax too much, which can cause our body to jerk or twitch. This sudden movement is known as a hypnic jerk or sleep start. It can feel like a sudden shock or jolt that wakes us up abruptly.",1
"I convinced my dad to play Runescape when I was doing stuff like this around 2006. (I was probably 11). 

That college educated motherfucker realized that different merchants on different servers had slightly different price-points for buying and selling stuff based on the economy of that specific server and he would jump around between servers using arbitrage to earn millions. 

He would buy a bunch of cool stuff and give it to me as gifts. Best Dad ever. ",0
"[An ejaculating penis has a bandwidth of around 15,000 Terabytes per second](https://everything2.com/title/Penises+have+higher+bandwidth+than+cable+modems)  
^(\*thanks for correcting me) /u/ckhs142",0
"It’s due to a phenomenon known as TIDAL LOCKING.

This happens when

1. a moon (or any celestial object) orbits another object at a close enough distance, so..

2. ..the gravity of the “host” object is strong enough to pull one side of the moon towards it in such a way that..

3. ...the moon rotates at the same rate as it orbits

In half a rotation, the moon would have turned 180 degrees and showed its other side. But at the same time, it went 180 degrees around the Earth, so now the other side of the earth sees the same side.
Here’s a handy [gif](https://en.m.wikipedia.org/wiki/Tidal_locking#/media/File%3ATidal_locking_of_the_Moon_with_the_Earth.gif) !

The one we actually see is the one on the left. The one on the right is what it would look like IF the Moon weren’t tidally locked.

Also, the “dark side” of the Moon is misleading!
Both sides of the Moon can be lit up or in darkness, depending on the time of month.
On a Full Moon, the face you see is bright.
On a New Moon, the face you see is dark.

What most people mean by “dark side” is generally the side not facing the Earth.

(Edit: 180 degrees, forgive my 3am high ass brain)

Edit: so here’s some extra explanations about the mechanism of tidal locking:

I actually simplified the answer a lot. It’s not about one side experiencing a stronger force, it’s about one AXIS of the Moon experiencing stronger force.

Just like how the oceans facing the Moon experience high tide as they are pulled towards it, the Earth itself is pulled towards the Moon. Thus, the Oceans on the opposite side also experience high tide.

It’s this ‘bulging’ effect that effects the spin of a body.
Back when the moon was molten rock, this bulging was large enough to ‘lock’ its rotation the way it is today.
[Here’s a pic!](https://en.m.wikipedia.org/wiki/File:%C3%81rap%C3%A1ly_forgat%C3%B3nyomat%C3%A9k.png) 

Edit: the dumb mobile app isn’t showing any new responses after yesterday. I might have made a few errors in the explanation, so please DM me if there’s any edits I should make to clear things up!",0
"Oh boy, something I actually have a response for! Someone with 6 years of Jesuit schooling here, and I actually posed this exact question in a Theology class:
The short answer is no.

The concept of marriage in its most basic sense is that you are wedding this person to help them live a better life as their partner. However, since the purpose of heaven is to be infinitely close to God, any bonds or attachments to people you have on Earth are basically null. Your husband/wife would be someone you view fondly and had a close relationship with, but they would no longer be your wife/husband.

I don't really agree with all this, but I'm not going to insert personal opinion here. Hope this explanation helps!


Edit: removed the part saying ""The reason for this is explained as 'it would be a distraction from why you're there.'""

/u/_Silly_Wizard_ offered a better explanation, saying that instead of remembering people fondly, it's more akin to everyone is married in the sense that there is perfect unity with God.

Edit 2: figured it would be more credible if I actually included some sources. Here's a passage from the Catechism on the issue:

1618 Christ is the center of all Christian life. The bond with him takes precedence over all other bonds, familial or social.113 From the very beginning of the Church there have been men and women who have renounced the great good of marriage to follow the Lamb wherever he goes, to be intent on the things of the Lord, to seek to please him, and to go out to meet the Bridegroom who is coming.114 Christ himself has invited certain persons to follow him in this way of life, of which he remains the model:

""For there are eunuchs who have been so from birth, and there are eunuchs who have been made eunuchs by men, and there are eunuchs who have made themselves eunuchs for the sake of the kingdom of heaven. He who is able to receive this, let him receive it.""115

Edit 3: To all the atheists arguing in the comment string below, keep in mind that just because someone is speaking on religious doctrine, it doesn't mean they're trying to 'biblethump' or 'spew nonsense'. Regardless of God's existence, you CANNOT dismiss that faith is historically integrated into culture, and the texts still provide profound lenses through which you can view the world. Stop acting like bloody conspiracy theorists and saying that everyone involved in the church is colluding to create some massive smokescreen. Religion has been around for tens of thousands of years--basically since humans started keeping a written language. And you think that you're some intellectual crusader who's saying ""Gods not real bro."" for the first time? Congratulations! There's no God! Now good luck wishing away the millennia of texts, scriptures, parables, and doctrines that he created.

Real? That will always be debated.
Impactful and crucial to understanding western civilization? Abso-fuckin'-lutely.",0
"First of all you're comparing a 55-year-old racing prototype to brand new road cars. The Ford GT40 was made to go as fast as it could, reliably enough to win a 24-hour race; a modern 'supercar' is designed to look pretty, be comfortable, meet government safety regulations, etc. reliably for years. A more apt comparison is between the GT40 and a new Le Mans prototype.

Compared to a modern Le Mans prototype it was more advantageous for the GT40 to be as fast on the Mulsanne Straight as it possibly could. The course at Le Mans now has chicanes deliberately added to the straight, to force drivers to slow down. There's only so much space to reach top speed now, so carrying as much speed through corners and accelerating as quickly as possible is more advantageous.

Last year's Le Mans winner, the Toyota TS050, had a top speed of 217.5 mph, which isn't all that much more than the GT40. However, it reached that top speed in much shorter straights and can corner much quicker than the GT40 ever could. Dan Gurney set the fastest lap in '66, at 3:30. Last year's fastest lap was 3:17, with two chicanes in the Mulsanne Straight. Overall the TS050 is **much** faster.",0
"The real problem, as I understand it, is that HSV (the herpes virus) can and does hide away within nerve cells in the body, only to reappear months later.  So, even if an anti-viral medicine is introduced to the body, it may kill all of the ""obvious"" HSV-infected cells, but not those that are ""hidden away"" inside the nerve cells.  It is very unusual behavior for a virus, and thus, medical scientists have had a difficult time come up with a suitable and effective permanent treatment.",0
For some fans the amount of power required to get the blades spinning is higher than the amount provided by the lowest setting. The lowest is enough to keep the blades spinning but not overcome the inertia at the start. Edit: as others have pointed out the fan will typically start but it is hard on the motor,0
"Sperm and eggs can be frozen successfully because they are relatively small cells with a lower water content compared to the rest of the body. When they are frozen, special techniques are used to protect them from damage caused by the formation of ice crystals. Additionally, they can be stored at very low temperatures, which slows down cellular processes and prevents them from aging or deteriorating.",1
"Short Answer: Because the MPAA says so, they have a monopoly on the rating process. 

Also, part of the distinction lies with the simulation of the act versus a graphic depiction of it. For instance, graphic violence gets you an R rating, but simulated violence doesn't (so if you see lots of blood it goes to an R but otherwise you can blow up as much stuff as you want.) Likewise, sex receives a lower rating the less graphically you depict it. 

Edit: For instance you can have Austin Powers and Two and a Half Men talk about sex all the time, but as long as you don't show anything besides a shirtless man and a woman covered up in bedsheets then you are in the firm PG-13 territory. 

Likewise, Wolverine can stab and slash tons of soldiers without any blood and stay PG-13, but if you show a realistic portrayal of war like in Saving Private Ryan then you move up to an R rating. 

Edit 2: [An example of a PG-13 sex scene from the Notebook](https://www.youtube.com/watch?v=2KXVizOUQVY&app=desktop)

Also, somehow Top Gun managed to stay PG with this [love scene](https://m.youtube.com/watch?v=uy6MseHSToI) although granted they still hadn't ironed out the kinks for what the PG-13 rating was going to be yet (it was only introduced 2 years prior to Top Gun). ",0
"Am cook for a restaurant: Ingredients are all fresh and of the highest quality around, but that doesnt mean we dont add extra fat for it to taste good. Heavy cream is subbed for milk, anything i can put sour cream into i will, always a little generous with the salt. My job is to make you a great meal, if you were going healthy you wouldnt order the 4 cheese mac and cheese with bacon on top. ",0
"Bees are kind of like the drug lords of their world. They make really desirable stuff and have to constantly watch out for attacks, so they have tons of guys just sitting around the hive with guns. 

Everyone wants their shit. But if you aren’t tricking them into joining your cartel (an apiary by a human) or grow them inside it, you’ll get a bloody mess trying to take them out.",0
"In hospitals at least, patients seem to know. They often say ""I'm going to die"" or they will ask to contact a relative. You do not take these requests lightly; dying people know when they're getting close.

Also, experience. People can look really really ill for weeks or months, but one day you'll look at someone and they'll look different from an hour ago. More grey in colour, or a different facial expression, or... you just *know*. 

All this of course aside from clinical findings, respiratory rate, BP, blood tests if you're still doing that stuff by this point, etc etc.",0
"I haven’t seen anyone mention the fact that once inside the recipient the donated organ will be under constant attack from the recipients immune system, which will prematurely age the organ. Because of this transplant recipients take immunosuppressive drugs to reduce the amount of damage their own immune system will do to the new organ. It’s a balancing act between suppressing the immune system enough to stave off rejection of the new organ (which is almost always inevitable) and having enough of an immune system to fight off basic infections. This is why it can be difficult to find a match when looking for an organ.  The closer the new organ is to the recipients own genetic markers the better. 

Source : I used to be an RN on an transplant unit. ",0
"After the end of World War II, Germany underwent a process of denazification to address the legacy of the Nazi regime. It involved dismantling the Nazi party, purging former Nazis from positions of power, and implementing educational programs to promote democratic values. However, it is important to note that not all Germans were Nazis, and many ordinary citizens were not actively involved in the atrocities committed during the war.",1
"Less ELI5, but might be interesting: 
 
Neurotransmitters don't have a single function. There are things called neural pathways, and each neurotransmitter does different things in each pathway. The most notable pathway to be affected by ADHD is the reward pathway. It goes through lots of parts of the brain, including memory, capacity to make decisions, capacity to plan.   
This pathway makes sure that if we do something good,we feel pleasure. That pleasure is then stored with the memory of the action, associated. When we try to make a decision, our brain brings back memories of things that brought us pleasure, meaning we remember things that worked, and prepares (primes) us to feel that satisfaction again.   
  
When you have ADHD, your brain doesn't keep enough dopamine floating in that particular pathway, which means you don't feel satisfaction when doing things that would normally be rewarding. That lack of satisfaction is stored, marking the memory as unimportant. When you try to plan for something, you don't have experiences to draw on at first.  
  
That is why ADHD causes executive disfunction, in the form of memory issues and complications with decision making.  
  
And, of course it means that while you are doing something, your brain doesn't show you that it is an worthwhile action. That means you have to fight to stay focused, as your body tells you shouldn't. And when you do hit on a loop of reward, your brain goes ""THIS MATTERS! DON'T STOP!"". and breaking free to pay attention to something else becomes almost impossible.    
    
ADHD is really interesting! If a big pain in the ass to have.   
  
Edit: *Please* keep in mind that I'm *not a doctor*. I'm taking some classes on the topic for an *education* minor, nothing to do with mental health. I'm also Autistic, so my own experience might be different from yours. Do not take my word for gospel, but only as a way to guide your own research in the future, and a way to ask good questions from your doctor.   
  
If you want a good *entry level* overview on ADHD, Dr Tracey Marks has some good information on her Youtube channel on the topic. **entry level** being the important part.  
  
Be well friends!",0
"When a houseplant is transplanted into a pot that is too big, it can actually hinder its growth rather than promote it. Here's why:

1. Soil Moisture: A pot that is too large holds more soil, which means it takes longer for the soil to dry out. This can lead to overwatering and root rot, as the excess moisture can't evaporate quickly enough.",1
"I thought aspiration had more to do with vomit

Edit: Pointing out this is more a case of someone knowing their vomit than their shit",0
"Yup, there are exceptions though, their rum is Sailor Jerry's, but their vodka is made in house, contrary to popular belief it is not grey goose, it is just made in a distillery down river, using the same wheat and water as grey goose. ",0
"""Activated charcoal"" is carbon - which, you know, is what charcoal is made out of, mostly. They press it into smaller bits with more surface area.

Carbon like that has a lot of porous area where chemical *adsorption* can occur. Adsorption is like absorption, kind of, in that your material is ""sucking up"" something from a liquid or gas. In the case of adsorption, atoms and molecules stick to the surface area of your material. Charcoal, particularly activated charcoal, has a *lot* of surface area, so it can adsorb a *lot* of stuff and it can do it quickly and efficiently, and also cheaply.

~~That's why they use charcoal to pump your stomach when you have alcohol poisoning. The carbon sucks up all the alcohol, then they remove the saturated carbon from your stomach, bringing the alcohol with it. What carbon remains to go through your digestive tract contains the alcohol so you can't absorb it into your blood.~~ [^^Or ^^not ^^maybe?](https://www.reddit.com/r/explainlikeimfive/comments/7hmd8m/eli5_how_does_activated_charcoal_work_and_why_has/dqs6r6a/) ^^I ^^dunno, ^^point ^^is ^^it ^^sucks ^^up ^^poisonous ^^stuff ^^in ^^your ^^stomach.

It's also used in aquariums to suck up waste in the water so it can be removed when you change your filter. It's especially good at removing heavy metals, which are sometimes present in your water at concentrations that are not high enough to hurt you but high enough to hurt your livestock. Similarly, it's used in water purification systems (like your Brita filter) to similarly suck up some stuff in your tap water that you are trying to filter out. EDIT: And some gas masks, and industrial air pollution scrubbers, and a number of similar filtering applications. It's quite good at actually sucking up toxic chemicals. Just, you know...not out of your face.

That tendency to suck up heavy metals has created a mystique about it, that it sucks up ""toxins"". If it can purify your water, why not purify your skin? It can suck up ""toxins"" out of your face! (Pro tip: it doesn't.)

Edit: when they make you swallow charcoal for a pill overdose or swallowing poison, *it does nothing to the drugs or poison in your blood*. If you still have some left in your stomach, it soaks that up so that your OD or poisoning doesn't get any worse. What's in your blood is still there and has to be dealt with in other ways. ",0
"If you're asking for the purposes of losing weight, I lost 30 pounds by counting calories. After the first week it averaged to about two pounds per week. Losing weight doesn't have to be strenuous and you don't have to be hungry. It's just boring. Find your caloric max, split your calories three ways between carbs fat and protein. If you feel hungry after you've reached your caloric max only intake protein. I'm talking plain chicken, turkey or tuna. If you don't have the appetite for any of those you're not hungry you're just craving. Cravings subside after about two weeks.

Once you get past two weeks you get used to your very plain and boring diet.

Alcohol and eating out aren't very practical during this time. If you don't have a lunch with you you can find stuff at the grocery store which won't mess up your three way carb/protein/fat split. Usually slices of meat from the deli or some of the soups. (Chicken noodle and chili mainly. Forget chowder or anything potato or dairy based.)

EDIT: Couple more tips: don't rely too heavily on protein bars or drinks. Once a week is fine and keeping them on hand so you don't have to make a choice between getting hangry or messing up for a day. (One day isn't a big deal in reality but it can fuck with your mental momentum.) You want a diet that you don't change much after you're at your target weight. Protein drinks and bars aren't a long term diet solution.

Tuna is great but get chunk light and keep the weekly servings low (2-3 a week generally) to avoid intaking too much Mercury.

Greek yogurt is great. 1/4 cup in a bowl and then fill a single spoon with honey. Keep it on the spoon and dip it. Draw a little of the honey with it. You can have nearly the same flavor with way less calories. Do something similar with salad dressing. Put a measured amount in a bowl and dip it. You'll realize some pieces of salad taste great on their own (cucumber, tomato, radish, carrot) and the more bitter leaves don't take much to flavor.

Crock pot stews are easy to measure, make and prep. Don't worry about figuring out the nutrition of each cup. Just average it out and by the time you finish it over a few days it'll balance out the numbers. I recommend chicken, chicken stock, beets, sweet potatoes and brown rice. Any combination of those will taste good just figure out what works with your fat/carb/protein split. And you can flavor it afterwards. (Greek yogurt and salt after microwaving can make a huge difference.)

EDIT 2: Someone reminded me, at the first sign of hunger drink some water. Sometimes you're just thirsty.

EDIT 3: Keep your diet somewhat varied. Don't eat chicken exclusively or something.

EDIT 4: Some people were confused why I was giving diet suggestions as if it affects how you lose weight. It doesn't. You can lose weight on Twinkies provided you stay under your caloric max it's just gonna be _way_ more difficult to stay under because you're going to feel like you're starving. Making sure to get the three way split should prevent you from feeling starving. If you feel that way after drinking water you probably need to allocate a higher percentage of your calories to protein. Plain protein (I'm talking chicken cooked in water not oil) isn't very appetizing unless you're actually hungry.",0
Is this why fat people dont really feel a breeze when their ass crack is showing or when their shirt is inadvertently left up exposing their lower belly? r/TooAfraidToAsk,0
""" No point in mentioning these bats, I thought. Poor bastard will see them soon enough. """,0
"This answer here :).
   
The slope is so that the snow will fall off the roof before it is too heavy and the roof caves in. The little fences stop chunks falling off in lighter snowfalls and to stop ice falling in the case of snow melting and refreezing, without them there would be a constant danger of snow and ice falling off.
   
Most houses also have a stick  attached to the wall which Indicates the safe distance you should stand away from the wall as in the case of heavy snowfall, there is still a risk of snow and ice falling down. ",0
Does mouthwash kill the commensal bacteria?,0
"One of he biggest killer of plants is over watering. (Yes, a lot of people pay too much attention to the plant and kill it)

When indoors you're completely changing a plants environment. There is so much that can affect a plant in how it - takes up nutrients, water and light. 

When you have a plant that in its native habitat gets a lot of light and a lot of warmth and you have it indoors with considerably lower light and temperature (when bought young they are more likely to adjust without going into shock) it is going to have to adjust to those conditions meaning - since it has less light it will need less water and less nutrients, the plant is not using all its resources up like it would so it doesn't need to ""re-up"" because it really hasn't lost much. 

So even though it uses less there are many factors that affect the soil- AC will dry out a well ventilated area rather quickly depending on the pot type. Also many plants indoors lack humidity they'd have in their native range. 

Having a larger pot for small roots is bad because it will not dry out as evenly, the top will become more dry while it could still be sopping wet at the bottom. Seeing that you go and water the plant again when it doesn't need it and the plant becomes water logged, suffocates and dies.  

The killing of indoor plants by overwatering is #1 most common, especially since when you bring a plant home, unless you have the same environment as the place you got it from in your home, it is going to go through some shock, many people don't know this and see their plant a little sad after they just got it and then panic and water the plant. 

Plants need time to adapt. You can't just keep changing things and expect to see results in a day or two. Often times even if they look a little sad it's best to leave them be, especially if they have enough water, light and humidity. They need time. 

Starting small when buying a plant in general is much better practice, for so many reasons. Just don't put it in a huge pot :)

Edit: Wow, didn't think this would get this big lol Thanks so much for such interest in plants and horticulture! 

To be clear I was mostly talking about house plants in this, since they are a plant that is usually not of the climate where you live and is why they're a house plant because the can't live outside. (But not always- in FL many ""house plants"" can do better outside, depending on the plant still) 

Doing my best to answer questions- you guys are lucky I'm sick and aren't busy outside :P

Edit 2: [Here](https://imgur.com/a/VXypP) are a couple pages from one of the best houseplant books I've seen and had. There is helpful info that can be used for other plant types as well. You can get the book off [Amazon](https://www.amazon.com/House-Plant-Expert-D-G-Hessayon/dp/0903505355#immersive-view_1504415707650), its rather inexpensive and 100% worth it.

Edit 3: Forgot to stress the fact that many of our tap water had loads of chlorine and chloramine in it. The chlorine can evaporate out if the water is left out before use but the chloramine will not.  These are things that can really affect a plant over time. Along with mineral build up from water and fertilizer. [Here's](https://extension.umd.edu/hgic/watering-and-soluble-salts-houseplants) another good resource.",0
"When you feel thirsty, it's because your body is dehydrated and needs water to replenish the lost fluids. While coffee is made with water, it also contains compounds like caffeine and tannins that have dehydrating effects on your body. These compounds can increase urine production, leading to more fluid loss. So, even though coffee contains water, its other components can actually make you feel more thirsty instead of quenching your thirst.",1
"Cocoa beans are surrounded by pulp, and coffee beans are found within a ""cherry"". Both of these are edible and consumed by animals. So there were reasons to nibble on them even if you discount the bitterness of the beans.

Humans have also been eating and processing foods for a long time, we've probably tried eating damn near everything on this planet at one point or another. Neither are particularly toxic nor require advanced processing, so they're pretty edible as things go. There may be specific events that lead to cultivation of these, but it's no more strange than the fact that we eat olives or beans.",0
I swear to God if I get home and there's a fucking ace of clubs in my ass I will SUE YOU,0
"I'll also add u/Walshy231231 's explaination that was shared above:


>u/Walshy231231 : Well, in the beginning, there was Eohippus. The proto-horse. It was a small hooved animal about the size of a dog, and it ate grass. It was a simple creature, and in my (factual) opinion it represents the last time that the Horse lineage was untainted by sin. Now, it is worth noting that life was not easy for this proto-horse, in fact life for early hooved mammals was so difficult, that some of them said ""fuck that"" and moonwalked back into the ocean to become cetaceans (Whales and Dolphins). That's right, The proto-horse had so stupid an existence, that hooved mammals went back into the ocean (lacking gills and flippers) and had more success than horses would have on land.

>Okay, So why was life so hard for Eohippus? Well, they are herbivores eating almost exclusively grasses. Grasses, as you may know, are not particularly nutritious. But more importantly, grasses are smarter than Horses. See, Grass does not want to be eaten, and evolutionary pressure caused the grasses to start incorporating silica (ie sand) into their structure. Silica is extremely hard. Hard enough to wear down Horse teeth. Now there is another evolutionary pressure acting on Eohippus; It's teeth wear down by the mere act of eating, to the point that it will starve to death. Eohippus teeth do not regrow, instead, Eohippus evolved bigger teeth. However, bigger teeth mean a bigger jaw, bigger head, and a bigger body to carry it.

>These opposing evolutionary pressures started an arms race in which the grasses incorporated more and more silica, and Horses got bigger and bigger, just so they would have big enough teeth to grow and reproduce before finally starving to death. And eventually our cute dog-sized pony evolved into the 1,500-pound, dumb-as-rocks prey animal i loathe today.

>But wait, there's more! See, Horses are extremely fragile. There is a reason why a ""horse doctor"" typically prescribes a dose of double-0 buckshot in the event of a leg injury. A horse is very heavy, and it has very thin legs to carry that weight. If any one leg gets fractured, it is exceptionally unlikely that it will heal well enough for the Horse to walk again, and is extremely likely to break again just carrying the weight of the horse. Remember, a human thigh bone is gigantic relative to the size of our bodies, a horse leg bone is absolutely minuscule relative to the weight it carries.

>Also, Hooves: I want you to imagine that instead of feet, you have a giant toenail at the end of your leg. That is how the Horse do. That is what a hoof is. A giant toenail. It is extremely delicate, and joined to the leg by a vast network of very fine connective tissue, and oh yeah it also bears the weight of a fucking HORSE. If a hoof gets infected (which is quite common, because imagine how often shit would get stuck under your toenails if you walked on them), the Horse immune system responds in the typical way: via inflammation of the area. The problem is, a horse hoof is a rigid ""cup"". It cannot accomodate the swelling from inflammatory response. The Horse hoof will basically pop off the leg like a sock. On top of that, remember the Horse is putting 1,500 pounds of weight on it (because Horses can't redistribute their weight very well since all of their legs can BARELY support their share of the total weight).

>So, Horse apologists will claim that Horses are good at one thing: Turning Grass into Fast. As the previous two paragraphs show, they can't even do that right. Locomotion is very dangerous for a Horse, and if the Fast doesn't kill them they'll starve to death just by eating.

>On top of that, they are dumb as all fuck. Horses will often do something called ""Cribbing"", which is when they decide to bite down on something (literally anything) as hard as they can, and suck in air. They just keep sucking in air until they inflate like a balloon. Eventually, the vet will show up and literally deflate the Horse with a long needle to let the air out of them, and hopefully get them to just... stop...

>First off, horses are obligate nasal breathers. If our noses are stuffed up we can breathe through our mouths. If our pets' noses are stuffed up (except for rabbits, who are also really fragile but unlike horses aren't stuck having only one baby a year) they can breathe through their mouths. If a horse can't breathe through its nose, it will suffocate and die.

>Horse eyes are exquisitely sensitive to steroids. Most animal eyes are, except for cows because cows are tanks, but horses are extremely sensitive. Corneal ulcers won't heal. They'll probably get worse. They might rupture and cause eyeball fluid to leak out.

>If you overexert a horse they can get exertional rhabodmyolysis. Basically you overwork their muscles and they break down and die and release their contents. Super painful, and then you get scarifying and necrosis. But that's not the problem. See, when muscles die hey release myoglobin, which goes into the blood and is filtered by the kidneys. If you dump a bucket of myoglobin into the blood then it shreds the kidneys, causing acutel renal failure. This kills the horse. People and other animals can get that too but in school we only talked about it in context of the horse.

>Horses can only have one foal at a time. Their uterus simply can't support two foals. If a pregnant horse has twins you have to abort one or they'll both die and possibly kill the mother with them. A lot of this has to do with the way horse placentas work.

>If a horse rears up on its hind legs it can fall over, hit the back of its head, and get a traumatic brain injury.

>Now to their digestive system. Oh boy. First of all, they can't vomit. There's an incredibly tight sphincter in between the stomach and esophagus that simply won't open up. If a horse is vomiting it's literally about to die. In many cases their stomach will rupture before they vomit. When treating colic you need to reflux the horse, which means shoving a tube into their stomach and pumping out any material to decompress the stomach and proximal GI tract. Their small intestines are 70+ feet long (which is expected for a big herbivore) and can get strangulated, which is fatal without surgery.

>Let's go to the large intestine. Horses are hindgut fermenters, not ruminants. I'll spare you the diagram and extended anatomy lesson but here's what you need to know: Their cecum is large enough to shove a person into, and the path of digesta doubles back on itself. The large intestine is very long, has segments of various diameters, multiple flexures, and doubles back on itself several times. It's not anchored to the body wall with mesentery like it is in many other animals. The spleen can get trapped. Parts of the colon can get filled with gas or digested food and/or get displaced. Parts of the large intestine can twist on themselves, causing torsions or volvulus. These conditions can range from mildly painful to excruciating. Many require surgery or intense medical therapy for the horse to have any chance of surviving. Any part of the large intestine can fail at any time and potentially kill the horse. A change in feed can cause colic. Giving birth can cause I believe a large colon volvulus I don't know at the moment I'm going into small animal medicine. Infections can cause colic. Lots of things can cause colic and you better hope it's an impaction that can be treated on the farm and not enteritis or a volvulus.

>And now the legs. Before we start with bones and hooves let's talk about the skin. The skin on horse legs, particularly their lower legs, is under a lot of tension and has basically no subcutaneous tissue. If a horse lacerated its legs and has a dangling flap of skin that's a fucking nightmare. That skin is incredibly difficult to successfully suture back together because it's under so much tension. There's basically no subcutaneous tissue underneath. You need to use releasing incisions and all sorts of undermining techniques to even get the skin loose enough to close without tearing itself apart afterwards. Also horses like to get this thing called proud flesh where scar tissue just builds up into this giant ugly mass that restricts movement. If a horse severely lacerated a leg it will take months to heal and the prognosis is not great.

>I hope this information has enlightened you, and that you will join me in hating these stupid goddamn bastard animals.",0
"You're one of today's lucky ten thousand, because you get to learn about the weird-ass thing called ""[chronostasis](https://en.wikipedia.org/wiki/Chronostasis).""

If you don't know, our eyes have two modes of movement. We can either engage in *smooth pursuit,* where we're focused on a moving object and our eyes move smoothly and continuously to follow it, or we can experience *saccades* -- a quick, immediate jump to another object. We'll focus on the latter.

We rely on a constant stream of information fed to our visual processing centers from our eyes in order to see the world around us. This isn't normally a problem, but when our eyes jump from one thing to another, that rapid motion means that the data we collect is degraded and useless because of the speed of motion. For this reason, when our brains receive a signal saying ""OK, the eyes are going to jump now,"" we actually discard and ignore any information our eyes collect during the jump through a process called ""saccadic masking"" -- in effect, we blind ourselves for the duration.

When our eyes settle on the new object, they drop the mask and visual information is interpreted once again. However, the brain is conscious of the fact that there was just a period of time where it wasn't receiving information, and so it does something really tricky:

It back-fills the information it thinks it should have received while it was blind with the information that it starts getting after the saccade -- we don't consciously perceive the gap because the brain convinces itself that it was seeing that image the whole time.

Because the brain fills in the gap with what it sees immediately following the jump, we subjectively perceive that the clock hand ""takes longer"" to move.",0
"Fuck. I'm a big Scandinavian dude, I wish I had that, im always so cold.",0
"I’m an Airline Pilot. There are a lot of separate factors here.

1) Different aircraft variants have different crosswind limits 

2) The same aircraft variants can have different crosswind limits between different operators/airlines 

3) different operators/airlines have different stable approach criteria and different restrictions on contaminated runway operations/adverse weather ops 

4) Airport facilities. Intense snow and severe icing. Can the deicing provider cope with everyone simultaneously (hint, outside of the hugely experienced airports who cope with snow every year, the answer is no)

5) Strong winds - we take off into headwinds. In the A320 we can accept crosswind up to 38kts and a tailwind up to 10kts. Every flight is different though - although allowed to take off in 10kts of tailwind, we are so heavy on this particular flight that our take off performance calculations show we can’t take off in accordance with the performance requirements. Can’t use that runway end, have to use the other. Can’t take off into aircraft approaching the other runway end. Big delays.


6) As pilots when we say ‘bad weather’ we are generally thinking about 

Strong gusty crosswinds 

Windshear and microbursts 

Thunderstorms (TS) & Cumulonimbus (CB) clouds that can generate moderate to severe turbulence, windshear, icing 

Heavy freezing rain 

Low visibility (<550m in fog/drizzle/low cloud)

Mountain waves 

Very strong gusty headwinds 

If there is TS/CB activity in the vicinity of the airport then everyone going in and out is going to need to take avoiding action and be vectored around it. ATC are going to be very, very busy indeed and consequently the flow rate of aircraft in and out will need to be chopped. When this happens most aircraft in and out end up being given what we call a slot/CTOT/CDM TSAT which is a designated time we’re allowed to go. This could be hours and hours after the scheduled departure time.

So your flight may be cancelled because 

1) The weather is out of limits 

2) The chopped flow rate means your flight has to be cancelled 

3) The slot means your flight crew will be ‘out of hours’ - the delay means our duty hours would breach the limits. There is a special procedure called discretion to extend the limits slightly but only to get home on the last flight after an unexpected delay, e.g a diversion due to a passenger medical emergency. In Europe it can’t really be used to leave home base for an expected delay like forecast severe weather. In these circumstances the airline call new crew from standby but if there aren’t any/enough available then the flight simply cannot operate.


I fly the A320. If the crosswind including gusts exceeds 38kts we simply can’t shoot the approach or take off. If the airport is covered in TS and CB’s we’ll just have to divert. If they’re isolated and we can try to pick through we’ll give it a go but if there is a sniff of safety being compromised we’ll have to go around and go off to the alternate.

Bear in mind we will have loaded lots of extra fuel (I’m talking several tons...as much as is necessary but also not so much that it causes landing performance problems) to give us lots of holding time. We try our best to achieve the schedule but if the weather is out of limits or other aircraft are reporting genuine windshear or severe turbulence etc it just can’t be done. Can’t take off into reported genuine windshear. No one is going to take off into a proper embedded thunderstorm. 


-edit-

Lots of questions asking me to explain windshear and microbursts and whether they are common.

Read this, it's an exceptionally good article on what WS actually is. 

https://safetyfirst.airbus.com/wind-shear-an-invisible-enemy-to-pilots/

Microbursts aren't, because we don't fly through thunderstorms. We also have doppler radar that measures the shear rates of water droplets in the atmosphere ahead to detect and warn of windshear i.e. microbursts and gust fronts.
Watch this from 1:10 onwards

https://youtu.be/9LMZGBN7rXY?t=70

If you guys are still really interested, read this.

https://skybrary.aero/bookshelf/books/164.pdf

-edit again-

Ok the amount of responses to this has gone a bit fucking mental. I’m busy atm but when I get back home in a few hours I’ll follow up on all your questions and messages.",0
"70% alcohol has 30% water, and that water is necessary for the alcohol to interact at all with the cells it’s killing.

It’s like cooking pancakes. You know how when your pan is really hot and you put in pancake batter, it cooks the outside really fast? And then you can flip it, but it does the same thing to the other side and the middle doesn’t cook very well? 90% alcohol is like that. It doesn’t penetrate well into cells or clumps of microbes because it just fries everything it touches on the outside. The 70% alcohol is like cooking on medium heat with a moderately hot pan. It contacts the outside, too, but the water helps it penetrate to cook the inside (denature proteins deeper) as well. 

From https://blog.gotopac.com/2017/05/15/why-is-70-isopropyl-alcohol-ipa-a-better-disinfectant-than-99-isopropanol-and-what-is-ipa-used-for/

> The presence of water is a crucial factor in destroying or inhibiting the growth of pathogenic microorganisms with isopropyl alcohol. Water acts as a catalyst and plays a key role in denaturing the proteins of vegetative cell membranes. 70% IPA solutions penetrate the cell wall more completely which permeates the entire cell, coagulates all proteins, and therefore the microorganism dies. Extra water content slows evaporation, therefore increasing surface contact time and enhancing effectiveness. Isopropyl alcohol concentrations over 91% coagulate proteins instantly. Consequently, a protective layer is created which protects other proteins from further coagulation.

> Solutions > 91% IPA may kill some bacteria, but require longer contact times for disinfection, and enable spores to lie in a dormant state without being killed. A 50% isopropyl alcohol solution kills Staphylococcus Aureus in less than 10 seconds (pg. 238), yet a 90% solution with a contact time of over two hours is ineffective.

Edit: Because there’s been some confusion, I’d like to add two points. First, higher concentrations of alcohol solutions (specifically isopropyl) may still be superior as solvents, for use on things like electronics for cleaning, because water is generally bad for electronics. Second, what we’re talking about above you should think of as referring only to ethanol and isopropyl alcohol (which is not safe to consume). There are other alcohols but we’re just sticking to the ones commonly used.

Edit 2: Some people have questioned the source, which is good and part of science. The source offered a decent write-up of what numerous PhD mentors have taught me, and it’s consistent with the science. At the risk of making this too long, here’s what the CDC has to say, from https://www.cdc.gov/infectioncontrol/guidelines/disinfection/disinfection-methods/chemical.html 

Adding water enhances effectiveness of isopropyl and ethyl alcohols: 

> The most feasible explanation for the antimicrobial action of alcohol is denaturation of proteins. This mechanism is supported by the observation that absolute ethyl alcohol, a dehydrating agent, is less bactericidal than mixtures of alcohol and water because proteins are denatured more quickly in the presence of water 

Isopropanol and ethanol effective bactericides

> The bactericidal activity of various concentrations of ethyl alcohol (ethanol) was examined against a variety of microorganisms in exposure periods ranging from 10 seconds to 1 hour 483. Pseudomonas aeruginosa was killed in 10 seconds by all concentrations of ethanol from 30% to 100% (v/v), and Serratia marcescens, E, coli and Salmonella typhosa were killed in 10 seconds by all concentrations of ethanol from 40% to 100%. The gram-positive organisms Staphylococcus aureus and Streptococcus pyogenes were slightly more resistant, being killed in 10 seconds by ethyl alcohol concentrations of 60%–95%. Isopropyl alcohol (isopropanol) was slightly more bactericidal than ethyl alcohol for E. coli and S. aureus 489.

Kills viruses at these concentrations 
> Ethyl alcohol, at concentrations of 60%–80%, is a potent virucidal agent inactivating all of the lipophilic viruses (e.g., herpes, vaccinia, and influenza virus) and many hydrophilic viruses (e.g., adenovirus, enterovirus, rhinovirus, and rotaviruses but not hepatitis A virus (HAV) 58 or poliovirus) 49. 

Isopropanol similar to chlorhexidine
https://www.sciencedirect.com/science/article/pii/0195670183900257",0
"As a pharmaceutical production technician,I use 70% Isopropyl alcohol daily. The 30% sterilized water allows it time to spread the alcohol over the surface without evaporating too fast. I believe it takes about 15 seconds of contact time for it to disinfect, but at 90% it would only last 5 seconds so not enough time to fully kill even half the bacteria/spores. 
Plus 70% IPA smells good as hell.",0
"The carbs measured on the label can not have water ""in them"". If I took 10g of sucrose and put it in 90g of water and called it high-sucrose-syrup (HSS), it would still only have 10g of carbs despite being 100g of HSS. 

The fact is that different carbohydrates have different available energies. The general rule of thumb of 4 kcal per gram is just a rough rule of thumb.

High fructose corn syrup is generally 42% fructose and 58% glucose. By weight excluding water.

Fructose is a monosaccharide that contains 3.68 kcal/g.

Glucose is a monosaccharide that contains 3.91 kcal/g.

43[g] x (3.68[kcal/g] x .42 + 3.91[kcal/g] x .58)

43[g] x (3.81[kcal/g]) = 164 kcal

164 kcal can be rounded down to 160 Calories, and thats it. HFCS42 has about 3.8 kcal/g (anhydrous), not 4.",0
"Yes, babies in the womb do produce waste, but it's not in the same way as adults. The waste produced by a developing baby is called meconium. It is made up of things like cells, proteins, and other materials that the baby's body does not need anymore. Instead of being excreted as poop like adults, meconium mixes with the amniotic fluid surrounding the baby. This is the fluid that protects and nourishes the baby in the womb.",1
"Oh god thank you so much. 

So follow up: if it works out perfectly like that, what is the purpose of leap day?",0
"There's a concept in food safety called the ""danger zone"". It extends from about 40 F / 5 C to about 140 F / 60 C. You can keep food below the lower end of that range, or above the upper end, for a while without it going bad. The reason is that this is approximately the range of temperatures that the bacteria that cause foodborne illness can tolerate well enough to grow. They can survive in lower temps, but they won't grow or will grow very slowly, which is why you can store meat in the fridge for a few days without problems. (They'll die at higher temps if they're kept that hot for a while, as in sous vide cooking, but can survive brief exposure.)

The food at the buffet is (at least supposed to be) kept above 140 F / 60 C, so that whatever bacteria may be present can't grow effectively. The trays at a buffet usually sit above a pool of very hot water in addition to the heat lamps, which keeps them hot. (This is if the food does stay out for hours. It may simply be changed out regularly, in which case it can be kept in the danger zone.)

You can take the ""keep it above the upper end of the range"" principle really far, because temps above 140 F / 60 C will kill almost all bacteria with prolonged exposure. This is the principle behind [perpetual stews](https://en.wikipedia.org/wiki/Perpetual_stew), foods which were effectively cooked for years or decades.",0
"Part of it is how accurately you want to emulate. Take the game Space Invaders. You may recall there's many enemies and as you kill them they speed up. That was not coded in, it was a happy side effect of the processor being able to render fewer faster (and one super fast lol). If the emulator is not coded to run at the same speed as the old processor then you won't get this effect.

Edit: I didn't learn this from Game Maker's Toolkit, never heard of that show.",0
"Toxins are absolutely a real thing; it's getting rid of them where the bullshit comes in. Toxins are processed by your liver and kidneys. That's it. If there's something toxic in your body, your body gets rid of it via your liver and/or kidneys. You cannot drink juice to make it work faster or better; there is no such thing as a ""cleanse."" If something is toxic to you (alcohol, for example), your body is already sending it out via your liver/kidneys, that system is already in place. If you ingest too much of a toxin, though, you can overwhelm the system and make yourself very ill or die.

Edited to add: you're also right that people claim lots of things are toxins that aren't, or you would have to ingest *such* a large quantity of them to reach toxicity that it's not realistically possible. Conversely, a lot of people get real upset when it's pointed out that alcohol is actually highly toxic.",0
If I could get my cheddar folded 1000 times by Hattori Hanzo himself I would. The sharper that shit is the better.,0
"Hot water is more effective than cold water for washing hands because it helps to remove dirt, oils, and other substances from your skin more effectively. While hot water alone might not kill bacteria, it can help to loosen and wash away microbes from your hands. Additionally, using soap with hot water creates a lather that can further break down oils and dirt, making it easier to rinse them off.",1
"Multiple snoozes is the closest I can get to multiple orgasims.

EDIT: I'm leaving this one.  Learn from my mistakes.  Tell my story.",0
"This makes PTSD interesting. Recently, 30 years after my abuse, I had a family reunion over the Thanksgiving holidays. There were a lot of details that matched my childhood family holiday reunions that resulted in abuse and rape. The modern family reunion was factually completely different⎯different family, I was older, different location⎯but there were enough similarities to trigger a response. I spent the first three days in a fugue state or panic. The brain, and its primal core, can be terribly powerful.",0
"For the purposes of hygenic cleaning (killing germs, removing dead skin, cleaning a wound), temperature doesn't matter and (in some scenarios eg washing off bodily fluids or with certain soaps.) cold water is actually preferable.

For the purposes of cosmetic cleaning (washing off stains, cleaning oily fingers, greasy marks), hot water can help soften long chain hydrocarbons like waxes, grease or oils and can help solubilise inks or other chemicals into the soap or water.

---
Tl;dr (Better ELI5) is:

If you want to kill germs, temperature doesn't matter. If you want to clean dirty hands, warm water can help.

In both cases, washing thoroughly (at least 15 seconds) with soap is the most important thing.",0
"They sorta can, but granted, no where near the extent of the typical muscles such as the quads, chest, lats, etc. 


One such example of a person with big facial muscles is Matt Stonie. He’s a competitive eater so he has to utilize those muscles quite often. Thus his jaw is. Very developed compared to most people.",0
"On a somewhat related note... if you are taking an antidepressant, DO NOT just decide to take yourself off it. Consult with the prescribing doctor. Doing it wrong can really fuck you up. 

Like.... ""week or longer of being completely unable to function on a basic level"" kind of fuck you up.",0
"Clinical health psychologist with particular expertise in sleep and there is so much wrong with this comment. There is no evidence (even with our evolutionary psychologist brethren) that what OP is claiming is remotely true. The last theory I heard on this was that when our simian ancestors slept in trees the jerk was our bodies way of keeping us from falling off a limb. Again, just ideas/theories. 

Your post sounds appealing but there is nothing substantive to back it up. You’re also confusing hypnagogic and hypnapomic jerks.  

Edit. People are asking for sources. There aren’t any, same reason OP isn’t providing any. This is in the realm of evolutionary psychology theory which can’t be disproven or substantiated.",0
When they invent hyperspace and I scratch my starships paintwork on that hydrogen atom I'm gonna be pissed,0
"They were so superstitious and stupid.  We're way better now. 

Welp, off to drink some antioxidants and get my detoxifying colonic irrigation.  'later!",0
"This individualist mentality is exactly what's so damaging to the American workforce. You shouldn't try to work 50% hard than your colleague to earn more, the two of you should band together and pressure your employer for a higher wage. It is a system of short-term gain that leads to long-term wage stagnation.

In the heavily unionized scandinavia, we have a livable minimum wage (~17 USD/hr in Denmark) and expected substantial wage increases over the course of a career (no 2 cent/hr raises). Other niceties include paid vacation (6 weeks per year,) paid maternity leave (1 year to share between the parents,) and overtime pay of 150% for hours exceeding 37/week.

This is because the unions are strong and can pressure the employers. Employers literally hold _all_ the cards. They want you to compete amongst yourself so you forget this fact.

EDIT: This blew the hell up! Thanks for the gold!",0
"I'm a scientist! So let me try to offer my insight:

So first of all, like every other job in the world, scientists need money in order to work on their projects/research. Unlike ""regular"" companies though, scientists don't really *sell* anything, so it's going to be hard to go to Wells Fargo and ask for money without being able to show them how you plan on paying them back.

Enter organizations like the National Science Foundation (NSF), the National Institutes of Health (NIH), Defense Advanced Research Projects Agency (DARPA), NASA, the European Commission, and the list goes on. These organizations have many purposes, and one of them is to allocate researching funding to promising projects. What they'll do is, for example, put out a ""call for proposals"" and then allow scientists to apply for funding. For example, the NSF might put out a call for proposal on the subject of say ""childhood education.""

So you're a scientist doing research in ""teenage education."" You have a lot of experience on research in education in teenagers, and you think that you might be able to apply your work to education in children as well. You just don't have the time, or money, or staff, to actually do it. But now that there's this call for proposal, it's your chance! So you write a grant proposal which basically outlines what you are going to do, how you are going to do it, why you are going to do it, and a lot of other things are involved. Will your project involve any ethical considerations? You'll need to include documentation showing how you will follow ethical approvals, for example. You'll also need to submit some kind of budget guidelines. If you are requesting $500,000, how will this be used? $500,000 sounds like a lot, but in terms of research it's not really. The NSF might award you the grant for $500,000, but you need to keep in mind that this money is for the duration of the project. Do you need equipment (you will)? Do you need lab space (you do)? Do you need to hire new staff (you might)? New staff could be other researchers or grad students to help you. They need to get paid, after all, and so do you.

In the end: my point is: we **need** money just like everybody else. But unlike Boeing, and unlike Intel, and unlike Apple, or Google, etc... the money that I am asking for to do my project, actually has no promise of monetary return to my investors.

What I promise to return to the NSF, or to NASA, etc, is the promise of advancement in research. I do this by using the money to conduct experiments, and then publishing papers about it or giving talks at conferences. From the journal articles, other scientists will be able to follow my findings and either use it or try to test it etc and build upon their own research. From the conferences, I show things that are essentially ""works in progress"" but hey, maybe my idea is exactly what someone else was missing, and if they see me talk about it, they might come find me later on (or email) asking to collaborate. These are things that we *all* benefit from (we as in scientists), and these are essentially the ""returns"" that I promise to the NSF when I write my proposals.

When I publish or talk at conferences, I am talking to my peers. I am talking to colleagues. I am talking to scientists. When I talk to my peers, I would never make claims like ""this line of research can, **will**, definitely improve childhood education by 500%!""

When I talk to my peers I am trying to discuss my work.

But when I am talking to media (be it the press, a TV program/interview, Twitter, my personal website/blog, message boards, or my university's press office, or hell, even my own non-scientist friends and family), I am not trying to discuss my work. **I am trying to sell my work.** I want to sell my work because, like I said, my work is entirely based on receiving money. Without money, there is no research, period. So I might exaggerate a tiny bit, or trump up all the benefits of what I'm doing and then throw in a very minute detail about how those gains are the theoretical maximum assuming that all the planets are aligned. I'm not really lying about anything, I'm just giving a, perhaps very, optimistic view of my research.

(After that, the journalists usually run off with it, and replace words like ""could maybe"" or ""might possibly"" into ""will definitely"" and so on.)

When I apply for funding, I like to think that the system is merit based, as in they'll review my track record and past research and so on. In general this is more or less true. So I'm not actually trying to sell my work to these agencies like NSF etc. Who I'm trying to sell to is to both the tax paying public and to the politicians in charge of appropriating money to the NSF. Since I am not making anything, or selling anything, I need to convince the public that their tax dollars are being used in a productive and/or beneficial manner. I need to convince the politicians not to defund the NSF, because I need that money to do my research. I need to convince the public that my work is crucial, vital even, so that they might complain loudly when a politician decides that they want to cut funding to the NSF.",0
Camel humps actually store fat and not water.,0
"Imagine putting 22 oranges into your cart, then put orange juice next to that.  Notice how much space each takes up?  If you weighed them, the oranges would weigh more (because of skin and other things that are removed when juicing).  The same thing goes for shipping!  That's why ""flat packed furniture,"" like what's sold at Walmart and Ikea, is so much cheaper than pre-built furniture stores.  It's also why a package of dehydrated or condensced food costs much less.  Like dry beans vs canned beans.  Or Gatorade powder vs Gatorade Bottles. 

When shipping, 2 things matter: Size and Weight.  Decrease either of these, and the costs go down.  If you imagine a semi-truck, one full of packing peanuts and one full of bricks, which costs less gas to move?  The one full of packing peanuts.  However, it still costs money to move the semi truck full of packing peanuts. 

The other thing to know is that not all Produce is created equal.  The produce that shows up at the grocery store is often larger and is *much better looking* than the produce used to make refined goods like juice.  This means that if you used pretty, large oranges, you might use less than the 22 needed, and they'd all look pretty, but only the juice-maker knows that. 

If you can find a local place that sells ""ugly"" produce (that tastes the same), you will save a lot of money.  The reason for this is that you essentially pay for all the ugly food that is thrown away.  [John Oliver did a piece on it.](https://youtu.be/i8xwLWb0lLY?t=470) The link takes you to the part showing a field after they've gone through and picked everything.  This is also why baby carrots are a thing... baby carrots are the ugliest carrots trimmed into the ""baby"" shape! ",0
"Lots. The cervix is a physical barrier. And then women have mucus that slows down the sperm. Then the actual distance to travel. 

The body is designed not to get pregnant constantly. Although some people are more fertile than others.",0
"Honey often contains botulism spores, growth of botulism is suppressed when there is low water activity (such as in honey), and it's suppressed when the pH is low. It needs to get 125'C to kill the spores. Botulism produces one of the most toxic poisons  known to man when it grows.

The temperature to kill botulism is too high for honey (it would ruin it). For adults this is a non-issue because it doesn't grow in honey, and when you eat it your stomach acid prevents it from growing. Babies don't have a low enough pH in their stomach (not enough stomach acid basically), so botulism can grow in a babies stomach after it mixes with water in their stomach which could be deadly.",0
Holy fuck I want whatever intergalactic adhesive that author was huffing. This is the kind of sci-fi I genuinely enjoy. Got a name or title on that??,0
"Fuck them kids, they can make they own damn oats",0
"Very nice analogy. So water slows evaporation, helps penetrate to kill bacteria.  
Less water --> evaporate fast, less agent to help with penetration to kill bacteria. Glad I asked this on ELI. Thanks !",0
"Life as a waitress after a busy day - oh god my feet

Life as a waitress after a slow day - oh god my back",0
">my dad

>motherfucker 


Story checks out, no internal inconsistencies. ",0
"Great comment!

“Drinking alcohol causes beer bellies. Taking in a lot of calories causes beer bellies. And most importantly, genetics causes beer bellies.” 

Totally true but missing a few factors specific to ‘alcohol-bellies’. Consistent alcohol consumption in men at a level most people would consider high end of moderate (4 drinks a night), can cause a shift in where fat is stored in the body. It pushes the balance away from peripheral subcutaneous fat into developing more central visceral fat (this also happens to a smaller degree in normal aging). 

More severe alcoholism will also cause inflammation and swelling of the intensities (transiently in the 24-36hrs after drinking) and, eventually ascites (free fluid buildup in the abdominal cavity). 

Any or all of these  processes might be adding on top of the caloric drivers of drinking causing a protuberant abdomen. ",0
"> And, yes, taking more of them than recommended can actually have detrimental effects in some instances.

To elaborate on this, I remember learning in health class that there are two kinds of vitamins:  Water-soluble and fat-soluble.  Water-soluble vitamins easily dissolve in water, so if you eat too much of them your body will just pee out the excess.  Fat-soluble vitamins don't dissolve in water, so it takes much longer for the body to get rid of.

Vitamin C and all of the B vitamins are water-soluble, so it's rather difficult (though not impossible) to eat too much of them.  Vitamins A, D, E, and K are all fat-soluble, and can easily cause [Hyperviatminosis](https://en.wikipedia.org/wiki/Hypervitaminosis) if you eat too much too quickly.  For example, it's a somewhat well-known factoid that polar bear livers are toxic to humans due to the large amounts of Vitamin A in them",0
"A large percentage of your skin is made up of a protein called keratin.

Keratin forms long chains and if you were to look at your skin under a really powerful microscope what you would see is a bunch of keratin chains crisscrossing together like a chain link fence.  But just like a chain link fence, those crisscrossing chains leave large holes between themselves that water, or anything else, could go through.

To stop water from passing through those holes, keratin absorbs a small amount of fat from your skin's oil.  That fat sits in those gaps in the keratin chain link fence, preventing water from getting through.

Because keratin absorbs fat from your skin oil like that, anything that is able to dissolve into that fat is able to use it to leapfrog past the keratin in your skin and be absorbed into your body.  There aren't a huge amount of substances like that but that's why very oily substances, like poison oak/ivy, are able to get absorbed into  your skin.",0
"It's notable that for animals that we've only found skeletons that artist depictions are probably missing things like loose skin and fat deposits. Unfortunately I can't find a better source so Buzzfeed it shall be. Two paleontologists took skeletons of modern animals and sketched them the way we've been historically sketching dinosaurs. 

https://www.buzzfeed.com/natashaumer/dinosaur-animals?utm_term=.vhojKmrBb#.fheOV5Y3X

Granted they took a good bit of artistic liberty for emphasis. ",0
"The plant only gets small amounts of micronutrients from soil. Things like fixed nitrogen, iron, and the small amounts of salt they need.

Every bit of macronutrients they get, all of the fuel for their growth and even the carbon that makes up their structure, comes from the air and the energy of the sun.

So the pot can run out of nutrients, but those are not the most important nutrients and it is very likely the plant can survive a pretty decent amount of time without them. Just like how a human can survive without vitamin c, but will have the symptoms of scurvy.",0
"Good god!
I asked this question on a whim
I didn't know reddit was waiting for it.",0
"Another reason is because when you look in the mirror, you are aware of your nonverbal behaviors/body language: facial expressions, gestures, etc.  You can see if you smile is crooked, you are squinting, slouching, your arm is bent oddly, or what not, and immediately change to a ""better"" looking face/pose.  When taking a photo or video, you don't have that immediate feedback, so don't change to a more flattering look. 

This is why actors practice facial expressions in the mirror, so they can look on camera or stage how they want to. [https://www.castittalent.com/blog/2011/10/mastering-your-facial-expressions/](https://www.castittalent.com/blog/2011/10/mastering-your-facial-expressions/) ",0
"Yep. Morning runs have taught me the best tactic to deal with a canada goose on the trail is to increase my stride to a full sprint, put my hands over my head and commit to charge at it with a loud throaty roar. They will fucking clear out. 

I lose all sense of shame after a few miles. ",0
"No, organ recipients are on immunosuppresants forever. But with newer drugs, their immune system mostly comes back. My wife's nephrologist says within a year you're up to about 95% immune capability when it comes to germs.

I know they've done some experimental stuff where they don't need immunosuppresants, like wiping out the recipient's immune system and giving them a bone marrow transplant to convert their immune system to the donor's, but that's pretty cutting-edge stuff.",0
"Twice as long as wide allows for an interlinking stack. All cubes would be unstable.  The slight slope to the sides makes it easier to pick up, also pops out of mold easier",0
"The real answer, in ELI5 fashion:

Companies like Coke switched from reusable glass to plastic because it was cheaper. When plastic waste became a big problem, they paid a lot of money to run ad campaigns that make it sound like it’s our fault for increasing plastic waste and not their fault for producing it all. 

When big companies spend lots of money to push an agenda, it usually works.

Edit: Just to clarify: The OP didn’t ask “why is there litter” he/she asked “why is the emphasis just on consumers and not producers”. Plastic waste is a complex subject that isn’t easily ELI5. The OP’s actual question however has a pretty clear answer. For more information I suggest reading over this [article](https://www.motherjones.com/politics/2006/05/origins-anti-litter-campaigns/)  that does a pretty thorough background on the origins of anti-littering campaigns.",0
"It is extremely unlikely for you to collide with anything in space. There is a gigantic amount nothing out there. So it's far more likely to drift through a near empty abyss without collisions. 

But in the unlikely event that you do end up heading towards a planet you don't know about, you'd feel it's gravity long before you hit it. So that's a warning sign. 

As for whether you can see it? Almost everything emits infrared light, so you probably have devices in your space ship that can detect that light, even if your naked eye can't. Also if you are in a galaxy, which is where most planets are, there will be stars nearby that light it up.",0
"I used to work on a loading bay near a really huge refrigerated warehouse which used ammonia as a refrigerating agent - we once had a fairly large leak (it was pouring out of a pipe like a smoke machine) and had to keep working after it had mostly dispersed.

I can see why it might work the way you describe, that stuff was horrendous even after leaving it. Holding your breath only did so much because it would irritate the absolute shit out of your eyes and nostrils even if you weren't breathing through them.

Thankfully we only had to deal with that once!

Edit: Apparently this was mega dangerous and I'm lucky to be alive, thankfully I don't work there any more.",0
The better question is....WHERE'S THE RUM?!?,0
"Insulins fucked it up.

Your body needs glucose for energy. Everything you eat is broken down into glucose.

1. Foods like ""sugary"" foods, starch or very simple carbohydrates like rice, bread, pasta, etc, are digested very fast and thus, broken down and turned into glucose very fast.

2. Now, when you eat lots of those foods, lots of glucose goes into your blood very fast.

And your pancrea produce little guys (hormones) called insulins who push that glucose into your cells.

3. When lots of glucose goes into your blood very fast, your organ pancrea freaks out and is like ""OMFG! Too much glucose in the blood. Too much glucose wandering around in the blood = poison!! Guys!!, Insulins!!, go go go!!! Go push them into the muscle cells and fat cells to properly store them."" and then Pancrea produce lots of fucking insulins. May be even too much insulins.

4. Those lots of little fuckers called insulins push all the glucose from your blood into the cells.

5. Then, very low level of glucose in your blood. So, your body thinks ""Hey, my blood needs glucose. I need sugar(glucose). Feed me.""",0
"Fun thing is that when you have your ears covered up with earplugs and you yawn, you can hear the outside noise better because you are actually listening through your mouth and Eustachian tube! That rarely happens but when it does you have this weird wtf feeling.",0
"Your DNA is the result of millions of your ancestors progressively adapting to all that and in many cases turning it to their advantage, at the cost of the lives of those that didn't make it.


You're literally built to single handedly, systematically disintegrate and metabolize all that scum and much more unless the toxin buildup is so high that it's obvious through bad smell or visual cues (there are exceptions,  just probably not in your fridge).


You'll be fine. Enjoy your food
 ",0
"And to add to this further, the reason visceral (or “hard”) fat is generally “worse for you” is that it contributes to the production and secretion of hormones, as opposed to subcutaneous fat (the soft, flabby stuff) which is much more inert, meaning it’s mostly just there to store energy",0
"Because most hard candies are a solid structure of sugar -- as you strip off the outermost layers of sugar, you reveal the next layer down.

Peppermints and a few other candies are actually filled with holes, so as you dissolve the outer layer, you get a new layer that also has holes.

And then you have candies like Mentos where the outer layer is peppermint with all its holes, but this is on top of a candy layer that has no holes... so as you suck on it, the holes disappear.",0
"Mother fucker, stop making me breathe manually and know where my tongue is, you fucking witch.

10 Month Edit: Every month or so, someone or another gives this comment another award, and yet again I'm breathing manually and I know where my tongue is. I love and hate you all.",0
"And how much wildlife there is.  Deer can strip everything up to about 6-7 feet off the ground.  You can see in some forests where there are a lot of deer, there's almost nothing at ground level.",0
"> “do you feel you spill food or drinks more than others” and “do you run into walls or trip over permanent items (door frames) even in familiar spaces”

Late-diagnosed (also inattentive), and my god this explains a lot.",0
Lot of people pulling funny faces and thinking about sucking titty right now,0
"There are two somewhat related properties of the plastic that affect its ability to evaporate water - its specific heat capacity and its thermal conductivity.

With specific heat capacity, that is the amount of energy it takes to heat an object up to a particular temperature.  Plastic has a higher heat capacity (1.67 KJ/Kg K) vs clay (0.92, or 1 for bricks (https://www.engineeringtoolbox.com/specific-heat-solids-d_154.html).  This means that if the dishwasher doesn't heat long enough, it is possible for the plastic to actually have a lower temperature than the ceramic.

Second, and more importantly, is the thermal conductivity.  This is the ability of an object to conduct heat through itself.  This means that even though one side of an object is 100°C, the other side could be room temperature if conductivity is poor (think of home insulation).  Plastics generally have really bad conductivity.  

https://www.engineeringtoolbox.com/thermal-conductivity-d_429.html shows HDPE has a thermal conductivity of 0.42-0.51 W/mK .  Ceramics are difficult to pin down...there are a lot of variations on what type of ceramic plate you have, but the value for slate is 2.01, sandstone is 1.7, and even Pyrex is 1.005 - all higher than HDPE.  

This means that there is more energy moving THROUGH the material and that the time it takes to heat up is less as conductivity is higher.  There is less resistance to getting warm and absorbing all the energy its specific heat capacity wants.  It takes energy to evaporate water, and higher thermal conductivity allows the container/plate to recover that lost energy faster.

A great example of how conductivity can really matter is if you put a stainless steel pot and a vacuum insulated container in the dishwasher.  They are made of the same material (same specific heat), but the container is insulated solely because of its shape, which helps prevent heat getting to the inside and reduces conductivity.  The vacuum insulated container may come out wet on the inside (depending on how you placed it in there).

Edit - okay, I realize I was kind of high level, sorry.

The point is that some things move heat faster than others, and water evaporating takes heat away from the dish. Ceramic replaces the lost heat faster, so it can evaporate the next drop of water faster. 

Edit 2 - thank you /u/unclefishbits for the gold! ",0
